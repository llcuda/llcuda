{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a9a36ca5","cell_type":"markdown","source":"# llcuda v2.2.0 - Kaggle 2√ó T4 Build Notebook\n\n## Architecture: Split-GPU Workload\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         GPU 0             ‚îÇ            GPU 1              ‚îÇ\n‚îÇ  llama-server (GGUF)      ‚îÇ  RAPIDS + Graphistry          ‚îÇ\n‚îÇ  LLM Inference            ‚îÇ  Graph Visualization (cuGraph)‚îÇ\n‚îÇ  15GB VRAM                ‚îÇ  15GB VRAM                    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\nThis notebook builds llcuda binaries for **split-GPU** operation:\n- **GPU 0**: llama-server with GGUF model (LLM inference)\n- **GPU 1**: RAPIDS/Graphistry with cuDF/cuGraph (graph simulation)\n\n## Step 1: Verify Kaggle GPU Environment","metadata":{}},{"id":"b107f3d4","cell_type":"code","source":"# Verify we have 2√ó T4 GPUs\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"KAGGLE GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check nvidia-smi\nresult = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\ngpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\nprint(f\"\\nüìä Detected GPUs: {len(gpu_lines)}\")\nfor line in gpu_lines:\n    print(f\"   {line}\")\n\n# Check CUDA version\nprint(\"\\nüìä CUDA Version:\")\n!nvcc --version | grep release\n\n# Check total VRAM\nprint(\"\\nüìä VRAM Summary:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n\n# Verify we have 2 GPUs\nif len(gpu_lines) >= 2:\n    print(\"\\n‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\")\nelse:\n    print(\"\\n‚ö†Ô∏è WARNING: Less than 2 GPUs detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-16T19:18:35.208484Z","iopub.execute_input":"2026-01-16T19:18:35.208760Z","iopub.status.idle":"2026-01-16T19:18:35.530897Z","shell.execute_reply.started":"2026-01-16T19:18:35.208733Z","shell.execute_reply":"2026-01-16T19:18:35.530233Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nKAGGLE GPU ENVIRONMENT CHECK\n======================================================================\n\nüìä Detected GPUs: 2\n   GPU 0: Tesla T4 (UUID: GPU-825b4c22-49b2-7f2d-08a8-ce11f4a5079c)\n   GPU 1: Tesla T4 (UUID: GPU-8f8f68e8-9eda-5d5e-92e3-fa61d364c3e1)\n\nüìä CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\nüìä VRAM Summary:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\n\n‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\n","output_type":"stream"}],"execution_count":1},{"id":"334985f0","cell_type":"markdown","source":"## Step 2: Verify/Install Build Dependencies\n\n**Note:** Kaggle 2√ó T4 comes with cmake 3.31.6 and ninja 1.13.0 pre-installed.\nWe only install what's missing.","metadata":{}},{"id":"2b3ce1a6","cell_type":"code","source":"%%time\n# Check pre-installed build tools (Kaggle 2√ó T4 has cmake/ninja)\nimport subprocess\n\nprint(\"Checking build dependencies...\")\n\n# Check CMake\ncmake_result = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\nif cmake_result.returncode == 0:\n    cmake_ver = cmake_result.stdout.split(\"\\n\")[0]\n    print(f\"‚úÖ {cmake_ver}\")\nelse:\n    print(\"‚ö†Ô∏è  CMake not found, installing...\")\n    !apt-get update -qq && apt-get install -y -qq cmake\n\n# Check Ninja  \nninja_result = subprocess.run([\"ninja\", \"--version\"], capture_output=True, text=True)\nif ninja_result.returncode == 0:\n    print(f\"‚úÖ Ninja {ninja_result.stdout.strip()}\")\nelse:\n    print(\"‚ö†Ô∏è  Ninja not found, installing...\")\n    !apt-get install -y -qq ninja-build\n\n# Check ccache (optional but speeds up rebuilds)\nccache_result = subprocess.run([\"which\", \"ccache\"], capture_output=True, text=True)\nif ccache_result.returncode != 0:\n    print(\"üì¶ Installing ccache...\")\n    !apt-get install -y -qq ccache\n\n# Install Python dependencies (minimal - most are pre-installed on Kaggle)\nprint(\"\\nüì¶ Checking Python packages...\")\nrequired_py = [\"huggingface_hub\", \"sseclient-py\"]\nfor pkg in required_py:\n    try:\n        __import__(pkg.replace(\"-\", \"_\"))\n        print(f\"   ‚úÖ {pkg}\")\n    except ImportError:\n        print(f\"   üì¶ Installing {pkg}...\")\n        !pip install -q {pkg}\n\nprint(\"\\n‚úÖ Build dependencies ready\")\n!cmake --version | head -1\n!ninja --version","metadata":{"execution":{"iopub.status.busy":"2026-01-16T19:18:49.212881Z","iopub.execute_input":"2026-01-16T19:18:49.213522Z","iopub.status.idle":"2026-01-16T19:19:03.187837Z","shell.execute_reply.started":"2026-01-16T19:18:49.213485Z","shell.execute_reply":"2026-01-16T19:19:03.187194Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Checking build dependencies...\n‚úÖ cmake version 3.31.6\n‚úÖ Ninja 1.13.0.git.kitware.jobserver-pipe-1\nüì¶ Installing ccache...\nSelecting previously unselected package libhiredis0.14:amd64.\n(Reading database ... 129073 files and directories currently installed.)\nPreparing to unpack .../libhiredis0.14_0.14.1-2_amd64.deb ...\nUnpacking libhiredis0.14:amd64 (0.14.1-2) ...\nSelecting previously unselected package ccache.\nPreparing to unpack .../ccache_4.5.1-1_amd64.deb ...\nUnpacking ccache (4.5.1-1) ...\nSetting up libhiredis0.14:amd64 (0.14.1-2) ...\nSetting up ccache (4.5.1-1) ...\nUpdating symlinks in /usr/lib/ccache ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.8) ...\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n\n/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n\nProcessing triggers for man-db (2.10.2-1) ...\n\nüì¶ Checking Python packages...\n   ‚úÖ huggingface_hub\n   üì¶ Installing sseclient-py...\n\n‚úÖ Build dependencies ready\ncmake version 3.31.6\n1.13.0.git.kitware.jobserver-pipe-1\nCPU times: user 130 ms, sys: 55.3 ms, total: 185 ms\nWall time: 14 s\n","output_type":"stream"}],"execution_count":2},{"id":"d9eee12f","cell_type":"markdown","source":"## Step 2b: Fix RAPIDS + Install cuGraph (GPU 1 Workload)\n\n**Issue:** Kaggle's pre-installed RAPIDS (25.6.0) has version conflicts with `cuda-python` and `numba-cuda`.\n\n**Solution:** Fix the `cuda-python` package version compatibility, then install `cugraph-cu12`.\n\n**Reference:** https://docs.rapids.ai/install/ (pip + CUDA 12 + Stable 25.12)","metadata":{}},{"id":"097adecf","cell_type":"code","source":"%%time\n# Install cuGraph matching Kaggle's pre-installed RAPIDS 25.6.0\n# CRITICAL: Do NOT upgrade cuda-python or numba-cuda - this breaks RAPIDS!\n\nprint(\"=\"*70)\nprint(\"INSTALLING CUGRAPH FOR GPU 1 (RAPIDS 25.6.0 COMPATIBLE)\")\nprint(\"=\"*70)\n\n# Step 1: Check pre-installed RAPIDS versions\nimport subprocess\nprint(\"\\nüì¶ Pre-installed RAPIDS packages on Kaggle:\")\nfor pkg in [\"cudf-cu12\", \"cuml-cu12\", \"pylibraft-cu12\", \"cuda-python\", \"numba-cuda\"]:\n    result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n    if \"Version:\" in result.stdout:\n        version = [l for l in result.stdout.split(\"\\n\") if l.startswith(\"Version:\")][0]\n        print(f\"   {pkg}: {version.split(': ')[1]}\")\n    else:\n        print(f\"   {pkg}: NOT INSTALLED\")\n\n# Step 2: Install cugraph-cu12 matching RAPIDS 25.6.* (Kaggle's version)\n# Using pypi.nvidia.com for RAPIDS packages\nprint(\"\\nüì¶ Installing cugraph-cu12==25.6.* (matching Kaggle's RAPIDS)...\")\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Step 3: Install graphistry (minimal, no [ai] extras to avoid conflicts)\nprint(\"\\nüì¶ Installing graphistry...\")\n!pip install -q graphistry\n\n# Step 4: Verify RAPIDS imports work\nprint(\"\\nüì¶ Final verification:\")\ntry:\n    import cudf\n    print(f\"   ‚úÖ cuDF: {cudf.__version__}\")\nexcept ImportError as e:\n    print(f\"   ‚ùå cuDF: {e}\")\n\ntry:\n    import cugraph\n    print(f\"   ‚úÖ cuGraph: {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"   ‚ùå cuGraph: {e}\")\n    print(\"   üí° If cuGraph fails, try: Runtime ‚Üí Restart runtime, then re-run this cell\")\n\ntry:\n    import graphistry\n    print(f\"   ‚úÖ Graphistry: {graphistry.__version__}\")\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  Graphistry: {e}\")\n\nprint(\"\\n‚úÖ RAPIDS packages installed! If imports fail, restart runtime and re-run.\")","metadata":{"execution":{"iopub.status.busy":"2026-01-16T19:19:43.546742Z","iopub.execute_input":"2026-01-16T19:19:43.547650Z","iopub.status.idle":"2026-01-16T19:20:25.946574Z","shell.execute_reply.started":"2026-01-16T19:19:43.547614Z","shell.execute_reply":"2026-01-16T19:20:25.945914Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nINSTALLING CUGRAPH FOR GPU 1 (RAPIDS 25.6.0 COMPATIBLE)\n======================================================================\n\nüì¶ Pre-installed RAPIDS packages on Kaggle:\n   cudf-cu12: 25.6.0\n   cuml-cu12: 25.6.0\n   pylibraft-cu12: 25.6.0\n   cuda-python: 12.6.2.post1\n   numba-cuda: 0.11.0\n\nüì¶ Installing cugraph-cu12==25.6.* (matching Kaggle's RAPIDS)...\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\nüì¶ Installing graphistry...\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\nüì¶ Final verification:\n   ‚úÖ cuDF: 25.06.00\n   ‚úÖ cuGraph: 25.06.00\n   ‚úÖ Graphistry: 0.50.4\n\n‚úÖ RAPIDS packages installed! If imports fail, restart runtime and re-run.\nCPU times: user 5.95 s, sys: 947 ms, total: 6.89 s\nWall time: 42.4 s\n","output_type":"stream"}],"execution_count":3},{"id":"98739d5f-299c-4f02-b142-b8cc82727ef0","cell_type":"code","source":"!pip list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T19:20:51.714624Z","iopub.execute_input":"2026-01-16T19:20:51.715821Z","iopub.status.idle":"2026-01-16T19:20:53.693973Z","shell.execute_reply.started":"2026-01-16T19:20:51.715788Z","shell.execute_reply":"2026-01-16T19:20:53.693046Z"}},"outputs":[{"name":"stdout","text":"Package                                  Version\n---------------------------------------- -------------------\na2a-sdk                                  0.3.22\nabsl-py                                  1.4.0\nabsolufy-imports                         0.3.1\naccelerate                               1.11.0\naiofiles                                 22.1.0\naiohappyeyeballs                         2.6.1\naiohttp                                  3.13.3\naiosignal                                1.4.0\naiosqlite                                0.22.1\nalabaster                                1.0.0\nalbucore                                 0.0.24\nalbumentations                           2.0.8\nale-py                                   0.11.2\nalembic                                  1.17.0\naltair                                   5.5.0\nannotated-doc                            0.0.4\nannotated-types                          0.7.0\nansicolors                               1.1.8\nantlr4-python3-runtime                   4.9.3\nanyio                                    4.12.1\nanywidget                                0.9.18\nargon2-cffi                              25.1.0\nargon2-cffi-bindings                     25.1.0\nargs                                     0.1.0\narray_record                             0.8.1\narrow                                    1.4.0\narviz                                    0.22.0\nasn1crypto                               1.5.1\nastropy                                  7.1.1\nastropy-iers-data                        0.2025.10.20.0.39.8\nasttokens                                3.0.1\nastunparse                               1.6.3\natpublic                                 5.1\nattrs                                    25.4.0\naudioread                                3.0.1\nAuthlib                                  1.6.5\nautograd                                 1.8.0\nbabel                                    2.17.0\nbackcall                                 0.2.0\nbayesian-optimization                    3.2.0\nbeartype                                 0.22.2\nbeautifulsoup4                           4.13.5\nbetterproto                              2.0.0b7\nbigframes                                2.26.0\nbigquery-magics                          0.10.3\nblack                                    25.12.0\nblake3                                   1.0.8\nbleach                                   6.2.0\nblinker                                  1.9.0\nblis                                     1.3.0\nblobfile                                 3.1.0\nblosc2                                   3.10.2\nbokeh                                    3.7.3\nBoruta                                   0.4.3\nboto3                                    1.42.27\nbotocore                                 1.42.27\nBottleneck                               1.4.2\nbqplot                                   0.12.45\nbranca                                   0.8.2\nBrotli                                   1.1.0\nbuild                                    1.3.0\nCacheControl                             0.14.3\ncachetools                               5.5.2\nCartopy                                  0.25.0\ncatalogue                                2.0.10\ncatboost                                 1.2.8\ncategory_encoders                        2.9.0\ncertifi                                  2026.1.4\ncesium                                   0.12.4\ncffi                                     2.0.0\nchardet                                  5.2.0\ncharset-normalizer                       3.4.4\nChessnut                                 0.4.1\nchex                                     0.1.90\nclarabel                                 0.11.1\nclick                                    8.3.1\nclick-plugins                            1.1.1.2\ncligj                                    0.7.2\nclint                                    0.5.1\ncloudpathlib                             0.23.0\ncloudpickle                              3.1.1\ncmake                                    3.31.6\ncmdstanpy                                1.2.5\ncolorama                                 0.4.6\ncolorcet                                 3.1.0\ncolorlog                                 6.10.1\ncolorlover                               0.3.0\ncolour                                   0.1.5\ncomm                                     0.2.3\ncommunity                                1.0.0b1\nconfection                               0.1.5\ncons                                     0.4.7\ncontourpy                                1.3.3\ncoverage                                 7.13.1\ncramjam                                  2.11.0\ncryptography                             46.0.3\ncuda-python                              12.6.2.post1\ncudf-cu12                                25.6.0\ncudf-polars-cu12                         25.6.0\ncufflinks                                0.17.3\ncugraph-cu12                             25.6.0\ncuml-cu12                                25.6.0\ncupy-cuda12x                             13.3.0\ncurl_cffi                                0.13.0\ncuvs-cu12                                25.6.1\ncvxopt                                   1.3.2\ncvxpy                                    1.6.7\ncycler                                   0.12.1\ncyipopt                                  1.5.0\ncymem                                    2.0.11\nCython                                   3.0.12\ncytoolz                                  1.1.0\ndaal                                     2025.10.0\ndacite                                   1.9.2\ndask                                     2025.5.0\ndask-cuda                                25.6.0\ndask-cudf-cu12                           25.6.0\ndataclasses-json                         0.6.7\ndataproc-spark-connect                   0.8.3\ndatasets                                 4.4.2\ndb-dtypes                                1.4.3\ndbus-python                              1.2.18\ndeap                                     1.4.3\ndebugpy                                  1.8.15\ndecorator                                4.4.2\ndeepdiff                                 8.6.1\ndefusedxml                               0.7.1\nDeprecated                               1.3.1\ndiffusers                                0.35.2\ndill                                     0.4.0\ndipy                                     1.11.0\ndistributed                              2025.5.0\ndistributed-ucxx-cu12                    0.44.0\ndistro                                   1.9.0\ndlib                                     19.24.6\ndm-tree                                  0.1.9\ndnspython                                2.8.0\ndocker                                   7.1.0\ndocstring_parser                         0.17.0\ndocstring-to-markdown                    0.17\ndocutils                                 0.21.2\ndopamine_rl                              4.1.2\nduckdb                                   1.3.2\nearthengine-api                          1.5.24\neasydict                                 1.13\neasyocr                                  1.7.2\neditdistance                             0.8.1\neerepr                                   0.1.2\neinops                                   0.8.1\nemail-validator                          2.3.0\nemoji                                    2.15.0\nen_core_web_sm                           3.8.0\nentrypoints                              0.4\net_xmlfile                               2.0.0\netils                                    1.13.0\netuples                                  0.3.10\nexecnb                                   0.1.18\nFarama-Notifications                     0.0.4\nfastai                                   2.8.4\nfastapi                                  0.123.10\nfastcore                                 1.11.3\nfastdownload                             0.0.7\nfastgit                                  0.0.1\nfastjsonschema                           2.21.2\nfastprogress                             1.0.3\nfastrlock                                0.8.3\nfasttext                                 0.9.3\nfasttransform                            0.0.2\nfastuuid                                 0.14.0\nfeaturetools                             1.31.0\nffmpy                                    0.6.3\nfilelock                                 3.20.3\nfiletype                                 1.2.0\nfiona                                    1.10.1\nfirebase-admin                           6.9.0\nFlask                                    3.1.2\nflatbuffers                              25.9.23\nflax                                     0.10.7\nfolium                                   0.20.0\nfonttools                                4.60.1\nfqdn                                     1.5.1\nfrozendict                               2.4.6\nfrozenlist                               1.8.0\nfsspec                                   2025.10.0\nfuncy                                    2.0\nfury                                     0.12.0\nfuture                                   1.0.0\nfuzzywuzzy                               0.18.0\ngast                                     0.6.0\ngatspy                                   0.3\ngcsfs                                    2025.3.0\nGDAL                                     3.8.4\ngdown                                    5.2.0\ngeemap                                   0.35.3\ngensim                                   4.4.0\ngeocoder                                 1.38.1\ngeographiclib                            2.1\ngeojson                                  3.2.0\ngeopandas                                1.1.1\ngeopy                                    2.4.1\nghapi                                    1.0.8\ngin-config                               0.5.0\ngitdb                                    4.0.12\nGitPython                                3.1.45\nglob2                                    0.7\ngoogle                                   2.0.3\ngoogle-adk                               1.22.1\ngoogle-ai-generativelanguage             0.6.15\ngoogle-api-core                          2.26.0\ngoogle-api-python-client                 2.185.0\ngoogle-auth                              2.47.0\ngoogle-auth-httplib2                     0.2.0\ngoogle-auth-oauthlib                     1.2.2\ngoogle-cloud-aiplatform                  1.133.0\ngoogle-cloud-appengine-logging           1.7.0\ngoogle-cloud-audit-log                   0.4.0\ngoogle-cloud-bigquery                    3.38.0\ngoogle-cloud-bigquery-connection         1.19.0\ngoogle-cloud-bigtable                    2.33.0\ngoogle-cloud-core                        2.4.3\ngoogle-cloud-dataproc                    5.23.0\ngoogle-cloud-datastore                   2.21.0\ngoogle-cloud-discoveryengine             0.13.12\ngoogle-cloud-firestore                   2.21.0\ngoogle-cloud-functions                   1.21.0\ngoogle-cloud-language                    2.18.0\ngoogle-cloud-logging                     3.12.1\ngoogle-cloud-monitoring                  2.28.0\ngoogle-cloud-pubsub                      2.34.0\ngoogle-cloud-resource-manager            1.15.0\ngoogle-cloud-secret-manager              2.25.0\ngoogle-cloud-spanner                     3.58.0\ngoogle-cloud-speech                      2.34.0\ngoogle-cloud-storage                     2.19.0\ngoogle-cloud-trace                       1.17.0\ngoogle-cloud-translate                   3.12.1\ngoogle-cloud-videointelligence           2.17.0\ngoogle-cloud-vision                      3.11.0\ngoogle-colab                             1.0.0\ngoogle-crc32c                            1.7.1\ngoogle-genai                             1.57.0\ngoogle-generativeai                      0.8.5\ngoogle-pasta                             0.2.0\ngoogle-resumable-media                   2.7.2\ngoogleapis-common-protos                 1.71.0\ngoogledrivedownloader                    1.1.0\ngpxpy                                    1.6.2\ngradio                                   5.49.1\ngradio_client                            1.13.3\ngraphistry                               0.50.4\ngraphviz                                 0.21\ngreenlet                                 3.2.4\ngroovy                                   0.1.2\ngrpc-google-iam-v1                       0.14.3\ngrpc-interceptor                         0.15.4\ngrpcio                                   1.75.1\ngrpcio-status                            1.71.2\ngrpclib                                  0.4.9\ngspread                                  6.2.1\ngspread-dataframe                        4.0.0\ngym                                      0.25.2\ngym-notices                              0.1.0\ngymnasium                                0.29.0\nh11                                      0.16.0\nh2                                       4.3.0\nh2o                                      3.46.0.9\nh5netcdf                                 1.7.2\nh5py                                     3.15.1\nhaversine                                2.9.0\nhdbscan                                  0.8.40\nhep_ml                                   0.8.0\nhf_transfer                              0.1.9\nhf-xet                                   1.2.1rc0\nhighspy                                  1.11.0\nholidays                                 0.82\nholoviews                                1.21.0\nhpack                                    4.1.0\nhtml5lib                                 1.1\nhttpcore                                 1.0.9\nhttpimport                               1.4.1\nhttplib2                                 0.31.0\nhttpx                                    0.28.1\nhttpx-sse                                0.4.3\nhuggingface-hub                          0.36.0\nhumanize                                 4.14.0\nhyperframe                               6.1.0\nhyperopt                                 0.2.7\nibis-framework                           9.5.0\nid                                       1.5.0\nidna                                     3.11\nigraph                                   1.0.0\nImageHash                                4.3.2\nimageio                                  2.37.0\nimageio-ffmpeg                           0.6.0\nimagesize                                1.4.1\nimbalanced-learn                         0.14.0\nimmutabledict                            4.2.2\nimportlib_metadata                       8.7.0\nimportlib_resources                      6.5.2\nimutils                                  0.5.4\nin-toto-attestation                      0.9.3\ninflect                                  7.5.0\niniconfig                                2.3.0\nintel-cmplr-lib-ur                       2025.2.1\nintel-openmp                             2025.2.1\nipyevents                                2.0.4\nipyfilechooser                           0.6.0\nipykernel                                6.17.1\nipyleaflet                               0.20.0\nipympl                                   0.9.8\nipyparallel                              8.8.0\nipython                                  7.34.0\nipython-genutils                         0.2.0\nipython_pygments_lexers                  1.1.1\nipython-sql                              0.5.0\nipytree                                  0.2.2\nipywidgets                               8.1.5\nisoduration                              20.11.0\nisoweek                                  1.3.3\nitsdangerous                             2.2.0\nJanome                                   0.5.0\njaraco.classes                           3.4.0\njaraco.context                           6.0.1\njaraco.functools                         4.3.0\njax                                      0.7.2\njax-cuda12-pjrt                          0.7.2\njax-cuda12-plugin                        0.7.2\njaxlib                                   0.7.2\njedi                                     0.19.2\njeepney                                  0.9.0\njieba                                    0.42.1\nJinja2                                   3.1.6\njiter                                    0.10.0\njmespath                                 1.0.1\njoblib                                   1.5.3\njson5                                    0.13.0\njsonpatch                                1.33\njsonpickle                               4.1.1\njsonpointer                              3.0.0\njsonschema                               4.25.1\njsonschema-specifications                2025.9.1\njupyter_client                           7.4.9\njupyter-console                          6.6.3\njupyter_core                             5.9.1\njupyter-events                           0.12.0\njupyter_kernel_gateway                   2.5.2\njupyter-leaflet                          0.20.0\njupyter-lsp                              1.5.1\njupyter_server                           2.12.5\njupyter_server_fileid                    0.9.3\njupyter_server_proxy                     4.4.0\njupyter_server_terminals                 0.5.3\njupyter_server_ydoc                      0.8.0\njupyter-ydoc                             0.2.5\njupyterlab                               3.6.8\njupyterlab-lsp                           3.10.2\njupyterlab_pygments                      0.3.0\njupyterlab_server                        2.28.0\njupyterlab_widgets                       3.0.15\njupytext                                 1.17.3\nkaggle                                   1.7.4.5\nkaggle-environments                      1.18.0\nkagglehub                                0.4.0\nkagglesdk                                0.1.14\nkeras                                    3.10.0\nkeras-core                               0.1.7\nkeras-cv                                 0.9.0\nkeras-hub                                0.21.1\nkeras-nlp                                0.21.1\nkeras-tuner                              1.4.8\nkeyring                                  25.6.0\nkeyrings.google-artifactregistry-auth    1.1.2\nkiwisolver                               1.4.9\nkornia                                   0.8.2\nkornia_rs                                0.1.10\nkt-legacy                                1.0.5\nlangchain                                0.3.27\nlangchain-core                           0.3.79\nlangchain-text-splitters                 0.3.11\nlangcodes                                3.5.0\nlangid                                   1.1.6\nlangsmith                                0.4.37\nlanguage_data                            1.3.0\nlark                                     1.3.0\nlaunchpadlib                             1.10.16\nlazr.restfulclient                       0.14.4\nlazr.uri                                 1.0.6\nlazy_loader                              0.4\nlearntools                               0.3.5\nlibclang                                 18.1.1\nlibcudf-cu12                             25.6.0\nlibcugraph-cu12                          25.6.0\nlibcuml-cu12                             25.6.0\nlibcuvs-cu12                             25.6.1\nlibkvikio-cu12                           25.6.0\nlibpysal                                 4.9.2\nlibraft-cu12                             25.6.0\nlibrmm-cu12                              25.6.0\nlibrosa                                  0.11.0\nlibucx-cu12                              1.18.1\nlibucxx-cu12                             0.44.0\nlightgbm                                 4.6.0\nlightning-utilities                      0.15.2\nlime                                     0.2.0.1\nline_profiler                            5.0.0\nlinkify-it-py                            2.0.3\nlitellm                                  1.80.16\nllvmlite                                 0.43.0\nlml                                      0.2.0\nlocket                                   1.0.0\nlogical-unification                      0.4.6\nlxml                                     5.4.0\nMako                                     1.3.10\nmamba                                    0.11.3\nmarisa-trie                              1.3.1\nMarkdown                                 3.9\nmarkdown-it-py                           4.0.0\nMarkupSafe                               3.0.3\nmarshmallow                              3.26.2\nmatplotlib                               3.10.0\nmatplotlib-inline                        0.1.7\nmatplotlib-venn                          1.1.2\nmcp                                      1.18.0\nmdit-py-plugins                          0.5.0\nmdurl                                    0.1.2\nminify_html                              0.18.1\nminiKanren                               1.0.5\nmissingno                                0.5.2\nmistune                                  0.8.4\nmizani                                   0.13.5\nmkl                                      2025.2.0\nml_collections                           1.1.0\nml_dtypes                                0.5.3\nmlcrate                                  0.2.0\nmlxtend                                  0.23.4\nmne                                      1.11.0\nmodel-signing                            1.1.1\nmore-itertools                           10.8.0\nmoviepy                                  1.0.3\nmpld3                                    0.5.12\nmpmath                                   1.3.0\nmsgpack                                  1.1.2\nmultidict                                6.7.0\nmultimethod                              1.12\nmultipledispatch                         1.0.0\nmultiprocess                             0.70.18\nmultitasking                             0.0.12\nmurmurhash                               1.0.13\nmusic21                                  9.3.0\nmypy_extensions                          1.1.0\nnamex                                    0.1.0\nnarwhals                                 2.9.0\nnatsort                                  8.4.0\nnbclassic                                1.3.3\nnbclient                                 0.5.13\nnbconvert                                6.4.5\nnbdev                                    2.4.10\nnbformat                                 5.10.4\nndindex                                  1.10.0\nnest-asyncio                             1.6.0\nnetworkx                                 3.5\nnibabel                                  5.3.2\nnilearn                                  0.13.0\nninja                                    1.13.0\nnltk                                     3.9.2\nnotebook                                 6.5.7\nnotebook_shim                            0.2.4\nnumba                                    0.60.0\nnumba-cuda                               0.11.0\nnumexpr                                  2.14.1\nnumpy                                    2.0.2\nnvidia-cublas-cu12                       12.6.4.1\nnvidia-cuda-cupti-cu12                   12.6.80\nnvidia-cuda-nvcc-cu12                    12.5.82\nnvidia-cuda-nvrtc-cu12                   12.6.77\nnvidia-cuda-runtime-cu12                 12.6.77\nnvidia-cudnn-cu12                        9.10.2.21\nnvidia-cufft-cu12                        11.3.0.4\nnvidia-cufile-cu12                       1.11.1.6\nnvidia-curand-cu12                       10.3.7.77\nnvidia-cusolver-cu12                     11.7.1.2\nnvidia-cusparse-cu12                     12.5.4.2\nnvidia-cusparselt-cu12                   0.7.1\nnvidia-ml-py                             12.575.51\nnvidia-nccl-cu12                         2.27.3\nnvidia-nvjitlink-cu12                    12.6.85\nnvidia-nvshmem-cu12                      3.4.5\nnvidia-nvtx-cu12                         12.6.77\nnvtx                                     0.2.13\nnx-cugraph-cu12                          25.6.0\noauth2client                             4.1.3\noauthlib                                 3.3.1\nodfpy                                    1.4.1\nolefile                                  0.47\nomegaconf                                2.3.0\nonnx                                     1.20.1\nopen_spiel                               1.6.11\nopenai                                   2.15.0\nopencv-contrib-python                    4.12.0.88\nopencv-python                            4.12.0.88\nopencv-python-headless                   4.12.0.88\nopenpyxl                                 3.1.5\nopenslide-bin                            4.0.0.11\nopenslide-python                         1.4.3\nopentelemetry-api                        1.37.0\nopentelemetry-exporter-gcp-logging       1.11.0a0\nopentelemetry-exporter-gcp-monitoring    1.10.0a0\nopentelemetry-exporter-gcp-trace         1.10.0\nopentelemetry-exporter-otlp-proto-common 1.37.0\nopentelemetry-exporter-otlp-proto-http   1.37.0\nopentelemetry-proto                      1.37.0\nopentelemetry-resourcedetector-gcp       1.10.0a0\nopentelemetry-sdk                        1.37.0\nopentelemetry-semantic-conventions       0.58b0\nopt_einsum                               3.4.0\noptax                                    0.2.6\noptree                                   0.17.0\noptuna                                   4.6.0\norbax-checkpoint                         0.11.25\norderly-set                              5.5.0\norjson                                   3.11.3\nosqp                                     1.0.5\noverrides                                7.7.0\npackaging                                26.0rc2\npalettable                               3.3.3\npandas                                   2.2.2\npandas-datareader                        0.10.0\npandas-gbq                               0.29.2\npandas-profiling                         3.6.6\npandas-stubs                             2.2.2.240909\npandasql                                 0.7.3\npandocfilters                            1.5.1\npanel                                    1.8.2\npapermill                                2.6.0\nparam                                    2.2.1\nparso                                    0.8.5\nparsy                                    2.2\npartd                                    1.4.2\npath                                     17.1.1\npath.py                                  12.5.0\npathos                                   0.3.2\npathspec                                 1.0.3\npatsy                                    1.0.2\npdf2image                                1.17.0\npeewee                                   3.18.2\npeft                                     0.17.1\npettingzoo                               1.24.0\npexpect                                  4.9.0\nphik                                     0.12.5\npickleshare                              0.7.5\npillow                                   11.3.0\npip                                      24.1.2\nplatformdirs                             4.5.1\nplotly                                   5.24.1\nplotly-express                           0.4.1\nplotnine                                 0.14.5\npluggy                                   1.6.0\nplum-dispatch                            2.5.8\nply                                      3.11\npolars                                   1.25.2\npooch                                    1.8.2\nportpicker                               1.5.2\npox                                      0.3.6\nppft                                     1.7.7\npreprocessing                            0.1.13\npreshed                                  3.0.10\nprettytable                              3.16.0\nproglog                                  0.1.12\nprogressbar2                             4.5.0\nprometheus_client                        0.23.1\npromise                                  2.3\nprompt_toolkit                           3.0.52\npropcache                                0.4.1\nprophet                                  1.1.7\nproto-plus                               1.26.1\nprotobuf                                 5.29.5\npsutil                                   5.9.5\npsycopg2                                 2.9.11\npsygnal                                  0.15.0\nptyprocess                               0.7.0\npudb                                     2025.1.5\npuremagic                                1.30\npy-cpuinfo                               9.0.0\npy4j                                     0.10.9.7\npyaml                                    25.7.0\nPyArabic                                 0.6.15\npyarrow                                  19.0.1\npyasn1                                   0.6.1\npyasn1_modules                           0.4.2\npybind11                                 3.0.1\npycairo                                  1.28.0\npyclipper                                1.4.0\npycocotools                              2.0.10\npycparser                                2.23\npycryptodome                             3.23.0\npycryptodomex                            3.23.0\npycuda                                   2025.1.2\npydantic                                 2.12.5\npydantic_core                            2.41.5\npydantic-settings                        2.11.0\npydata-google-auth                       1.9.1\npydicom                                  3.0.1\npydot                                    3.0.4\npydotplus                                2.0.2\nPyDrive2                                 1.21.3\npydub                                    0.25.1\npyemd                                    1.0.0\npyerfa                                   2.0.1.5\npyexcel-io                               0.6.7\npyexcel-ods                              0.6.0\npygame                                   2.6.1\npygit2                                   1.18.2\npygltflib                                1.16.5\nPygments                                 2.19.2\nPyGObject                                3.42.0\nPyJWT                                    2.10.1\npyLDAvis                                 3.4.1\npylibcudf-cu12                           25.6.0\npylibcugraph-cu12                        25.6.0\npylibraft-cu12                           25.6.0\npymc                                     5.26.1\npymongo                                  4.16.0\nPympler                                  1.1\npynndescent                              0.5.13\npynvjitlink-cu12                         0.7.0\npynvml                                   12.0.0\npyogrio                                  0.11.1\npyomo                                    6.9.5\nPyOpenGL                                 3.1.10\npyOpenSSL                                25.3.0\npyparsing                                3.2.5\npypdf                                    6.6.0\npyperclip                                1.11.0\npyproj                                   3.7.2\npyproject_hooks                          1.2.0\npyshp                                    3.0.2.post1\nPySocks                                  1.7.1\npyspark                                  3.5.1\npytensor                                 2.35.1\npytesseract                              0.3.13\npytest                                   8.4.2\npython-apt                               0.0.0\npython-bidi                              0.6.7\npython-box                               7.3.2\npython-dateutil                          2.9.0.post0\npython-dotenv                            1.1.1\npython-json-logger                       4.0.0\npython-louvain                           0.16\npython-lsp-jsonrpc                       1.1.2\npython-lsp-server                        1.14.0\npython-multipart                         0.0.20\npython-slugify                           8.0.4\npython-snappy                            0.7.3\npython-utils                             3.9.1\npytokens                                 0.3.0\npytools                                  2025.2.5\npytorch-ignite                           0.5.3\npytorch-lightning                        2.6.0\npytz                                     2025.2\nPyUpSet                                  0.1.1.post7\npyviz_comms                              3.0.6\nPyWavelets                               1.9.0\nPyYAML                                   6.0.3\npyzmq                                    26.2.1\nqgrid                                    1.3.1\nqtconsole                                5.7.0\nQtPy                                     2.4.3\nraft-dask-cu12                           25.6.0\nrapids-dask-dependency                   25.6.0\nrapids-logger                            0.1.19\nratelim                                  0.1.6\nray                                      2.53.0\nreferencing                              0.37.0\nregex                                    2025.11.3\nrequests                                 2.32.5\nrequests-oauthlib                        2.0.0\nrequests-toolbelt                        1.0.0\nrequirements-parser                      0.9.0\nrfc3161-client                           1.0.5\nrfc3339-validator                        0.1.4\nrfc3986-validator                        0.1.1\nrfc3987-syntax                           1.1.0\nrfc8785                                  0.1.4\nrgf-python                               3.12.0\nrich                                     14.2.0\nrmm-cu12                                 25.6.0\nroman-numerals-py                        3.1.0\nrouge_score                              0.1.2\nrpds-py                                  0.27.1\nrpy2                                     3.5.17\nrsa                                      4.9.1\nrtree                                    1.4.1\nruamel.yaml                              0.19.1\nruff                                     0.14.1\ns3fs                                     0.4.2\ns3transfer                               0.16.0\nsafehttpx                                0.1.6\nsafetensors                              0.6.2\nscikit-image                             0.25.2\nscikit-learn                             1.6.1\nscikit-learn-intelex                     2025.10.0\nscikit-multilearn                        0.2.0\nscikit-optimize                          0.10.2\nscikit-plot                              0.3.7\nscikit-surprise                          1.1.4\nscipy                                    1.15.3\nscooby                                   0.10.2\nscs                                      3.2.9\nseaborn                                  0.13.2\nSecretStorage                            3.4.0\nsecuresystemslib                         1.3.1\nsegment_anything                         1.0\nsemantic-version                         2.10.0\nSend2Trash                               1.8.3\nsentence-transformers                    5.1.1\nsentencepiece                            0.2.1\nsentry-sdk                               2.42.1\nsetuptools                               75.2.0\nsetuptools-scm                           9.2.2\nshap                                     0.49.1\nshapely                                  2.1.2\nshellingham                              1.5.4\nShimmy                                   1.3.0\nsigstore                                 4.1.0\nsigstore-models                          0.0.5\nsigstore-rekor-types                     0.0.18\nsimpervisor                              1.0.0\nsimple-parsing                           0.1.7\nsimpleitk                                2.5.3\nsimplejson                               3.20.2\nsimsimd                                  6.5.3\nsiphash24                                1.8\nsix                                      1.17.0\nsklearn-pandas                           2.2.0\nslicer                                   0.0.8\nsmart_open                               7.4.0\nsmmap                                    5.0.2\nsniffio                                  1.3.1\nsnowballstemmer                          3.0.1\nsortedcontainers                         2.4.0\nsoundfile                                0.13.1\nsoupsieve                                2.8\nsoxr                                     1.0.0\nspacy                                    3.8.7\nspacy-legacy                             3.0.12\nspacy-loggers                            1.0.5\nspanner-graph-notebook                   1.1.8\nSphinx                                   8.2.3\nsphinx-rtd-theme                         0.2.4\nsphinxcontrib-applehelp                  2.0.0\nsphinxcontrib-devhelp                    2.0.0\nsphinxcontrib-htmlhelp                   2.1.0\nsphinxcontrib-jsmath                     1.0.1\nsphinxcontrib-qthelp                     2.0.0\nsphinxcontrib-serializinghtml            2.0.0\nSQLAlchemy                               2.0.44\nsqlalchemy-spanner                       1.17.0\nsqlglot                                  25.20.2\nsqlparse                                 0.5.3\nsquarify                                 0.4.4\nsrsly                                    2.5.1\nsse-starlette                            3.0.2\nsseclient-py                             1.9.0\nstable-baselines3                        2.1.0\nstanio                                   0.5.1\nstarlette                                0.50.0\nstatsmodels                              0.14.5\nstopit                                   1.1.2\nstringzilla                              4.2.1\nstumpy                                   1.13.0\nsympy                                    1.13.3\ntables                                   3.10.2\ntabulate                                 0.9.0\ntbb                                      2022.2.0\ntblib                                    3.1.0\ntcmlib                                   1.4.0\ntenacity                                 9.1.2\ntensorboard                              2.19.0\ntensorboard-data-server                  0.7.2\ntensorflow                               2.19.0\ntensorflow-cloud                         0.1.5\ntensorflow-datasets                      4.9.9\ntensorflow_decision_forests              1.12.0\ntensorflow-hub                           0.16.1\ntensorflow-io                            0.37.1\ntensorflow-io-gcs-filesystem             0.37.1\ntensorflow-metadata                      1.17.2\ntensorflow-probability                   0.25.0\ntensorflow-text                          2.19.0\ntensorstore                              0.1.78\ntermcolor                                3.1.0\nterminado                                0.18.1\ntestpath                                 0.6.0\ntext-unidecode                           1.3\ntextblob                                 0.19.0\ntexttable                                1.7.0\ntf_keras                                 2.19.0\ntf-slim                                  1.1.0\nthinc                                    8.3.6\nthreadpoolctl                            3.6.0\ntifffile                                 2025.10.16\ntiktoken                                 0.12.0\ntimm                                     1.0.20\ntinycss2                                 1.4.0\ntokenizers                               0.22.1\ntoml                                     0.10.2\ntomlkit                                  0.13.3\ntoolz                                    0.12.1\ntorch                                    2.8.0+cu126\ntorchao                                  0.10.0\ntorchaudio                               2.8.0+cu126\ntorchdata                                0.11.0\ntorchinfo                                1.8.0\ntorchmetrics                             1.8.2\ntorchsummary                             1.5.1\ntorchtune                                0.6.1\ntorchvision                              0.23.0+cu126\ntornado                                  6.5.1\nTPOT                                     0.12.2\ntqdm                                     4.67.1\ntraitlets                                5.7.1\ntraittypes                               0.2.1\ntransformers                             4.57.1\ntreelite                                 4.4.1\ntreescope                                0.1.10\ntriton                                   3.4.0\ntrx-python                               0.3\ntsfresh                                  0.21.1\ntuf                                      6.0.0\ntweepy                                   4.16.0\ntypeguard                                4.4.4\ntyper                                    0.20.0\ntyper-slim                               0.21.1\ntypes-pytz                               2025.2.0.20250809\ntypes-setuptools                         80.9.0.20250822\ntyping_extensions                        4.15.0\ntyping-inspect                           0.9.0\ntyping-inspection                        0.4.2\ntzdata                                   2025.2\ntzlocal                                  5.3.1\nuc-micro-py                              1.0.3\nucx-py-cu12                              0.44.0\nucxx-cu12                                0.44.0\nujson                                    5.11.0\numap-learn                               0.5.9.post2\numf                                      0.11.0\nupdate-checker                           0.18.0\nuri-template                             1.3.0\nuritemplate                              4.2.0\nurllib3                                  2.6.3\nurwid                                    3.0.3\nurwid_readline                           0.15.1\nuvicorn                                  0.38.0\nvega-datasets                            0.9.0\nvisions                                  0.8.1\nvtk                                      9.3.1\nwadllib                                  1.3.6\nWand                                     0.6.13\nwandb                                    0.22.2\nwasabi                                   1.1.3\nwatchdog                                 6.0.0\nwavio                                    0.0.9\nwcwidth                                  0.2.14\nweasel                                   0.4.1\nwebcolors                                24.11.1\nwebencodings                             0.5.1\nwebsocket-client                         1.9.0\nwebsockets                               15.0.1\nWerkzeug                                 3.1.3\nwheel                                    0.45.1\nwidgetsnbextension                       4.0.15\nwoodwork                                 0.31.0\nwordcloud                                1.9.4\nwrapt                                    2.0.0\nwurlitzer                                3.1.1\nxarray                                   2025.10.1\nxarray-einstats                          0.9.1\nxgboost                                  3.1.0\nxlrd                                     2.0.2\nxvfbwrapper                              0.2.18\nxxhash                                   3.6.0\nxyzservices                              2025.4.0\ny-py                                     0.6.2\nyarl                                     1.22.0\nydata-profiling                          4.18.1\nydf                                      0.13.0\nyellowbrick                              1.5\nyfinance                                 0.2.66\nypy-websocket                            0.8.4\nzict                                     3.0.0\nzipp                                     3.23.0\nzstandard                                0.25.0\n","output_type":"stream"}],"execution_count":4},{"id":"a7531433-318e-4795-8e50-f27ae7ff1c45","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"81a4e8f2-7cd6-4227-b60f-8f388e101915","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cfa4bf76-69a7-4d55-854b-6971b01c6f5f","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5c69ed34","cell_type":"markdown","source":"## Step 3: Clone llama.cpp (Latest Stable)","metadata":{}},{"id":"c5fa6016","cell_type":"code","source":"%%time\nimport os\n\n# Set working directory\nWORK_DIR = \"/kaggle/working\"\nos.chdir(WORK_DIR)\n\n# Clean any previous build\n!rm -rf llama.cpp\n\n# Clone llama.cpp\nprint(\"Cloning llama.cpp...\")\n!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n\nos.chdir(\"llama.cpp\")\n\n# Get commit info\nprint(\"\\nüì¶ llama.cpp Version:\")\n!git log -1 --oneline\n!git describe --tags --always 2>/dev/null || echo \"(no tag)\"","metadata":{"execution":{"iopub.status.busy":"2026-01-16T20:12:38.308655Z","iopub.execute_input":"2026-01-16T20:12:38.309456Z","iopub.status.idle":"2026-01-16T20:12:42.563774Z","shell.execute_reply.started":"2026-01-16T20:12:38.309427Z","shell.execute_reply":"2026-01-16T20:12:42.563048Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning llama.cpp...\nCloning into 'llama.cpp'...\nremote: Enumerating objects: 2395, done.\u001b[K\nremote: Counting objects: 100% (2395/2395), done.\u001b[K\nremote: Compressing objects: 100% (1875/1875), done.\u001b[K\nremote: Total 2395 (delta 518), reused 1568 (delta 448), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (2395/2395), 27.25 MiB | 18.04 MiB/s, done.\nResolving deltas: 100% (518/518), done.\n\nüì¶ llama.cpp Version:\n\u001b[33m388ce82\u001b[m\u001b[33m (\u001b[m\u001b[1;34mgrafted\u001b[m\u001b[33m, \u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;33mtag: b7760\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m ggml : extend ggml_pool_1d + metal (#16429)\nb7760\nCPU times: user 67.5 ms, sys: 53.2 ms, total: 121 ms\nWall time: 4.25 s\n","output_type":"stream"}],"execution_count":15},{"id":"6fa16d20","cell_type":"markdown","source":"## Step 4: Configure CMake for Dual T4 (SM 7.5)","metadata":{}},{"id":"bbf67503","cell_type":"code","source":"%%time\nimport os\nos.chdir(\"/kaggle/working/llama.cpp\")\n\n# Clean previous build\n!rm -rf build\n\nprint(\"=\"*70)\nprint(\"STEP 4: CREATE CUDA DRIVER STUB + CONFIGURE CMAKE (VMM DISABLED)\")\nprint(\"=\"*70)\n\n# ============================================================================\n# CRITICAL FIX: Create libcuda.so stub in WRITABLE location\n# Kaggle's /usr/local/cuda is read-only, so we use /kaggle/working/\n# ============================================================================\nprint(\"\\nüîß Creating CUDA driver stub library...\")\n\nSTUBS_DIR = \"/kaggle/working/cuda_stubs\"\nos.makedirs(STUBS_DIR, exist_ok=True)\n\n# Create a minimal C file that provides empty symbols for libcuda.so\n# NOTE: We disable VMM via -DGGML_CUDA_NO_VMM compile flag so we don't need\n# the advanced memory management APIs (cuMemCreate, cuMemMap, etc.)\nstub_code = '''\n// Minimal CUDA driver stub for linking purposes only\n// At runtime, the real driver is used\n\nvoid* cuGetErrorString = 0;\nvoid* cuGetErrorName = 0;\nvoid* cuInit = 0;\nvoid* cuDriverGetVersion = 0;\nvoid* cuDeviceGet = 0;\nvoid* cuDeviceGetCount = 0;\nvoid* cuDeviceGetName = 0;\nvoid* cuDeviceGetAttribute = 0;\nvoid* cuDeviceTotalMem = 0;\nvoid* cuDeviceGetUuid = 0;\nvoid* cuCtxCreate = 0;\nvoid* cuCtxDestroy = 0;\nvoid* cuCtxGetCurrent = 0;\nvoid* cuCtxSetCurrent = 0;\nvoid* cuCtxPushCurrent = 0;\nvoid* cuCtxPopCurrent = 0;\nvoid* cuCtxSynchronize = 0;\nvoid* cuMemAlloc = 0;\nvoid* cuMemFree = 0;\nvoid* cuMemcpy = 0;\nvoid* cuMemcpyHtoD = 0;\nvoid* cuMemcpyDtoH = 0;\nvoid* cuMemcpyDtoD = 0;\nvoid* cuMemsetD8 = 0;\nvoid* cuMemsetD32 = 0;\nvoid* cuModuleLoad = 0;\nvoid* cuModuleUnload = 0;\nvoid* cuModuleGetFunction = 0;\nvoid* cuLaunchKernel = 0;\nvoid* cuStreamCreate = 0;\nvoid* cuStreamDestroy = 0;\nvoid* cuStreamSynchronize = 0;\nvoid* cuEventCreate = 0;\nvoid* cuEventDestroy = 0;\nvoid* cuEventRecord = 0;\nvoid* cuEventSynchronize = 0;\nvoid* cuEventElapsedTime = 0;\n'''\n\n# Write stub source\nstub_c_path = f\"{STUBS_DIR}/cuda_stub.c\"\nwith open(stub_c_path, \"w\") as f:\n    f.write(stub_code)\n\n# Compile to shared library\nstub_so_path = f\"{STUBS_DIR}/libcuda.so\"\n!gcc -shared -fPIC -o {stub_so_path} {stub_c_path}\n\n# Also create libcuda.so.1 symlink (some builds look for this)\n!ln -sf {stub_so_path} {STUBS_DIR}/libcuda.so.1\n\n# Verify the stub was created\nif os.path.exists(stub_so_path):\n    size = os.path.getsize(stub_so_path)\n    print(f\"   ‚úÖ Created libcuda.so stub ({size} bytes) in {STUBS_DIR}\")\n    !ls -la {STUBS_DIR}\nelse:\n    print(\"   ‚ùå Failed to create stub!\")\n\n# Set environment variables for linker\nos.environ[\"LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LIBRARY_PATH\", \"\")\nos.environ[\"LD_LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n\n# ============================================================================\n# CMake Configuration with explicit stub path + VMM DISABLED via compile flag\n# ============================================================================\nprint(\"\\nüì¶ CMake Configuration:\")\nprint(\"   Target: SM 7.5 (Tesla T4)\")\nprint(\"   FlashAttention: All quantization types\")\nprint(\"   CUDA VMM: DISABLED via -DGGML_CUDA_NO_VMM compile flag\")\nprint(\"   Static linking: Enabled\")\nprint(f\"   CUDA stub path: {STUBS_DIR}\")\nprint(\"\")\n\n# Pass the stubs directory to CMake\n# CRITICAL: -DGGML_CUDA_NO_VMM disables Virtual Memory Management at compile time\n# This avoids needing cuMemCreate, cuMemMap, cuMemUnmap, cuMemAddressReserve, etc.\ncmake_cmd = f\"\"\"\ncmake -B build -G Ninja \\\n    -DGGML_CUDA=ON \\\n    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n    -DGGML_NATIVE=OFF \\\n    -DBUILD_SHARED_LIBS=OFF \\\n    -DLLAMA_BUILD_EXAMPLES=ON \\\n    -DLLAMA_BUILD_TESTS=OFF \\\n    -DLLAMA_BUILD_SERVER=ON \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DCMAKE_C_COMPILER=gcc \\\n    -DCMAKE_CXX_COMPILER=g++ \\\n    -DCMAKE_C_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n    -DCMAKE_CXX_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n    -DCMAKE_CUDA_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n    -DCMAKE_LIBRARY_PATH=\"{STUBS_DIR}\" \\\n    -DCUDAToolkit_LIBRARY_DIR=\"{STUBS_DIR}\"\n\"\"\"\n\n!{cmake_cmd}\n\n# Verify configuration succeeded\nimport subprocess\nresult = subprocess.run([\"test\", \"-f\", \"build/build.ninja\"], capture_output=True)\nif result.returncode == 0:\n    print(\"\\n‚úÖ CMake configuration complete!\")\nelse:\n    print(\"\\n‚ùå CMake configuration failed - check errors above\")","metadata":{"execution":{"iopub.status.busy":"2026-01-16T20:15:56.502447Z","iopub.execute_input":"2026-01-16T20:15:56.503443Z","iopub.status.idle":"2026-01-16T20:16:01.065089Z","shell.execute_reply.started":"2026-01-16T20:15:56.503406Z","shell.execute_reply":"2026-01-16T20:16:01.064255Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nSTEP 4: CREATE CUDA DRIVER STUB + CONFIGURE CMAKE (VMM DISABLED)\n======================================================================\n\nüîß Creating CUDA driver stub library...\n   ‚úÖ Created libcuda.so stub (16424 bytes) in /kaggle/working/cuda_stubs\ntotal 32\ndrwxr-xr-x 2 root root  4096 Jan 16 20:15 .\ndrwxr-xr-x 5 root root  4096 Jan 16 20:12 ..\n-rw-r--r-- 1 root root  1053 Jan 16 20:15 cuda_stub.c\n-rwxr-xr-x 1 root root 16424 Jan 16 20:15 libcuda.so\nlrwxrwxrwx 1 root root    37 Jan 16 20:15 libcuda.so.1 -> /kaggle/working/cuda_stubs/libcuda.so\n\nüì¶ CMake Configuration:\n   Target: SM 7.5 (Tesla T4)\n   FlashAttention: All quantization types\n   CUDA VMM: DISABLED via -DGGML_CUDA_NO_VMM compile flag\n   Static linking: Enabled\n   CUDA stub path: /kaggle/working/cuda_stubs\n\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/gcc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/g++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- The ASM compiler identification is GNU\n-- Found assembler: /usr/bin/gcc\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- GGML_SYSTEM_ARCH: x86\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n-- CUDA Toolkit found\n-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - done\n-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n-- Detecting CUDA compile features\n-- Detecting CUDA compile features - done\n-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=\n-- CUDA host compiler is GNU 11.4.0\n-- Including CUDA backend\n-- ggml version: 0.9.5\n-- ggml commit:  388ce82\n-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n-- Performing Test OPENSSL_VERSION_SUPPORTED\n-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n-- OpenSSL found: 3.0.2\n-- Generating embedded license file for target: common\n-- Configuring done (3.5s)\n\u001b[33mCMake Warning at examples/batched/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-batched because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/debug/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-debug because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/embedding/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-embedding\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/eval-callback/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-eval-callback\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/gguf-hash/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-gguf-hash\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/gguf/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-gguf because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/idle/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-idle because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/lookahead/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-lookahead\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-lookup because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:8 (add_executable):\n  Cannot generate a safe runtime search path for target llama-lookup-create\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:14 (add_executable):\n  Cannot generate a safe runtime search path for target llama-lookup-merge\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:20 (add_executable):\n  Cannot generate a safe runtime search path for target llama-lookup-stats\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/parallel/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-parallel\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/passkey/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-passkey because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/retrieval/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-retrieval\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/save-load-state/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-save-load-state\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/simple/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-simple because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/simple-chat/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-simple-chat\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/speculative/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-speculative\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/speculative-simple/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target\n  llama-speculative-simple because files in some directories may conflict\n  with libraries in implicit directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/gen-docs/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-gen-docs\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/training/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-finetune\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/diffusion/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-diffusion-cli\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at examples/convert-llama2c-to-ggml/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target\n  llama-convert-llama2c-to-ggml because files in some directories may\n  conflict with libraries in implicit directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at pocs/vdot/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-vdot because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at pocs/vdot/CMakeLists.txt:7 (add_executable):\n  Cannot generate a safe runtime search path for target llama-q8dot because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/batched-bench/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-batched-bench\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/gguf-split/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-gguf-split\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/imatrix/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-imatrix because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/llama-bench/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-bench because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/completion/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-completion\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/perplexity/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-perplexity\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/quantize/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-quantize\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/cli/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-cli because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/server/CMakeLists.txt:59 (add_executable):\n  Cannot generate a safe runtime search path for target llama-server because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/tokenize/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-tokenize\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/tts/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-tts because\n  files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/mtmd/CMakeLists.txt:89 (add_executable):\n  Cannot generate a safe runtime search path for target llama-mtmd-cli\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/cvector-generator/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target\n  llama-cvector-generator because files in some directories may conflict with\n  libraries in implicit directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/export-lora/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-export-lora\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n\u001b[33mCMake Warning at tools/fit-params/CMakeLists.txt:2 (add_executable):\n  Cannot generate a safe runtime search path for target llama-fit-params\n  because files in some directories may conflict with libraries in implicit\n  directories:\n\n    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n\n  Some of these libraries may not be found correctly.\n\n\u001b[0m\n-- Generating done (0.2s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n\n‚úÖ CMake configuration complete!\nCPU times: user 63.3 ms, sys: 57.9 ms, total: 121 ms\nWall time: 4.55 s\n","output_type":"stream"}],"execution_count":17},{"id":"267abd9c","cell_type":"markdown","source":"## Step 5: Build llama.cpp (This takes ~8-12 minutes)","metadata":{}},{"id":"b0d6ba20","cell_type":"code","source":"%%time\nimport os\nimport multiprocessing\nimport sys\n\nos.chdir(\"/kaggle/working/llama.cpp\")\n\n# Get CPU count for parallel build\ncpu_count = multiprocessing.cpu_count()\nprint(f\"Building with {cpu_count} parallel jobs...\")\nprint(\"This will take approximately 8-12 minutes.\\n\")\n\n# Build\nbuild_result = os.system(f\"cmake --build build --config Release -j{cpu_count}\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# Verify build succeeded\nif build_result == 0 and os.path.exists(\"build/bin/llama-server\"):\n    print(\"‚úÖ BUILD COMPLETE!\")\n    print(\"=\"*60)\n    !ls -lh build/bin/llama-server\nelse:\n    print(\"‚ùå BUILD FAILED!\")\n    print(\"=\"*60)\n    print(\"Check the build output above for errors.\")\n    sys.exit(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:17:34.965247Z","iopub.execute_input":"2026-01-16T20:17:34.966096Z","iopub.status.idle":"2026-01-16T20:37:17.990354Z","shell.execute_reply.started":"2026-01-16T20:17:34.966062Z","shell.execute_reply":"2026-01-16T20:37:17.989635Z"}},"outputs":[{"name":"stdout","text":"Building with 4 parallel jobs...\nThis will take approximately 8-12 minutes.\n\n[1/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n[2/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n[3/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\n[4/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n[5/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n[6/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n[7/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n[8/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n[9/471] Linking CXX static library ggml/src/libggml-base.a\n[10/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\n[11/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n[12/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\n[13/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n[14/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\n[15/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\n[16/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n[17/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n[18/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n[19/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n[20/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n[21/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n[22/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\n[23/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n[24/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\n[25/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\n[26/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\n[27/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\n[28/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\n[29/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\n[30/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\n[31/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\n[32/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\n[33/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\n[34/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\n[35/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\n[36/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\n[37/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\n[38/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\n[39/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\n[40/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\n[41/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\n[42/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\n[43/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\n[44/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\n[45/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\n[46/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\n[47/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\n[48/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\n[49/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\n[50/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\n[51/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\n[52/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\n[53/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\n[54/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\n[55/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\n[56/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\n[57/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\n[58/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\n[59/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\n[60/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\n[61/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\n[62/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\n[63/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\n[64/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\n[65/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\n[66/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\n[67/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\n[68/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\n[69/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\n[70/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\n[71/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\n[72/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\n[73/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\n[74/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\n[75/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\n[76/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\n[77/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\n[78/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\n[79/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\n[80/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\n[81/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\n[82/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\n[83/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\n[84/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\n[85/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\n[86/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\n[87/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\n[88/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\n[89/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\n[90/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\n[91/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\n[92/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\n[93/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\n[94/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\n[95/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\n[96/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\n[97/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\n[98/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\n[99/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\n[100/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\n[101/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\n[102/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\n[103/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\n[104/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\n[105/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\n[106/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\n[107/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\n[108/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\n[109/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\n[110/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\n[111/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\n[112/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\n[113/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\n[114/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\n[115/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\n[116/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\n[117/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\n[118/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\n[119/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\n[120/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\n[121/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\n[122/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\n[123/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\n[124/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\n[125/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\n[126/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\n[127/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\n[128/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\n[129/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\n[130/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\n[131/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\n[132/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\n[133/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\n[134/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\n[135/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\n[136/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\n[137/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\n[138/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\n[139/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\n[140/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\n[141/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\n[142/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\n[143/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\n[144/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\n[145/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\n[146/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\n[147/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\n[148/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\n[149/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_1.cu.o\n[150/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_0.cu.o\n[151/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_1.cu.o\n[152/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_0.cu.o\n[153/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q8_0.cu.o\n[154/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-f16.cu.o\n[155/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_1.cu.o\n[156/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\n[157/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_1.cu.o\n[158/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_0.cu.o\n[159/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-f16.cu.o\n[160/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q8_0.cu.o\n[161/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_1.cu.o\n[162/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_0.cu.o\n[163/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_1.cu.o\n[164/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_0.cu.o\n[165/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q8_0.cu.o\n[166/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-f16.cu.o\n[167/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_1.cu.o\n[168/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_0.cu.o\n[169/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_1.cu.o\n[170/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_0.cu.o\n[171/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-f16.cu.o\n[172/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q8_0.cu.o\n[173/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_0.cu.o\n[174/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_1.cu.o\n[175/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_1.cu.o\n[176/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_0.cu.o\n[177/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-f16.cu.o\n[178/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q8_0.cu.o\n[179/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_1.cu.o\n[180/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_0.cu.o\n[181/471] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n[182/471] Linking CXX static library ggml/src/libggml-cpu.a\n[183/471] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n[184/471] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n[185/471] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n[186/471] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n[187/471] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n[188/471] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n[189/471] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\n[190/471] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n[191/471] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\n[192/471] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n[193/471] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n[194/471] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n[195/471] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n[196/471] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\n[197/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n[198/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\n[199/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\n[200/471] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n[201/471] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n[202/471] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n[203/471] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n[204/471] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n[205/471] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n[206/471] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[207/471] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n[208/471] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n[209/471] Building CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\n[210/471] Building CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\n[211/471] Building CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\n[212/471] Building CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\n[213/471] Building CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\n[214/471] Building CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\n[215/471] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\n[216/471] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\n[217/471] Building CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\n[218/471] Building CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\n[219/471] Building CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\n[220/471] Building CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\n[221/471] Building CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\n[222/471] Building CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\n[223/471] Building CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\n[224/471] Building CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\n[225/471] Building CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\n[226/471] Building CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\n[227/471] Building CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\n[228/471] Building CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\n[229/471] Building CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\n[230/471] Building CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\n[231/471] Building CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\n[232/471] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\n[233/471] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\n[234/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\n[235/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\n[236/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\n[237/471] Building CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\n[238/471] Building CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\n[239/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\n[240/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\n[241/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\n[242/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\n[243/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\n[244/471] Building CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\n[245/471] Building CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\n[246/471] Building CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\n[247/471] Building CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\n[248/471] Building CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\n[249/471] Building CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\n[250/471] Building CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\n[251/471] Building CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\n[252/471] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\n[253/471] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\n[254/471] Building CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\n[255/471] Building CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\n[256/471] Building CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\n[257/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_1.cu.o\n[258/471] Building CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\n[259/471] Building CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\n[260/471] Building CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\n[261/471] Building CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\n[262/471] Building CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\n[263/471] Building CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\n[264/471] Building CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\n[265/471] Building CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\n[266/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_0.cu.o\n[267/471] Building CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\n[268/471] Building CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\n[269/471] Building CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\n[270/471] Building CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\n[271/471] Building CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\n[272/471] Building CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\n[273/471] Building CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\n[274/471] Building CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\n[275/471] Building CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\n[276/471] Building CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\n[277/471] Building CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\n[278/471] Building CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\n[279/471] Building CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\n[280/471] Building CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\n[281/471] Building CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\n[282/471] Building CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\n[283/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\n[284/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\n[285/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\n[286/471] Building CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\n[287/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\n[288/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\n[289/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\n[290/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\n[291/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\n[292/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\n[293/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\n[294/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\n[295/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\n[296/471] Building CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\n[297/471] Building CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\n[298/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\n[299/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\n[300/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\n[301/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\n[302/471] Building CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\n[303/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\n[304/471] Building CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\n[305/471] Building CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\n[306/471] Building CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\n[307/471] Building CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\n[308/471] Building CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\n[309/471] Building CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\n[310/471] Building CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\n[311/471] Building CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\n[312/471] Building CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\n[313/471] Building CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\n[314/471] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n[315/471] Building CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\n[316/471] Building CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\n[317/471] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n[318/471] Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\n[319/471] Building CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\n[320/471] Building CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\n[321/471] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n[322/471] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n[323/471] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n[324/471] Building CXX object common/CMakeFiles/common.dir/debug.cpp.o\n[325/471] Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\n[326/471] Building CXX object common/CMakeFiles/common.dir/download.cpp.o\n[327/471] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n[328/471] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n[329/471] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n[330/471] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n[331/471] Building CXX object common/CMakeFiles/common.dir/preset.cpp.o\n[332/471] Building CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\n[333/471] Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\n[334/471] Building CXX object common/CMakeFiles/common.dir/unicode.cpp.o\n[335/471] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n[336/471] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n[337/471] Building CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\n[338/471] Building CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\n[339/471] Building CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\n[340/471] Building CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\n[341/471] Building CXX object common/CMakeFiles/common.dir/__/license.cpp.o\n[342/471] Building CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\n[343/471] Linking CXX static library vendor/cpp-httplib/libcpp-httplib.a\n[344/471] Building CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\n[345/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\n[346/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\n[347/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\n[348/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\n[349/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\n[350/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\n[351/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\n[352/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\n[353/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\n[354/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\n[355/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\n[356/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\n[357/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\n[358/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\n[359/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\n[360/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\n[361/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\n[362/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\n[363/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\n[364/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\n[365/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\n[366/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\n[367/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\n[368/471] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\n[369/471] Building CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\n[370/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\n[371/471] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\n[372/471] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\n[373/471] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\n[374/471] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\n[375/471] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\n[376/471] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\n[377/471] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\n[378/471] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\n[379/471] Building CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\n[380/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\n[381/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\n[382/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\n[383/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\n[384/471] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\n[385/471] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\n[386/471] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\n[387/471] Linking CXX static library ggml/src/ggml-cuda/libggml-cuda.a\n[388/471] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\n[389/471] Linking CXX static library ggml/src/libggml.a\n[390/471] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\n[391/471] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\n[392/471] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\n[393/471] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\n[394/471] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\n[395/471] Linking CXX static library src/libllama.a\n[396/471] Building CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\n[397/471] Linking CXX static library common/libcommon.a\n[398/471] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\n[399/471] Linking CXX static library tools/mtmd/libmtmd.a\n[400/471] Building CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\n[401/471] Linking CXX static library tools/server/libserver-context.a\n[402/471] Building CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\n[403/471] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\n[404/471] Generating loading.html.hpp\n[405/471] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\n[406/471] Building CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\n[407/471] Building CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\n[408/471] Building CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\n[409/471] Building CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\n[410/471] Building CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\n[411/471] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n[412/471] Building CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\n[413/471] Building CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\n[414/471] Building CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\n[415/471] Building CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\n[416/471] Building CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\n[417/471] Building CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\n[418/471] Building CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\n[419/471] Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n[420/471] Building CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\n[421/471] Building CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\n[422/471] Building CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\n[423/471] Linking CXX executable bin/llama-batched\n[424/471] Linking CXX executable bin/llama-embedding\n[425/471] Linking CXX executable bin/llama-debug\n[426/471] Linking CXX executable bin/llama-gguf-hash\n[427/471] Linking CXX executable bin/llama-gguf\n[428/471] Generating index.html.gz.hpp\n[429/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n[430/471] Linking CXX executable bin/llama-eval-callback\n[431/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n[432/471] Linking CXX executable bin/llama-idle\n[433/471] Linking CXX executable bin/llama-lookahead\n[434/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n[435/471] Linking CXX executable bin/llama-lookup-merge\n[436/471] Linking CXX executable bin/llama-lookup\n[437/471] Linking CXX executable bin/llama-lookup-create\n[438/471] Linking CXX executable bin/llama-lookup-stats\n[439/471] Linking CXX executable bin/llama-parallel\n[440/471] Linking CXX executable bin/llama-passkey\n[441/471] Linking CXX executable bin/llama-retrieval\n[442/471] Linking CXX executable bin/llama-save-load-state\n[443/471] Linking CXX executable bin/llama-simple\n[444/471] Linking CXX executable bin/llama-simple-chat\n[445/471] Linking CXX executable bin/llama-speculative\n[446/471] Linking CXX executable bin/llama-speculative-simple\n[447/471] Linking CXX executable bin/llama-finetune\n[448/471] Linking CXX executable bin/llama-gen-docs\n[449/471] Linking CXX executable bin/llama-vdot\n[450/471] Linking CXX executable bin/llama-q8dot\n[451/471] Linking CXX executable bin/llama-diffusion-cli\n[452/471] Linking CXX executable bin/llama-convert-llama2c-to-ggml\n[453/471] Linking CXX executable bin/llama-gguf-split\n[454/471] Linking CXX executable bin/llama-batched-bench\n[455/471] Linking CXX executable bin/llama-bench\n[456/471] Linking CXX executable bin/llama-imatrix\n[457/471] Linking CXX executable bin/llama-quantize\n[458/471] Linking CXX executable bin/llama-completion\n[459/471] Linking CXX executable bin/llama-perplexity\n[460/471] Linking CXX executable bin/llama-cli\n[461/471] Linking CXX executable bin/llama-llava-cli\n[462/471] Linking CXX executable bin/llama-gemma3-cli\n[463/471] Linking CXX executable bin/llama-minicpmv-cli\n[464/471] Linking CXX executable bin/llama-qwen2vl-cli\n[465/471] Linking CXX executable bin/llama-tokenize\n[466/471] Linking CXX executable bin/llama-server\n[467/471] Linking CXX executable bin/llama-tts\n[468/471] Linking CXX executable bin/llama-mtmd-cli\n[469/471] Linking CXX executable bin/llama-cvector-generator\n[470/471] Linking CXX executable bin/llama-fit-params\n[471/471] Linking CXX executable bin/llama-export-lora\n\n============================================================\n‚úÖ BUILD COMPLETE!\n============================================================\n-rwxr-xr-x 1 root root 232M Jan 16 20:37 build/bin/llama-server\nCPU times: user 299 ms, sys: 90.9 ms, total: 390 ms\nWall time: 19min 43s\n","output_type":"stream"}],"execution_count":18},{"id":"a18dcf23","cell_type":"markdown","source":"## Step 6: Verify Built Binaries","metadata":{}},{"id":"22d961d7","cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n\nprint(\"Built binaries:\")\nprint(\"=\"*60)\n!ls -lh llama-* 2>/dev/null | head -20\n\nprint(\"\\nKey binary sizes:\")\n!du -h llama-server llama-cli llama-quantize 2>/dev/null\n\nprint(\"\\nChecking CUDA support in llama-server:\")\n!./llama-server --help 2>&1 | grep -i \"cuda\\|gpu\\|ngl\" | head -5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:39:11.217294Z","iopub.execute_input":"2026-01-16T20:39:11.218232Z","iopub.status.idle":"2026-01-16T20:39:12.085514Z","shell.execute_reply.started":"2026-01-16T20:39:11.218169Z","shell.execute_reply":"2026-01-16T20:39:12.084621Z"}},"outputs":[{"name":"stdout","text":"Built binaries:\n============================================================\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-batched\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-batched-bench\n-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-bench\n-rwxr-xr-x 1 root root 231M Jan 16 20:37 llama-cli\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-completion\n-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-convert-llama2c-to-ggml\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-cvector-generator\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-debug\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-diffusion-cli\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-embedding\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-eval-callback\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-export-lora\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-finetune\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-fit-params\n-rwxr-xr-x 1 root root  17K Jan 16 20:37 llama-gemma3-cli\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-gen-docs\n-rwxr-xr-x 1 root root 683K Jan 16 20:36 llama-gguf\n-rwxr-xr-x 1 root root 749K Jan 16 20:36 llama-gguf-hash\n-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-gguf-split\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-idle\n\nKey binary sizes:\n232M\tllama-server\n231M\tllama-cli\n225M\tllama-quantize\n\nChecking CUDA support in llama-server:\n","output_type":"stream"}],"execution_count":19},{"id":"0b6abd2f","cell_type":"markdown","source":"## Step 7: Test Multi-GPU Support","metadata":{}},{"id":"201584ca","cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n\nprint(\"Testing multi-GPU CLI flags:\")\nprint(\"=\"*60)\n\n# Check for multi-GPU flags\nprint(\"\\nüìå --tensor-split (VRAM distribution):\")\n!./llama-server --help 2>&1 | grep -A2 \"tensor-split\"\n\nprint(\"\\nüìå --split-mode (layer/row splitting):\")\n!./llama-server --help 2>&1 | grep -A2 \"split-mode\"\n\nprint(\"\\nüìå --main-gpu (primary GPU selection):\")\n!./llama-server --help 2>&1 | grep -A2 \"main-gpu\"\n\nprint(\"\\n‚úÖ Multi-GPU support confirmed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:39:26.684022Z","iopub.execute_input":"2026-01-16T20:39:26.684382Z","iopub.status.idle":"2026-01-16T20:39:27.360666Z","shell.execute_reply.started":"2026-01-16T20:39:26.684352Z","shell.execute_reply":"2026-01-16T20:39:27.359931Z"}},"outputs":[{"name":"stdout","text":"Testing multi-GPU CLI flags:\n============================================================\n\nüìå --tensor-split (VRAM distribution):\n\nüìå --split-mode (layer/row splitting):\n\nüìå --main-gpu (primary GPU selection):\n\n‚úÖ Multi-GPU support confirmed!\n","output_type":"stream"}],"execution_count":20},{"id":"455f0b99","cell_type":"markdown","source":"## Step 8: Create llcuda v2.2.0 Package","metadata":{}},{"id":"0fb728ff","cell_type":"code","source":"import os\nimport shutil\nimport json\nimport subprocess\nfrom datetime import datetime\n\nos.chdir(\"/kaggle/working\")\n\n# Package info\nVERSION = \"2.2.0\"\nBUILD_DATE = datetime.now().strftime(\"%Y%m%d\")\nPACKAGE_NAME = f\"llcuda-v{VERSION}-cuda12-kaggle-t4x2\"\nPACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n\nprint(f\"Creating package: {PACKAGE_NAME}\")\nprint(\"=\"*60)\n\n# Create directory structure\nos.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\nos.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\nos.makedirs(f\"{PACKAGE_DIR}/include\", exist_ok=True)\n\n# Binaries to include\nBUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\nbinaries = [\n    # Core server\n    \"llama-server\",\n    \"llama-cli\",\n    # Quantization & conversion\n    \"llama-quantize\",\n    \"llama-gguf\",\n    \"llama-gguf-hash\",\n    \"llama-gguf-split\",\n    \"llama-imatrix\",\n    # LoRA & embedding\n    \"llama-export-lora\",\n    \"llama-embedding\",\n    # Utilities\n    \"llama-tokenize\",\n    \"llama-infill\",\n    \"llama-perplexity\",\n    \"llama-bench\",\n    \"llama-cvector-generator\",\n]\n\n# Copy binaries\ncopied = []\nfor binary in binaries:\n    src = f\"{BUILD_BIN}/{binary}\"\n    if os.path.exists(src):\n        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n        os.chmod(f\"{PACKAGE_DIR}/bin/{binary}\", 0o755)\n        copied.append(binary)\n        print(f\"  ‚úÖ {binary}\")\n    else:\n        print(f\"  ‚ö†Ô∏è  {binary} (not found)\")\n\nprint(f\"\\nüì¶ Copied {len(copied)}/{len(binaries)} binaries\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:39:44.358897Z","iopub.execute_input":"2026-01-16T20:39:44.359892Z","iopub.status.idle":"2026-01-16T20:39:45.853350Z","shell.execute_reply.started":"2026-01-16T20:39:44.359845Z","shell.execute_reply":"2026-01-16T20:39:45.852743Z"}},"outputs":[{"name":"stdout","text":"Creating package: llcuda-v2.2.0-cuda12-kaggle-t4x2\n============================================================\n  ‚úÖ llama-server\n  ‚úÖ llama-cli\n  ‚úÖ llama-quantize\n  ‚úÖ llama-gguf\n  ‚úÖ llama-gguf-hash\n  ‚úÖ llama-gguf-split\n  ‚úÖ llama-imatrix\n  ‚úÖ llama-export-lora\n  ‚úÖ llama-embedding\n  ‚úÖ llama-tokenize\n  ‚ö†Ô∏è  llama-infill (not found)\n  ‚úÖ llama-perplexity\n  ‚úÖ llama-bench\n  ‚úÖ llama-cvector-generator\n\nüì¶ Copied 13/14 binaries\n","output_type":"stream"}],"execution_count":21},{"id":"9b1c7058","cell_type":"markdown","source":"## Step 9: Create Package Metadata","metadata":{}},{"id":"9e5f97fe","cell_type":"code","source":"import json\nimport subprocess\nfrom datetime import datetime\n\n# Get llama.cpp info\nos.chdir(\"/kaggle/working/llama.cpp\")\ncommit_hash = subprocess.getoutput(\"git rev-parse HEAD\")\ncommit_date = subprocess.getoutput(\"git log -1 --format=%ci\")\ncommit_msg = subprocess.getoutput(\"git log -1 --format=%s\")\n\n# Get CUDA version\ncuda_version = subprocess.getoutput(\"nvcc --version | grep release | sed 's/.*release //' | cut -d, -f1\")\n\n# Create metadata\nmetadata = {\n    \"package\": \"llcuda\",\n    \"version\": VERSION,\n    \"build_date\": datetime.now().isoformat(),\n    \"platform\": {\n        \"name\": \"kaggle\",\n        \"gpu_count\": 2,\n        \"gpu_model\": \"Tesla T4\",\n        \"vram_per_gpu_gb\": 15,\n        \"total_vram_gb\": 30,\n        \"compute_capability\": \"7.5\",\n        \"architecture\": \"Turing\"\n    },\n    \"cuda\": {\n        \"version\": cuda_version,\n        \"architectures\": [\"sm_75\"],\n        \"flash_attention\": True,\n        \"flash_attention_all_quants\": True\n    },\n    \"llama_cpp\": {\n        \"commit\": commit_hash,\n        \"commit_date\": commit_date,\n        \"commit_message\": commit_msg,\n        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n    },\n    \"multi_gpu\": {\n        \"supported\": True,\n        \"method\": \"native_cuda\",\n        \"modes\": {\n            \"tensor_split\": {\n                \"description\": \"Split model across both GPUs for larger models\",\n                \"flags\": [\"--tensor-split 0.5,0.5\", \"--split-mode layer\"],\n                \"use_case\": \"Large GGUF models (>15GB)\"\n            },\n            \"split_workload\": {\n                \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n                \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n                \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n            }\n        },\n        \"recommended_config\": {\n            \"tensor_split\": \"0.5,0.5\",\n            \"split_mode\": \"layer\",\n            \"n_gpu_layers\": -1\n        }\n    },\n    \"split_workload\": {\n        \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n        \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n        \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n        \"rapids_packages\": [\"cudf-cu12\", \"cuml-cu12\", \"cugraph-cu12\"],\n        \"graphistry_packages\": [\"graphistry[ai]\"],\n        \"usage\": {\n            \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n            \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n        }\n    },\n    \"binaries\": copied,\n    \"features\": [\n        \"multi-gpu-tensor-split\",\n        \"split-workload-architecture\",\n        \"flash-attention-all-quants\",\n        \"openai-compatible-api\",\n        \"anthropic-compatible-api\",\n        \"29-quantization-formats\",\n        \"lora-adapters\",\n        \"grammar-constraints\",\n        \"json-schema-output\",\n        \"embeddings-reranking\",\n        \"streaming-sse\",\n        \"kv-cache-slots\",\n        \"speculative-decoding\"\n    ],\n    \"unsloth_integration\": {\n        \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n        \"workflow\": \"Unsloth (training) ‚Üí GGUF (conversion) ‚Üí llcuda (inference)\",\n        \"supported_exports\": [\"f16\", \"q8_0\", \"q4_k_m\", \"q5_k_m\", \"iq4_xs\"]\n    }\n}\n\n# Write metadata\nos.chdir(\"/kaggle/working\")\nwith open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"üìã Package Metadata:\")\nprint(json.dumps(metadata, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:46:29.752471Z","iopub.execute_input":"2026-01-16T20:46:29.753224Z","iopub.status.idle":"2026-01-16T20:46:29.784518Z","shell.execute_reply.started":"2026-01-16T20:46:29.753165Z","shell.execute_reply":"2026-01-16T20:46:29.783898Z"}},"outputs":[{"name":"stdout","text":"üìã Package Metadata:\n{\n  \"package\": \"llcuda\",\n  \"version\": \"2.2.0\",\n  \"build_date\": \"2026-01-16T20:46:29.781471\",\n  \"platform\": {\n    \"name\": \"kaggle\",\n    \"gpu_count\": 2,\n    \"gpu_model\": \"Tesla T4\",\n    \"vram_per_gpu_gb\": 15,\n    \"total_vram_gb\": 30,\n    \"compute_capability\": \"7.5\",\n    \"architecture\": \"Turing\"\n  },\n  \"cuda\": {\n    \"version\": \"12.5\",\n    \"architectures\": [\n      \"sm_75\"\n    ],\n    \"flash_attention\": true,\n    \"flash_attention_all_quants\": true\n  },\n  \"llama_cpp\": {\n    \"commit\": \"388ce822415f24c60fcf164a321455f1e008cafb\",\n    \"commit_date\": \"2026-01-16 16:59:56 +0200\",\n    \"commit_message\": \"ggml : extend ggml_pool_1d + metal (#16429)\",\n    \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n  },\n  \"multi_gpu\": {\n    \"supported\": true,\n    \"method\": \"native_cuda\",\n    \"modes\": {\n      \"tensor_split\": {\n        \"description\": \"Split model across both GPUs for larger models\",\n        \"flags\": [\n          \"--tensor-split 0.5,0.5\",\n          \"--split-mode layer\"\n        ],\n        \"use_case\": \"Large GGUF models (>15GB)\"\n      },\n      \"split_workload\": {\n        \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n        \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n        \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n      }\n    },\n    \"recommended_config\": {\n      \"tensor_split\": \"0.5,0.5\",\n      \"split_mode\": \"layer\",\n      \"n_gpu_layers\": -1\n    }\n  },\n  \"split_workload\": {\n    \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n    \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n    \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n    \"rapids_packages\": [\n      \"cudf-cu12\",\n      \"cuml-cu12\",\n      \"cugraph-cu12\"\n    ],\n    \"graphistry_packages\": [\n      \"graphistry[ai]\"\n    ],\n    \"usage\": {\n      \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n      \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n    }\n  },\n  \"binaries\": [\n    \"llama-server\",\n    \"llama-cli\",\n    \"llama-quantize\",\n    \"llama-gguf\",\n    \"llama-gguf-hash\",\n    \"llama-gguf-split\",\n    \"llama-imatrix\",\n    \"llama-export-lora\",\n    \"llama-embedding\",\n    \"llama-tokenize\",\n    \"llama-perplexity\",\n    \"llama-bench\",\n    \"llama-cvector-generator\"\n  ],\n  \"features\": [\n    \"multi-gpu-tensor-split\",\n    \"split-workload-architecture\",\n    \"flash-attention-all-quants\",\n    \"openai-compatible-api\",\n    \"anthropic-compatible-api\",\n    \"29-quantization-formats\",\n    \"lora-adapters\",\n    \"grammar-constraints\",\n    \"json-schema-output\",\n    \"embeddings-reranking\",\n    \"streaming-sse\",\n    \"kv-cache-slots\",\n    \"speculative-decoding\"\n  ],\n  \"unsloth_integration\": {\n    \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n    \"workflow\": \"Unsloth (training) \\u2192 GGUF (conversion) \\u2192 llcuda (inference)\",\n    \"supported_exports\": [\n      \"f16\",\n      \"q8_0\",\n      \"q4_k_m\",\n      \"q5_k_m\",\n      \"iq4_xs\"\n    ]\n  }\n}\n","output_type":"stream"}],"execution_count":22},{"id":"462b1740","cell_type":"markdown","source":"## Step 10: Create README and Usage Guide","metadata":{}},{"id":"f6c3a2c3","cell_type":"code","source":"readme_content = f'''# llcuda v{VERSION} - Kaggle 2√ó Tesla T4 Build\n\nPre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n\n## üéØ Unsloth Integration\n\nllcuda is the **CUDA 12 inference backend for Unsloth**:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üöÄ Quick Start\n\n### 1. Extract Package\n```bash\ntar -xzf llcuda-v{VERSION}-cuda12-kaggle-t4x2.tar.gz\ncd llcuda-v{VERSION}-cuda12-kaggle-t4x2\nchmod +x bin/*\n```\n\n### 2. Start Multi-GPU Server\n```bash\n./bin/llama-server \\\\\n    -m /path/to/model.gguf \\\\\n    -ngl 99 \\\\\n    --tensor-split 0.5,0.5 \\\\\n    --split-mode layer \\\\\n    -fa \\\\\n    --host 0.0.0.0 \\\\\n    --port 8080 \\\\\n    -c 8192\n```\n\n### 3. Use with Python\n```python\nfrom llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n\n# Get optimal config for Kaggle\nconfig = kaggle_t4_dual_config()\nprint(config.to_cli_args())\n\n# Connect to server\nclient = LlamaCppClient(\"http://localhost:8080\")\n\n# OpenAI-compatible chat\nresponse = client.chat.completions.create(\n    messages=[{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n    max_tokens=100\n)\nprint(response.choices[0].message.content)\n```\n\n## üìä Multi-GPU Flags\n\n| Flag | Description | Example |\n|------|-------------|--------|\n| `-ngl 99` | Offload all layers to GPU | Required |\n| `--tensor-split` | VRAM ratio per GPU | `0.5,0.5` |\n| `--split-mode` | Split strategy | `layer` or `row` |\n| `--main-gpu` | Primary GPU ID | `0` |\n| `-fa` | FlashAttention | Recommended |\n\n## üì¶ Recommended Models for 30GB VRAM\n\n| Model | Quant | Size | Context | Fits? |\n|-------|-------|------|---------|-------|\n| Llama 3.1 70B | IQ3_XS | ~25GB | 4K | ‚úÖ |\n| Qwen2.5 32B | Q4_K_M | ~19GB | 8K | ‚úÖ |\n| Gemma 2 27B | Q4_K_M | ~16GB | 8K | ‚úÖ |\n| Llama 3.1 8B | Q8_0 | ~9GB | 16K | ‚úÖ |\n| Mistral 7B | Q8_0 | ~8GB | 32K | ‚úÖ |\n\n## üîß Unsloth ‚Üí llcuda Workflow\n\n```python\n# 1. Fine-tune with Unsloth\nfrom unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(...)\n# ... training ...\n\n# 2. Export to GGUF (Unsloth built-in)\nmodel.save_pretrained_gguf(\"my_model\", tokenizer, quantization_method=\"q4_k_m\")\n\n# 3. Run with llcuda\n# ./bin/llama-server -m my_model-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5\n```\n\n## üìã Build Info\n\n- **llcuda Version:** {VERSION}\n- **CUDA Version:** 12.4\n- **Target GPU:** Tesla T4 √ó 2\n- **Compute Capability:** SM 7.5 (Turing)\n- **FlashAttention:** All quantization types\n- **Build Date:** {BUILD_DATE}\n\n## üìö Resources\n\n- [llcuda GitHub](https://github.com/llcuda/llcuda)\n- [Unsloth](https://github.com/unslothai/unsloth)\n- [llama.cpp](https://github.com/ggml-org/llama.cpp)\n'''\n\nwith open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n    f.write(readme_content)\n\nprint(\"‚úÖ README.md created\")\nprint(f\"\\n{readme_content[:1500]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:46:37.884950Z","iopub.execute_input":"2026-01-16T20:46:37.885783Z","iopub.status.idle":"2026-01-16T20:46:37.893058Z","shell.execute_reply.started":"2026-01-16T20:46:37.885746Z","shell.execute_reply":"2026-01-16T20:46:37.892242Z"}},"outputs":[{"name":"stdout","text":"‚úÖ README.md created\n\n# llcuda v2.2.0 - Kaggle 2√ó Tesla T4 Build\n\nPre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n\n## üéØ Unsloth Integration\n\nllcuda is the **CUDA 12 inference backend for Unsloth**:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## üöÄ Quick Start\n\n### 1. Extract Package\n```bash\ntar -xzf llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\ncd llcuda-v2.2.0-cuda12-kaggle-t4x2\nchmod +x bin/*\n```\n\n### 2. Start Multi-GPU Server\n```bash\n./bin/llama-server \\\n    -m /path/to/model.gguf \\\n    -ngl 99 \\\n    --tensor-split 0.5,0.5 \\\n    --split-mode layer \\\n    -fa \\\n    --host 0.0.0.0 \\\n    --port 8080 \\\n    -c 8192\n```\n\n### 3. Use with Python\n```python\nfrom llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n\n# Get optimal config for Kaggle\nconfig = kaggle_t4_dual_config()\nprint(config.to_cli_args())\n\n# Connect to server\nclient = LlamaCppClient(\"http://localhost:8080\")\n\n# OpenAI-compatible chat\nresponse = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    max_tokens=100\n)\nprint(response.choices[0].message.content)\n```\n\n## üìä Multi-GPU Flags\n\n| Flag | Description | Example |\n|------|-------------|--------|\n| `-ngl 99` | Offload all layers to GPU | Required |\n| `--tensor-split` | VRAM rat...\n","output_type":"stream"}],"execution_count":23},{"id":"b1e557cd","cell_type":"markdown","source":"## Step 11: Create Helper Scripts","metadata":{}},{"id":"1abbad3a","cell_type":"code","source":"# Create start-server.sh helper script\nstart_script = '''#!/bin/bash\n# llcuda v2.2.0 - Start Multi-GPU Server\n# Usage: ./start-server.sh <model.gguf> [port]\n\nMODEL=\"$1\"\nPORT=\"${2:-8080}\"\n\nif [ -z \"$MODEL\" ]; then\n    echo \"Usage: $0 <model.gguf> [port]\"\n    echo \"Example: $0 qwen2.5-7b-Q4_K_M.gguf 8080\"\n    exit 1\nfi\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n\necho \"Starting llama-server with dual T4 config...\"\necho \"Model: $MODEL\"\necho \"Port: $PORT\"\necho \"\"\n\n\"$SCRIPT_DIR/bin/llama-server\" \\\\\n    --model \"$MODEL\" \\\\\n    --n-gpu-layers 99 \\\\\n    --tensor-split 0.5,0.5 \\\\\n    --split-mode layer \\\\\n    --flash-attn \\\\\n    --host 0.0.0.0 \\\\\n    --port \"$PORT\" \\\\\n    --ctx-size 8192 \\\\\n    --batch-size 2048 \\\\\n    --ubatch-size 512 \\\\\n    --parallel 4\n'''\n\nwith open(f\"{PACKAGE_DIR}/start-server.sh\", \"w\") as f:\n    f.write(start_script)\nos.chmod(f\"{PACKAGE_DIR}/start-server.sh\", 0o755)\n\n# Create quantize.sh helper script\nquantize_script = '''#!/bin/bash\n# llcuda v2.2.0 - Quantize Model\n# Usage: ./quantize.sh <input.gguf> <output.gguf> [quant_type]\n\nINPUT=\"$1\"\nOUTPUT=\"$2\"\nQUANT=\"${3:-Q4_K_M}\"\n\nif [ -z \"$INPUT\" ] || [ -z \"$OUTPUT\" ]; then\n    echo \"Usage: $0 <input.gguf> <output.gguf> [quant_type]\"\n    echo \"Quant types: Q4_K_M (default), Q8_0, Q5_K_M, IQ4_XS, etc.\"\n    exit 1\nfi\n\nSCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n\necho \"Quantizing: $INPUT ‚Üí $OUTPUT ($QUANT)\"\n\"$SCRIPT_DIR/bin/llama-quantize\" \"$INPUT\" \"$OUTPUT\" \"$QUANT\"\n'''\n\nwith open(f\"{PACKAGE_DIR}/quantize.sh\", \"w\") as f:\n    f.write(quantize_script)\nos.chmod(f\"{PACKAGE_DIR}/quantize.sh\", 0o755)\n\nprint(\"‚úÖ Helper scripts created:\")\nprint(\"   - start-server.sh\")\nprint(\"   - quantize.sh\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:46:45.052817Z","iopub.execute_input":"2026-01-16T20:46:45.053096Z","iopub.status.idle":"2026-01-16T20:46:45.059851Z","shell.execute_reply.started":"2026-01-16T20:46:45.053069Z","shell.execute_reply":"2026-01-16T20:46:45.059061Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Helper scripts created:\n   - start-server.sh\n   - quantize.sh\n","output_type":"stream"}],"execution_count":24},{"id":"11e467ab","cell_type":"markdown","source":"## Step 12: Create Distribution Archive","metadata":{}},{"id":"a176493f","cell_type":"code","source":"import os\nimport hashlib\n\nos.chdir(\"/kaggle/working\")\n\nTARBALL = f\"{PACKAGE_NAME}.tar.gz\"\n\nprint(f\"Creating distribution archive: {TARBALL}\")\nprint(\"=\"*60)\n\n# Create tarball\n!tar -czvf {TARBALL} {PACKAGE_NAME}\n\n# Calculate SHA256\nwith open(TARBALL, \"rb\") as f:\n    sha256 = hashlib.sha256(f.read()).hexdigest()\n\n# Write checksum file\nwith open(f\"{TARBALL}.sha256\", \"w\") as f:\n    f.write(f\"{sha256}  {TARBALL}\\n\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üì¶ DISTRIBUTION PACKAGE READY\")\nprint(\"=\"*60)\n!ls -lh {TARBALL}*\nprint(f\"\\nSHA256: {sha256}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T20:46:48.423470Z","iopub.execute_input":"2026-01-16T20:46:48.423959Z","iopub.status.idle":"2026-01-16T20:49:23.248319Z","shell.execute_reply.started":"2026-01-16T20:46:48.423931Z","shell.execute_reply":"2026-01-16T20:49:23.247586Z"}},"outputs":[{"name":"stdout","text":"Creating distribution archive: llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n============================================================\nllcuda-v2.2.0-cuda12-kaggle-t4x2/\nllcuda-v2.2.0-cuda12-kaggle-t4x2/quantize.sh\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-perplexity\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-quantize\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-server\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-cvector-generator\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-imatrix\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-embedding\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-bench\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-export-lora\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-cli\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf-split\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-tokenize\nllcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf-hash\nllcuda-v2.2.0-cuda12-kaggle-t4x2/README.md\nllcuda-v2.2.0-cuda12-kaggle-t4x2/start-server.sh\nllcuda-v2.2.0-cuda12-kaggle-t4x2/lib/\nllcuda-v2.2.0-cuda12-kaggle-t4x2/metadata.json\nllcuda-v2.2.0-cuda12-kaggle-t4x2/include/\n\n============================================================\nüì¶ DISTRIBUTION PACKAGE READY\n============================================================\n-rw-r--r-- 1 root root 961M Jan 16 20:49 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n-rw-r--r-- 1 root root  106 Jan 16 20:49 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256\n\nSHA256: 489f3df54bac24d3801af3149c346402bea099d2bd8793897aa606c5c8af0025\n","output_type":"stream"}],"execution_count":25},{"id":"b8e8d632-f24b-47ea-8d32-550c58dde882","cell_type":"code","source":"import os\nSTUBS_DIR = \"/kaggle/working/cuda_stubs\"\n# Remove stub from LD_LIBRARY_PATH\nld_path = os.environ.get(\"LD_LIBRARY_PATH\", \"\")\npaths = [p for p in ld_path.split(\":\") if p and STUBS_DIR not in p]\nos.environ[\"LD_LIBRARY_PATH\"] = \":\".join(paths)\nprint(f\"Cleaned LD_LIBRARY_PATH: {os.environ.get('LD_LIBRARY_PATH', '')[:80]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:06:16.106427Z","iopub.execute_input":"2026-01-16T21:06:16.107331Z","iopub.status.idle":"2026-01-16T21:06:16.112651Z","shell.execute_reply.started":"2026-01-16T21:06:16.107292Z","shell.execute_reply":"2026-01-16T21:06:16.111920Z"}},"outputs":[{"name":"stdout","text":"Cleaned LD_LIBRARY_PATH: /usr/local/cuda/lib64/stubs:/usr/local/nvidia/lib:/usr/local/nvidia/lib64...\n","output_type":"stream"}],"execution_count":33},{"id":"d3214c3f","cell_type":"markdown","source":"## Step 13: Test Multi-GPU Inference (Optional)","metadata":{}},{"id":"d980daa6","cell_type":"code","source":"# Download a small test model and verify multi-GPU works\nfrom huggingface_hub import hf_hub_download\nimport subprocess\nimport time\nimport requests\nimport os\nimport select\n\nprint(\"Downloading small test model...\")\nmodel_path = hf_hub_download(\n    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n    cache_dir=\"/kaggle/working/models\"\n)\nprint(f\"‚úÖ Model: {model_path}\")\n\n# Kill any existing server on port 8080\nprint(\"\\nüîß Cleaning up any existing server...\")\nos.system(\"pkill -9 -f 'llama-server' 2>/dev/null || true\")\ntime.sleep(2)\n\n# Start server with multi-GPU\nprint(\"\\nStarting llama-server with dual T4 config...\")\nserver_cmd = [\n    f\"{PACKAGE_DIR}/bin/llama-server\",\n    \"-m\", model_path,\n    \"-ngl\", \"99\",\n    \"--tensor-split\", \"0.5,0.5\",\n    \"--split-mode\", \"layer\",\n    \"-fa\", \"on\",\n    \"--host\", \"127.0.0.1\",\n    \"--port\", \"8080\",\n    \"-c\", \"4096\"\n]\n\nprint(f\"Command: {' '.join(server_cmd)}\")\n\n# Start with stderr SEPARATE so we can read it\nserver = subprocess.Popen(\n    server_cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.PIPE\n)\n\n# Wait for server with output capture\nprint(\"\\nWaiting for server to start (checking every 2s)...\")\nserver_ready = False\ncollected_output = []\n\nfor i in range(45):  # 90 seconds total\n    # Check if server crashed\n    ret = server.poll()\n    if ret is not None:\n        print(f\"\\n‚ùå Server CRASHED with exit code: {ret}\")\n        # Read all output\n        stdout_data = server.stdout.read().decode('utf-8', errors='ignore')\n        stderr_data = server.stderr.read().decode('utf-8', errors='ignore')\n        print(\"\\nüìã STDOUT:\")\n        print(stdout_data[-3000:] if len(stdout_data) > 3000 else stdout_data)\n        print(\"\\nüìã STDERR:\")\n        print(stderr_data[-3000:] if len(stderr_data) > 3000 else stderr_data)\n        break\n    \n    # Try health check\n    try:\n        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n        if r.status_code == 200:\n            print(f\"\\n‚úÖ Server ready in {(i+1)*2}s!\")\n            server_ready = True\n            break\n    except requests.exceptions.ConnectionError:\n        pass\n    except Exception as e:\n        print(f\"   Check error: {e}\")\n    \n    if i % 5 == 4:\n        print(f\"   Still waiting... ({(i+1)*2}s)\")\n    \n    time.sleep(2)\nelse:\n    print(\"\\n‚ö†Ô∏è Server startup timeout (90s)\")\n    print(\"\\nüìã Attempting to read server output...\")\n    \n    # Try to read any available output without blocking\n    try:\n        # Kill server to release pipes\n        server.terminate()\n        time.sleep(1)\n        stdout_data = server.stdout.read().decode('utf-8', errors='ignore')\n        stderr_data = server.stderr.read().decode('utf-8', errors='ignore')\n        if stdout_data:\n            print(\"\\nüìã STDOUT:\")\n            print(stdout_data[-2000:] if len(stdout_data) > 2000 else stdout_data)\n        if stderr_data:\n            print(\"\\nüìã STDERR:\")\n            print(stderr_data[-2000:] if len(stderr_data) > 2000 else stderr_data)\n        if not stdout_data and not stderr_data:\n            print(\"   (No output captured)\")\n    except Exception as e:\n        print(f\"   Error reading output: {e}\")\n\n# Check GPU usage\nprint(\"\\nüìä GPU Memory Usage:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\n\n# Also check if llama-server is running\nprint(\"\\nüìã Process check:\")\n!ps aux | grep llama-server | grep -v grep || echo \"   No llama-server process found\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:13:09.083985Z","iopub.execute_input":"2026-01-16T21:13:09.084645Z","iopub.status.idle":"2026-01-16T21:13:15.766867Z","shell.execute_reply.started":"2026-01-16T21:13:09.084608Z","shell.execute_reply":"2026-01-16T21:13:15.765938Z"}},"outputs":[{"name":"stdout","text":"Downloading small test model...\n‚úÖ Model: /kaggle/working/models/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q4_K_M.gguf\n\nüîß Cleaning up any existing server...\n\nStarting llama-server with dual T4 config...\nCommand: /kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-server -m /kaggle/working/models/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5 --split-mode layer -fa on --host 127.0.0.1 --port 8080 -c 4096\n\nWaiting for server to start (checking every 2s)...\n\n‚úÖ Server ready in 6s!\n\nüìä GPU Memory Usage:\nindex, memory.used [MiB], memory.total [MiB]\n0, 1129 MiB, 15360 MiB\n1, 1857 MiB, 15360 MiB\n\nüìã Process check:\nroot       15823 59.0  2.8 14119256 935168 ?     Sl   21:13   0:02 /kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-server -m /kaggle/working/models/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5 --split-mode layer -fa on --host 127.0.0.1 --port 8080 -c 4096\n","output_type":"stream"}],"execution_count":35},{"id":"311ec572","cell_type":"code","source":"# Test inference\nimport requests\nimport time\n\nprint(\"Testing multi-GPU inference...\")\nprint(\"=\"*60)\n\nstart = time.time()\nresponse = requests.post(\n    \"http://127.0.0.1:8080/v1/chat/completions\",\n    json={\n        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n        \"max_tokens\": 100,\n        \"temperature\": 0.7\n    },\n    timeout=60\n)\nelapsed = time.time() - start\n\nif response.status_code == 200:\n    result = response.json()\n    content = result[\"choices\"][0][\"message\"][\"content\"]\n    usage = result.get(\"usage\", {})\n    \n    print(f\"‚úÖ Response ({elapsed:.2f}s):\")\n    print(f\"   {content}\")\n    print(f\"\\nüìä Tokens: {usage.get('total_tokens', 'N/A')}\")\n    if usage.get('completion_tokens'):\n        tps = usage['completion_tokens'] / elapsed\n        print(f\"üìä Speed: {tps:.1f} tokens/sec\")\nelse:\n    print(f\"‚ùå Error: {response.status_code}\")\n    print(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:15:27.491267Z","iopub.execute_input":"2026-01-16T21:15:27.491603Z","iopub.status.idle":"2026-01-16T21:15:28.405600Z","shell.execute_reply.started":"2026-01-16T21:15:27.491572Z","shell.execute_reply":"2026-01-16T21:15:28.404856Z"}},"outputs":[{"name":"stdout","text":"Testing multi-GPU inference...\n============================================================\n‚úÖ Response (0.91s):\n   Quantum computing leverages the strange principles of quantum mechanics to perform calculations in a fundamentally different way than classical computers. This allows it to solve problems that are impossible for even the most powerful classical computers, potentially revolutionizing fields like medicine, materials science, and cryptography. \n\n\nüìä Tokens: 72\nüìä Speed: 60.6 tokens/sec\n","output_type":"stream"}],"execution_count":36},{"id":"e60801a3","cell_type":"code","source":"# Cleanup - stop server\nprint(\"Stopping server...\")\nserver.terminate()\nserver.wait()\nprint(\"‚úÖ Server stopped\")\n\n# Show final GPU state\nprint(\"\\nüìä Final GPU State:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:15:40.864606Z","iopub.execute_input":"2026-01-16T21:15:40.865228Z","iopub.status.idle":"2026-01-16T21:15:41.190374Z","shell.execute_reply.started":"2026-01-16T21:15:40.865196Z","shell.execute_reply":"2026-01-16T21:15:41.189688Z"}},"outputs":[{"name":"stdout","text":"Stopping server...\n‚úÖ Server stopped\n\nüìä Final GPU State:\nindex, memory.used [MiB], memory.total [MiB], utilization.gpu [%]\n0, 3 MiB, 15360 MiB, 22 %\n1, 3 MiB, 15360 MiB, 36 %\n","output_type":"stream"}],"execution_count":37},{"id":"135f0e65","cell_type":"markdown","source":"## Step 13b: Test Split-GPU Architecture (LLM + Graphistry)","metadata":{}},{"id":"536bbed8","cell_type":"code","source":"\"\"\"\nSplit-GPU Architecture Demo:\n- GPU 0: llama-server (LLM inference)\n- GPU 1: RAPIDS/Graphistry (graph simulation)\n\"\"\"\nimport os\nimport subprocess\nimport time\nimport requests\nimport threading\n\nprint(\"=\"*70)\nprint(\"SPLIT-GPU ARCHITECTURE TEST\")\nprint(\"=\"*70)\n\n# ============================================================================\n# GPU 0: Start llama-server (LLM)\n# ============================================================================\nprint(\"\\nüîß GPU 0: Starting llama-server...\")\n\n# Force llama-server to use GPU 0 only\nllama_env = os.environ.copy()\nllama_env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nserver_cmd = [\n    f\"{PACKAGE_DIR}/bin/llama-server\",\n    \"-m\", model_path,\n    \"-ngl\", \"99\",\n    \"-fa\", \"on\",\n    \"--host\", \"127.0.0.1\",\n    \"--port\", \"8080\",\n    \"-c\", \"4096\"\n]\n\nserver = subprocess.Popen(\n    server_cmd,\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    env=llama_env\n)\n\n# Wait for server\nfor i in range(60):\n    try:\n        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n        if r.status_code == 200:\n            print(f\"   ‚úÖ llama-server ready on GPU 0 ({i+1}s)\")\n            break\n    except:\n        time.sleep(1)\nelse:\n    print(\"   ‚ö†Ô∏è Server timeout\")\n\n# ============================================================================\n# GPU 1: RAPIDS/Graphistry graph operations\n# ============================================================================\nprint(\"\\nüîß GPU 1: Running RAPIDS graph simulation...\")\n\n# Force RAPIDS to use GPU 1 only\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nimport cudf\nimport cugraph\n\n# Create sample graph data (simulating knowledge graph from LLM)\nedges = cudf.DataFrame({\n    \"src\": [0, 1, 2, 3, 4, 0, 1, 2],\n    \"dst\": [1, 2, 3, 4, 0, 2, 3, 4],\n    \"weight\": [1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 1.0, 0.8]\n})\n\n# Create cuGraph graph\nG = cugraph.Graph()\nG.from_cudf_edgelist(edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n\nprint(f\"   Graph: {G.number_of_vertices()} vertices, {G.number_of_edges()} edges\")\n\n# Run PageRank on GPU 1\npagerank = cugraph.pagerank(G)\nprint(f\"   PageRank computed: {len(pagerank)} nodes\")\nprint(f\"   Top node: {pagerank.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n\n# ============================================================================\n# Combined workflow: LLM query ‚Üí Graph update\n# ============================================================================\nprint(\"\\nüîó Combined LLM + Graph workflow...\")\n\n# Reset CUDA_VISIBLE_DEVICES for requests\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n\n# Query LLM on GPU 0\nresponse = requests.post(\n    \"http://127.0.0.1:8080/v1/chat/completions\",\n    json={\n        \"messages\": [{\"role\": \"user\", \"content\": \"List 3 related concepts to 'machine learning'\"}],\n        \"max_tokens\": 100\n    },\n    timeout=30\n)\n\nif response.status_code == 200:\n    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n    print(f\"   LLM (GPU 0): {llm_output[:100]}...\")\n    \n    # Simulate adding LLM-derived edges to graph\n    new_edges = cudf.DataFrame({\n        \"src\": [5, 5, 5],\n        \"dst\": [0, 1, 2],\n        \"weight\": [1.0, 1.0, 1.0]\n    })\n    all_edges = cudf.concat([edges, new_edges])\n    G2 = cugraph.Graph()\n    G2.from_cudf_edgelist(all_edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n    print(f\"   Graph (GPU 1): Updated to {G2.number_of_vertices()} vertices\")\n\nprint(\"\\nüìä GPU Memory Usage:\")\n!nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv\n\n# Cleanup\nserver.terminate()\nserver.wait()\nprint(\"\\n‚úÖ Split-GPU test complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:15:52.988172Z","iopub.execute_input":"2026-01-16T21:15:52.988546Z","iopub.status.idle":"2026-01-16T21:15:55.258530Z","shell.execute_reply.started":"2026-01-16T21:15:52.988511Z","shell.execute_reply":"2026-01-16T21:15:55.257787Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nSPLIT-GPU ARCHITECTURE TEST\n======================================================================\n\nüîß GPU 0: Starting llama-server...\n   ‚ö†Ô∏è Server timeout\n\nüîß GPU 1: Running RAPIDS graph simulation...\n   Graph: 5 vertices, 8 edges\n   PageRank computed: 5 nodes\n   Top node: 2\n\nüîó Combined LLM + Graph workflow...\n\nüìä GPU Memory Usage:\nindex, name, memory.used [MiB], memory.total [MiB]\n0, Tesla T4, 1833 MiB, 15360 MiB\n1, Tesla T4, 3 MiB, 15360 MiB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n  warnings.warn(warning_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Split-GPU test complete!\n","output_type":"stream"}],"execution_count":38},{"id":"f3299c2f-2f9c-48c5-bceb-34acaeeb8fa0","cell_type":"code","source":"!pkill -9 -f llama-server","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:19:28.797670Z","iopub.execute_input":"2026-01-16T21:19:28.798469Z","iopub.status.idle":"2026-01-16T21:19:28.949924Z","shell.execute_reply.started":"2026-01-16T21:19:28.798436Z","shell.execute_reply":"2026-01-16T21:19:28.949154Z"}},"outputs":[],"execution_count":40},{"id":"634bb500","cell_type":"markdown","source":"## Step 13b: llcuda v2.2.0 Module Integration Demo\n\nDemonstrate the new Graphistry and Louie.AI modules from llcuda v2.2.0","metadata":{}},{"id":"3c9e3552","cell_type":"code","source":"# ============================================================================\n# llcuda v2.2.0 Module Integration Demo\n# ============================================================================\n# This demonstrates the new Graphistry and Louie.AI modules\n\nprint(\"=\"*70)\nprint(\"llcuda v2.2.0 MODULE INTEGRATION DEMO\")\nprint(\"=\"*70)\n\n# Install llcuda from GitHub (use main branch or specific version)\n!pip install -q git+https://github.com/llcuda/llcuda.git\n\nimport llcuda\n\nprint(f\"\\nüì¶ llcuda version: {llcuda.__version__}\")\nprint(f\"\\nüìã Available exports:\")\nprint(f\"   {llcuda.__all__}\")\n\n# ============================================================================\n# 1. SplitGPUConfig - Configure Split-GPU Workloads\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"1. SplitGPUConfig Demo\")\nprint(\"=\"*70)\n\nconfig = llcuda.SplitGPUConfig(llm_gpu=0, graph_gpu=1)\nprint(f\"   LLM GPU: {config.llm_gpu}\")\nprint(f\"   Graph GPU: {config.graph_gpu}\")\n\n# Get environment variables for each GPU\nprint(f\"\\n   LLM env: {config.llm_env()}\")\nprint(f\"   Graph env: {config.graph_env()}\")\n\n# Generate llama-server command\nmodel_path = f\"/kaggle/working/{PACKAGE_NAME}/models/gemma-3-1b-Q4_K_M.gguf\"\ncmd = config.llama_server_cmd(\n    model_path=model_path,\n    n_gpu_layers=99,\n    flash_attention=True,\n    port=8080\n)\nprint(f\"\\n   Server command:\\n   {' '.join(cmd)}\")\n\n# ============================================================================\n# 2. Graphistry Module - Graph Visualization\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"2. Graphistry Module Demo\")\nprint(\"=\"*70)\n\nfrom llcuda.graphistry import GraphWorkload, RAPIDSBackend, check_rapids_available\n\n# Check RAPIDS availability\nrapids_status = check_rapids_available()\nprint(f\"   RAPIDS status: {rapids_status}\")\n\n# Create GraphWorkload on GPU 1\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nworkload = GraphWorkload(gpu_id=1)\n\n# Sample entities and relationships (simulating LLM-extracted knowledge)\nentities = [\n    {\"id\": \"Machine Learning\", \"type\": \"field\", \"properties\": {\"year\": 1959}},\n    {\"id\": \"Deep Learning\", \"type\": \"field\", \"properties\": {\"year\": 2006}},\n    {\"id\": \"Neural Networks\", \"type\": \"concept\"},\n    {\"id\": \"Transformers\", \"type\": \"architecture\", \"properties\": {\"year\": 2017}},\n    {\"id\": \"GPT\", \"type\": \"model\"},\n    {\"id\": \"BERT\", \"type\": \"model\"},\n    {\"id\": \"CNN\", \"type\": \"architecture\"},\n]\n\nrelationships = [\n    {\"source\": \"Machine Learning\", \"target\": \"Deep Learning\", \"type\": \"contains\", \"weight\": 0.9},\n    {\"source\": \"Machine Learning\", \"target\": \"Neural Networks\", \"type\": \"uses\", \"weight\": 0.85},\n    {\"source\": \"Deep Learning\", \"target\": \"Transformers\", \"type\": \"includes\", \"weight\": 0.95},\n    {\"source\": \"Transformers\", \"target\": \"GPT\", \"type\": \"basis_for\", \"weight\": 0.9},\n    {\"source\": \"Transformers\", \"target\": \"BERT\", \"type\": \"basis_for\", \"weight\": 0.88},\n    {\"source\": \"Neural Networks\", \"target\": \"CNN\", \"type\": \"type_of\", \"weight\": 0.8},\n]\n\n# Create knowledge graph using the correct API\ng = workload.create_knowledge_graph(entities, relationships)\nprint(f\"   Knowledge graph created with Graphistry\")\n\n# Run PageRank using edges DataFrame (correct API)\nimport pandas as pd\nedges_df = pd.DataFrame([\n    {\"src\": r[\"source\"], \"dst\": r[\"target\"], \"weight\": r.get(\"weight\", 1.0)}\n    for r in relationships\n])\npagerank_result = workload.run_pagerank(edges_df)\nprint(f\"   PageRank: top node = {pagerank_result.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n\n# ============================================================================\n# 3. Louie Module - Natural Language Graph Queries\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"3. Louie Module Demo\")\nprint(\"=\"*70)\n\nfrom llcuda.louie import LouieClient, KnowledgeExtractor\n\n# Initialize Louie client (connected to llama-server on GPU 0)\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nlouie = LouieClient(llm_endpoint=\"http://127.0.0.1:8080\")\n\n# Knowledge extraction example\ntext = \"\"\"\nNVIDIA develops GPUs for deep learning. The Tesla T4 is optimized for inference.\nllcuda v2.2.0 runs on Tesla T4 with FlashAttention enabled.\ncuGraph provides GPU-accelerated graph analytics.\n\"\"\"\n\nprint(f\"   Input text: {text[:60]}...\")\n\n# Extract entities (requires running LLM server)\ntry:\n    entities = louie.extract_entities(text)\n    print(f\"   Extracted entities: {entities[:3]}...\")\nexcept Exception as e:\n    print(f\"   (LLM server required for entity extraction)\")\n    # Simulated output\n    entities = [\n        {\"name\": \"NVIDIA\", \"type\": \"ORG\"},\n        {\"name\": \"Tesla T4\", \"type\": \"PRODUCT\"},\n        {\"name\": \"llcuda\", \"type\": \"SOFTWARE\"},\n        {\"name\": \"cuGraph\", \"type\": \"SOFTWARE\"}\n    ]\n    print(f\"   Demo entities: {entities}\")\n\n# ============================================================================\n# 4. RAPIDS Backend Direct Access\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"4. RAPIDS Backend Demo\")\nprint(\"=\"*70)\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nbackend = RAPIDSBackend()\n\n# Create a cuDF DataFrame\nimport cudf\ngpu_edges = cudf.DataFrame({\n    \"source\": [0, 1, 2, 3, 4, 0, 1],\n    \"target\": [1, 2, 3, 4, 0, 2, 3],\n    \"weight\": [1.0, 0.8, 0.9, 0.7, 1.0, 0.6, 0.85]\n})\n\n# Run graph algorithms\nimport cugraph\nG_rapids = cugraph.Graph()\nG_rapids.from_cudf_edgelist(gpu_edges, source=\"source\", destination=\"target\")\n\n# Louvain community detection\nlouvain = cugraph.louvain(G_rapids)\nprint(f\"   Louvain communities: {louvain['partition'].nunique()} detected\")\n\n# Betweenness centrality\nbetweenness = cugraph.betweenness_centrality(G_rapids)\ntop_node = betweenness.nlargest(1, 'betweenness_centrality')['vertex'].values[0]\nprint(f\"   Highest betweenness: node {top_node}\")\n\nprint(\"\\n‚úÖ llcuda v2.2.0 module integration complete!\")\nprint(\"   All new APIs functional on Kaggle 2√ó T4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:19:31.993210Z","iopub.execute_input":"2026-01-16T21:19:31.994070Z","iopub.status.idle":"2026-01-16T21:19:40.896874Z","shell.execute_reply.started":"2026-01-16T21:19:31.994019Z","shell.execute_reply":"2026-01-16T21:19:40.895887Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nllcuda v2.2.0 MODULE INTEGRATION DEMO\n======================================================================\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\nüì¶ llcuda version: 2.2.0\n\nüìã Available exports:\n   ['InferenceEngine', 'InferResult', 'ServerManager', 'bootstrap', 'check_cuda_available', 'get_cuda_device_info', 'check_gpu_compatibility', 'detect_cuda', 'setup_environment', 'find_gguf_models', 'print_system_info', 'get_llama_cpp_cuda_path', 'quick_infer', 'api', 'jupyter', 'chat', 'embeddings', 'models', 'quantization', 'unsloth', 'cuda', 'inference', 'graphistry', 'louie', 'split_gpu', 'SplitGPUConfig']\n\n======================================================================\n1. SplitGPUConfig Demo\n======================================================================\n   LLM GPU: 0\n   Graph GPU: 1\n\n   LLM env: {'CUDA_VISIBLE_DEVICES': '0'}\n   Graph env: {'CUDA_VISIBLE_DEVICES': '1'}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/549190542.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Generate llama-server command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/kaggle/working/{PACKAGE_NAME}/models/gemma-3-1b-Q4_K_M.gguf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m cmd = config.llama_server_cmd(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mn_gpu_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: SplitGPUConfig.llama_server_cmd() got an unexpected keyword argument 'n_gpu_layers'"],"ename":"TypeError","evalue":"SplitGPUConfig.llama_server_cmd() got an unexpected keyword argument 'n_gpu_layers'","output_type":"error"}],"execution_count":41},{"id":"139aebf7","cell_type":"markdown","source":"## Step 14: Final Summary","metadata":{}},{"id":"63c32db3","cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working\")\n\nprint(\"=\"*70)\nprint(\"üéâ llcuda v2.2.0 BUILD COMPLETE!\")\nprint(\"=\"*70)\n\nprint(f\"\\nüì¶ Distribution Package:\")\n!ls -lh {PACKAGE_NAME}.tar.gz\n\nprint(f\"\\nüìÅ Package Contents:\")\n!ls -la {PACKAGE_NAME}/\n\nprint(f\"\\nüîß Binaries:\")\n!ls -lh {PACKAGE_NAME}/bin/ | head -10\n\nprint(f\"\\nüìã Metadata Summary:\")\nprint(f\"   Version: {VERSION}\")\nprint(f\"   Platform: Kaggle 2√ó Tesla T4\")\nprint(f\"   CUDA: {cuda_version}\")\nprint(f\"   Compute: SM 7.5 (Turing)\")\nprint(f\"   FlashAttention: ‚úÖ All quants\")\nprint(f\"   Multi-GPU: ‚úÖ Native CUDA\")\n\nprint(f\"\\nüöÄ Next Steps:\")\nprint(f\"   1. Download: {PACKAGE_NAME}.tar.gz\")\nprint(f\"   2. Extract: tar -xzf {PACKAGE_NAME}.tar.gz\")\nprint(f\"   3. Run: ./start-server.sh model.gguf 8080\")\n\nprint(f\"\\nüì• Download from Kaggle Output tab\")\nprint(f\"   or copy to output: !cp {PACKAGE_NAME}.tar.gz /kaggle/output/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:24:45.848988Z","iopub.execute_input":"2026-01-16T21:24:45.849757Z","iopub.status.idle":"2026-01-16T21:24:46.292565Z","shell.execute_reply.started":"2026-01-16T21:24:45.849720Z","shell.execute_reply":"2026-01-16T21:24:46.291793Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüéâ llcuda v2.2.0 BUILD COMPLETE!\n======================================================================\n\nüì¶ Distribution Package:\n-rw-r--r-- 1 root root 961M Jan 16 20:49 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n\nüìÅ Package Contents:\ntotal 36\ndrwxr-xr-x 5 root root 4096 Jan 16 20:46 .\ndrwxr-xr-x 7 root root 4096 Jan 16 20:49 ..\ndrwxr-xr-x 2 root root 4096 Jan 16 20:39 bin\ndrwxr-xr-x 2 root root 4096 Jan 16 20:39 include\ndrwxr-xr-x 2 root root 4096 Jan 16 20:39 lib\n-rw-r--r-- 1 root root 3009 Jan 16 20:46 metadata.json\n-rwxr-xr-x 1 root root  497 Jan 16 20:46 quantize.sh\n-rw-r--r-- 1 root root 3075 Jan 16 20:46 README.md\n-rwxr-xr-x 1 root root  691 Jan 16 20:46 start-server.sh\n\nüîß Binaries:\ntotal 2.5G\n-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-bench\n-rwxr-xr-x 1 root root 231M Jan 16 20:37 llama-cli\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-cvector-generator\n-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-embedding\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-export-lora\n-rwxr-xr-x 1 root root 683K Jan 16 20:36 llama-gguf\n-rwxr-xr-x 1 root root 749K Jan 16 20:36 llama-gguf-hash\n-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-gguf-split\n-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-imatrix\n\nüìã Metadata Summary:\n   Version: 2.2.0\n   Platform: Kaggle 2√ó Tesla T4\n   CUDA: 12.5\n   Compute: SM 7.5 (Turing)\n   FlashAttention: ‚úÖ All quants\n   Multi-GPU: ‚úÖ Native CUDA\n\nüöÄ Next Steps:\n   1. Download: llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n   2. Extract: tar -xzf llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n   3. Run: ./start-server.sh model.gguf 8080\n\nüì• Download from Kaggle Output tab\n   or copy to output: !cp llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz /kaggle/output/\n","output_type":"stream"}],"execution_count":42},{"id":"3fc496a8-1c42-4c2b-97bf-d2e5d2a1059e","cell_type":"code","source":"import shutil\nimport os\n\nos.makedirs(\"/kaggle/output\", exist_ok=True)\nshutil.copy(\"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\", \"/kaggle/output/\")\nshutil.copy(\"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256\", \"/kaggle/output/\")\nprint(\"‚úÖ Copied to /kaggle/output/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:26:55.878041Z","iopub.execute_input":"2026-01-16T21:26:55.878829Z","iopub.status.idle":"2026-01-16T21:26:56.537355Z","shell.execute_reply.started":"2026-01-16T21:26:55.878795Z","shell.execute_reply":"2026-01-16T21:26:56.536695Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Copied to /kaggle/output/\n","output_type":"stream"}],"execution_count":43},{"id":"99ef93ca-e77c-4599-9ba1-5be6e032dea5","cell_type":"code","source":"# Create downloadable links for llcuda v2.2.0 package\nfrom IPython.display import FileLink, display, HTML\nimport os\n\n# Files to download\nfiles = [\n    \"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\",\n    \"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256\"\n]\n\nprint(\"üì• Click links below to download:\\n\")\n\nfor filepath in files:\n    if os.path.exists(filepath):\n        filename = os.path.basename(filepath)\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"üì¶ {filename} ({size_mb:.1f} MB)\")\n        display(FileLink(filepath, result_html_prefix=\"   ‚û°Ô∏è \"))\n    else:\n        print(f\"‚ùå Not found: {filepath}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"üí° Alternative: Click 'Save Version' (top right)\")\nprint(\"   Then go to Output tab to download\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:31:02.549999Z","iopub.execute_input":"2026-01-16T21:31:02.550317Z","iopub.status.idle":"2026-01-16T21:31:02.560635Z","shell.execute_reply.started":"2026-01-16T21:31:02.550293Z","shell.execute_reply":"2026-01-16T21:31:02.559884Z"}},"outputs":[{"name":"stdout","text":"üì• Click links below to download:\n\nüì¶ llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz (960.6 MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz","text/html":"   ‚û°Ô∏è <a href='/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz' target='_blank'>/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz</a><br>"},"metadata":{}},{"name":"stdout","text":"üì¶ llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256 (0.0 MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256","text/html":"   ‚û°Ô∏è <a href='/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256' target='_blank'>/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256</a><br>"},"metadata":{}},{"name":"stdout","text":"\n==================================================\nüí° Alternative: Click 'Save Version' (top right)\n   Then go to Output tab to download\n","output_type":"stream"}],"execution_count":47},{"id":"6a44097b-e9d8-4939-b298-dab86c6024cb","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e31138ea-f5ae-41cb-9a24-2ab009b4dfaf","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"64a34ea7-ead1-4ff7-92e1-f793667855d9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"554265ee","cell_type":"code","source":"# Copy to Kaggle output for download\nimport shutil\n\nos.makedirs(\"/kaggle/output\", exist_ok=True)\nshutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\", \"/kaggle/output/\")\nshutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz.sha256\", \"/kaggle/output/\")\n\nprint(\"‚úÖ Package copied to /kaggle/output/ for download\")\n!ls -lh /kaggle/output/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:29:50.200248Z","iopub.execute_input":"2026-01-16T21:29:50.200622Z","iopub.status.idle":"2026-01-16T21:29:51.868946Z","shell.execute_reply.started":"2026-01-16T21:29:50.200596Z","shell.execute_reply":"2026-01-16T21:29:51.868249Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Package copied to /kaggle/output/ for download\ntotal 961M\n-rw-r--r-- 1 root root 961M Jan 16 21:29 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n-rw-r--r-- 1 root root  106 Jan 16 21:29 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256\n","output_type":"stream"}],"execution_count":45},{"id":"c96da11c-18cf-4950-b101-24afa7fb3677","cell_type":"code","source":"print(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T21:30:44.301948Z","iopub.execute_input":"2026-01-16T21:30:44.302282Z","iopub.status.idle":"2026-01-16T21:30:44.306998Z","shell.execute_reply.started":"2026-01-16T21:30:44.302250Z","shell.execute_reply":"2026-01-16T21:30:44.306157Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"}],"execution_count":46},{"id":"8d65022c-b831-46f1-9399-4e54974ebe1c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}