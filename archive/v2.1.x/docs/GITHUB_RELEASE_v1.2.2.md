# llcuda v1.2.2 - CUDA 12 Support for GeForce 940M & Tesla T4

Official CUDA 12 binary release with GPU-specific optimizations and FlashAttention support.

## üéâ What's New

### GPU-Specific Binary Bundles
- ‚úÖ **Automatic GPU Detection**: Detects your GPU and downloads the right binaries
- ‚úÖ **GeForce 940M Package (26 MB)**: Optimized for Maxwell architecture with forced cuBLAS
- ‚úÖ **Tesla T4 Package (264 MB)**: FlashAttention support for 2x faster inference
- ‚úÖ **Smart Selection**: Auto-selects based on Compute Capability

### FlashAttention Support
- üöÄ **2x Faster Inference** on modern GPUs (CC 7.5+)
- üéÆ **Supported GPUs**: Tesla T4, RTX 20xx/30xx/40xx, A100, and newer
- ‚ö° **Automatic**: Enabled when GPU supports it

### Critical Bug Fixes
- ‚úÖ Fixed `AttributeError: 'NoneType' object has no attribute 'read'` in Google Colab silent mode
- ‚úÖ Improved library path detection for different build configurations
- ‚úÖ Better error messages and logging

## üì¶ Binary Packages

### üéÆ GeForce 940M (26 MB)
**File:** `llcuda-binaries-cuda12-940m.tar.gz`

**Target GPUs:**
- NVIDIA GeForce 940M/930M/920M
- Maxwell architecture (Compute Capability 5.0-5.9)
- GTX 950, GTX 960

**Optimizations:**
- Forced cuBLAS for better compatibility
- CUDA graphs enabled
- Optimized for limited VRAM

**Performance:**
- **Speed**: 10-20 tokens/sec
- **Best For**: 1-3B parameter models (Q4_K_M quantization)
- **GPU Layers**: 10-15
- **Context**: 512-1024 tokens

**Example:**
```
Model: Gemma 3-1B Q4_K_M
Speed: 15 tok/s
Latency: 67ms per token
VRAM: ~1 GB
```

### ‚òÅÔ∏è Tesla T4 (264 MB)
**File:** `llcuda-binaries-cuda12-t4.tar.gz`

**Target GPUs:**
- Tesla T4, P100, V100
- Volta architecture (CC 7.0)
- Turing architecture (CC 7.5): RTX 2060/2070/2080
- Ampere architecture (CC 8.0-8.6): RTX 3060/3070/3080/3090, A100
- Ada Lovelace (CC 8.9): RTX 4060/4070/4080/4090

**Optimizations:**
- **FlashAttention enabled** (2x faster than standard)
- Tensor cores utilized
- CUDA graphs enabled
- Custom CUDA kernels

**Performance:**
- **Speed**: 25-60 tokens/sec with FlashAttention
- **Best For**: 1-13B parameter models (Q4_K_M/Q5_K_M quantization)
- **GPU Layers**: 26-35
- **Context**: 2048-8192 tokens

**Example (Tesla T4):**
```
Model: Gemma 3-1B Q4_K_M
Speed: 45 tok/s
Latency: 22ms per token
VRAM: ~1 GB
FlashAttention: Enabled
```

**Example (RTX 4090):**
```
Model: Gemma 3-1B Q4_K_M
Speed: 125 tok/s
Latency: 8ms per token
VRAM: ~1 GB
FlashAttention: Enabled
```

## üì• Installation

### From PyPI

```bash
pip install llcuda
```

On first import, llcuda will:
1. Detect your GPU using `nvidia-smi`
2. Download appropriate binary package (26 MB or 264 MB)
3. Extract binaries and configure paths
4. Ready to use!

### Manual Binary Installation (Optional)

If you prefer to manually install binaries:

1. Download appropriate package from this release
2. Extract to `~/.cache/llcuda/`
3. Run Python import to complete setup

## üöÄ Quick Start

```python
import llcuda

# Initialize engine (auto-downloads optimized binaries on first run)
engine = llcuda.InferenceEngine()

# Load model (downloads from HuggingFace on first use)
engine.load_model("gemma-3-1b-Q4_K_M", silent=True)

# Run inference
result = engine.infer("What is artificial intelligence?", max_tokens=100)
print(result.text)
print(f"Speed: {result.tokens_per_sec:.1f} tokens/sec")
```

## üéÆ GPU Compatibility

| GPU Family | Compute Cap | Package | Download Size | Features |
|------------|-------------|---------|---------------|----------|
| Maxwell | 5.0-5.2 | 940M | 26 MB | cuBLAS optimized |
| Pascal | 6.0-6.2 | T4 | 264 MB | Tensor cores |
| Volta | 7.0 | T4 | 264 MB | Tensor cores |
| Turing | 7.5 | T4 | 264 MB | **FlashAttention** |
| Ampere | 8.0-8.6 | T4 | 264 MB | **FlashAttention** |
| Ada | 8.9 | T4 | 264 MB | **FlashAttention** |

## üìä Performance Comparison

### Gemma 3-1B Q4_K_M (769 MB model)

| GPU | Package | Speed | Latency | GPU Layers | FlashAttention |
|-----|---------|-------|---------|------------|----------------|
| GeForce 940M | 940M (26 MB) | 15 tok/s | 67ms | 10-15 | ‚ùå |
| Tesla T4 | T4 (264 MB) | 45 tok/s | 22ms | 26-35 | ‚úÖ |
| RTX 3080 | T4 (264 MB) | 95 tok/s | 11ms | 35 | ‚úÖ |
| RTX 4090 | T4 (264 MB) | 125 tok/s | 8ms | 35 | ‚úÖ |

### Llama 3.2-3B Q4_K_M (2 GB model)

| GPU | Package | Speed | Latency | GPU Layers | FlashAttention |
|-----|---------|-------|---------|------------|----------------|
| GeForce 940M | 940M (26 MB) | 8-12 tok/s | 100ms | 10-12 | ‚ùå |
| Tesla T4 | T4 (264 MB) | 30 tok/s | 33ms | 26-30 | ‚úÖ |
| RTX 4090 | T4 (264 MB) | 85 tok/s | 12ms | 35 | ‚úÖ |

## üîß What's Changed

### Added
- GPU-specific binary bundles for optimized performance
- Automatic GPU detection in bootstrap using nvidia-smi
- FlashAttention support for CC 7.5+ GPUs (2x faster inference)
- GPU compute capability detection function
- Smart binary selection logic based on GPU architecture
- Platform detection for Colab/Kaggle/local systems

### Fixed
- **Critical**: Fixed `AttributeError` when reading stderr in silent mode (Google Colab issue)
- Library path detection for different CMake build configurations
- Script termination bug in packaging script
- Better error messages for missing binaries

### Changed
- Bootstrap now downloads GPU-specific binaries (reduces download for old GPUs by 90%)
- Improved LD_LIBRARY_PATH configuration
- Updated package structure for multiple binary variants
- Version bumped to 1.2.2

## üìã Requirements

- **Python**: 3.11+
- **CUDA**: 12.x runtime (12.8 recommended)
- **NVIDIA GPU**: Compute Capability 5.0+ (Maxwell or newer)
- **VRAM**:
  - Minimum 1GB for small models (1-3B params)
  - 8GB+ recommended for larger models (7-13B params)

## üåê Platform Support

### Local Systems
- ‚úÖ Ubuntu 22.04+
- ‚úÖ Windows 11 with WSL2
- ‚ö†Ô∏è macOS (CPU only, no CUDA)

### Cloud Notebooks
- ‚úÖ Google Colab (Tesla T4/P100/V100/A100)
- ‚úÖ Kaggle Notebooks
- ‚úÖ Paperspace Gradient
- ‚úÖ AWS SageMaker

## üìö Documentation

- **GitHub**: https://github.com/waqasm86/llcuda
- **PyPI**: https://pypi.org/project/llcuda/
- **Changelog**: [CHANGELOG.md](https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md)
- **Build Guide**: [BUILD_GUIDE.md](https://github.com/waqasm86/llcuda/blob/main/BUILD_GUIDE.md)

## üêõ Known Issues

None at this time. Report issues at: https://github.com/waqasm86/llcuda/issues

## üîÑ Upgrading from v1.1.x

**No breaking changes!** Simply upgrade:

```bash
pip install --upgrade llcuda
```

On first import after upgrade:
- Old binaries will be replaced with GPU-specific ones
- Appropriate package downloaded based on your GPU
- All existing code continues to work

## üí° Tips

### Check Your GPU
```python
import llcuda
compat = llcuda.check_gpu_compatibility()
print(f"GPU: {compat['gpu_name']}")
print(f"Compute: {compat['compute_capability']}")
```

### Force Specific Bundle (if needed)
```bash
export LLCUDA_FORCE_BUNDLE="940m"  # or "t4"
python your_script.py
```

### Verify Installed Binaries
```python
from pathlib import Path
import llcuda

binaries_dir = Path(llcuda.__file__).parent / "binaries" / "cuda12"
print(f"llama-server: {(binaries_dir / 'llama-server').exists()}")
```

## üôè Acknowledgments

- Built on [llama.cpp](https://github.com/ggerganov/llama.cpp)
- CUDA toolkit from NVIDIA
- FlashAttention implementation from Dao et al.

## üìÑ License

MIT License

---

**Full Changelog**: https://github.com/waqasm86/llcuda/compare/v1.1.9...v1.2.2

**Download llcuda from PyPI**: https://pypi.org/project/llcuda/1.2.2/
