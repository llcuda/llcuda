{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# llcuda v1.1.9 - Google Colab Test\n",
    "\n",
    "This notebook tests all the fixes in v1.1.9:\n",
    "1. ✅ llama-server detection from package binaries directory\n",
    "2. ✅ Silent mode to suppress llama-server warnings\n",
    "3. ✅ Binary auto-download on first import\n",
    "4. ✅ Model download only when explicitly called\n",
    "\n",
    "**Recommended Runtime**: Python 3.11+ with T4 GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install"
   },
   "source": [
    "## Step 1: Install llcuda v1.1.9\n",
    "\n",
    "Install the latest version from PyPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_cell"
   },
   "outputs": [],
   "source": [
    "!pip install llcuda==1.1.9 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import"
   },
   "source": [
    "## Step 2: Import llcuda\n",
    "\n",
    "**Expected behavior:**\n",
    "- First run: Downloads binaries (~161MB) - takes 30-60 seconds\n",
    "- Subsequent runs: Instant (binaries cached)\n",
    "- **NO MODEL DOWNLOAD** on import (fixed in v1.1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_cell"
   },
   "outputs": [],
   "source": [
    "import llcuda\n",
    "\n",
    "print(f\"✅ llcuda version: {llcuda.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sysinfo"
   },
   "source": [
    "## Step 3: System Information\n",
    "\n",
    "Check GPU and CUDA availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sysinfo_cell"
   },
   "outputs": [],
   "source": [
    "llcuda.print_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "server_check"
   },
   "source": [
    "## Step 4: Verify llama-server Detection\n",
    "\n",
    "**This is the critical fix in v1.1.9!**\n",
    "\n",
    "The server manager now checks the package binaries directory as priority #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "server_check_cell"
   },
   "outputs": [],
   "source": [
    "from llcuda import ServerManager\n",
    "import os\n",
    "\n",
    "server = ServerManager()\n",
    "llama_server_path = server.find_llama_server()\n",
    "\n",
    "if llama_server_path:\n",
    "    print(f\"✅ llama-server found at: {llama_server_path}\")\n",
    "    print(f\"   Exists: {llama_server_path.exists()}\")\n",
    "    print(f\"   Executable: {os.access(llama_server_path, os.X_OK)}\")\n",
    "    \n",
    "    ld_path = os.environ.get(\"LD_LIBRARY_PATH\", \"Not set\")\n",
    "    print(f\"   LD_LIBRARY_PATH: {ld_path[:100]}...\")\n",
    "else:\n",
    "    print(\"❌ llama-server NOT FOUND - this is a bug!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_model"
   },
   "source": [
    "## Step 5: Load Model with Silent Mode\n",
    "\n",
    "**New in v1.1.9:** `silent=True` parameter suppresses all llama-server output.\n",
    "\n",
    "**Expected behavior:**\n",
    "- First run: Downloads model (~800MB for gemma-3-1b-Q4_K_M) - takes 1-2 minutes\n",
    "- Subsequent runs: Instant (model cached)\n",
    "- **NO LLAMA-SERVER WARNINGS** in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model_cell"
   },
   "outputs": [],
   "source": [
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load model with silent mode enabled\n",
    "engine.load_model(\n",
    "    \"gemma-3-1b-Q4_K_M\",\n",
    "    gpu_layers=20,  # Conservative for T4 GPU (4GB VRAM)\n",
    "    ctx_size=2048,\n",
    "    auto_start=True,\n",
    "    silent=True,  # ← NEW: Suppress llama-server warnings\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## Step 6: Run Inference\n",
    "\n",
    "Test the model with a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_cell"
   },
   "outputs": [],
   "source": [
    "prompt = \"What is artificial intelligence? Answer in one sentence.\"\n",
    "\n",
    "result = engine.infer(prompt, max_tokens=50)\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"\\nResponse: {result.text}\")\n",
    "print(f\"\\n✅ Tokens generated: {result.tokens_generated}\")\n",
    "print(f\"✅ Time: {result.generation_time_ms:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "batch"
   },
   "source": [
    "## Step 7: Batch Inference\n",
    "\n",
    "Test with multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_cell"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "results = engine.batch_infer(prompts, max_tokens=30)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {prompts[i-1]}\")\n",
    "    print(f\"   → {result.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metrics"
   },
   "source": [
    "## Step 8: Performance Metrics\n",
    "\n",
    "Get throughput and latency statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics_cell"
   },
   "outputs": [],
   "source": [
    "metrics = engine.get_metrics()\n",
    "\n",
    "if metrics and 'throughput' in metrics:\n",
    "    print(f\"✅ Throughput: {metrics['throughput']['tokens_per_sec']:.2f} tokens/sec\")\n",
    "    \n",
    "    if 'latency' in metrics:\n",
    "        print(f\"✅ Mean latency: {metrics['latency']['mean_ms']:.2f}ms\")\n",
    "        if 'p95_ms' in metrics['latency']:\n",
    "            print(f\"✅ P95 latency: {metrics['latency']['p95_ms']:.2f}ms\")\n",
    "else:\n",
    "    print(\"⚠️  Metrics not available yet (need more inferences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## Step 9: Cleanup\n",
    "\n",
    "Stop the server when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup_cell"
   },
   "outputs": [],
   "source": [
    "if engine.server.is_running():\n",
    "    engine.server.stop_server()\n",
    "    print(\"✅ Server stopped\")\n",
    "else:\n",
    "    print(\"Server already stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "---\n",
    "\n",
    "## ✅ Test Summary\n",
    "\n",
    "If all cells ran successfully, v1.1.9 is working correctly!\n",
    "\n",
    "**Key Features Verified:**\n",
    "- ✅ llama-server detected from package binaries directory\n",
    "- ✅ Silent mode - no llama-server warnings\n",
    "- ✅ Binary auto-download on first import\n",
    "- ✅ Model download only when `load_model()` called\n",
    "- ✅ Inference working correctly\n",
    "- ✅ Performance metrics available\n",
    "\n",
    "### Links\n",
    "- **PyPI**: https://pypi.org/project/llcuda/1.1.9/\n",
    "- **GitHub**: https://github.com/waqasm86/llcuda\n",
    "- **Documentation**: https://waqasm86.github.io/\n",
    "- **Issues**: https://github.com/waqasm86/llcuda/issues\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
