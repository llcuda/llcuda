import torch
import sys
import subprocess
import os
import platform
import importlib.metadata

def get_system_info():
    """Get system and GPU information"""
    print("=" * 70)
    print("SYSTEM & NVIDIA GPU INFORMATION")
    print("=" * 70)
    
    print(f"\nüìä SYSTEM DETAILS")
    print(f"{'-' * 40}")
    print(f"Operating System: {platform.system()} {platform.release()}")
    print(f"System Version: {platform.version()[:80]}")
    print(f"Machine: {platform.machine()}")
    print(f"Processor: {platform.processor()}")
    
    print(f"\nüêç PYTHON & PYTORCH")
    print(f"{'-' * 40}")
    print(f"Python Version: {sys.version.split()[0]}")
    print(f"PyTorch Version: {torch.__version__}")

    print(f"\n‚ö° CUDA STATUS")
    print(f"{'-' * 40}")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA Version (PyTorch): {torch.version.cuda}")
        try:
            print(f"cuDNN Version: {torch.backends.cudnn.version()}")
        except:
            print(f"cuDNN Version: N/A")

        print(f"\nüéÆ GPU HARDWARE")
        print(f"{'-' * 40}")
        gpu_count = torch.cuda.device_count()
        print(f"Number of GPUs Detected: {gpu_count}")
        
        for i in range(gpu_count):
            print(f"\nGPU {i}:")
            print("-" * 30)
            props = torch.cuda.get_device_properties(i)
            print(f"  Name: {props.name}")
            print(f"  Compute Capability: {props.major}.{props.minor}")
            print(f"  Total Memory: {props.total_memory / 1024**3:.2f} GB")
            print(f"  Multiprocessors: {props.multi_processor_count}")
            
            print(f"\n  üìä MEMORY INFORMATION:")
            print(f"    Allocated: {torch.cuda.memory_allocated(i) / 1024**3:.4f} GB")
            print(f"    Reserved:  {torch.cuda.memory_reserved(i) / 1024**3:.4f} GB")

def get_nvidia_smi_info():
    """Get NVIDIA-SMI output"""
    print("\n" + "=" * 70)
    print("NVIDIA-SMI DETAILED REPORT")
    print("=" * 70)
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print(result.stdout.strip())
        else:
            print("nvidia-smi failed:", result.stderr.strip())
    except FileNotFoundError:
        print("nvidia-smi not found in PATH")
    except Exception as e:
        print(f"Error running nvidia-smi: {e}")

def check_cuda_toolkit():
    """Check nvcc and CUDA environment"""
    print("\n" + "=" * 70)
    print("CUDA TOOLKIT CHECK")
    print("=" * 70)
    
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ nvcc found and working:")
            for line in result.stdout.splitlines()[:3]:
                if line.strip():
                    print(f"   {line.strip()}")
        else:
            print("‚ùå nvcc not working")
    except FileNotFoundError:
        print("‚ùå nvcc not found")

    print("\nüåê Relevant Environment Variables:")
    for var in ['CUDA_HOME', 'CUDA_PATH', 'LD_LIBRARY_PATH']:
        val = os.environ.get(var)
        if val:
            print(f"   {var}: {val[:100]}{'...' if len(val) > 100 else ''}")
        else:
            print(f"   {var}: Not set")

def check_pytorch_cuda_compatibility():
    """Fixed function - now properly uses global torch"""
    print("\n" + "=" * 70)
    print("PYTORCH CUDA COMPATIBILITY")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available in PyTorch")
        return
    
    print(f"PyTorch Version: {torch.__version__}")
    print(f"CUDA Version (PyTorch build): {torch.version.cuda}")
    print(f"Current Device: {torch.cuda.current_device()}")
    print(f"Device Name: {torch.cuda.get_device_name(0)}")
    
    print("\nüîß Key NVIDIA CUDA packages (from pip):")
    packages = [
        'torch', 'nvidia-cuda-runtime-cu12', 'nvidia-cudnn-cu12',
        'nvidia-cublas-cu12', 'nvidia-nccl-cu12'
    ]
    for pkg in packages:
        try:
            ver = importlib.metadata.version(pkg)
            print(f"   {pkg:30}: {ver}")
        except importlib.metadata.PackageNotFoundError:
            print(f"   {pkg:30}: Not installed")

def run_quick_tests():
    """Quick functionality tests"""
    print("\n" + "=" * 70)
    print("QUICK GPU FUNCTIONALITY TESTS")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA not available - skipping tests")
        return
    
    try:
        print("üß™ Basic tensor on GPU:")
        a = torch.tensor([1.0, 2.0, 3.0]).cuda()
        b = torch.tensor([4.0, 5.0, 6.0]).cuda()
        c = a + b
        print(f"   Result: {c.tolist()} ‚úÖ")
        
        print("üß™ Small matrix multiplication:")
        x = torch.randn(50, 50).cuda()
        y = torch.randn(50, 50).cuda()
        z = torch.matmul(x, y)
        print(f"   Output shape: {z.shape} ‚úÖ")
        
        print("üß™ Memory allocation check:")
        mem = torch.cuda.memory_allocated(0) / 1024**2
        print(f"   Currently allocated: {mem:.1f} MB ‚úÖ")
        
        print("\nüéâ All quick tests passed!")
    except Exception as e:
        print(f"‚ùå Test failed: {e}")

# Main execution
if __name__ == "__main__":
    print("\nüîç NVIDIA GPU DIAGNOSTIC TOOL - FIXED VERSION")
    print("Optimized for GeForce 940M (CC 5.0)")
    print("=" * 70)
    
    try:
        get_system_info()
        get_nvidia_smi_info()
        check_cuda_toolkit()
        check_pytorch_cuda_compatibility()
        run_quick_tests()
        
        print("\n" + "=" * 70)
        print("DIAGNOSTIC COMPLETE üéâ")
        print("=" * 70)
        print("Your GeForce 940M with PyTorch CUDA is working correctly!")
        print("Note: Due to limited VRAM (~0.95 GB) and CC 5.0,")
        print("use only small models, low batch sizes, and consider quantization.")
        
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        
        
        

üîç NVIDIA GPU DIAGNOSTIC TOOL - FIXED VERSION
Optimized for GeForce 940M (CC 5.0)
======================================================================
======================================================================
SYSTEM & NVIDIA GPU INFORMATION
======================================================================

üìä SYSTEM DETAILS
----------------------------------------
Operating System: Linux 6.8.0-90-generic
System Version: #91~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Nov 20 15:20:45 UTC 2
Machine: x86_64
Processor: x86_64

üêç PYTHON & PYTORCH
----------------------------------------
Python Version: 3.11.11
PyTorch Version: 2.4.1+cu124

‚ö° CUDA STATUS
----------------------------------------
CUDA Available: True
CUDA Version (PyTorch): 12.4
cuDNN Version: 90100

üéÆ GPU HARDWARE
----------------------------------------
Number of GPUs Detected: 1

GPU 0:
------------------------------
  Name: NVIDIA GeForce 940M
  Compute Capability: 5.0
  Total Memory: 0.95 GB
  Multiprocessors: 3

  üìä MEMORY INFORMATION:
    Allocated: 0.0000 GB
    Reserved:  0.0000 GB

======================================================================
NVIDIA-SMI DETAILED REPORT
======================================================================
Sat Jan  3 19:05:16 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce 940M            Off |   00000000:04:00.0 Off |                  N/A |
| N/A   42C    P8            N/A  /  200W |       7MiB /   1024MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A             974      G   /usr/lib/xorg/Xorg                        2MiB |
+-----------------------------------------------------------------------------------------+

======================================================================
CUDA TOOLKIT CHECK
======================================================================
‚úÖ nvcc found and working:
   nvcc: NVIDIA (R) Cuda compiler driver
   Copyright (c) 2005-2025 NVIDIA Corporation
   Built on Wed_Jan_15_19:20:09_PST_2025

üåê Relevant Environment Variables:
   CUDA_HOME: Not set
   CUDA_PATH: Not set
   LD_LIBRARY_PATH: /usr/local/openmpi/lib:/usr/local/cuda-12.8/lib64:/usr/local/cuda-12.8/lib64:/usr/local/ompi-ucx/lib...

======================================================================
PYTORCH CUDA COMPATIBILITY
======================================================================
PyTorch Version: 2.4.1+cu124
CUDA Version (PyTorch build): 12.4
Current Device: 0
Device Name: NVIDIA GeForce 940M

üîß Key NVIDIA CUDA packages (from pip):
   torch                         : 2.4.1+cu124
   nvidia-cuda-runtime-cu12      : 12.4.99
   nvidia-cudnn-cu12             : 9.1.0.70
   nvidia-cublas-cu12            : 12.4.2.65
   nvidia-nccl-cu12              : 2.20.5

======================================================================
QUICK GPU FUNCTIONALITY TESTS
======================================================================
üß™ Basic tensor on GPU:
   Result: [5.0, 7.0, 9.0] ‚úÖ
üß™ Small matrix multiplication:
   Output shape: torch.Size([50, 50]) ‚úÖ
üß™ Memory allocation check:
   Currently allocated: 8.2 MB ‚úÖ

üéâ All quick tests passed!

======================================================================
DIAGNOSTIC COMPLETE üéâ
======================================================================
Your GeForce 940M with PyTorch CUDA is working correctly!
Note: Due to limited VRAM (~0.95 GB) and CC 5.0,
use only small models, low batch sizes, and consider quantization.
