# llcuda v2.2.0

[![Version](https://img.shields.io/badge/version-2.2.0-blue.svg)](https://github.com/llcuda/llcuda/releases/tag/v2.2.0)
[![Python](https://img.shields.io/badge/python-3.11+-brightgreen.svg)](https://python.org)
[![CUDA](https://img.shields.io/badge/CUDA-12.x-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![Kaggle](https://img.shields.io/badge/Kaggle-2Ã—T4-orange.svg)](https://kaggle.com)
[![Documentation](https://img.shields.io/badge/docs-llcuda.github.io-blue.svg)](https://llcuda.github.io)

**CUDA 12-first backend inference for Unsloth on Kaggle** â€” Optimized for small GGUF models (1B-5B) on dual Tesla T4 GPUs (15GB each, SM 7.5). Built-in C++ libraries (llama.cpp llama-server, NVIDIA NCCL). Split-GPU architecture: GPU 0 for LLM inference, GPU 1 for Graphistry dashboard visualization of internal neural network architecture.

ğŸŒ **[Official Documentation](https://llcuda.github.io/)** | ğŸ“– **[Tutorial Notebooks](https://llcuda.github.io/tutorials/index/)** | ğŸš€ **[Quick Start](https://llcuda.github.io/guides/quickstart/)** | ğŸ”§ **[API Reference](https://llcuda.github.io/api/overview/)**

---

## ğŸ“– Table of Contents

- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Multi-GPU Inference](#-multi-gpu-kaggle-2-t4)
- [Unsloth Integration](#-unsloth-integration)
- [Split-GPU Architecture](#-split-gpu-architecture)
- [Features](#-features)
- [Performance](#-performance)
- [Tutorial Notebooks](#-tutorial-notebooks)
- [Documentation](#-documentation)
- [Requirements](#-requirements)

---

## ğŸš€ Installation

### From GitHub (Recommended)
```bash
pip install git+https://github.com/llcuda/llcuda.git@v2.2.0
```

### Development Install
```bash
git clone https://github.com/llcuda/llcuda.git
cd llcuda
pip install -e .
```

### Verify Installation
```python
import llcuda
print(f"llcuda {llcuda.__version__}")  # 2.2.0
```

ğŸ“˜ **[Full Installation Guide â†’](docs/INSTALLATION.md)**

---

## âš¡ Quick Start (Kaggle Dual T4)

### Prerequisites
- **Platform:** Kaggle notebook
- **GPUs:** 2Ã— Tesla T4 (15GB VRAM each, SM 7.5)
- **Model Range:** 1B-5B parameters (GGUF Q4_K_M quantization)
- **Settings:** Internet enabled, GPU T4 Ã— 2 selected

### Basic Inference (Single GPU 0)
```python
import llcuda
from huggingface_hub import hf_hub_download

# Download small GGUF model (1B-5B range)
model_path = hf_hub_download(
    repo_id="unsloth/gemma-3-1b-it-GGUF",
    filename="gemma-3-1b-it-Q4_K_M.gguf",
    local_dir="/kaggle/working/models"
)

# Load on GPU 0 (15GB VRAM)
engine = llcuda.InferenceEngine()
engine.load_model(model_path, silent=True)
result = engine.infer("What is AI?", max_tokens=100)
print(result.text)
```

### Split-GPU Architecture (GPU 0: LLM, GPU 1: Graphistry)
```python
from llcuda.server import ServerManager

# Start llama-server on GPU 0 (100% allocation)
server = ServerManager()
server.start_server(
    model_path=model_path,
    gpu_layers=99,
    tensor_split="1.0,0.0",  # 100% GPU 0, 0% GPU 1
    flash_attn=1,
)

# GPU 1 now available for Graphistry visualization
# See Notebook 11 for complete visualization workflow
```

ğŸ“˜ **[Quick Start Guide â†’](QUICK_START.md)** | ğŸ““ **[Notebook 01 â†’](notebooks/01-quickstart-llcuda-v2.2.0.ipynb)**

---

## ğŸ¯ Split-GPU Architecture (Kaggle 2Ã— T4)

### Recommended: GPU 0 for LLM, GPU 1 for Graphistry
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         KAGGLE DUAL T4 SPLIT-GPU ARCHITECTURE (Optimized)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   GPU 0: Tesla T4 (15GB VRAM, SM 7.5)                           â”‚
â”‚   â”œâ”€ llama.cpp llama-server (C++)                               â”‚
â”‚   â”œâ”€ GGUF Model: 1B-5B params (Q4_K_M)                          â”‚
â”‚   â”œâ”€ VRAM Usage: ~2-6 GB                                        â”‚
â”‚   â”œâ”€ Built-in: FlashAttention, CUDA Graphs                      â”‚
â”‚   â””â”€ tensor-split: "1.0,0.0" (100% GPU 0)                       â”‚
â”‚                                                                 â”‚
â”‚   GPU 1: Tesla T4 (15GB VRAM, SM 7.5)                           â”‚
â”‚   â”œâ”€ Graphistry[ai] Python SDK                                  â”‚
â”‚   â”œâ”€ RAPIDS cuGraph (GPU-accelerated PageRank)                  â”‚
â”‚   â”œâ”€ Neural Network Visualization (929 nodes)                   â”‚
â”‚   â”œâ”€ VRAM Usage: ~0.5-2 GB                                      â”‚
â”‚   â””â”€ Free VRAM: ~13 GB for analytics                            â”‚
â”‚                                                                 â”‚
â”‚   Built-in C++ Libraries: llama.cpp + NVIDIA NCCL               â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Alternative: Tensor-Split for Large Models (Advanced)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       KAGGLE DUAL T4 TENSOR-SPLIT (For models >15GB VRAM)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   GPU 0: Tesla T4 (15GB)    GPU 1: Tesla T4 (15GB)              â”‚
â”‚   â”œâ”€ Model Layers 0-39      â”œâ”€ Model Layers 40-79               â”‚
â”‚   â””â”€ ~14GB VRAM             â””â”€ ~14GB VRAM                       â”‚
â”‚                                                                 â”‚
â”‚           â† tensor-split 0.5,0.5 (NCCL-based) â†’                 â”‚
â”‚                                                                 â”‚
â”‚   Total: 30GB VRAM for models up to 70B (IQ3_XS)                â”‚
â”‚   Note: Not recommended for 1B-5B models (use split-GPU)        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Start Multi-GPU Server
```bash
./bin/llama-server \
    -m model.gguf \
    -ngl 99 \
    --tensor-split 0.5,0.5 \
    --split-mode layer \
    -fa \
    --host 0.0.0.0 \
    --port 8080
```

### Python API
```python
from llcuda.server import ServerManager
from llcuda.api.multigpu import kaggle_t4_dual_config
from llcuda.api.client import LlamaCppClient

# Get optimized configuration for Kaggle dual T4
config = kaggle_t4_dual_config()

# Start server with multi-GPU configuration
server = ServerManager()
tensor_split_str = ",".join(str(x) for x in config.tensor_split)
server.start_server(
    model_path="model.gguf",
    gpu_layers=config.n_gpu_layers,
    tensor_split=tensor_split_str,
    split_mode="layer",
    flash_attn=1 if config.flash_attention else 0,
)

# Use OpenAI-compatible API
client = LlamaCppClient("http://localhost:8080")
response = client.chat.create(
    messages=[{"role": "user", "content": "Hello!"}],
    max_tokens=100
)
print(response.choices[0].message.content)
```

> **Note:** llama.cpp uses **native CUDA tensor-split**, NOT NCCL.
> NCCL is available for PyTorch distributed workloads.

ğŸ“˜ **[Kaggle Multi-GPU Guide â†’](docs/KAGGLE_GUIDE.md)**

---

## ğŸ”— Unsloth Integration

Complete workflow from fine-tuning to deployment:

```python
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 1: Fine-tune with Unsloth
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-1.5B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# Add LoRA and train...

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 2: Export to GGUF
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
model.save_pretrained_gguf(
    "my_model",
    tokenizer,
    quantization_method="q4_k_m"  # Recommended for T4
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STEP 3: Deploy with llcuda
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from llcuda.server import ServerManager, ServerConfig

server = ServerManager()
server.start_with_config(ServerConfig(
    model_path="my_model-Q4_K_M.gguf",
    n_gpu_layers=99,
    tensor_split="0.5,0.5",  # Dual T4
    flash_attn=True,
))
```

ğŸ“˜ **[Unsloth Integration Guide â†’](notebooks/05-unsloth-integration-llcuda-v2.2.0.ipynb)**

---

## ğŸ”§ Split-GPU Architecture

Run LLM inference on GPU 0 while using GPU 1 for RAPIDS/Graphistry analytics:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU 0 (T4)    â”‚      â”‚   GPU 1 (T4)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama-server    â”‚      â”‚ RAPIDS cuDF     â”‚
â”‚ LLM Inference   â”‚      â”‚ cuGraph         â”‚
â”‚ ~5-12 GB        â”‚      â”‚ Graphistry      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
from llcuda import SplitGPUConfig

config = SplitGPUConfig(llm_gpu=0, graph_gpu=1)
# GPU 0: llama-server (LLM inference)
# GPU 1: RAPIDS cuGraph (graph visualization)
```

ğŸ“˜ **[Split-GPU Tutorial â†’](notebooks/06-split-gpu-graphistry-llcuda-v2.2.0.ipynb)**

---

## ğŸ¨ GGUF Architecture Visualization â­ NEW

**Visualize your GGUF models as interactive graphs** with Notebook 11:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GGUF NEURAL NETWORK ARCHITECTURE VISUALIZATION          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚   ğŸ“Š 929 Nodes: Complete Llama-3.2-3B structure                 â”‚
â”‚   ğŸ”— 981 Edges: All connections and data flows                  â”‚
â”‚   ğŸ¯ 896 Attention Heads: Multi-head attention visualized       â”‚
â”‚   ğŸ“¦ 112 Quantization Blocks: Q4_K_M structure revealed         â”‚
â”‚   ğŸŒ Interactive Graphistry Dashboards: Cloud + offline HTML    â”‚
â”‚                                                                 â”‚
â”‚   âœ¨ First comprehensive GGUF visualization tool                â”‚
â”‚   âœ¨ GPU-accelerated graph analytics (PageRank, centrality)     â”‚
â”‚   âœ¨ Dual-GPU architecture (inference + visualization)          â”‚
â”‚   âœ¨ Multi-scale: From overview to individual attention heads   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What You Can Visualize:**
- Layer-by-layer transformer structure (35 nodes per layer)
- Attention head importance and connectivity
- Quantization block memory layout
- Information flow through the network
- Critical components via PageRank analysis

ğŸ“˜ **[GGUF Visualization Guide â†’](docs/GGUF_NEURAL_NETWORK_VISUALIZATION.md)** | ğŸ““ **[Notebook 11 â†’](notebooks/11-gguf-neural-network-graphistry-visualization.ipynb)**

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| **Kaggle-Optimized** | Built specifically for Kaggle dual Tesla T4 (15GB Ã— 2, SM 7.5) |
| **Small Models** | Optimized for 1B-5B params GGUF (Q4_K_M) on single T4 |
| **Split-GPU** | GPU 0: LLM inference, GPU 1: Graphistry visualization |
| **Built-in C++ Libraries** | llama.cpp llama-server + NVIDIA NCCL (no compilation needed) |
| **FlashAttention** | Built-in for all quantizations (2Ã— speedup) |
| **Unsloth Backend** | CUDA 12-first inference for Unsloth-trained models |
| **Graphistry Dashboards** | Interactive neural network visualization (929 nodes) |
| **OpenAI API** | Full llama.cpp server compatibility |
| **GGUF Tools** | Parse, quantize, analyze GGUF files |
| **Auto-download** | 62KB package, 961MB binaries from GitHub Releases |

---

## ğŸ“Š Performance (Kaggle Single Tesla T4)

### Optimized for 1B-5B Models

| Model | Size | Quantization | VRAM | Tokens/sec | Recommended |
|-------|------|--------------|------|------------|-------------|
| Gemma-3 1B | 1.0B | Q4_K_M | ~1.2 GB | ~50 tok/s | â­ Best for fast inference |
| Llama-3.2 1B | 1.2B | Q4_K_M | ~1.3 GB | ~48 tok/s | â­ Excellent quality |
| Gemma-2 2B | 2.0B | Q4_K_M | ~1.8 GB | ~45 tok/s | â­ Balanced |
| Qwen2.5 3B | 3.0B | Q4_K_M | ~2.3 GB | ~40 tok/s | â­ High quality |
| Llama-3.2 3B | 3.2B | Q4_K_M | ~2.5 GB | ~38 tok/s | â­ Very capable |
| Gemma-3 4B | 4.0B | Q4_K_M | ~3.0 GB | ~35 tok/s | â­ Best quality |

**All tested on single Tesla T4 (15GB VRAM, SM 7.5) with FlashAttention enabled**

### VRAM Availability (Split-GPU Architecture)

```
Configuration: GPU 0 for LLM, GPU 1 for Graphistry

GPU 0 Usage:
â”œâ”€ 1B model: ~1.2 GB â†’ 13.8 GB free
â”œâ”€ 2B model: ~1.8 GB â†’ 13.2 GB free
â”œâ”€ 3B model: ~2.5 GB â†’ 12.5 GB free
â”œâ”€ 4B model: ~3.0 GB â†’ 12.0 GB free
â””â”€ 5B model: ~3.8 GB â†’ 11.2 GB free

GPU 1 Available:
â”œâ”€ Graphistry: ~0.5-2 GB
â”œâ”€ RAPIDS cuGraph: ~0.3 GB
â””â”€ Free for analytics: ~13 GB
```

---

## ğŸ““ Tutorial Notebooks

Comprehensive Kaggle-ready tutorials in [`notebooks/`](notebooks/):

| # | Notebook | Description |
|---|----------|-------------|
| 01 | [Quick Start](notebooks/01-quickstart-llcuda-v2.2.0.ipynb) | 5-minute introduction |
| 02 | [Server Setup](notebooks/02-llama-server-setup-llcuda-v2.2.0.ipynb) | Advanced server configuration |
| 03 | [Multi-GPU](notebooks/03-multi-gpu-inference-llcuda-v2.2.0.ipynb) | Dual T4 tensor-split |
| 04 | [GGUF Quantization](notebooks/04-gguf-quantization-llcuda-v2.2.0.ipynb) | Complete quantization guide |
| 05 | [Unsloth Integration](notebooks/05-unsloth-integration-llcuda-v2.2.0.ipynb) | Train â†’ Export â†’ Deploy |
| 06 | [Split-GPU + Graphistry](notebooks/06-split-gpu-graphistry-llcuda-v2.2.0.ipynb) | LLM + RAPIDS analytics |
| 07 | [OpenAI API Client](notebooks/07-openai-api-client-llcuda-v2.2.0.ipynb) | Full API reference |
| 08 | [NCCL + PyTorch](notebooks/08-nccl-pytorch-llcuda-v2.2.0.ipynb) | Distributed training |
| 09 | [Large Models](notebooks/09-large-models-kaggle-llcuda-v2.2.0.ipynb) | 70B on dual T4 |
| 10 | [Complete Workflow](notebooks/10-complete-workflow-llcuda-v2.2.0.ipynb) | End-to-end tutorial |
| 11 | [**GGUF Visualization**](notebooks/11-gguf-neural-network-graphistry-visualization.ipynb) | â­ Interactive architecture graphs |

ğŸ“˜ **[Notebooks Index â†’](notebooks/README.md)**

---

## ğŸ“š Documentation

### Core Documentation
| Document | Description |
|----------|-------------|
| [QUICK_START.md](QUICK_START.md) | Get started in 5 minutes |
| [INSTALL.md](INSTALL.md) | Detailed installation guide |
| [CHANGELOG.md](CHANGELOG.md) | Version history |

### In-Depth Guides
| Document | Description |
|----------|-------------|
| [docs/INSTALLATION.md](docs/INSTALLATION.md) | Complete installation reference |
| [docs/CONFIGURATION.md](docs/CONFIGURATION.md) | Server & client configuration |
| [docs/API_REFERENCE.md](docs/API_REFERENCE.md) | Python API documentation |
| [docs/KAGGLE_GUIDE.md](docs/KAGGLE_GUIDE.md) | Kaggle-specific guide |
| [docs/GGUF_GUIDE.md](docs/GGUF_GUIDE.md) | GGUF format & quantization |
| [docs/TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md) | Common issues & solutions |

### Contributing
| Document | Description |
|----------|-------------|
| [CONTRIBUTING.md](CONTRIBUTING.md) | How to contribute |
| [docs/BUILD_GUIDE.md](docs/BUILD_GUIDE.md) | Building from source |

---

## ğŸ“‹ Requirements

### Platform (Required)
- **Platform:** Kaggle notebooks (https://kaggle.com/code)
- **GPUs:** 2Ã— Tesla T4 (15GB VRAM each, Compute Capability SM 7.5)
- **Python:** 3.11+ (pre-installed on Kaggle)
- **CUDA:** 12.x (pre-installed on Kaggle)

### Kaggle Settings (Required)
- **Accelerator:** GPU T4 Ã— 2 (must select dual T4)
- **Internet:** Enabled (for package installation)
- **Persistence:** Enabled (for downloaded models)

### Model Requirements
- **Size:** 1B-5B parameters recommended
- **Format:** GGUF (from HuggingFace)
- **Quantization:** Q4_K_M (best quality/speed balance)
- **Source:** Unsloth-compatible models preferred

**Note:** llcuda v2.2.0 is designed and tested exclusively for Kaggle dual T4 environment. Other platforms (Colab, local) are not officially supported.

---

## ğŸ“¦ Binary Package

| File | Size | Platform |
|------|------|----------|
| `llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz` | 961 MB | Kaggle 2Ã— T4 |

**Build Info:**
- CUDA 12.5, SM 7.5 (Turing)
- llama.cpp b7760 (commit 388ce82)
- Build Date: 2026-01-16
- Contents: 13 binaries (llama-server, llama-cli, llama-quantize, etc.)

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

```bash
# Development setup
git clone https://github.com/llcuda/llcuda.git
cd llcuda
pip install -e ".[dev]"
pytest tests/
```

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE)

---

## ğŸ““ Tutorial Notebooks (10 notebooks)

Complete tutorial series for llcuda v2.2.0 on Kaggle dual T4 GPUs. Click the badges to open directly in Kaggle or view on GitHub.

| # | Notebook | Open in Kaggle | Description |
|---|----------|----------------|-------------|
| 01 | [Quick Start](notebooks/01-quickstart-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/01-quickstart-llcuda-v2.2.0.ipynb) | 5-minute introduction to llcuda |
| 02 | [Llama Server Setup](notebooks/02-llama-server-setup-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/02-llama-server-setup-llcuda-v2.2.0.ipynb) | Server configuration & lifecycle |
| 03 | [Multi-GPU Inference](notebooks/03-multi-gpu-inference-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/03-multi-gpu-inference-llcuda-v2.2.0.ipynb) | Dual T4 tensor-split configuration |
| 04 | [GGUF Quantization](notebooks/04-gguf-quantization-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/04-gguf-quantization-llcuda-v2.2.0.ipynb) | K-quants, I-quants, GGUF parsing |
| 05 | [Unsloth Integration](notebooks/05-unsloth-integration-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/05-unsloth-integration-llcuda-v2.2.0.ipynb) | Fine-tune â†’ GGUF â†’ Deploy |
| 06 | [Split-GPU + Graphistry](notebooks/06-split-gpu-graphistry-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/06-split-gpu-graphistry-llcuda-v2.2.0.ipynb) | LLM on GPU 0 + RAPIDS on GPU 1 |
| 07 | [OpenAI API Client](notebooks/07-openai-api-client-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/07-openai-api-client-llcuda-v2.2.0.ipynb) | Drop-in OpenAI SDK replacement |
| 08 | [NCCL + PyTorch](notebooks/08-nccl-pytorch-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/08-nccl-pytorch-llcuda-v2.2.0.ipynb) | NCCL for distributed PyTorch |
| 09 | [Large Models (70B)](notebooks/09-large-models-kaggle-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/09-large-models-kaggle-llcuda-v2.2.0.ipynb) | 70B models on Kaggle with IQ3_XS |
| 10 | [Complete Workflow](notebooks/10-complete-workflow-llcuda-v2.2.0.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/10-complete-workflow-llcuda-v2.2.0.ipynb) | End-to-end production workflow |
| 11 | [**GGUF Visualization** â­](notebooks/11-gguf-neural-network-graphistry-visualization.ipynb) | [![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/llcuda/llcuda/blob/main/notebooks/11-gguf-neural-network-graphistry-visualization.ipynb) | **MOST IMPORTANT**: Interactive architecture graphs |

### ğŸ¯ Learning Paths

| Path | Notebooks | Time | Focus |
|------|-----------|------|-------|
| **Quick Start** | 01 â†’ 02 â†’ 03 | 1 hour | Get running fast |
| **Full Course** | 01 â†’ 11 (all) | 4.5 hours | Complete mastery + visualization |
| **Unsloth Focus** | 01 â†’ 04 â†’ 05 â†’ 10 | 2 hours | Fine-tuning workflow |
| **Large Models** | 01 â†’ 03 â†’ 09 | 1.5 hours | 70B on Kaggle |
| **Visualization** â­ | 01 â†’ 03 â†’ 04 â†’ 06 â†’ 11 | 2.5 hours | Architecture analysis |

ğŸ“˜ **[Full Notebook Guide â†’](notebooks/README.md)**
