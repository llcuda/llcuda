{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Step 1: Verify Kaggle GPU Environment\n",
    "!nvidia-smi\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 2: Install Build Dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq cmake ninja-build ccache\n",
    "!pip install -q huggingface_hub tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì• Step 3: Clone llama.cpp (Latest)\n",
    "import os\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp\n",
    "!git log -1 --oneline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfa327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî® Step 4: Build llama.cpp with CUDA 12 for Dual T4 (SM 7.5)\n",
    "import os\n",
    "os.chdir('/kaggle/working/llama.cpp')\n",
    "\n",
    "# Clean any previous build\n",
    "!rm -rf build\n",
    "\n",
    "# Configure with CMake\n",
    "# Key flags:\n",
    "# - GGML_CUDA=ON: Enable CUDA backend\n",
    "# - CMAKE_CUDA_ARCHITECTURES=75: Tesla T4 (Turing)\n",
    "# - GGML_CUDA_FA_ALL_QUANTS=ON: FlashAttention for all quantization types\n",
    "# - BUILD_SHARED_LIBS=OFF: Static linking for portability\n",
    "# - LLAMA_SERVER_SSL=OFF: No SSL (simpler for local use)\n",
    "\n",
    "!cmake -B build -G Ninja \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DBUILD_SHARED_LIBS=OFF \\\n",
    "    -DLLAMA_BUILD_EXAMPLES=ON \\\n",
    "    -DLLAMA_BUILD_TESTS=OFF \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DCMAKE_BUILD_TYPE=Release\n",
    "\n",
    "print(\"\\n‚úÖ CMake configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17113a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Step 5: Compile (this takes ~5-10 minutes)\n",
    "import os\n",
    "os.chdir('/kaggle/working/llama.cpp')\n",
    "\n",
    "# Build with all available CPU cores\n",
    "!cmake --build build --config Release -j$(nproc)\n",
    "\n",
    "print(\"\\n‚úÖ Build complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìã Step 6: Verify Built Binaries\n",
    "import os\n",
    "os.chdir('/kaggle/working/llama.cpp/build/bin')\n",
    "\n",
    "print(\"Built binaries:\")\n",
    "!ls -lh llama-*\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Testing llama-server version:\")\n",
    "!./llama-server --version 2>/dev/null || echo \"Version flag not supported\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Binary sizes:\")\n",
    "!du -sh llama-server llama-cli llama-quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70787bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 7: Package Binaries for llcuda v2.1.2\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Create package directory structure\n",
    "PACKAGE_NAME = f\"llcuda-v2.1.2-cuda12-kaggle-t4x2-{datetime.now().strftime('%Y%m%d')}\"\n",
    "PACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n",
    "\n",
    "os.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\n",
    "\n",
    "# Copy binaries\n",
    "BUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\n",
    "binaries = [\n",
    "    \"llama-server\",\n",
    "    \"llama-cli\", \n",
    "    \"llama-quantize\",\n",
    "    \"llama-embedding\",\n",
    "    \"llama-gguf\",\n",
    "    \"llama-gguf-hash\",\n",
    "    \"llama-imatrix\",\n",
    "    \"llama-export-lora\",\n",
    "    \"llama-tokenize\",\n",
    "    \"llama-infill\",\n",
    "    \"llama-perplexity\",\n",
    "]\n",
    "\n",
    "for binary in binaries:\n",
    "    src = f\"{BUILD_BIN}/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n",
    "        print(f\"‚úÖ Copied {binary}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Not found: {binary}\")\n",
    "\n",
    "# Copy shared libraries if any\n",
    "BUILD_LIB = \"/kaggle/working/llama.cpp/build\"\n",
    "for lib in [\"libllama.so\", \"libggml.so\"]:\n",
    "    src = f\"{BUILD_LIB}/{lib}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/lib/{lib}\")\n",
    "        print(f\"‚úÖ Copied {lib}\")\n",
    "\n",
    "print(f\"\\nüì¶ Package created: {PACKAGE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f88d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Step 8: Create Package Metadata\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "# Get llama.cpp commit hash\n",
    "os.chdir('/kaggle/working/llama.cpp')\n",
    "commit_hash = subprocess.getoutput('git rev-parse HEAD')\n",
    "commit_date = subprocess.getoutput('git log -1 --format=%ci')\n",
    "\n",
    "metadata = {\n",
    "    \"package\": \"llcuda\",\n",
    "    \"version\": \"2.1.2\",\n",
    "    \"build_date\": datetime.now().isoformat(),\n",
    "    \"target_platform\": \"kaggle\",\n",
    "    \"gpu_config\": {\n",
    "        \"count\": 2,\n",
    "        \"model\": \"Tesla T4\",\n",
    "        \"vram_per_gpu_gb\": 15,\n",
    "        \"total_vram_gb\": 30,\n",
    "        \"compute_capability\": \"7.5\",\n",
    "        \"architecture\": \"Turing\"\n",
    "    },\n",
    "    \"cuda\": {\n",
    "        \"version\": subprocess.getoutput('nvcc --version | grep release | sed \"s/.*release //\" | cut -d, -f1'),\n",
    "        \"architectures\": [\"sm_75\"],\n",
    "        \"flash_attention\": True\n",
    "    },\n",
    "    \"llama_cpp\": {\n",
    "        \"commit\": commit_hash,\n",
    "        \"commit_date\": commit_date,\n",
    "        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
    "    },\n",
    "    \"binaries\": binaries,\n",
    "    \"features\": [\n",
    "        \"multi-gpu-tensor-split\",\n",
    "        \"flash-attention\",\n",
    "        \"openai-compatible-api\",\n",
    "        \"all-quantization-formats\",\n",
    "        \"lora-adapters\",\n",
    "        \"grammar-constraints\",\n",
    "        \"embeddings\",\n",
    "        \"reranking\",\n",
    "        \"streaming\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "os.chdir('/kaggle/working')\n",
    "with open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Metadata written\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìö Step 9: Create Quick Start Guide\n",
    "readme_content = '''# llcuda v2.1.2 - Kaggle 2√ó Tesla T4 Build\n",
    "\n",
    "Pre-built CUDA 12 binaries for Kaggle's dual Tesla T4 GPU configuration.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "```bash\n",
    "# Extract and set permissions\n",
    "tar -xzf llcuda-v2.1.2-cuda12-kaggle-t4x2-*.tar.gz\n",
    "chmod +x bin/*\n",
    "\n",
    "# Run with dual GPU tensor splitting\n",
    "./bin/llama-server \\\n",
    "    -m /path/to/model.gguf \\\n",
    "    -ngl 99 \\\n",
    "    --split-mode layer \\\n",
    "    --tensor-split 0.5,0.5 \\\n",
    "    -fa \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080\n",
    "```\n",
    "\n",
    "## Multi-GPU Configuration\n",
    "\n",
    "| Flag | Description |\n",
    "|------|-------------|\n",
    "| `-ngl 99` | Offload all layers to GPU |\n",
    "| `--split-mode layer` | Split model layers across GPUs |\n",
    "| `--tensor-split 0.5,0.5` | Equal VRAM split between GPUs |\n",
    "| `-fa` | Enable FlashAttention |\n",
    "\n",
    "## Python Usage\n",
    "\n",
    "```python\n",
    "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
    "\n",
    "# Get optimal config for Kaggle dual T4\n",
    "config = kaggle_t4_dual_config()\n",
    "print(config.to_cli_args())\n",
    "\n",
    "# Connect to server\n",
    "client = LlamaCppClient(\"http://localhost:8080\")\n",
    "\n",
    "# Chat completion (OpenAI-compatible)\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "## Recommended Models for 30GB VRAM\n",
    "\n",
    "| Model | Quantization | Size | Fits? |\n",
    "|-------|--------------|------|-------|\n",
    "| Llama 3.1 70B | Q4_K_M | ~40GB | ‚ùå |\n",
    "| Llama 3.1 70B | IQ3_XS | ~25GB | ‚úÖ |\n",
    "| Llama 3.1 8B | Q8_0 | ~9GB | ‚úÖ |\n",
    "| Qwen2.5 32B | Q4_K_M | ~19GB | ‚úÖ |\n",
    "| Qwen2.5 14B | Q8_0 | ~15GB | ‚úÖ |\n",
    "| Gemma 2 27B | Q4_K_M | ~16GB | ‚úÖ |\n",
    "\n",
    "## Build Info\n",
    "\n",
    "- **CUDA Version:** 12.4\n",
    "- **Architecture:** SM 7.5 (Turing)\n",
    "- **FlashAttention:** Enabled\n",
    "- **Target:** Kaggle 2√ó Tesla T4\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README.md created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 10: Create Distribution Archive\n",
    "import os\n",
    "os.chdir('/kaggle/working')\n",
    "\n",
    "!tar -czvf {PACKAGE_NAME}.tar.gz {PACKAGE_NAME}\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "!ls -lh {PACKAGE_NAME}.tar.gz\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚úÖ Package ready: {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"\\nüì• Download from Kaggle Output tab or use:\")\n",
    "print(f\"   !cp /kaggle/working/{PACKAGE_NAME}.tar.gz /kaggle/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155edf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Step 11: Test Multi-GPU Server (Optional)\n",
    "# Download a small model and test\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Download a small test model\n",
    "print(\"Downloading test model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"‚úÖ Model downloaded: {model_path}\")\n",
    "\n",
    "# Start server with multi-GPU config\n",
    "print(\"\\nStarting llama-server with dual T4 config...\")\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"--split-mode\", \"layer\",\n",
    "    \"--tensor-split\", \"0.5,0.5\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait for server to start\n",
    "print(\"Waiting for server to start...\")\n",
    "for i in range(30):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"‚úÖ Server ready! ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Server startup timeout\")\n",
    "\n",
    "# Test inference\n",
    "print(\"\\nTesting inference...\")\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Say hello!\"}],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(f\"‚úÖ Response: {result['choices'][0]['message']['content']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "print(\"\\nüõë Server stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b9405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Final Summary\n",
    "print(\"=\"*60)\n",
    "print(\"  llcuda v2.1.2 Build Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüì¶ Package: {PACKAGE_NAME}.tar.gz\")\n",
    "!ls -lh /kaggle/working/{PACKAGE_NAME}.tar.gz\n",
    "\n",
    "print(\"\\nüìÅ Contents:\")\n",
    "!ls -la /kaggle/working/{PACKAGE_NAME}/bin/\n",
    "\n",
    "print(\"\\nüéØ Target: Kaggle 2√ó Tesla T4 (30 GB VRAM)\")\n",
    "print(\"   CUDA: 12.4 | SM: 7.5 | FlashAttention: ‚úÖ\")\n",
    "print(\"\\nüìñ See README.md for usage instructions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
