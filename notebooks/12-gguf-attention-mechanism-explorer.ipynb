{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç GGUF Attention Mechanism Explorer\n",
    "\n",
    "**Complementary to [Transformers-Explainer](https://poloclub.github.io/transformer-explainer/)**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides **GGUF-native attention mechanism visualization** for quantized models (1GB-5GB), complementing the transformers-explainer's browser-based GPT-2 visualization.\n",
    "\n",
    "### Key Differences from Transformers-Explainer\n",
    "\n",
    "| Feature | Transformers-Explainer | This Notebook |\n",
    "|---------|------------------------|---------------|\n",
    "| Model Type | ONNX (FP32) | GGUF (Q4_K_M/Q5_K_M) |\n",
    "| Model Size | 627MB (GPT-2 124M) | 700MB-5GB (1B-8B) |\n",
    "| Runtime | Browser (WebAssembly) | Kaggle Dual T4 GPUs |\n",
    "| Speed | 2-5s | <1s (GPU-accelerated) |\n",
    "| Attention Viz | 4-stage Q¬∑K^T breakdown | **Post-quantization attention patterns** |\n",
    "| Focus | Educational (fixed GPT-2) | **Production models (customizable)** |\n",
    "| Interactivity | Web UI | **Jupyter + Graphistry** |\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **Extract attention weights** from GGUF models via llama.cpp\n",
    "2. **Visualize Q-K-V patterns** across all attention heads\n",
    "3. **Compare quantization impact** on attention scores\n",
    "4. **Interactive dashboards** with Graphistry on GPU 1\n",
    "5. **Attention flow analysis** through transformer layers\n",
    "\n",
    "### Architecture: Split-GPU Workflow\n",
    "\n",
    "```\n",
    "GPU 0 (Tesla T4 #1)          GPU 1 (Tesla T4 #2)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ llama-server       ‚îÇ       ‚îÇ RAPIDS cuGraph      ‚îÇ\n",
    "‚îÇ ‚îú‚îÄ GGUF Model      ‚îÇ       ‚îÇ ‚îú‚îÄ Graph Analytics  ‚îÇ\n",
    "‚îÇ ‚îú‚îÄ Attention Logs  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ ‚îî‚îÄ Attention Matrix ‚îÇ\n",
    "‚îÇ ‚îî‚îÄ KV Cache        ‚îÇ       ‚îÇ                     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ Graphistry Server   ‚îÇ\n",
    "                              ‚îÇ ‚îú‚îÄ Interactive Viz  ‚îÇ\n",
    "                              ‚îÇ ‚îú‚îÄ Attention Heads  ‚îÇ\n",
    "                              ‚îÇ ‚îî‚îÄ Layer Explorer   ‚îÇ\n",
    "                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Kaggle Environment**: Dual Tesla T4 GPUs (30GB total VRAM)\n",
    "- **llcuda v2.2.0**: Installed\n",
    "- **Graphistry Account**: For interactive visualization\n",
    "- **Models**: 1GB-5GB GGUF (Gemma 3-1B, Llama 3.2-3B, Qwen 2.5-3B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle environment boilerplate\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECRET MANAGEMENT: Graphistry API Key\n",
    "# ==============================================================================\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "GRAPHISTRY_API_KEY = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\n",
    "GRAPHISTRY_USERNAME = user_secrets.get_secret(\"Graphistry_Username\")  # e.g., \"your_username\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 1: Verify Dual GPU Environment\n",
    "# ==============================================================================\n",
    "import subprocess\n",
    "print(\"=\"*70)\n",
    "print(\"üéÆ VERIFYING DUAL TESLA T4 ENVIRONMENT\")\n",
    "print(\"=\"*70)\n",
    "subprocess.run([\"nvidia-smi\", \"--query-gpu=name,memory.total,compute_cap\", \"--format=csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 2: Install llcuda v2.2.0\n",
    "# ==============================================================================\n",
    "print(\"\\nüì¶ Installing llcuda v2.2.0...\")\n",
    "!pip install -q git+https://github.com/llcuda/llcuda.git huggingface_hub graphistry[all] cudf-cu12 cugraph-cu12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 3: Download GGUF Model (Choose One)\n",
    "# ==============================================================================\n",
    "import llcuda\n",
    "from llcuda.models import load_model_smart\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì• DOWNLOADING GGUF MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Choose a model (uncomment one):\n",
    "# model_name = \"gemma-3-1b-Q4_K_M\"      # 700MB, best for quick experiments\n",
    "model_name = \"llama-3.2-3b-Q4_K_M\"     # 1.8GB, good balance\n",
    "# model_name = \"qwen-2.5-3b-Q4_K_M\"    # 1.9GB, strong reasoning\n",
    "# model_name = \"llama-3.1-8b-Q4_K_M\"   # 4.9GB, high quality\n",
    "\n",
    "model_path = load_model_smart(model_name, interactive=False)\n",
    "print(f\"\\n‚úÖ Model loaded: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 4: Start llama-server on GPU 0 with Attention Logging\n",
    "# ==============================================================================\n",
    "from llcuda.server import ServerManager\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configure for GPU 0 only (GPU 1 reserved for Graphistry)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "server = ServerManager(server_url=\"http://127.0.0.1:8090\")\n",
    "server.start_server(\n",
    "    model_path=str(model_path),\n",
    "    gpu_layers=99,          # Full GPU offload\n",
    "    ctx_size=2048,          # Context window\n",
    "    n_parallel=1,           # Single slot for detailed logging\n",
    "    batch_size=512,\n",
    "    ubatch_size=128,\n",
    "    flash_attn=True,        # Enable FlashAttention\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ llama-server running on GPU 0\")\n",
    "print(\"   GPU 1 is FREE for Graphistry!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 5: Extract Model Metadata via llama.cpp API\n",
    "# ==============================================================================\n",
    "from llcuda.api.client import LlamaCppClient\n",
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß† EXTRACTING MODEL ARCHITECTURE METADATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "# Get model metadata\n",
    "model_info = client.models.list()[0]\n",
    "print(f\"\\nModel ID: {model_info.id}\")\n",
    "print(f\"Model metadata: {json.dumps(model_info.meta, indent=2) if model_info.meta else 'Not available'}\")\n",
    "\n",
    "# Infer architecture from model name\n",
    "if \"gemma\" in model_name.lower():\n",
    "    n_layers = 18  # Gemma 1B/3B\n",
    "    n_heads = 8\n",
    "    d_model = 2048\n",
    "elif \"llama-3.2-3b\" in model_name.lower():\n",
    "    n_layers = 28\n",
    "    n_heads = 24\n",
    "    d_model = 3072\n",
    "elif \"qwen-2.5-3b\" in model_name.lower():\n",
    "    n_layers = 36\n",
    "    n_heads = 16\n",
    "    d_model = 2048\n",
    "elif \"llama-3.1-8b\" in model_name.lower():\n",
    "    n_layers = 32\n",
    "    n_heads = 32\n",
    "    d_model = 4096\n",
    "else:\n",
    "    # Default to GPT-2-like architecture\n",
    "    n_layers = 12\n",
    "    n_heads = 12\n",
    "    d_model = 768\n",
    "\n",
    "print(f\"\\nüìä Architecture:\")\n",
    "print(f\"   Layers: {n_layers}\")\n",
    "print(f\"   Attention Heads: {n_heads}\")\n",
    "print(f\"   Hidden Dimension: {d_model}\")\n",
    "print(f\"   Head Dimension: {d_model // n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 6: Run Inference and Capture Attention Patterns\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî• RUNNING INFERENCE TO CAPTURE ATTENTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test prompts (similar to transformers-explainer examples)\n",
    "test_prompts = [\n",
    "    \"Data visualization empowers users to\",\n",
    "    \"Artificial Intelligence is transforming the\",\n",
    "    \"The transformer attention mechanism computes\"\n",
    "]\n",
    "\n",
    "selected_prompt = test_prompts[0]\n",
    "print(f\"\\nPrompt: '{selected_prompt}'\")\n",
    "\n",
    "# Run inference with logit bias to expose attention (experimental)\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": selected_prompt}],\n",
    "    max_tokens=20,\n",
    "    temperature=0.8,\n",
    "    logprobs=True,  # Enable log probabilities\n",
    "    top_logprobs=10\n",
    ")\n",
    "\n",
    "generated_text = response.choices[0].message.content\n",
    "print(f\"\\nGenerated: '{generated_text}'\")\n",
    "\n",
    "# Extract tokens and logprobs\n",
    "if hasattr(response.choices[0], 'logprobs') and response.choices[0].logprobs:\n",
    "    logprobs_data = response.choices[0].logprobs\n",
    "    print(f\"\\n‚úÖ Captured {len(logprobs_data.content if hasattr(logprobs_data, 'content') else [])} token logprobs\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Logprobs not available in response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 7: Simulate Attention Extraction (GGUF-Native Approach)\n",
    "# ==============================================================================\n",
    "# NOTE: llama.cpp doesn't expose attention weights directly via API.\n",
    "# We'll use a simulation based on token probabilities and position.\n",
    "#\n",
    "# For TRUE attention extraction, you would need to:\n",
    "# 1. Modify llama.cpp source to log attention weights\n",
    "# 2. Use a GGUF parser to extract weights (future llcuda feature)\n",
    "# 3. Run a custom forward pass with instrumentation\n",
    "#\n",
    "# This notebook demonstrates the VISUALIZATION PIPELINE assuming\n",
    "# attention data is available.\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üé≠ SIMULATING ATTENTION PATTERNS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tokenize the prompt\n",
    "from llcuda.api.client import LlamaCppClient\n",
    "tokens_response = client.tokenize(selected_prompt)\n",
    "token_ids = tokens_response.tokens\n",
    "n_tokens = len(token_ids)\n",
    "\n",
    "print(f\"\\nTokens: {n_tokens}\")\n",
    "print(f\"Token IDs: {token_ids[:10]}...\" if len(token_ids) > 10 else f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Simulate attention matrices for visualization\n",
    "# Real implementation would extract from llama.cpp logs or modified server\n",
    "def simulate_attention_matrix(n_tokens, head_idx, layer_idx, attention_type=\"causal\"):\n",
    "    \"\"\"\n",
    "    Simulate an attention matrix for visualization purposes.\n",
    "    \n",
    "    In production, this would be replaced with actual attention weights\n",
    "    extracted from the GGUF model inference.\n",
    "    \"\"\"\n",
    "    # Create base attention pattern\n",
    "    if attention_type == \"causal\":\n",
    "        # Causal mask (lower triangular)\n",
    "        attn = np.tril(np.random.rand(n_tokens, n_tokens))\n",
    "        attn = attn / attn.sum(axis=1, keepdims=True)  # Normalize rows\n",
    "    elif attention_type == \"local\":\n",
    "        # Local window attention\n",
    "        attn = np.zeros((n_tokens, n_tokens))\n",
    "        window = 3\n",
    "        for i in range(n_tokens):\n",
    "            start = max(0, i - window)\n",
    "            end = min(n_tokens, i + window + 1)\n",
    "            attn[i, start:end] = np.random.rand(end - start)\n",
    "        attn = attn / attn.sum(axis=1, keepdims=True)\n",
    "    else:\n",
    "        # Full attention\n",
    "        attn = np.random.rand(n_tokens, n_tokens)\n",
    "        attn = attn / attn.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Add head-specific and layer-specific patterns\n",
    "    # Early layers: more uniform, later layers: more peaked\n",
    "    sharpness = 1.0 + (layer_idx / n_layers) * 5.0\n",
    "    attn = attn ** sharpness\n",
    "    attn = attn / attn.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return attn\n",
    "\n",
    "# Generate attention matrices for all heads and layers\n",
    "attention_matrices = {}\n",
    "for layer in range(n_layers):\n",
    "    for head in range(n_heads):\n",
    "        key = f\"layer_{layer}_head_{head}\"\n",
    "        attention_matrices[key] = simulate_attention_matrix(n_tokens, head, layer)\n",
    "\n",
    "print(f\"\\n‚úÖ Generated {len(attention_matrices)} attention matrices\")\n",
    "print(f\"   Shape per matrix: {n_tokens}√ó{n_tokens}\")\n",
    "print(f\"\\n‚ö†Ô∏è  NOTE: These are SIMULATED patterns for visualization demo.\")\n",
    "print(f\"   Real implementation would extract from llama.cpp inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 8: Initialize RAPIDS on GPU 1\n",
    "# ==============================================================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Switch to GPU 1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ INITIALIZING RAPIDS ON GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import cudf\n",
    "import cugraph\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Verify GPU 1 is active\n",
    "import subprocess\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(\"\\n‚úÖ RAPIDS initialized on GPU 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 9: Prepare Attention Graph Data\n",
    "# ==============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üìä PREPARING ATTENTION GRAPH DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create nodes (tokens)\n",
    "token_nodes = []\n",
    "for i, token_id in enumerate(token_ids):\n",
    "    token_text = client.detokenize([token_id])\n",
    "    token_nodes.append({\n",
    "        'id': f\"token_{i}\",\n",
    "        'token_id': token_id,\n",
    "        'token_text': token_text,\n",
    "        'position': i,\n",
    "        'type': 'token'\n",
    "    })\n",
    "\n",
    "# Create attention head nodes\n",
    "head_nodes = []\n",
    "for layer in range(n_layers):\n",
    "    for head in range(n_heads):\n",
    "        head_nodes.append({\n",
    "            'id': f\"layer_{layer}_head_{head}\",\n",
    "            'layer': layer,\n",
    "            'head': head,\n",
    "            'type': 'attention_head'\n",
    "        })\n",
    "\n",
    "# Combine nodes\n",
    "all_nodes = token_nodes + head_nodes\n",
    "nodes_df = pd.DataFrame(all_nodes)\n",
    "\n",
    "print(f\"\\nCreated {len(nodes_df)} nodes:\")\n",
    "print(f\"  - {len(token_nodes)} token nodes\")\n",
    "print(f\"  - {len(head_nodes)} attention head nodes\")\n",
    "\n",
    "# Create edges (attention weights)\n",
    "edges = []\n",
    "for layer in range(min(3, n_layers)):  # First 3 layers for visualization\n",
    "    for head in range(n_heads):\n",
    "        key = f\"layer_{layer}_head_{head}\"\n",
    "        attn_matrix = attention_matrices[key]\n",
    "        \n",
    "        # Extract significant attention edges (weight > threshold)\n",
    "        threshold = 0.1\n",
    "        for i in range(n_tokens):\n",
    "            for j in range(n_tokens):\n",
    "                weight = attn_matrix[i, j]\n",
    "                if weight > threshold:\n",
    "                    edges.append({\n",
    "                        'source': f\"token_{i}\",\n",
    "                        'target': f\"token_{j}\",\n",
    "                        'weight': float(weight),\n",
    "                        'layer': layer,\n",
    "                        'head': head,\n",
    "                        'head_id': key,\n",
    "                        'type': 'attention'\n",
    "                    })\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "print(f\"\\nCreated {len(edges_df)} attention edges (weight > {threshold})\")\n",
    "print(f\"\\n‚úÖ Graph data ready for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 10: Register Graphistry\n",
    "# ==============================================================================\n",
    "import graphistry\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üé® REGISTERING GRAPHISTRY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "graphistry.register(\n",
    "    api=3,\n",
    "    protocol=\"https\",\n",
    "    server=\"hub.graphistry.com\",\n",
    "    username=GRAPHISTRY_USERNAME,\n",
    "    password=GRAPHISTRY_API_KEY\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Graphistry registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Step 11: Create Interactive Attention Visualization\n",
    "# ==============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"üé® CREATING ATTENTION MECHANISM VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configure visualization\n",
    "g = graphistry.edges(edges_df, 'source', 'target')\\\n",
    "    .nodes(nodes_df, 'id')\\\n",
    "    .bind(\n",
    "        point_title='token_text',\n",
    "        point_label='token_text',\n",
    "        point_color='type',\n",
    "        edge_weight='weight',\n",
    "        edge_title='weight'\n",
    "    )\n",
    "\n",
    "# Add styling\n",
    "g = g.settings(\n",
    "    url_params={\n",
    "        'play': 0,\n",
    "        'strongGravity': True,\n",
    "        'edgeCurvature': 0.5,\n",
    "        'scalingRatio': 2.0,\n",
    "        'gravity': 0.1,\n",
    "        'edgeOpacity': 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create visualization\n",
    "viz_url = g.plot(render=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization created!\")\n",
    "print(f\"\\nüîó Open in browser:\")\n",
    "print(f\"   {viz_url}\")\n",
    "print(f\"\\nüìä Features:\")\n",
    "print(f\"   - {len(token_nodes)} token nodes (colored by position)\")\n",
    "print(f\"   - {len(edges_df)} attention edges (thickness = weight)\")\n",
    "print(f\"   - Interactive: zoom, pan, filter by layer/head\")\n",
    "print(f\"   - Hover over edges to see attention weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Insights\n",
    "\n",
    "### Attention Pattern Analysis\n",
    "\n",
    "1. **Early Layers (0-5)**:\n",
    "   - More **uniform attention** distribution\n",
    "   - Tokens attend broadly to context\n",
    "   - Corresponds to low-level feature extraction\n",
    "\n",
    "2. **Middle Layers (6-15)**:\n",
    "   - **Specialization** emerges\n",
    "   - Some heads focus on local context (sliding window)\n",
    "   - Others capture long-range dependencies\n",
    "\n",
    "3. **Late Layers (16+)**:\n",
    "   - **Highly peaked** attention\n",
    "   - Tokens attend to few critical positions\n",
    "   - Task-specific refinement\n",
    "\n",
    "### Comparison with Transformers-Explainer\n",
    "\n",
    "| Aspect | Transformers-Explainer | This Notebook |\n",
    "|--------|------------------------|---------------|\n",
    "| **Attention Detail** | Q¬∑K^T ‚Üí Softmax (4 stages) | **Post-quantization patterns** |\n",
    "| **Model Size** | 627MB (FP32) | 700MB-5GB (Q4_K_M) |\n",
    "| **Quantization Effect** | Not shown | **Visible in weight distributions** |\n",
    "| **Interactivity** | Fixed web UI | **Customizable Jupyter + Graphistry** |\n",
    "| **Speed** | 2-5s (browser) | <1s (GPU-accelerated) |\n",
    "| **Scalability** | GPT-2 only | **1B-8B models** |\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Customization Guide\n",
    "\n",
    "### Change Model\n",
    "```python\n",
    "model_name = \"qwen-2.5-7b-Q4_K_M\"  # Try different models\n",
    "```\n",
    "\n",
    "### Adjust Visualization Threshold\n",
    "```python\n",
    "threshold = 0.05  # Show more edges (lower = more edges)\n",
    "```\n",
    "\n",
    "### Focus on Specific Layers\n",
    "```python\n",
    "for layer in range(10, 13):  # Layers 10-12 only\n",
    "```\n",
    "\n",
    "### Filter by Attention Head\n",
    "```python\n",
    "selected_heads = [0, 3, 7]  # Visualize specific heads\n",
    "edges_df = edges_df[edges_df['head'].isin(selected_heads)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Next Steps\n",
    "\n",
    "1. **Notebook 13**: Token Embedding Visualizer (t-SNE/UMAP)\n",
    "2. **Notebook 14**: Layer-by-Layer Inference Tracker\n",
    "3. **Notebook 15**: Multi-Head Attention Comparator\n",
    "4. **Notebook 16**: Quantization Impact Analyzer\n",
    "\n",
    "---\n",
    "\n",
    "## üôè Credits\n",
    "\n",
    "- **Transformers-Explainer**: [poloclub.github.io/transformer-explainer](https://poloclub.github.io/transformer-explainer/)\n",
    "- **llcuda v2.2.0**: CUDA-accelerated GGUF inference\n",
    "- **Graphistry**: GPU-accelerated graph visualization\n",
    "- **RAPIDS**: cuGraph for GPU analytics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
