{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# llcuda v2.0.6 + Unsloth Gemma 3-1B Tutorial\n",
        "\n",
        "**Complete Guide**: Using llcuda v2.0.6 with Unsloth GGUF models on Tesla T4 GPU\n",
        "\n",
        "**What This Demonstrates**:\n",
        "1. âœ… Install llcuda v2.0.6 from GitHub (no PyPI)\n",
        "2. âœ… Auto-download CUDA binaries from GitHub Releases\n",
        "3. âœ… Use Gemma 3-1B-IT GGUF from Unsloth\n",
        "4. âœ… Fast inference on Tesla T4 with FlashAttention\n",
        "5. âœ… Performance benchmarking and optimization\n",
        "\n",
        "**Requirements**:\n",
        "- Google Colab with Tesla T4 GPU\n",
        "- Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- **llcuda v2.0.6**: https://github.com/waqasm86/llcuda\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth GGUF Models**: https://huggingface.co/unsloth\n",
        "- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Verify Tesla T4 GPU\n",
        "\n",
        "llcuda v2.0.6 binaries are optimized for Tesla T4 (SM 7.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n",
        "\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "gpu_info = result.stdout.strip().split(',')\n",
        "gpu_name = gpu_info[0].strip()\n",
        "compute_cap = gpu_info[1].strip()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Compute Capability: SM {compute_cap}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'T4' in gpu_name and compute_cap == '7.5':\n",
        "    print(\"\\nâœ… Tesla T4 detected - Perfect for llcuda v2.0.6!\")\n",
        "    print(\"   Binaries include FlashAttention and Tensor Core optimization\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  {gpu_name} detected\")\n",
        "    print(\"   llcuda v2.0.6 is optimized for Tesla T4\")\n",
        "    print(\"   Performance may vary on other GPUs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Install llcuda v2.0.6 from GitHub\n",
        "\n",
        "**Note**: llcuda is now **GitHub-only** (no longer on PyPI)\n",
        "\n",
        "CUDA binaries (~266 MB) will auto-download from GitHub Releases on first import."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_llcuda"
      },
      "outputs": [],
      "source": [
        "# Install llcuda v2.0.6 from GitHub\n",
        "print(\"ğŸ“¥ Installing llcuda v2.0.6 from GitHub...\\n\")\n",
        "\n",
        "!pip install -q git+https://github.com/waqasm86/llcuda.git\n",
        "\n",
        "print(\"\\nâœ… llcuda v2.0.6 installed successfully!\")\n",
        "print(\"\\nâ„¹ï¸  CUDA binaries (~266 MB) will auto-download on first import\")\n",
        "print(\"   This happens once - subsequent runs use cached binaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Import llcuda (Triggers Binary Download)\n",
        "\n",
        "First import will download CUDA binaries from:\n",
        "https://github.com/waqasm86/llcuda/releases/download/v2.0.6/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_llcuda"
      },
      "outputs": [],
      "source": [
        "import llcuda\n",
        "import time\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"llcuda version: {llcuda.__version__}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nâœ… llcuda imported successfully!\")\n",
        "print(\"\\nâ„¹ï¸  If this was the first run:\")\n",
        "print(\"   - CUDA binaries were downloaded to ~/.cache/llcuda/\")\n",
        "print(\"   - Includes: llama-server, libggml-cuda.so, and supporting libs\")\n",
        "print(\"   - Next imports will be instant (binaries cached)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Verify GPU Compatibility\n",
        "\n",
        "Check if llcuda can detect and use the GPU properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_compat"
      },
      "outputs": [],
      "source": [
        "# Check GPU compatibility with llcuda\n",
        "compat = llcuda.check_gpu_compatibility()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"GPU COMPATIBILITY CHECK\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"GPU Name: {compat['gpu_name']}\")\n",
        "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
        "print(f\"Platform: {compat['platform']}\")\n",
        "print(f\"Compatible: {compat['compatible']}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if compat['compatible']:\n",
        "    print(\"\\nâœ… GPU is compatible with llcuda!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  GPU compatibility issue detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Load Gemma 3-1B-IT from Unsloth\n",
        "\n",
        "**Three methods to load models:**\n",
        "\n",
        "1. **HuggingFace Hub** (recommended): Direct from Unsloth's repo\n",
        "2. **Model Registry**: Pre-configured model names\n",
        "3. **Local Path**: From downloaded GGUF file\n",
        "\n",
        "We'll use Method 1: Direct from Unsloth HuggingFace repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [],
      "source": [
        "# Initialize inference engine\n",
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "print(\"\\nğŸ“¥ Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\")\n",
        "print(\"   Repository: unsloth/gemma-3-1b-it-GGUF\")\n",
        "print(\"   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\")\n",
        "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
        "\n",
        "# Load model from Unsloth HuggingFace repository\n",
        "# Format: repo_id:filename\n",
        "start_time = time.time()\n",
        "\n",
        "engine.load_model(\n",
        "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "    silent=True,        # Suppress llama-server output\n",
        "    auto_start=True     # Start server automatically\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Model loaded successfully in {load_time:.1f}s!\")\n",
        "print(\"\\nğŸš€ Ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: First Inference - General Knowledge\n",
        "\n",
        "Test the model with a general knowledge question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "first_inference"
      },
      "outputs": [],
      "source": [
        "# Test with a general knowledge prompt\n",
        "prompt = \"Explain quantum computing in simple terms that a beginner can understand.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PROMPT:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(prompt)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Generating response...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    prompt,\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"RESPONSE:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Performance:\")\n",
        "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
        "print(f\"   Latency: {result.latency_ms:.1f} ms\")\n",
        "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
        "print(f\"\\nğŸ’¡ Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Code Generation\n",
        "\n",
        "Test the model's ability to generate Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_generation"
      },
      "outputs": [],
      "source": [
        "# Test code generation\n",
        "code_prompt = \"\"\"Write a Python function to calculate the fibonacci sequence using dynamic programming. \n",
        "Include docstring and example usage.\"\"\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CODE GENERATION TEST\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {code_prompt}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Generating code...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    code_prompt,\n",
        "    max_tokens=300,\n",
        "    temperature=0.3,  # Lower temperature for more deterministic code\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tokens/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Batch Inference\n",
        "\n",
        "Process multiple prompts efficiently with batch inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_inference"
      },
      "outputs": [],
      "source": [
        "# Prepare multiple prompts\n",
        "prompts = [\n",
        "    \"What is machine learning in one sentence?\",\n",
        "    \"Explain neural networks briefly.\",\n",
        "    \"What is the difference between AI and ML?\",\n",
        "    \"Define deep learning concisely.\"\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"BATCH INFERENCE - Processing 4 prompts\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"Query {i}: {prompt}\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "    print(result.text)\n",
        "    print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tok/s | Latency: {result.latency_ms:.0f}ms\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Total batch time: {total_time:.1f}s for {len(prompts)} prompts\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Performance Metrics\n",
        "\n",
        "Analyze aggregated performance metrics across all requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics"
      },
      "outputs": [],
      "source": [
        "# Get comprehensive metrics\n",
        "metrics = engine.get_metrics()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Throughput:\")\n",
        "print(f\"   Total requests: {metrics['throughput']['total_requests']}\")\n",
        "print(f\"   Total tokens: {metrics['throughput']['total_tokens']}\")\n",
        "print(f\"   Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
        "\n",
        "print(f\"\\nâ±ï¸  Latency Distribution:\")\n",
        "print(f\"   Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
        "print(f\"   Median (P50): {metrics['latency']['p50_ms']:.1f} ms\")\n",
        "print(f\"   P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
        "print(f\"   P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
        "print(f\"   Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
        "print(f\"   Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Sample count: {metrics['latency']['sample_count']}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: Advanced Generation Parameters\n",
        "\n",
        "Explore different generation strategies and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_params"
      },
      "outputs": [],
      "source": [
        "# Test creative writing with higher temperature\n",
        "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CREATIVE GENERATION (High Temperature)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {creative_prompt}\")\n",
        "print(f\"Parameters: temperature=0.9, top_p=0.95, top_k=50\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    creative_prompt,\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,     # High creativity\n",
        "    top_p=0.95,          # Nucleus sampling\n",
        "    top_k=50,            # Top-k sampling\n",
        "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
        ")\n",
        "\n",
        "print(result.text)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step11"
      },
      "source": [
        "## Step 11: Alternative Model Loading Methods\n",
        "\n",
        "Demonstrate other ways to load models with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alternative_loading"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALTERNATIVE MODEL LOADING METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£  HuggingFace Hub (Current method - RECOMMENDED):\")\n",
        "print(\"   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\")\n",
        "print(\"   âœ… Direct from Unsloth repository\")\n",
        "print(\"   âœ… Auto-downloads and caches\")\n",
        "print(\"   âœ… Always up-to-date\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£  Model Registry (Pre-configured shortcuts):\")\n",
        "print(\"   engine.load_model('gemma-3-1b-Q4_K_M')\")\n",
        "print(\"   âœ… Simple one-word names\")\n",
        "print(\"   âœ… Curated model list\")\n",
        "print(\"   âš ï¸  May not include all Unsloth models\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£  Local Path (Pre-downloaded GGUF):\")\n",
        "print(\"   engine.load_model('/path/to/model.gguf')\")\n",
        "print(\"   âœ… Full control over model files\")\n",
        "print(\"   âœ… No network dependency after download\")\n",
        "print(\"   âš ï¸  Manual model management\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Tip: For Unsloth models, use method 1 (HuggingFace Hub)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step12"
      },
      "source": [
        "## Step 12: Unsloth Fine-tuning â†’ llcuda Inference Workflow\n",
        "\n",
        "Example workflow showing how to integrate llcuda with Unsloth fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "workflow_demo"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘          UNSLOTH FINE-TUNING â†’ llcuda INFERENCE WORKFLOW             â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "STEP 1: Fine-tune with Unsloth\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load base model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "# Fine-tune on your dataset\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    ...\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "STEP 2: Export to GGUF\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Save fine-tuned model as GGUF (Q4_K_M quantization)\n",
        "model.save_pretrained_gguf(\n",
        "    \"my_finetuned_model\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"\n",
        ")\n",
        "\n",
        "# This creates: my_finetuned_model/unsloth.Q4_K_M.gguf\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "STEP 3: Deploy with llcuda v2.0.6\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import llcuda\n",
        "\n",
        "# Load your fine-tuned GGUF model\n",
        "engine = llcuda.InferenceEngine()\n",
        "engine.load_model(\"my_finetuned_model/unsloth.Q4_K_M.gguf\")\n",
        "\n",
        "# Run inference\n",
        "result = engine.infer(\"Your task-specific prompt\", max_tokens=200)\n",
        "print(result.text)\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "âœ… BENEFITS:\n",
        "   â€¢ Fast training with Unsloth (2x faster, 70% less VRAM)\n",
        "   â€¢ Fast inference with llcuda (FlashAttention, T4-optimized)\n",
        "   â€¢ Easy deployment (GGUF = single portable file)\n",
        "   â€¢ Full llama.cpp ecosystem compatibility\n",
        "   â€¢ Production-ready inference server\n",
        "\n",
        "ğŸ“Š EXPECTED PERFORMANCE (Tesla T4):\n",
        "   â€¢ Gemma 3-1B Q4_K_M: ~45 tok/s\n",
        "   â€¢ Llama 3.2-3B Q4_K_M: ~30 tok/s\n",
        "   â€¢ Qwen 2.5-7B Q4_K_M: ~18 tok/s\n",
        "   â€¢ Llama 3.1-8B Q4_K_M: ~15 tok/s\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step13"
      },
      "source": [
        "## Step 13: Context Manager Usage\n",
        "\n",
        "Use llcuda with Python context managers for automatic cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "context_manager"
      },
      "outputs": [],
      "source": [
        "# Context manager automatically cleans up resources\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONTEXT MANAGER DEMO\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
        "    print(\"ğŸ“¥ Loading model in context...\")\n",
        "    temp_engine.load_model(\n",
        "        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "        silent=True\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ¤– Running quick test...\\n\")\n",
        "    result = temp_engine.infer(\n",
        "        \"What is the capital of France?\",\n",
        "        max_tokens=30\n",
        "    )\n",
        "    \n",
        "    print(f\"Response: {result.text}\")\n",
        "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "print(\"\\nâœ… Context exited - engine automatically cleaned up\")\n",
        "print(\"   Server stopped, resources released\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step14"
      },
      "source": [
        "## Step 14: Available Unsloth Models\n",
        "\n",
        "Browse popular Unsloth GGUF models compatible with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "available_models"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘              POPULAR UNSLOTH GGUF MODELS FOR llcuda v2.0.6           â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ğŸ”¹ SMALL MODELS (1-3B) - Best for T4\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "1. Gemma 3-1B Instruct\n",
        "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
        "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
        "   Speed on T4: ~45 tok/s | VRAM: ~1.2 GB\n",
        "   Use case: General chat, Q&A, reasoning\n",
        "\n",
        "2. Llama 3.2-3B Instruct\n",
        "   Repository: unsloth/Llama-3.2-3B-Instruct-GGUF\n",
        "   File: Llama-3.2-3B-Instruct-Q4_K_M.gguf (~2 GB)\n",
        "   Speed on T4: ~30 tok/s | VRAM: ~2.0 GB\n",
        "   Use case: Instruction following, chat\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ”¹ MEDIUM MODELS (7-8B) - Fits on T4\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "3. Qwen 2.5-7B Instruct\n",
        "   Repository: unsloth/Qwen2.5-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Multilingual, coding, math\n",
        "\n",
        "4. Llama 3.1-8B Instruct\n",
        "   Repository: unsloth/Meta-Llama-3.1-8B-Instruct-GGUF\n",
        "   File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB)\n",
        "   Speed on T4: ~15 tok/s | VRAM: ~5.5 GB\n",
        "   Use case: Advanced reasoning, long context\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ”¹ CODE MODELS\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "5. Qwen 2.5-Coder-7B\n",
        "   Repository: unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Code generation, debugging\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ’¡ LOADING SYNTAX:\n",
        "\n",
        "engine.load_model(\"unsloth/REPO-NAME:filename.gguf\")\n",
        "\n",
        "Example:\n",
        "engine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\")\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ“¦ QUANTIZATION GUIDE:\n",
        "   â€¢ Q4_K_M: Best balance (recommended for T4)\n",
        "   â€¢ Q5_K_M: Higher quality, slower\n",
        "   â€¢ Q8_0: Near full precision, much slower\n",
        "   â€¢ Q2_K: Smallest, lowest quality\n",
        "\n",
        "ğŸ¯ T4 GPU LIMITS:\n",
        "   â€¢ Total VRAM: 16 GB\n",
        "   â€¢ Recommended max model: 8B parameters (Q4_K_M)\n",
        "   â€¢ Leave ~2-3 GB for context and processing\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## ğŸ“Š Summary\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "âœ… **Installed llcuda v2.0.6** from GitHub (GitHub-only distribution)\n",
        "âœ… **Auto-downloaded binaries** from GitHub Releases (~266 MB)\n",
        "âœ… **Loaded Gemma 3-1B-IT** GGUF from Unsloth HuggingFace\n",
        "âœ… **Ran inference** with ~45 tok/s on Tesla T4\n",
        "âœ… **Batch processing** multiple prompts efficiently\n",
        "âœ… **Performance analysis** with detailed metrics\n",
        "âœ… **Demonstrated workflow** from Unsloth fine-tuning to llcuda deployment\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Results (Tesla T4)\n",
        "\n",
        "| Model | Quantization | Speed | VRAM | Context |\n",
        "|-------|--------------|-------|------|---------|\n",
        "| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB | 2048 |\n",
        "| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB | 4096 |\n",
        "| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB | 8192 |\n",
        "| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB | 8192 |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Features of llcuda v2.0.6\n",
        "\n",
        "ğŸš€ **GitHub-Only Distribution**\n",
        "- No PyPI dependency\n",
        "- Install: `pip install git+https://github.com/waqasm86/llcuda.git`\n",
        "- Binaries auto-download from GitHub Releases\n",
        "\n",
        "âš¡ **Performance Optimizations**\n",
        "- FlashAttention support (2-3x faster for long contexts)\n",
        "- Tensor Core optimization for SM 7.5 (Tesla T4)\n",
        "- CUDA Graphs for reduced overhead\n",
        "- All quantization formats supported\n",
        "\n",
        "ğŸ”„ **Seamless Unsloth Integration**\n",
        "- Direct loading from Unsloth HuggingFace repos\n",
        "- Compatible with Unsloth fine-tuned GGUF exports\n",
        "- Full llama.cpp ecosystem support\n",
        "\n",
        "---\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **llcuda GitHub**: https://github.com/waqasm86/llcuda\n",
        "- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "- **Releases**: https://github.com/waqasm86/llcuda/releases\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth Models**: https://huggingface.co/unsloth\n",
        "- **Unsloth GGUF Docs**: https://docs.unsloth.ai/basics/saving-to-gguf\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try different models**: Experiment with larger models from Unsloth\n",
        "2. **Fine-tune with Unsloth**: Train on your custom dataset\n",
        "3. **Export to GGUF**: Use Unsloth's export functionality\n",
        "4. **Deploy with llcuda**: Fast inference on your fine-tuned models\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: llcuda v2.0.6 | Tesla T4 | CUDA 12 | Unsloth Integration | FlashAttention\n",
        "\n",
        "**Author**: Waqas Muhammad (waqasm86@gmail.com)\n",
        "**License**: MIT\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
