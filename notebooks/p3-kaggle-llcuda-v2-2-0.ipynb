{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a36ca5",
   "metadata": {},
   "source": [
    "# llcuda v2.2.0 - Kaggle 2√ó T4 Build Notebook\n",
    "\n",
    "## Architecture: Split-GPU Workload\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         GPU 0             ‚îÇ            GPU 1              ‚îÇ\n",
    "‚îÇ  llama-server (GGUF)      ‚îÇ  RAPIDS + Graphistry          ‚îÇ\n",
    "‚îÇ  LLM Inference            ‚îÇ  Graph Visualization (cuGraph)‚îÇ\n",
    "‚îÇ  15GB VRAM                ‚îÇ  15GB VRAM                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "This notebook builds llcuda binaries for **split-GPU** operation:\n",
    "- **GPU 0**: llama-server with GGUF model (LLM inference)\n",
    "- **GPU 1**: RAPIDS/Graphistry with cuDF/cuGraph (graph simulation)\n",
    "\n",
    "## Step 1: Verify Kaggle GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b107f3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T19:18:35.208760Z",
     "iopub.status.busy": "2026-01-16T19:18:35.208484Z",
     "iopub.status.idle": "2026-01-16T19:18:35.530897Z",
     "shell.execute_reply": "2026-01-16T19:18:35.530233Z",
     "shell.execute_reply.started": "2026-01-16T19:18:35.208733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KAGGLE GPU ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "\n",
      "üìä Detected GPUs: 2\n",
      "   GPU 0: Tesla T4 (UUID: GPU-825b4c22-49b2-7f2d-08a8-ce11f4a5079c)\n",
      "   GPU 1: Tesla T4 (UUID: GPU-8f8f68e8-9eda-5d5e-92e3-fa61d364c3e1)\n",
      "\n",
      "üìä CUDA Version:\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "\n",
      "üìä VRAM Summary:\n",
      "index, name, memory.total [MiB]\n",
      "0, Tesla T4, 15360 MiB\n",
      "1, Tesla T4, 15360 MiB\n",
      "\n",
      "‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\n"
     ]
    }
   ],
   "source": [
    "# Verify we have 2√ó T4 GPUs\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check nvidia-smi\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "gpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\n",
    "print(f\"\\nüìä Detected GPUs: {len(gpu_lines)}\")\n",
    "for line in gpu_lines:\n",
    "    print(f\"   {line}\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(\"\\nüìä CUDA Version:\")\n",
    "!nvcc --version | grep release\n",
    "\n",
    "# Check total VRAM\n",
    "print(\"\\nüìä VRAM Summary:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# Verify we have 2 GPUs\n",
    "if len(gpu_lines) >= 2:\n",
    "    print(\"\\n‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Less than 2 GPUs detected!\")\n",
    "    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334985f0",
   "metadata": {},
   "source": [
    "## Step 2: Verify/Install Build Dependencies\n",
    "\n",
    "**Note:** Kaggle 2√ó T4 comes with cmake 3.31.6 and ninja 1.13.0 pre-installed.\n",
    "We only install what's missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3ce1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T19:18:49.213522Z",
     "iopub.status.busy": "2026-01-16T19:18:49.212881Z",
     "iopub.status.idle": "2026-01-16T19:19:03.187837Z",
     "shell.execute_reply": "2026-01-16T19:19:03.187194Z",
     "shell.execute_reply.started": "2026-01-16T19:18:49.213485Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking build dependencies...\n",
      "‚úÖ cmake version 3.31.6\n",
      "‚úÖ Ninja 1.13.0.git.kitware.jobserver-pipe-1\n",
      "üì¶ Installing ccache...\n",
      "Selecting previously unselected package libhiredis0.14:amd64.\n",
      "(Reading database ... 129073 files and directories currently installed.)\n",
      "Preparing to unpack .../libhiredis0.14_0.14.1-2_amd64.deb ...\n",
      "Unpacking libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Selecting previously unselected package ccache.\n",
      "Preparing to unpack .../ccache_4.5.1-1_amd64.deb ...\n",
      "Unpacking ccache (4.5.1-1) ...\n",
      "Setting up libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Setting up ccache (4.5.1-1) ...\n",
      "Updating symlinks in /usr/lib/ccache ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\n",
      "üì¶ Checking Python packages...\n",
      "   ‚úÖ huggingface_hub\n",
      "   üì¶ Installing sseclient-py...\n",
      "\n",
      "‚úÖ Build dependencies ready\n",
      "cmake version 3.31.6\n",
      "1.13.0.git.kitware.jobserver-pipe-1\n",
      "CPU times: user 130 ms, sys: 55.3 ms, total: 185 ms\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check pre-installed build tools (Kaggle 2√ó T4 has cmake/ninja)\n",
    "import subprocess\n",
    "\n",
    "print(\"Checking build dependencies...\")\n",
    "\n",
    "# Check CMake\n",
    "cmake_result = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\n",
    "if cmake_result.returncode == 0:\n",
    "    cmake_ver = cmake_result.stdout.split(\"\\n\")[0]\n",
    "    print(f\"‚úÖ {cmake_ver}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CMake not found, installing...\")\n",
    "    !apt-get update -qq && apt-get install -y -qq cmake\n",
    "\n",
    "# Check Ninja  \n",
    "ninja_result = subprocess.run([\"ninja\", \"--version\"], capture_output=True, text=True)\n",
    "if ninja_result.returncode == 0:\n",
    "    print(f\"‚úÖ Ninja {ninja_result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ninja not found, installing...\")\n",
    "    !apt-get install -y -qq ninja-build\n",
    "\n",
    "# Check ccache (optional but speeds up rebuilds)\n",
    "ccache_result = subprocess.run([\"which\", \"ccache\"], capture_output=True, text=True)\n",
    "if ccache_result.returncode != 0:\n",
    "    print(\"üì¶ Installing ccache...\")\n",
    "    !apt-get install -y -qq ccache\n",
    "\n",
    "# Install Python dependencies (minimal - most are pre-installed on Kaggle)\n",
    "print(\"\\nüì¶ Checking Python packages...\")\n",
    "required_py = [\"huggingface_hub\", \"sseclient-py\"]\n",
    "for pkg in required_py:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"   ‚úÖ {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"   üì¶ Installing {pkg}...\")\n",
    "        !pip install -q {pkg}\n",
    "\n",
    "print(\"\\n‚úÖ Build dependencies ready\")\n",
    "!cmake --version | head -1\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eee12f",
   "metadata": {},
   "source": [
    "## Step 2b: Fix RAPIDS + Install cuGraph (GPU 1 Workload)\n",
    "\n",
    "**Issue:** Kaggle's pre-installed RAPIDS (25.6.0) has version conflicts with `cuda-python` and `numba-cuda`.\n",
    "\n",
    "**Solution:** Fix the `cuda-python` package version compatibility, then install `cugraph-cu12`.\n",
    "\n",
    "**Reference:** https://docs.rapids.ai/install/ (pip + CUDA 12 + Stable 25.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097adecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T19:19:43.547650Z",
     "iopub.status.busy": "2026-01-16T19:19:43.546742Z",
     "iopub.status.idle": "2026-01-16T19:20:25.946574Z",
     "shell.execute_reply": "2026-01-16T19:20:25.945914Z",
     "shell.execute_reply.started": "2026-01-16T19:19:43.547614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INSTALLING CUGRAPH FOR GPU 1 (RAPIDS 25.6.0 COMPATIBLE)\n",
      "======================================================================\n",
      "\n",
      "üì¶ Pre-installed RAPIDS packages on Kaggle:\n",
      "   cudf-cu12: 25.6.0\n",
      "   cuml-cu12: 25.6.0\n",
      "   pylibraft-cu12: 25.6.0\n",
      "   cuda-python: 12.6.2.post1\n",
      "   numba-cuda: 0.11.0\n",
      "\n",
      "üì¶ Installing cugraph-cu12==25.6.* (matching Kaggle's RAPIDS)...\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "üì¶ Installing graphistry...\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "üì¶ Final verification:\n",
      "   ‚úÖ cuDF: 25.06.00\n",
      "   ‚úÖ cuGraph: 25.06.00\n",
      "   ‚úÖ Graphistry: 0.50.4\n",
      "\n",
      "‚úÖ RAPIDS packages installed! If imports fail, restart runtime and re-run.\n",
      "CPU times: user 5.95 s, sys: 947 ms, total: 6.89 s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install cuGraph matching Kaggle's pre-installed RAPIDS 25.6.0\n",
    "# CRITICAL: Do NOT upgrade cuda-python or numba-cuda - this breaks RAPIDS!\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSTALLING CUGRAPH FOR GPU 1 (RAPIDS 25.6.0 COMPATIBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Check pre-installed RAPIDS versions\n",
    "import subprocess\n",
    "print(\"\\nüì¶ Pre-installed RAPIDS packages on Kaggle:\")\n",
    "for pkg in [\"cudf-cu12\", \"cuml-cu12\", \"pylibraft-cu12\", \"cuda-python\", \"numba-cuda\"]:\n",
    "    result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n",
    "    if \"Version:\" in result.stdout:\n",
    "        version = [l for l in result.stdout.split(\"\\n\") if l.startswith(\"Version:\")][0]\n",
    "        print(f\"   {pkg}: {version.split(': ')[1]}\")\n",
    "    else:\n",
    "        print(f\"   {pkg}: NOT INSTALLED\")\n",
    "\n",
    "# Step 2: Install cugraph-cu12 matching RAPIDS 25.6.* (Kaggle's version)\n",
    "# Using pypi.nvidia.com for RAPIDS packages\n",
    "print(\"\\nüì¶ Installing cugraph-cu12==25.6.* (matching Kaggle's RAPIDS)...\")\n",
    "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n",
    "\n",
    "# Step 3: Install graphistry (minimal, no [ai] extras to avoid conflicts)\n",
    "print(\"\\nüì¶ Installing graphistry...\")\n",
    "!pip install -q graphistry\n",
    "\n",
    "# Step 4: Verify RAPIDS imports work\n",
    "print(\"\\nüì¶ Final verification:\")\n",
    "try:\n",
    "    import cudf\n",
    "    print(f\"   ‚úÖ cuDF: {cudf.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå cuDF: {e}\")\n",
    "\n",
    "try:\n",
    "    import cugraph\n",
    "    print(f\"   ‚úÖ cuGraph: {cugraph.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   ‚ùå cuGraph: {e}\")\n",
    "    print(\"   üí° If cuGraph fails, try: Runtime ‚Üí Restart runtime, then re-run this cell\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"   ‚úÖ Graphistry: {graphistry.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Graphistry: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ RAPIDS packages installed! If imports fail, restart runtime and re-run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98739d5f-299c-4f02-b142-b8cc82727ef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T19:20:51.715821Z",
     "iopub.status.busy": "2026-01-16T19:20:51.714624Z",
     "iopub.status.idle": "2026-01-16T19:20:53.693973Z",
     "shell.execute_reply": "2026-01-16T19:20:53.693046Z",
     "shell.execute_reply.started": "2026-01-16T19:20:51.715788Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                                  Version\n",
      "---------------------------------------- -------------------\n",
      "a2a-sdk                                  0.3.22\n",
      "absl-py                                  1.4.0\n",
      "absolufy-imports                         0.3.1\n",
      "accelerate                               1.11.0\n",
      "aiofiles                                 22.1.0\n",
      "aiohappyeyeballs                         2.6.1\n",
      "aiohttp                                  3.13.3\n",
      "aiosignal                                1.4.0\n",
      "aiosqlite                                0.22.1\n",
      "alabaster                                1.0.0\n",
      "albucore                                 0.0.24\n",
      "albumentations                           2.0.8\n",
      "ale-py                                   0.11.2\n",
      "alembic                                  1.17.0\n",
      "altair                                   5.5.0\n",
      "annotated-doc                            0.0.4\n",
      "annotated-types                          0.7.0\n",
      "ansicolors                               1.1.8\n",
      "antlr4-python3-runtime                   4.9.3\n",
      "anyio                                    4.12.1\n",
      "anywidget                                0.9.18\n",
      "argon2-cffi                              25.1.0\n",
      "argon2-cffi-bindings                     25.1.0\n",
      "args                                     0.1.0\n",
      "array_record                             0.8.1\n",
      "arrow                                    1.4.0\n",
      "arviz                                    0.22.0\n",
      "asn1crypto                               1.5.1\n",
      "astropy                                  7.1.1\n",
      "astropy-iers-data                        0.2025.10.20.0.39.8\n",
      "asttokens                                3.0.1\n",
      "astunparse                               1.6.3\n",
      "atpublic                                 5.1\n",
      "attrs                                    25.4.0\n",
      "audioread                                3.0.1\n",
      "Authlib                                  1.6.5\n",
      "autograd                                 1.8.0\n",
      "babel                                    2.17.0\n",
      "backcall                                 0.2.0\n",
      "bayesian-optimization                    3.2.0\n",
      "beartype                                 0.22.2\n",
      "beautifulsoup4                           4.13.5\n",
      "betterproto                              2.0.0b7\n",
      "bigframes                                2.26.0\n",
      "bigquery-magics                          0.10.3\n",
      "black                                    25.12.0\n",
      "blake3                                   1.0.8\n",
      "bleach                                   6.2.0\n",
      "blinker                                  1.9.0\n",
      "blis                                     1.3.0\n",
      "blobfile                                 3.1.0\n",
      "blosc2                                   3.10.2\n",
      "bokeh                                    3.7.3\n",
      "Boruta                                   0.4.3\n",
      "boto3                                    1.42.27\n",
      "botocore                                 1.42.27\n",
      "Bottleneck                               1.4.2\n",
      "bqplot                                   0.12.45\n",
      "branca                                   0.8.2\n",
      "Brotli                                   1.1.0\n",
      "build                                    1.3.0\n",
      "CacheControl                             0.14.3\n",
      "cachetools                               5.5.2\n",
      "Cartopy                                  0.25.0\n",
      "catalogue                                2.0.10\n",
      "catboost                                 1.2.8\n",
      "category_encoders                        2.9.0\n",
      "certifi                                  2026.1.4\n",
      "cesium                                   0.12.4\n",
      "cffi                                     2.0.0\n",
      "chardet                                  5.2.0\n",
      "charset-normalizer                       3.4.4\n",
      "Chessnut                                 0.4.1\n",
      "chex                                     0.1.90\n",
      "clarabel                                 0.11.1\n",
      "click                                    8.3.1\n",
      "click-plugins                            1.1.1.2\n",
      "cligj                                    0.7.2\n",
      "clint                                    0.5.1\n",
      "cloudpathlib                             0.23.0\n",
      "cloudpickle                              3.1.1\n",
      "cmake                                    3.31.6\n",
      "cmdstanpy                                1.2.5\n",
      "colorama                                 0.4.6\n",
      "colorcet                                 3.1.0\n",
      "colorlog                                 6.10.1\n",
      "colorlover                               0.3.0\n",
      "colour                                   0.1.5\n",
      "comm                                     0.2.3\n",
      "community                                1.0.0b1\n",
      "confection                               0.1.5\n",
      "cons                                     0.4.7\n",
      "contourpy                                1.3.3\n",
      "coverage                                 7.13.1\n",
      "cramjam                                  2.11.0\n",
      "cryptography                             46.0.3\n",
      "cuda-python                              12.6.2.post1\n",
      "cudf-cu12                                25.6.0\n",
      "cudf-polars-cu12                         25.6.0\n",
      "cufflinks                                0.17.3\n",
      "cugraph-cu12                             25.6.0\n",
      "cuml-cu12                                25.6.0\n",
      "cupy-cuda12x                             13.3.0\n",
      "curl_cffi                                0.13.0\n",
      "cuvs-cu12                                25.6.1\n",
      "cvxopt                                   1.3.2\n",
      "cvxpy                                    1.6.7\n",
      "cycler                                   0.12.1\n",
      "cyipopt                                  1.5.0\n",
      "cymem                                    2.0.11\n",
      "Cython                                   3.0.12\n",
      "cytoolz                                  1.1.0\n",
      "daal                                     2025.10.0\n",
      "dacite                                   1.9.2\n",
      "dask                                     2025.5.0\n",
      "dask-cuda                                25.6.0\n",
      "dask-cudf-cu12                           25.6.0\n",
      "dataclasses-json                         0.6.7\n",
      "dataproc-spark-connect                   0.8.3\n",
      "datasets                                 4.4.2\n",
      "db-dtypes                                1.4.3\n",
      "dbus-python                              1.2.18\n",
      "deap                                     1.4.3\n",
      "debugpy                                  1.8.15\n",
      "decorator                                4.4.2\n",
      "deepdiff                                 8.6.1\n",
      "defusedxml                               0.7.1\n",
      "Deprecated                               1.3.1\n",
      "diffusers                                0.35.2\n",
      "dill                                     0.4.0\n",
      "dipy                                     1.11.0\n",
      "distributed                              2025.5.0\n",
      "distributed-ucxx-cu12                    0.44.0\n",
      "distro                                   1.9.0\n",
      "dlib                                     19.24.6\n",
      "dm-tree                                  0.1.9\n",
      "dnspython                                2.8.0\n",
      "docker                                   7.1.0\n",
      "docstring_parser                         0.17.0\n",
      "docstring-to-markdown                    0.17\n",
      "docutils                                 0.21.2\n",
      "dopamine_rl                              4.1.2\n",
      "duckdb                                   1.3.2\n",
      "earthengine-api                          1.5.24\n",
      "easydict                                 1.13\n",
      "easyocr                                  1.7.2\n",
      "editdistance                             0.8.1\n",
      "eerepr                                   0.1.2\n",
      "einops                                   0.8.1\n",
      "email-validator                          2.3.0\n",
      "emoji                                    2.15.0\n",
      "en_core_web_sm                           3.8.0\n",
      "entrypoints                              0.4\n",
      "et_xmlfile                               2.0.0\n",
      "etils                                    1.13.0\n",
      "etuples                                  0.3.10\n",
      "execnb                                   0.1.18\n",
      "Farama-Notifications                     0.0.4\n",
      "fastai                                   2.8.4\n",
      "fastapi                                  0.123.10\n",
      "fastcore                                 1.11.3\n",
      "fastdownload                             0.0.7\n",
      "fastgit                                  0.0.1\n",
      "fastjsonschema                           2.21.2\n",
      "fastprogress                             1.0.3\n",
      "fastrlock                                0.8.3\n",
      "fasttext                                 0.9.3\n",
      "fasttransform                            0.0.2\n",
      "fastuuid                                 0.14.0\n",
      "featuretools                             1.31.0\n",
      "ffmpy                                    0.6.3\n",
      "filelock                                 3.20.3\n",
      "filetype                                 1.2.0\n",
      "fiona                                    1.10.1\n",
      "firebase-admin                           6.9.0\n",
      "Flask                                    3.1.2\n",
      "flatbuffers                              25.9.23\n",
      "flax                                     0.10.7\n",
      "folium                                   0.20.0\n",
      "fonttools                                4.60.1\n",
      "fqdn                                     1.5.1\n",
      "frozendict                               2.4.6\n",
      "frozenlist                               1.8.0\n",
      "fsspec                                   2025.10.0\n",
      "funcy                                    2.0\n",
      "fury                                     0.12.0\n",
      "future                                   1.0.0\n",
      "fuzzywuzzy                               0.18.0\n",
      "gast                                     0.6.0\n",
      "gatspy                                   0.3\n",
      "gcsfs                                    2025.3.0\n",
      "GDAL                                     3.8.4\n",
      "gdown                                    5.2.0\n",
      "geemap                                   0.35.3\n",
      "gensim                                   4.4.0\n",
      "geocoder                                 1.38.1\n",
      "geographiclib                            2.1\n",
      "geojson                                  3.2.0\n",
      "geopandas                                1.1.1\n",
      "geopy                                    2.4.1\n",
      "ghapi                                    1.0.8\n",
      "gin-config                               0.5.0\n",
      "gitdb                                    4.0.12\n",
      "GitPython                                3.1.45\n",
      "glob2                                    0.7\n",
      "google                                   2.0.3\n",
      "google-adk                               1.22.1\n",
      "google-ai-generativelanguage             0.6.15\n",
      "google-api-core                          2.26.0\n",
      "google-api-python-client                 2.185.0\n",
      "google-auth                              2.47.0\n",
      "google-auth-httplib2                     0.2.0\n",
      "google-auth-oauthlib                     1.2.2\n",
      "google-cloud-aiplatform                  1.133.0\n",
      "google-cloud-appengine-logging           1.7.0\n",
      "google-cloud-audit-log                   0.4.0\n",
      "google-cloud-bigquery                    3.38.0\n",
      "google-cloud-bigquery-connection         1.19.0\n",
      "google-cloud-bigtable                    2.33.0\n",
      "google-cloud-core                        2.4.3\n",
      "google-cloud-dataproc                    5.23.0\n",
      "google-cloud-datastore                   2.21.0\n",
      "google-cloud-discoveryengine             0.13.12\n",
      "google-cloud-firestore                   2.21.0\n",
      "google-cloud-functions                   1.21.0\n",
      "google-cloud-language                    2.18.0\n",
      "google-cloud-logging                     3.12.1\n",
      "google-cloud-monitoring                  2.28.0\n",
      "google-cloud-pubsub                      2.34.0\n",
      "google-cloud-resource-manager            1.15.0\n",
      "google-cloud-secret-manager              2.25.0\n",
      "google-cloud-spanner                     3.58.0\n",
      "google-cloud-speech                      2.34.0\n",
      "google-cloud-storage                     2.19.0\n",
      "google-cloud-trace                       1.17.0\n",
      "google-cloud-translate                   3.12.1\n",
      "google-cloud-videointelligence           2.17.0\n",
      "google-cloud-vision                      3.11.0\n",
      "google-colab                             1.0.0\n",
      "google-crc32c                            1.7.1\n",
      "google-genai                             1.57.0\n",
      "google-generativeai                      0.8.5\n",
      "google-pasta                             0.2.0\n",
      "google-resumable-media                   2.7.2\n",
      "googleapis-common-protos                 1.71.0\n",
      "googledrivedownloader                    1.1.0\n",
      "gpxpy                                    1.6.2\n",
      "gradio                                   5.49.1\n",
      "gradio_client                            1.13.3\n",
      "graphistry                               0.50.4\n",
      "graphviz                                 0.21\n",
      "greenlet                                 3.2.4\n",
      "groovy                                   0.1.2\n",
      "grpc-google-iam-v1                       0.14.3\n",
      "grpc-interceptor                         0.15.4\n",
      "grpcio                                   1.75.1\n",
      "grpcio-status                            1.71.2\n",
      "grpclib                                  0.4.9\n",
      "gspread                                  6.2.1\n",
      "gspread-dataframe                        4.0.0\n",
      "gym                                      0.25.2\n",
      "gym-notices                              0.1.0\n",
      "gymnasium                                0.29.0\n",
      "h11                                      0.16.0\n",
      "h2                                       4.3.0\n",
      "h2o                                      3.46.0.9\n",
      "h5netcdf                                 1.7.2\n",
      "h5py                                     3.15.1\n",
      "haversine                                2.9.0\n",
      "hdbscan                                  0.8.40\n",
      "hep_ml                                   0.8.0\n",
      "hf_transfer                              0.1.9\n",
      "hf-xet                                   1.2.1rc0\n",
      "highspy                                  1.11.0\n",
      "holidays                                 0.82\n",
      "holoviews                                1.21.0\n",
      "hpack                                    4.1.0\n",
      "html5lib                                 1.1\n",
      "httpcore                                 1.0.9\n",
      "httpimport                               1.4.1\n",
      "httplib2                                 0.31.0\n",
      "httpx                                    0.28.1\n",
      "httpx-sse                                0.4.3\n",
      "huggingface-hub                          0.36.0\n",
      "humanize                                 4.14.0\n",
      "hyperframe                               6.1.0\n",
      "hyperopt                                 0.2.7\n",
      "ibis-framework                           9.5.0\n",
      "id                                       1.5.0\n",
      "idna                                     3.11\n",
      "igraph                                   1.0.0\n",
      "ImageHash                                4.3.2\n",
      "imageio                                  2.37.0\n",
      "imageio-ffmpeg                           0.6.0\n",
      "imagesize                                1.4.1\n",
      "imbalanced-learn                         0.14.0\n",
      "immutabledict                            4.2.2\n",
      "importlib_metadata                       8.7.0\n",
      "importlib_resources                      6.5.2\n",
      "imutils                                  0.5.4\n",
      "in-toto-attestation                      0.9.3\n",
      "inflect                                  7.5.0\n",
      "iniconfig                                2.3.0\n",
      "intel-cmplr-lib-ur                       2025.2.1\n",
      "intel-openmp                             2025.2.1\n",
      "ipyevents                                2.0.4\n",
      "ipyfilechooser                           0.6.0\n",
      "ipykernel                                6.17.1\n",
      "ipyleaflet                               0.20.0\n",
      "ipympl                                   0.9.8\n",
      "ipyparallel                              8.8.0\n",
      "ipython                                  7.34.0\n",
      "ipython-genutils                         0.2.0\n",
      "ipython_pygments_lexers                  1.1.1\n",
      "ipython-sql                              0.5.0\n",
      "ipytree                                  0.2.2\n",
      "ipywidgets                               8.1.5\n",
      "isoduration                              20.11.0\n",
      "isoweek                                  1.3.3\n",
      "itsdangerous                             2.2.0\n",
      "Janome                                   0.5.0\n",
      "jaraco.classes                           3.4.0\n",
      "jaraco.context                           6.0.1\n",
      "jaraco.functools                         4.3.0\n",
      "jax                                      0.7.2\n",
      "jax-cuda12-pjrt                          0.7.2\n",
      "jax-cuda12-plugin                        0.7.2\n",
      "jaxlib                                   0.7.2\n",
      "jedi                                     0.19.2\n",
      "jeepney                                  0.9.0\n",
      "jieba                                    0.42.1\n",
      "Jinja2                                   3.1.6\n",
      "jiter                                    0.10.0\n",
      "jmespath                                 1.0.1\n",
      "joblib                                   1.5.3\n",
      "json5                                    0.13.0\n",
      "jsonpatch                                1.33\n",
      "jsonpickle                               4.1.1\n",
      "jsonpointer                              3.0.0\n",
      "jsonschema                               4.25.1\n",
      "jsonschema-specifications                2025.9.1\n",
      "jupyter_client                           7.4.9\n",
      "jupyter-console                          6.6.3\n",
      "jupyter_core                             5.9.1\n",
      "jupyter-events                           0.12.0\n",
      "jupyter_kernel_gateway                   2.5.2\n",
      "jupyter-leaflet                          0.20.0\n",
      "jupyter-lsp                              1.5.1\n",
      "jupyter_server                           2.12.5\n",
      "jupyter_server_fileid                    0.9.3\n",
      "jupyter_server_proxy                     4.4.0\n",
      "jupyter_server_terminals                 0.5.3\n",
      "jupyter_server_ydoc                      0.8.0\n",
      "jupyter-ydoc                             0.2.5\n",
      "jupyterlab                               3.6.8\n",
      "jupyterlab-lsp                           3.10.2\n",
      "jupyterlab_pygments                      0.3.0\n",
      "jupyterlab_server                        2.28.0\n",
      "jupyterlab_widgets                       3.0.15\n",
      "jupytext                                 1.17.3\n",
      "kaggle                                   1.7.4.5\n",
      "kaggle-environments                      1.18.0\n",
      "kagglehub                                0.4.0\n",
      "kagglesdk                                0.1.14\n",
      "keras                                    3.10.0\n",
      "keras-core                               0.1.7\n",
      "keras-cv                                 0.9.0\n",
      "keras-hub                                0.21.1\n",
      "keras-nlp                                0.21.1\n",
      "keras-tuner                              1.4.8\n",
      "keyring                                  25.6.0\n",
      "keyrings.google-artifactregistry-auth    1.1.2\n",
      "kiwisolver                               1.4.9\n",
      "kornia                                   0.8.2\n",
      "kornia_rs                                0.1.10\n",
      "kt-legacy                                1.0.5\n",
      "langchain                                0.3.27\n",
      "langchain-core                           0.3.79\n",
      "langchain-text-splitters                 0.3.11\n",
      "langcodes                                3.5.0\n",
      "langid                                   1.1.6\n",
      "langsmith                                0.4.37\n",
      "language_data                            1.3.0\n",
      "lark                                     1.3.0\n",
      "launchpadlib                             1.10.16\n",
      "lazr.restfulclient                       0.14.4\n",
      "lazr.uri                                 1.0.6\n",
      "lazy_loader                              0.4\n",
      "learntools                               0.3.5\n",
      "libclang                                 18.1.1\n",
      "libcudf-cu12                             25.6.0\n",
      "libcugraph-cu12                          25.6.0\n",
      "libcuml-cu12                             25.6.0\n",
      "libcuvs-cu12                             25.6.1\n",
      "libkvikio-cu12                           25.6.0\n",
      "libpysal                                 4.9.2\n",
      "libraft-cu12                             25.6.0\n",
      "librmm-cu12                              25.6.0\n",
      "librosa                                  0.11.0\n",
      "libucx-cu12                              1.18.1\n",
      "libucxx-cu12                             0.44.0\n",
      "lightgbm                                 4.6.0\n",
      "lightning-utilities                      0.15.2\n",
      "lime                                     0.2.0.1\n",
      "line_profiler                            5.0.0\n",
      "linkify-it-py                            2.0.3\n",
      "litellm                                  1.80.16\n",
      "llvmlite                                 0.43.0\n",
      "lml                                      0.2.0\n",
      "locket                                   1.0.0\n",
      "logical-unification                      0.4.6\n",
      "lxml                                     5.4.0\n",
      "Mako                                     1.3.10\n",
      "mamba                                    0.11.3\n",
      "marisa-trie                              1.3.1\n",
      "Markdown                                 3.9\n",
      "markdown-it-py                           4.0.0\n",
      "MarkupSafe                               3.0.3\n",
      "marshmallow                              3.26.2\n",
      "matplotlib                               3.10.0\n",
      "matplotlib-inline                        0.1.7\n",
      "matplotlib-venn                          1.1.2\n",
      "mcp                                      1.18.0\n",
      "mdit-py-plugins                          0.5.0\n",
      "mdurl                                    0.1.2\n",
      "minify_html                              0.18.1\n",
      "miniKanren                               1.0.5\n",
      "missingno                                0.5.2\n",
      "mistune                                  0.8.4\n",
      "mizani                                   0.13.5\n",
      "mkl                                      2025.2.0\n",
      "ml_collections                           1.1.0\n",
      "ml_dtypes                                0.5.3\n",
      "mlcrate                                  0.2.0\n",
      "mlxtend                                  0.23.4\n",
      "mne                                      1.11.0\n",
      "model-signing                            1.1.1\n",
      "more-itertools                           10.8.0\n",
      "moviepy                                  1.0.3\n",
      "mpld3                                    0.5.12\n",
      "mpmath                                   1.3.0\n",
      "msgpack                                  1.1.2\n",
      "multidict                                6.7.0\n",
      "multimethod                              1.12\n",
      "multipledispatch                         1.0.0\n",
      "multiprocess                             0.70.18\n",
      "multitasking                             0.0.12\n",
      "murmurhash                               1.0.13\n",
      "music21                                  9.3.0\n",
      "mypy_extensions                          1.1.0\n",
      "namex                                    0.1.0\n",
      "narwhals                                 2.9.0\n",
      "natsort                                  8.4.0\n",
      "nbclassic                                1.3.3\n",
      "nbclient                                 0.5.13\n",
      "nbconvert                                6.4.5\n",
      "nbdev                                    2.4.10\n",
      "nbformat                                 5.10.4\n",
      "ndindex                                  1.10.0\n",
      "nest-asyncio                             1.6.0\n",
      "networkx                                 3.5\n",
      "nibabel                                  5.3.2\n",
      "nilearn                                  0.13.0\n",
      "ninja                                    1.13.0\n",
      "nltk                                     3.9.2\n",
      "notebook                                 6.5.7\n",
      "notebook_shim                            0.2.4\n",
      "numba                                    0.60.0\n",
      "numba-cuda                               0.11.0\n",
      "numexpr                                  2.14.1\n",
      "numpy                                    2.0.2\n",
      "nvidia-cublas-cu12                       12.6.4.1\n",
      "nvidia-cuda-cupti-cu12                   12.6.80\n",
      "nvidia-cuda-nvcc-cu12                    12.5.82\n",
      "nvidia-cuda-nvrtc-cu12                   12.6.77\n",
      "nvidia-cuda-runtime-cu12                 12.6.77\n",
      "nvidia-cudnn-cu12                        9.10.2.21\n",
      "nvidia-cufft-cu12                        11.3.0.4\n",
      "nvidia-cufile-cu12                       1.11.1.6\n",
      "nvidia-curand-cu12                       10.3.7.77\n",
      "nvidia-cusolver-cu12                     11.7.1.2\n",
      "nvidia-cusparse-cu12                     12.5.4.2\n",
      "nvidia-cusparselt-cu12                   0.7.1\n",
      "nvidia-ml-py                             12.575.51\n",
      "nvidia-nccl-cu12                         2.27.3\n",
      "nvidia-nvjitlink-cu12                    12.6.85\n",
      "nvidia-nvshmem-cu12                      3.4.5\n",
      "nvidia-nvtx-cu12                         12.6.77\n",
      "nvtx                                     0.2.13\n",
      "nx-cugraph-cu12                          25.6.0\n",
      "oauth2client                             4.1.3\n",
      "oauthlib                                 3.3.1\n",
      "odfpy                                    1.4.1\n",
      "olefile                                  0.47\n",
      "omegaconf                                2.3.0\n",
      "onnx                                     1.20.1\n",
      "open_spiel                               1.6.11\n",
      "openai                                   2.15.0\n",
      "opencv-contrib-python                    4.12.0.88\n",
      "opencv-python                            4.12.0.88\n",
      "opencv-python-headless                   4.12.0.88\n",
      "openpyxl                                 3.1.5\n",
      "openslide-bin                            4.0.0.11\n",
      "openslide-python                         1.4.3\n",
      "opentelemetry-api                        1.37.0\n",
      "opentelemetry-exporter-gcp-logging       1.11.0a0\n",
      "opentelemetry-exporter-gcp-monitoring    1.10.0a0\n",
      "opentelemetry-exporter-gcp-trace         1.10.0\n",
      "opentelemetry-exporter-otlp-proto-common 1.37.0\n",
      "opentelemetry-exporter-otlp-proto-http   1.37.0\n",
      "opentelemetry-proto                      1.37.0\n",
      "opentelemetry-resourcedetector-gcp       1.10.0a0\n",
      "opentelemetry-sdk                        1.37.0\n",
      "opentelemetry-semantic-conventions       0.58b0\n",
      "opt_einsum                               3.4.0\n",
      "optax                                    0.2.6\n",
      "optree                                   0.17.0\n",
      "optuna                                   4.6.0\n",
      "orbax-checkpoint                         0.11.25\n",
      "orderly-set                              5.5.0\n",
      "orjson                                   3.11.3\n",
      "osqp                                     1.0.5\n",
      "overrides                                7.7.0\n",
      "packaging                                26.0rc2\n",
      "palettable                               3.3.3\n",
      "pandas                                   2.2.2\n",
      "pandas-datareader                        0.10.0\n",
      "pandas-gbq                               0.29.2\n",
      "pandas-profiling                         3.6.6\n",
      "pandas-stubs                             2.2.2.240909\n",
      "pandasql                                 0.7.3\n",
      "pandocfilters                            1.5.1\n",
      "panel                                    1.8.2\n",
      "papermill                                2.6.0\n",
      "param                                    2.2.1\n",
      "parso                                    0.8.5\n",
      "parsy                                    2.2\n",
      "partd                                    1.4.2\n",
      "path                                     17.1.1\n",
      "path.py                                  12.5.0\n",
      "pathos                                   0.3.2\n",
      "pathspec                                 1.0.3\n",
      "patsy                                    1.0.2\n",
      "pdf2image                                1.17.0\n",
      "peewee                                   3.18.2\n",
      "peft                                     0.17.1\n",
      "pettingzoo                               1.24.0\n",
      "pexpect                                  4.9.0\n",
      "phik                                     0.12.5\n",
      "pickleshare                              0.7.5\n",
      "pillow                                   11.3.0\n",
      "pip                                      24.1.2\n",
      "platformdirs                             4.5.1\n",
      "plotly                                   5.24.1\n",
      "plotly-express                           0.4.1\n",
      "plotnine                                 0.14.5\n",
      "pluggy                                   1.6.0\n",
      "plum-dispatch                            2.5.8\n",
      "ply                                      3.11\n",
      "polars                                   1.25.2\n",
      "pooch                                    1.8.2\n",
      "portpicker                               1.5.2\n",
      "pox                                      0.3.6\n",
      "ppft                                     1.7.7\n",
      "preprocessing                            0.1.13\n",
      "preshed                                  3.0.10\n",
      "prettytable                              3.16.0\n",
      "proglog                                  0.1.12\n",
      "progressbar2                             4.5.0\n",
      "prometheus_client                        0.23.1\n",
      "promise                                  2.3\n",
      "prompt_toolkit                           3.0.52\n",
      "propcache                                0.4.1\n",
      "prophet                                  1.1.7\n",
      "proto-plus                               1.26.1\n",
      "protobuf                                 5.29.5\n",
      "psutil                                   5.9.5\n",
      "psycopg2                                 2.9.11\n",
      "psygnal                                  0.15.0\n",
      "ptyprocess                               0.7.0\n",
      "pudb                                     2025.1.5\n",
      "puremagic                                1.30\n",
      "py-cpuinfo                               9.0.0\n",
      "py4j                                     0.10.9.7\n",
      "pyaml                                    25.7.0\n",
      "PyArabic                                 0.6.15\n",
      "pyarrow                                  19.0.1\n",
      "pyasn1                                   0.6.1\n",
      "pyasn1_modules                           0.4.2\n",
      "pybind11                                 3.0.1\n",
      "pycairo                                  1.28.0\n",
      "pyclipper                                1.4.0\n",
      "pycocotools                              2.0.10\n",
      "pycparser                                2.23\n",
      "pycryptodome                             3.23.0\n",
      "pycryptodomex                            3.23.0\n",
      "pycuda                                   2025.1.2\n",
      "pydantic                                 2.12.5\n",
      "pydantic_core                            2.41.5\n",
      "pydantic-settings                        2.11.0\n",
      "pydata-google-auth                       1.9.1\n",
      "pydicom                                  3.0.1\n",
      "pydot                                    3.0.4\n",
      "pydotplus                                2.0.2\n",
      "PyDrive2                                 1.21.3\n",
      "pydub                                    0.25.1\n",
      "pyemd                                    1.0.0\n",
      "pyerfa                                   2.0.1.5\n",
      "pyexcel-io                               0.6.7\n",
      "pyexcel-ods                              0.6.0\n",
      "pygame                                   2.6.1\n",
      "pygit2                                   1.18.2\n",
      "pygltflib                                1.16.5\n",
      "Pygments                                 2.19.2\n",
      "PyGObject                                3.42.0\n",
      "PyJWT                                    2.10.1\n",
      "pyLDAvis                                 3.4.1\n",
      "pylibcudf-cu12                           25.6.0\n",
      "pylibcugraph-cu12                        25.6.0\n",
      "pylibraft-cu12                           25.6.0\n",
      "pymc                                     5.26.1\n",
      "pymongo                                  4.16.0\n",
      "Pympler                                  1.1\n",
      "pynndescent                              0.5.13\n",
      "pynvjitlink-cu12                         0.7.0\n",
      "pynvml                                   12.0.0\n",
      "pyogrio                                  0.11.1\n",
      "pyomo                                    6.9.5\n",
      "PyOpenGL                                 3.1.10\n",
      "pyOpenSSL                                25.3.0\n",
      "pyparsing                                3.2.5\n",
      "pypdf                                    6.6.0\n",
      "pyperclip                                1.11.0\n",
      "pyproj                                   3.7.2\n",
      "pyproject_hooks                          1.2.0\n",
      "pyshp                                    3.0.2.post1\n",
      "PySocks                                  1.7.1\n",
      "pyspark                                  3.5.1\n",
      "pytensor                                 2.35.1\n",
      "pytesseract                              0.3.13\n",
      "pytest                                   8.4.2\n",
      "python-apt                               0.0.0\n",
      "python-bidi                              0.6.7\n",
      "python-box                               7.3.2\n",
      "python-dateutil                          2.9.0.post0\n",
      "python-dotenv                            1.1.1\n",
      "python-json-logger                       4.0.0\n",
      "python-louvain                           0.16\n",
      "python-lsp-jsonrpc                       1.1.2\n",
      "python-lsp-server                        1.14.0\n",
      "python-multipart                         0.0.20\n",
      "python-slugify                           8.0.4\n",
      "python-snappy                            0.7.3\n",
      "python-utils                             3.9.1\n",
      "pytokens                                 0.3.0\n",
      "pytools                                  2025.2.5\n",
      "pytorch-ignite                           0.5.3\n",
      "pytorch-lightning                        2.6.0\n",
      "pytz                                     2025.2\n",
      "PyUpSet                                  0.1.1.post7\n",
      "pyviz_comms                              3.0.6\n",
      "PyWavelets                               1.9.0\n",
      "PyYAML                                   6.0.3\n",
      "pyzmq                                    26.2.1\n",
      "qgrid                                    1.3.1\n",
      "qtconsole                                5.7.0\n",
      "QtPy                                     2.4.3\n",
      "raft-dask-cu12                           25.6.0\n",
      "rapids-dask-dependency                   25.6.0\n",
      "rapids-logger                            0.1.19\n",
      "ratelim                                  0.1.6\n",
      "ray                                      2.53.0\n",
      "referencing                              0.37.0\n",
      "regex                                    2025.11.3\n",
      "requests                                 2.32.5\n",
      "requests-oauthlib                        2.0.0\n",
      "requests-toolbelt                        1.0.0\n",
      "requirements-parser                      0.9.0\n",
      "rfc3161-client                           1.0.5\n",
      "rfc3339-validator                        0.1.4\n",
      "rfc3986-validator                        0.1.1\n",
      "rfc3987-syntax                           1.1.0\n",
      "rfc8785                                  0.1.4\n",
      "rgf-python                               3.12.0\n",
      "rich                                     14.2.0\n",
      "rmm-cu12                                 25.6.0\n",
      "roman-numerals-py                        3.1.0\n",
      "rouge_score                              0.1.2\n",
      "rpds-py                                  0.27.1\n",
      "rpy2                                     3.5.17\n",
      "rsa                                      4.9.1\n",
      "rtree                                    1.4.1\n",
      "ruamel.yaml                              0.19.1\n",
      "ruff                                     0.14.1\n",
      "s3fs                                     0.4.2\n",
      "s3transfer                               0.16.0\n",
      "safehttpx                                0.1.6\n",
      "safetensors                              0.6.2\n",
      "scikit-image                             0.25.2\n",
      "scikit-learn                             1.6.1\n",
      "scikit-learn-intelex                     2025.10.0\n",
      "scikit-multilearn                        0.2.0\n",
      "scikit-optimize                          0.10.2\n",
      "scikit-plot                              0.3.7\n",
      "scikit-surprise                          1.1.4\n",
      "scipy                                    1.15.3\n",
      "scooby                                   0.10.2\n",
      "scs                                      3.2.9\n",
      "seaborn                                  0.13.2\n",
      "SecretStorage                            3.4.0\n",
      "securesystemslib                         1.3.1\n",
      "segment_anything                         1.0\n",
      "semantic-version                         2.10.0\n",
      "Send2Trash                               1.8.3\n",
      "sentence-transformers                    5.1.1\n",
      "sentencepiece                            0.2.1\n",
      "sentry-sdk                               2.42.1\n",
      "setuptools                               75.2.0\n",
      "setuptools-scm                           9.2.2\n",
      "shap                                     0.49.1\n",
      "shapely                                  2.1.2\n",
      "shellingham                              1.5.4\n",
      "Shimmy                                   1.3.0\n",
      "sigstore                                 4.1.0\n",
      "sigstore-models                          0.0.5\n",
      "sigstore-rekor-types                     0.0.18\n",
      "simpervisor                              1.0.0\n",
      "simple-parsing                           0.1.7\n",
      "simpleitk                                2.5.3\n",
      "simplejson                               3.20.2\n",
      "simsimd                                  6.5.3\n",
      "siphash24                                1.8\n",
      "six                                      1.17.0\n",
      "sklearn-pandas                           2.2.0\n",
      "slicer                                   0.0.8\n",
      "smart_open                               7.4.0\n",
      "smmap                                    5.0.2\n",
      "sniffio                                  1.3.1\n",
      "snowballstemmer                          3.0.1\n",
      "sortedcontainers                         2.4.0\n",
      "soundfile                                0.13.1\n",
      "soupsieve                                2.8\n",
      "soxr                                     1.0.0\n",
      "spacy                                    3.8.7\n",
      "spacy-legacy                             3.0.12\n",
      "spacy-loggers                            1.0.5\n",
      "spanner-graph-notebook                   1.1.8\n",
      "Sphinx                                   8.2.3\n",
      "sphinx-rtd-theme                         0.2.4\n",
      "sphinxcontrib-applehelp                  2.0.0\n",
      "sphinxcontrib-devhelp                    2.0.0\n",
      "sphinxcontrib-htmlhelp                   2.1.0\n",
      "sphinxcontrib-jsmath                     1.0.1\n",
      "sphinxcontrib-qthelp                     2.0.0\n",
      "sphinxcontrib-serializinghtml            2.0.0\n",
      "SQLAlchemy                               2.0.44\n",
      "sqlalchemy-spanner                       1.17.0\n",
      "sqlglot                                  25.20.2\n",
      "sqlparse                                 0.5.3\n",
      "squarify                                 0.4.4\n",
      "srsly                                    2.5.1\n",
      "sse-starlette                            3.0.2\n",
      "sseclient-py                             1.9.0\n",
      "stable-baselines3                        2.1.0\n",
      "stanio                                   0.5.1\n",
      "starlette                                0.50.0\n",
      "statsmodels                              0.14.5\n",
      "stopit                                   1.1.2\n",
      "stringzilla                              4.2.1\n",
      "stumpy                                   1.13.0\n",
      "sympy                                    1.13.3\n",
      "tables                                   3.10.2\n",
      "tabulate                                 0.9.0\n",
      "tbb                                      2022.2.0\n",
      "tblib                                    3.1.0\n",
      "tcmlib                                   1.4.0\n",
      "tenacity                                 9.1.2\n",
      "tensorboard                              2.19.0\n",
      "tensorboard-data-server                  0.7.2\n",
      "tensorflow                               2.19.0\n",
      "tensorflow-cloud                         0.1.5\n",
      "tensorflow-datasets                      4.9.9\n",
      "tensorflow_decision_forests              1.12.0\n",
      "tensorflow-hub                           0.16.1\n",
      "tensorflow-io                            0.37.1\n",
      "tensorflow-io-gcs-filesystem             0.37.1\n",
      "tensorflow-metadata                      1.17.2\n",
      "tensorflow-probability                   0.25.0\n",
      "tensorflow-text                          2.19.0\n",
      "tensorstore                              0.1.78\n",
      "termcolor                                3.1.0\n",
      "terminado                                0.18.1\n",
      "testpath                                 0.6.0\n",
      "text-unidecode                           1.3\n",
      "textblob                                 0.19.0\n",
      "texttable                                1.7.0\n",
      "tf_keras                                 2.19.0\n",
      "tf-slim                                  1.1.0\n",
      "thinc                                    8.3.6\n",
      "threadpoolctl                            3.6.0\n",
      "tifffile                                 2025.10.16\n",
      "tiktoken                                 0.12.0\n",
      "timm                                     1.0.20\n",
      "tinycss2                                 1.4.0\n",
      "tokenizers                               0.22.1\n",
      "toml                                     0.10.2\n",
      "tomlkit                                  0.13.3\n",
      "toolz                                    0.12.1\n",
      "torch                                    2.8.0+cu126\n",
      "torchao                                  0.10.0\n",
      "torchaudio                               2.8.0+cu126\n",
      "torchdata                                0.11.0\n",
      "torchinfo                                1.8.0\n",
      "torchmetrics                             1.8.2\n",
      "torchsummary                             1.5.1\n",
      "torchtune                                0.6.1\n",
      "torchvision                              0.23.0+cu126\n",
      "tornado                                  6.5.1\n",
      "TPOT                                     0.12.2\n",
      "tqdm                                     4.67.1\n",
      "traitlets                                5.7.1\n",
      "traittypes                               0.2.1\n",
      "transformers                             4.57.1\n",
      "treelite                                 4.4.1\n",
      "treescope                                0.1.10\n",
      "triton                                   3.4.0\n",
      "trx-python                               0.3\n",
      "tsfresh                                  0.21.1\n",
      "tuf                                      6.0.0\n",
      "tweepy                                   4.16.0\n",
      "typeguard                                4.4.4\n",
      "typer                                    0.20.0\n",
      "typer-slim                               0.21.1\n",
      "types-pytz                               2025.2.0.20250809\n",
      "types-setuptools                         80.9.0.20250822\n",
      "typing_extensions                        4.15.0\n",
      "typing-inspect                           0.9.0\n",
      "typing-inspection                        0.4.2\n",
      "tzdata                                   2025.2\n",
      "tzlocal                                  5.3.1\n",
      "uc-micro-py                              1.0.3\n",
      "ucx-py-cu12                              0.44.0\n",
      "ucxx-cu12                                0.44.0\n",
      "ujson                                    5.11.0\n",
      "umap-learn                               0.5.9.post2\n",
      "umf                                      0.11.0\n",
      "update-checker                           0.18.0\n",
      "uri-template                             1.3.0\n",
      "uritemplate                              4.2.0\n",
      "urllib3                                  2.6.3\n",
      "urwid                                    3.0.3\n",
      "urwid_readline                           0.15.1\n",
      "uvicorn                                  0.38.0\n",
      "vega-datasets                            0.9.0\n",
      "visions                                  0.8.1\n",
      "vtk                                      9.3.1\n",
      "wadllib                                  1.3.6\n",
      "Wand                                     0.6.13\n",
      "wandb                                    0.22.2\n",
      "wasabi                                   1.1.3\n",
      "watchdog                                 6.0.0\n",
      "wavio                                    0.0.9\n",
      "wcwidth                                  0.2.14\n",
      "weasel                                   0.4.1\n",
      "webcolors                                24.11.1\n",
      "webencodings                             0.5.1\n",
      "websocket-client                         1.9.0\n",
      "websockets                               15.0.1\n",
      "Werkzeug                                 3.1.3\n",
      "wheel                                    0.45.1\n",
      "widgetsnbextension                       4.0.15\n",
      "woodwork                                 0.31.0\n",
      "wordcloud                                1.9.4\n",
      "wrapt                                    2.0.0\n",
      "wurlitzer                                3.1.1\n",
      "xarray                                   2025.10.1\n",
      "xarray-einstats                          0.9.1\n",
      "xgboost                                  3.1.0\n",
      "xlrd                                     2.0.2\n",
      "xvfbwrapper                              0.2.18\n",
      "xxhash                                   3.6.0\n",
      "xyzservices                              2025.4.0\n",
      "y-py                                     0.6.2\n",
      "yarl                                     1.22.0\n",
      "ydata-profiling                          4.18.1\n",
      "ydf                                      0.13.0\n",
      "yellowbrick                              1.5\n",
      "yfinance                                 0.2.66\n",
      "ypy-websocket                            0.8.4\n",
      "zict                                     3.0.0\n",
      "zipp                                     3.23.0\n",
      "zstandard                                0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7531433-318e-4795-8e50-f27ae7ff1c45",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e8f2-7cd6-4227-b60f-8f388e101915",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4bf76-69a7-4d55-854b-6971b01c6f5f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c69ed34",
   "metadata": {},
   "source": [
    "## Step 3: Clone llama.cpp (Latest Stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5fa6016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:12:38.309456Z",
     "iopub.status.busy": "2026-01-16T20:12:38.308655Z",
     "iopub.status.idle": "2026-01-16T20:12:42.563774Z",
     "shell.execute_reply": "2026-01-16T20:12:42.563048Z",
     "shell.execute_reply.started": "2026-01-16T20:12:38.309427Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning llama.cpp...\n",
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 2395, done.\u001b[K\n",
      "remote: Counting objects: 100% (2395/2395), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1875/1875), done.\u001b[K\n",
      "remote: Total 2395 (delta 518), reused 1568 (delta 448), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (2395/2395), 27.25 MiB | 18.04 MiB/s, done.\n",
      "Resolving deltas: 100% (518/518), done.\n",
      "\n",
      "üì¶ llama.cpp Version:\n",
      "\u001b[33m388ce82\u001b[m\u001b[33m (\u001b[m\u001b[1;34mgrafted\u001b[m\u001b[33m, \u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;33mtag: b7760\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m ggml : extend ggml_pool_1d + metal (#16429)\n",
      "b7760\n",
      "CPU times: user 67.5 ms, sys: 53.2 ms, total: 121 ms\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Clean any previous build\n",
    "!rm -rf llama.cpp\n",
    "\n",
    "# Clone llama.cpp\n",
    "print(\"Cloning llama.cpp...\")\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
    "\n",
    "os.chdir(\"llama.cpp\")\n",
    "\n",
    "# Get commit info\n",
    "print(\"\\nüì¶ llama.cpp Version:\")\n",
    "!git log -1 --oneline\n",
    "!git describe --tags --always 2>/dev/null || echo \"(no tag)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa16d20",
   "metadata": {},
   "source": [
    "## Step 4: Configure CMake for Dual T4 (SM 7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf67503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:15:56.503443Z",
     "iopub.status.busy": "2026-01-16T20:15:56.502447Z",
     "iopub.status.idle": "2026-01-16T20:16:01.065089Z",
     "shell.execute_reply": "2026-01-16T20:16:01.064255Z",
     "shell.execute_reply.started": "2026-01-16T20:15:56.503406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: CREATE CUDA DRIVER STUB + CONFIGURE CMAKE (VMM DISABLED)\n",
      "======================================================================\n",
      "\n",
      "üîß Creating CUDA driver stub library...\n",
      "   ‚úÖ Created libcuda.so stub (16424 bytes) in /kaggle/working/cuda_stubs\n",
      "total 32\n",
      "drwxr-xr-x 2 root root  4096 Jan 16 20:15 .\n",
      "drwxr-xr-x 5 root root  4096 Jan 16 20:12 ..\n",
      "-rw-r--r-- 1 root root  1053 Jan 16 20:15 cuda_stub.c\n",
      "-rwxr-xr-x 1 root root 16424 Jan 16 20:15 libcuda.so\n",
      "lrwxrwxrwx 1 root root    37 Jan 16 20:15 libcuda.so.1 -> /kaggle/working/cuda_stubs/libcuda.so\n",
      "\n",
      "üì¶ CMake Configuration:\n",
      "   Target: SM 7.5 (Tesla T4)\n",
      "   FlashAttention: All quantization types\n",
      "   CUDA VMM: DISABLED via -DGGML_CUDA_NO_VMM compile flag\n",
      "   Static linking: Enabled\n",
      "   CUDA stub path: /kaggle/working/cuda_stubs\n",
      "\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/gcc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n",
      "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
      "-- CUDA Toolkit found\n",
      "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=\n",
      "-- CUDA host compiler is GNU 11.4.0\n",
      "-- Including CUDA backend\n",
      "-- ggml version: 0.9.5\n",
      "-- ggml commit:  388ce82\n",
      "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
      "-- OpenSSL found: 3.0.2\n",
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done (3.5s)\n",
      "\u001b[33mCMake Warning at examples/batched/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-batched because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/debug/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-debug because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/embedding/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-embedding\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/eval-callback/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-eval-callback\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/gguf-hash/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-gguf-hash\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/gguf/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-gguf because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/idle/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-idle because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/lookahead/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-lookahead\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-lookup because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:8 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-lookup-create\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:14 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-lookup-merge\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/lookup/CMakeLists.txt:20 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-lookup-stats\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/parallel/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-parallel\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/passkey/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-passkey because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/retrieval/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-retrieval\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/save-load-state/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-save-load-state\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/simple/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-simple because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/simple-chat/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-simple-chat\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/speculative/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-speculative\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/speculative-simple/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target\n",
      "  llama-speculative-simple because files in some directories may conflict\n",
      "  with libraries in implicit directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/gen-docs/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-gen-docs\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/training/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-finetune\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/diffusion/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-diffusion-cli\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at examples/convert-llama2c-to-ggml/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target\n",
      "  llama-convert-llama2c-to-ggml because files in some directories may\n",
      "  conflict with libraries in implicit directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at pocs/vdot/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-vdot because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at pocs/vdot/CMakeLists.txt:7 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-q8dot because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/batched-bench/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-batched-bench\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/gguf-split/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-gguf-split\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/imatrix/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-imatrix because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/llama-bench/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-bench because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/completion/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-completion\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/perplexity/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-perplexity\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/quantize/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-quantize\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/cli/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-cli because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/server/CMakeLists.txt:59 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-server because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/tokenize/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-tokenize\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/tts/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-tts because\n",
      "  files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/mtmd/CMakeLists.txt:89 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-mtmd-cli\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/cvector-generator/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target\n",
      "  llama-cvector-generator because files in some directories may conflict with\n",
      "  libraries in implicit directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/export-lora/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-export-lora\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[33mCMake Warning at tools/fit-params/CMakeLists.txt:2 (add_executable):\n",
      "  Cannot generate a safe runtime search path for target llama-fit-params\n",
      "  because files in some directories may conflict with libraries in implicit\n",
      "  directories:\n",
      "\n",
      "    runtime library [libcuda.so] in /kaggle/working/cuda_stubs may be hidden by files in:\n",
      "      /usr/local/cuda-12.5/targets/x86_64-linux/lib\n",
      "\n",
      "  Some of these libraries may not be found correctly.\n",
      "\n",
      "\u001b[0m\n",
      "-- Generating done (0.2s)\n",
      "-- Build files have been written to: /kaggle/working/llama.cpp/build\n",
      "\n",
      "‚úÖ CMake configuration complete!\n",
      "CPU times: user 63.3 ms, sys: 57.9 ms, total: 121 ms\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Clean previous build\n",
    "!rm -rf build\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: CREATE CUDA DRIVER STUB + CONFIGURE CMAKE (VMM DISABLED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL FIX: Create libcuda.so stub in WRITABLE location\n",
    "# Kaggle's /usr/local/cuda is read-only, so we use /kaggle/working/\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Creating CUDA driver stub library...\")\n",
    "\n",
    "STUBS_DIR = \"/kaggle/working/cuda_stubs\"\n",
    "os.makedirs(STUBS_DIR, exist_ok=True)\n",
    "\n",
    "# Create a minimal C file that provides empty symbols for libcuda.so\n",
    "# NOTE: We disable VMM via -DGGML_CUDA_NO_VMM compile flag so we don't need\n",
    "# the advanced memory management APIs (cuMemCreate, cuMemMap, etc.)\n",
    "stub_code = '''\n",
    "// Minimal CUDA driver stub for linking purposes only\n",
    "// At runtime, the real driver is used\n",
    "\n",
    "void* cuGetErrorString = 0;\n",
    "void* cuGetErrorName = 0;\n",
    "void* cuInit = 0;\n",
    "void* cuDriverGetVersion = 0;\n",
    "void* cuDeviceGet = 0;\n",
    "void* cuDeviceGetCount = 0;\n",
    "void* cuDeviceGetName = 0;\n",
    "void* cuDeviceGetAttribute = 0;\n",
    "void* cuDeviceTotalMem = 0;\n",
    "void* cuDeviceGetUuid = 0;\n",
    "void* cuCtxCreate = 0;\n",
    "void* cuCtxDestroy = 0;\n",
    "void* cuCtxGetCurrent = 0;\n",
    "void* cuCtxSetCurrent = 0;\n",
    "void* cuCtxPushCurrent = 0;\n",
    "void* cuCtxPopCurrent = 0;\n",
    "void* cuCtxSynchronize = 0;\n",
    "void* cuMemAlloc = 0;\n",
    "void* cuMemFree = 0;\n",
    "void* cuMemcpy = 0;\n",
    "void* cuMemcpyHtoD = 0;\n",
    "void* cuMemcpyDtoH = 0;\n",
    "void* cuMemcpyDtoD = 0;\n",
    "void* cuMemsetD8 = 0;\n",
    "void* cuMemsetD32 = 0;\n",
    "void* cuModuleLoad = 0;\n",
    "void* cuModuleUnload = 0;\n",
    "void* cuModuleGetFunction = 0;\n",
    "void* cuLaunchKernel = 0;\n",
    "void* cuStreamCreate = 0;\n",
    "void* cuStreamDestroy = 0;\n",
    "void* cuStreamSynchronize = 0;\n",
    "void* cuEventCreate = 0;\n",
    "void* cuEventDestroy = 0;\n",
    "void* cuEventRecord = 0;\n",
    "void* cuEventSynchronize = 0;\n",
    "void* cuEventElapsedTime = 0;\n",
    "'''\n",
    "\n",
    "# Write stub source\n",
    "stub_c_path = f\"{STUBS_DIR}/cuda_stub.c\"\n",
    "with open(stub_c_path, \"w\") as f:\n",
    "    f.write(stub_code)\n",
    "\n",
    "# Compile to shared library\n",
    "stub_so_path = f\"{STUBS_DIR}/libcuda.so\"\n",
    "!gcc -shared -fPIC -o {stub_so_path} {stub_c_path}\n",
    "\n",
    "# Also create libcuda.so.1 symlink (some builds look for this)\n",
    "!ln -sf {stub_so_path} {STUBS_DIR}/libcuda.so.1\n",
    "\n",
    "# Verify the stub was created\n",
    "if os.path.exists(stub_so_path):\n",
    "    size = os.path.getsize(stub_so_path)\n",
    "    print(f\"   ‚úÖ Created libcuda.so stub ({size} bytes) in {STUBS_DIR}\")\n",
    "    !ls -la {STUBS_DIR}\n",
    "else:\n",
    "    print(\"   ‚ùå Failed to create stub!\")\n",
    "\n",
    "# Set environment variables for linker\n",
    "os.environ[\"LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LIBRARY_PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "# ============================================================================\n",
    "# CMake Configuration with explicit stub path + VMM DISABLED via compile flag\n",
    "# ============================================================================\n",
    "print(\"\\nüì¶ CMake Configuration:\")\n",
    "print(\"   Target: SM 7.5 (Tesla T4)\")\n",
    "print(\"   FlashAttention: All quantization types\")\n",
    "print(\"   CUDA VMM: DISABLED via -DGGML_CUDA_NO_VMM compile flag\")\n",
    "print(\"   Static linking: Enabled\")\n",
    "print(f\"   CUDA stub path: {STUBS_DIR}\")\n",
    "print(\"\")\n",
    "\n",
    "# Pass the stubs directory to CMake\n",
    "# CRITICAL: -DGGML_CUDA_NO_VMM disables Virtual Memory Management at compile time\n",
    "# This avoids needing cuMemCreate, cuMemMap, cuMemUnmap, cuMemAddressReserve, etc.\n",
    "cmake_cmd = f\"\"\"\n",
    "cmake -B build -G Ninja \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DBUILD_SHARED_LIBS=OFF \\\n",
    "    -DLLAMA_BUILD_EXAMPLES=ON \\\n",
    "    -DLLAMA_BUILD_TESTS=OFF \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DCMAKE_C_COMPILER=gcc \\\n",
    "    -DCMAKE_CXX_COMPILER=g++ \\\n",
    "    -DCMAKE_C_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_CXX_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_CUDA_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_LIBRARY_PATH=\"{STUBS_DIR}\" \\\n",
    "    -DCUDAToolkit_LIBRARY_DIR=\"{STUBS_DIR}\"\n",
    "\"\"\"\n",
    "\n",
    "!{cmake_cmd}\n",
    "\n",
    "# Verify configuration succeeded\n",
    "import subprocess\n",
    "result = subprocess.run([\"test\", \"-f\", \"build/build.ninja\"], capture_output=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"\\n‚úÖ CMake configuration complete!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CMake configuration failed - check errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267abd9c",
   "metadata": {},
   "source": [
    "## Step 5: Build llama.cpp (This takes ~8-12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0d6ba20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:17:34.966096Z",
     "iopub.status.busy": "2026-01-16T20:17:34.965247Z",
     "iopub.status.idle": "2026-01-16T20:37:17.990354Z",
     "shell.execute_reply": "2026-01-16T20:37:17.989635Z",
     "shell.execute_reply.started": "2026-01-16T20:17:34.966062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building with 4 parallel jobs...\n",
      "This will take approximately 8-12 minutes.\n",
      "\n",
      "[1/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n",
      "[2/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n",
      "[3/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\n",
      "[4/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n",
      "[5/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n",
      "[6/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n",
      "[7/471] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n",
      "[8/471] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n",
      "[9/471] Linking CXX static library ggml/src/libggml-base.a\n",
      "[10/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\n",
      "[11/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n",
      "[12/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\n",
      "[13/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n",
      "[14/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\n",
      "[15/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\n",
      "[16/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n",
      "[17/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\n",
      "[18/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n",
      "[19/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\n",
      "[20/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\n",
      "[21/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n",
      "[22/471] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\n",
      "[23/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\n",
      "[24/471] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\n",
      "[25/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\n",
      "[26/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\n",
      "[27/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\n",
      "[28/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\n",
      "[29/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\n",
      "[30/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\n",
      "[31/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\n",
      "[32/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\n",
      "[33/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\n",
      "[34/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\n",
      "[35/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\n",
      "[36/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\n",
      "[37/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\n",
      "[38/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\n",
      "[39/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\n",
      "[40/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\n",
      "[41/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\n",
      "[42/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\n",
      "[43/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\n",
      "[44/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\n",
      "[45/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\n",
      "[46/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\n",
      "[47/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\n",
      "[48/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\n",
      "[49/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\n",
      "[50/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\n",
      "[51/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\n",
      "[52/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\n",
      "[53/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\n",
      "[54/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\n",
      "[55/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\n",
      "[56/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\n",
      "[57/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\n",
      "[58/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\n",
      "[59/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\n",
      "[60/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\n",
      "[61/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\n",
      "[62/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\n",
      "[63/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\n",
      "[64/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\n",
      "[65/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\n",
      "[66/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\n",
      "[67/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\n",
      "[68/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\n",
      "[69/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\n",
      "[70/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\n",
      "[71/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\n",
      "[72/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\n",
      "[73/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\n",
      "[74/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\n",
      "[75/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\n",
      "[76/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\n",
      "[77/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\n",
      "[78/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\n",
      "[79/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\n",
      "[80/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\n",
      "[81/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\n",
      "[82/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\n",
      "[83/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\n",
      "[84/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\n",
      "[85/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\n",
      "[86/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\n",
      "[87/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\n",
      "[88/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\n",
      "[89/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\n",
      "[90/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\n",
      "[91/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\n",
      "[92/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\n",
      "[93/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\n",
      "[94/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\n",
      "[95/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\n",
      "[96/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\n",
      "[97/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\n",
      "[98/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\n",
      "[99/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\n",
      "[100/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\n",
      "[101/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\n",
      "[102/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\n",
      "[103/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\n",
      "[104/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\n",
      "[105/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\n",
      "[106/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\n",
      "[107/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\n",
      "[108/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\n",
      "[109/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\n",
      "[110/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\n",
      "[111/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\n",
      "[112/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\n",
      "[113/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\n",
      "[114/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\n",
      "[115/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\n",
      "[116/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\n",
      "[117/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\n",
      "[118/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\n",
      "[119/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\n",
      "[120/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\n",
      "[121/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\n",
      "[122/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\n",
      "[123/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\n",
      "[124/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\n",
      "[125/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\n",
      "[126/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\n",
      "[127/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\n",
      "[128/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\n",
      "[129/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\n",
      "[130/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\n",
      "[131/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\n",
      "[132/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\n",
      "[133/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\n",
      "[134/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\n",
      "[135/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\n",
      "[136/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\n",
      "[137/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\n",
      "[138/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\n",
      "[139/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\n",
      "[140/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\n",
      "[141/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\n",
      "[142/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\n",
      "[143/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\n",
      "[144/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\n",
      "[145/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\n",
      "[146/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\n",
      "[147/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\n",
      "[148/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\n",
      "[149/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_1.cu.o\n",
      "[150/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_0.cu.o\n",
      "[151/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_1.cu.o\n",
      "[152/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_0.cu.o\n",
      "[153/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q8_0.cu.o\n",
      "[154/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-f16.cu.o\n",
      "[155/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_1.cu.o\n",
      "[156/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\n",
      "[157/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_1.cu.o\n",
      "[158/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_0.cu.o\n",
      "[159/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-f16.cu.o\n",
      "[160/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q8_0.cu.o\n",
      "[161/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_1.cu.o\n",
      "[162/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_0.cu.o\n",
      "[163/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_1.cu.o\n",
      "[164/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_0.cu.o\n",
      "[165/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q8_0.cu.o\n",
      "[166/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-f16.cu.o\n",
      "[167/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_1.cu.o\n",
      "[168/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_0.cu.o\n",
      "[169/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_1.cu.o\n",
      "[170/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_0.cu.o\n",
      "[171/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-f16.cu.o\n",
      "[172/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q8_0.cu.o\n",
      "[173/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_0.cu.o\n",
      "[174/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_1.cu.o\n",
      "[175/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_1.cu.o\n",
      "[176/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_0.cu.o\n",
      "[177/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-f16.cu.o\n",
      "[178/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q8_0.cu.o\n",
      "[179/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_1.cu.o\n",
      "[180/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_0.cu.o\n",
      "[181/471] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n",
      "[182/471] Linking CXX static library ggml/src/libggml-cpu.a\n",
      "[183/471] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n",
      "[184/471] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n",
      "[185/471] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n",
      "[186/471] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n",
      "[187/471] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n",
      "[188/471] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n",
      "[189/471] Building CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\n",
      "[190/471] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n",
      "[191/471] Building CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\n",
      "[192/471] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n",
      "[193/471] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n",
      "[194/471] Building CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\n",
      "[195/471] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n",
      "[196/471] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\n",
      "[197/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\n",
      "[198/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\n",
      "[199/471] Building CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\n",
      "[200/471] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n",
      "[201/471] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n",
      "[202/471] Building CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\n",
      "[203/471] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n",
      "[204/471] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n",
      "[205/471] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n",
      "[206/471] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n",
      "[207/471] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n",
      "[208/471] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n",
      "[209/471] Building CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\n",
      "[210/471] Building CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\n",
      "[211/471] Building CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\n",
      "[212/471] Building CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\n",
      "[213/471] Building CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\n",
      "[214/471] Building CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\n",
      "[215/471] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\n",
      "[216/471] Building CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\n",
      "[217/471] Building CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\n",
      "[218/471] Building CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\n",
      "[219/471] Building CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\n",
      "[220/471] Building CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\n",
      "[221/471] Building CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\n",
      "[222/471] Building CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\n",
      "[223/471] Building CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\n",
      "[224/471] Building CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\n",
      "[225/471] Building CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\n",
      "[226/471] Building CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\n",
      "[227/471] Building CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\n",
      "[228/471] Building CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\n",
      "[229/471] Building CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\n",
      "[230/471] Building CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\n",
      "[231/471] Building CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\n",
      "[232/471] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\n",
      "[233/471] Building CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\n",
      "[234/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\n",
      "[235/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\n",
      "[236/471] Building CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\n",
      "[237/471] Building CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\n",
      "[238/471] Building CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\n",
      "[239/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\n",
      "[240/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\n",
      "[241/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\n",
      "[242/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\n",
      "[243/471] Building CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\n",
      "[244/471] Building CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\n",
      "[245/471] Building CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\n",
      "[246/471] Building CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\n",
      "[247/471] Building CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\n",
      "[248/471] Building CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\n",
      "[249/471] Building CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\n",
      "[250/471] Building CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\n",
      "[251/471] Building CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\n",
      "[252/471] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\n",
      "[253/471] Building CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\n",
      "[254/471] Building CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\n",
      "[255/471] Building CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\n",
      "[256/471] Building CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\n",
      "[257/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_1.cu.o\n",
      "[258/471] Building CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\n",
      "[259/471] Building CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\n",
      "[260/471] Building CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\n",
      "[261/471] Building CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\n",
      "[262/471] Building CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\n",
      "[263/471] Building CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\n",
      "[264/471] Building CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\n",
      "[265/471] Building CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\n",
      "[266/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_0.cu.o\n",
      "[267/471] Building CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\n",
      "[268/471] Building CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\n",
      "[269/471] Building CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\n",
      "[270/471] Building CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\n",
      "[271/471] Building CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\n",
      "[272/471] Building CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\n",
      "[273/471] Building CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\n",
      "[274/471] Building CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\n",
      "[275/471] Building CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\n",
      "[276/471] Building CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\n",
      "[277/471] Building CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\n",
      "[278/471] Building CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\n",
      "[279/471] Building CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\n",
      "[280/471] Building CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\n",
      "[281/471] Building CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\n",
      "[282/471] Building CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\n",
      "[283/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\n",
      "[284/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\n",
      "[285/471] Building CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\n",
      "[286/471] Building CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\n",
      "[287/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\n",
      "[288/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\n",
      "[289/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\n",
      "[290/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\n",
      "[291/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\n",
      "[292/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\n",
      "[293/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\n",
      "[294/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\n",
      "[295/471] Building CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\n",
      "[296/471] Building CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\n",
      "[297/471] Building CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\n",
      "[298/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\n",
      "[299/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\n",
      "[300/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\n",
      "[301/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\n",
      "[302/471] Building CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\n",
      "[303/471] Building CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\n",
      "[304/471] Building CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\n",
      "[305/471] Building CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\n",
      "[306/471] Building CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\n",
      "[307/471] Building CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\n",
      "[308/471] Building CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\n",
      "[309/471] Building CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\n",
      "[310/471] Building CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\n",
      "[311/471] Building CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\n",
      "[312/471] Building CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\n",
      "[313/471] Building CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\n",
      "[314/471] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n",
      "[315/471] Building CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\n",
      "[316/471] Building CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\n",
      "[317/471] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n",
      "[318/471] Building CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\n",
      "[319/471] Building CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\n",
      "[320/471] Building CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\n",
      "[321/471] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n",
      "[322/471] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n",
      "[323/471] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n",
      "[324/471] Building CXX object common/CMakeFiles/common.dir/debug.cpp.o\n",
      "[325/471] Building CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\n",
      "[326/471] Building CXX object common/CMakeFiles/common.dir/download.cpp.o\n",
      "[327/471] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n",
      "[328/471] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n",
      "[329/471] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n",
      "[330/471] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n",
      "[331/471] Building CXX object common/CMakeFiles/common.dir/preset.cpp.o\n",
      "[332/471] Building CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\n",
      "[333/471] Building CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\n",
      "[334/471] Building CXX object common/CMakeFiles/common.dir/unicode.cpp.o\n",
      "[335/471] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n",
      "[336/471] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n",
      "[337/471] Building CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\n",
      "[338/471] Building CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\n",
      "[339/471] Building CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\n",
      "[340/471] Building CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\n",
      "[341/471] Building CXX object common/CMakeFiles/common.dir/__/license.cpp.o\n",
      "[342/471] Building CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\n",
      "[343/471] Linking CXX static library vendor/cpp-httplib/libcpp-httplib.a\n",
      "[344/471] Building CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\n",
      "[345/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\n",
      "[346/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\n",
      "[347/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\n",
      "[348/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\n",
      "[349/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\n",
      "[350/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\n",
      "[351/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\n",
      "[352/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\n",
      "[353/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\n",
      "[354/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\n",
      "[355/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\n",
      "[356/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\n",
      "[357/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\n",
      "[358/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\n",
      "[359/471] Building CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\n",
      "[360/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\n",
      "[361/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\n",
      "[362/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\n",
      "[363/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\n",
      "[364/471] Building CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\n",
      "[365/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\n",
      "[366/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\n",
      "[367/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\n",
      "[368/471] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\n",
      "[369/471] Building CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\n",
      "[370/471] Building CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\n",
      "[371/471] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\n",
      "[372/471] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\n",
      "[373/471] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\n",
      "[374/471] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\n",
      "[375/471] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\n",
      "[376/471] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\n",
      "[377/471] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\n",
      "[378/471] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\n",
      "[379/471] Building CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\n",
      "[380/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\n",
      "[381/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\n",
      "[382/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\n",
      "[383/471] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\n",
      "[384/471] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\n",
      "[385/471] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\n",
      "[386/471] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\n",
      "[387/471] Linking CXX static library ggml/src/ggml-cuda/libggml-cuda.a\n",
      "[388/471] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\n",
      "[389/471] Linking CXX static library ggml/src/libggml.a\n",
      "[390/471] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\n",
      "[391/471] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\n",
      "[392/471] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\n",
      "[393/471] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\n",
      "[394/471] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\n",
      "[395/471] Linking CXX static library src/libllama.a\n",
      "[396/471] Building CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\n",
      "[397/471] Linking CXX static library common/libcommon.a\n",
      "[398/471] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\n",
      "[399/471] Linking CXX static library tools/mtmd/libmtmd.a\n",
      "[400/471] Building CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\n",
      "[401/471] Linking CXX static library tools/server/libserver-context.a\n",
      "[402/471] Building CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\n",
      "[403/471] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\n",
      "[404/471] Generating loading.html.hpp\n",
      "[405/471] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\n",
      "[406/471] Building CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\n",
      "[407/471] Building CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\n",
      "[408/471] Building CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\n",
      "[409/471] Building CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\n",
      "[410/471] Building CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\n",
      "[411/471] Building CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n",
      "[412/471] Building CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\n",
      "[413/471] Building CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\n",
      "[414/471] Building CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\n",
      "[415/471] Building CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\n",
      "[416/471] Building CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\n",
      "[417/471] Building CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\n",
      "[418/471] Building CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\n",
      "[419/471] Building CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n",
      "[420/471] Building CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\n",
      "[421/471] Building CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\n",
      "[422/471] Building CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\n",
      "[423/471] Linking CXX executable bin/llama-batched\n",
      "[424/471] Linking CXX executable bin/llama-embedding\n",
      "[425/471] Linking CXX executable bin/llama-debug\n",
      "[426/471] Linking CXX executable bin/llama-gguf-hash\n",
      "[427/471] Linking CXX executable bin/llama-gguf\n",
      "[428/471] Generating index.html.gz.hpp\n",
      "[429/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n",
      "[430/471] Linking CXX executable bin/llama-eval-callback\n",
      "[431/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n",
      "[432/471] Linking CXX executable bin/llama-idle\n",
      "[433/471] Linking CXX executable bin/llama-lookahead\n",
      "[434/471] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n",
      "[435/471] Linking CXX executable bin/llama-lookup-merge\n",
      "[436/471] Linking CXX executable bin/llama-lookup\n",
      "[437/471] Linking CXX executable bin/llama-lookup-create\n",
      "[438/471] Linking CXX executable bin/llama-lookup-stats\n",
      "[439/471] Linking CXX executable bin/llama-parallel\n",
      "[440/471] Linking CXX executable bin/llama-passkey\n",
      "[441/471] Linking CXX executable bin/llama-retrieval\n",
      "[442/471] Linking CXX executable bin/llama-save-load-state\n",
      "[443/471] Linking CXX executable bin/llama-simple\n",
      "[444/471] Linking CXX executable bin/llama-simple-chat\n",
      "[445/471] Linking CXX executable bin/llama-speculative\n",
      "[446/471] Linking CXX executable bin/llama-speculative-simple\n",
      "[447/471] Linking CXX executable bin/llama-finetune\n",
      "[448/471] Linking CXX executable bin/llama-gen-docs\n",
      "[449/471] Linking CXX executable bin/llama-vdot\n",
      "[450/471] Linking CXX executable bin/llama-q8dot\n",
      "[451/471] Linking CXX executable bin/llama-diffusion-cli\n",
      "[452/471] Linking CXX executable bin/llama-convert-llama2c-to-ggml\n",
      "[453/471] Linking CXX executable bin/llama-gguf-split\n",
      "[454/471] Linking CXX executable bin/llama-batched-bench\n",
      "[455/471] Linking CXX executable bin/llama-bench\n",
      "[456/471] Linking CXX executable bin/llama-imatrix\n",
      "[457/471] Linking CXX executable bin/llama-quantize\n",
      "[458/471] Linking CXX executable bin/llama-completion\n",
      "[459/471] Linking CXX executable bin/llama-perplexity\n",
      "[460/471] Linking CXX executable bin/llama-cli\n",
      "[461/471] Linking CXX executable bin/llama-llava-cli\n",
      "[462/471] Linking CXX executable bin/llama-gemma3-cli\n",
      "[463/471] Linking CXX executable bin/llama-minicpmv-cli\n",
      "[464/471] Linking CXX executable bin/llama-qwen2vl-cli\n",
      "[465/471] Linking CXX executable bin/llama-tokenize\n",
      "[466/471] Linking CXX executable bin/llama-server\n",
      "[467/471] Linking CXX executable bin/llama-tts\n",
      "[468/471] Linking CXX executable bin/llama-mtmd-cli\n",
      "[469/471] Linking CXX executable bin/llama-cvector-generator\n",
      "[470/471] Linking CXX executable bin/llama-fit-params\n",
      "[471/471] Linking CXX executable bin/llama-export-lora\n",
      "\n",
      "============================================================\n",
      "‚úÖ BUILD COMPLETE!\n",
      "============================================================\n",
      "-rwxr-xr-x 1 root root 232M Jan 16 20:37 build/bin/llama-server\n",
      "CPU times: user 299 ms, sys: 90.9 ms, total: 390 ms\n",
      "Wall time: 19min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Get CPU count for parallel build\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Building with {cpu_count} parallel jobs...\")\n",
    "print(\"This will take approximately 8-12 minutes.\\n\")\n",
    "\n",
    "# Build\n",
    "build_result = os.system(f\"cmake --build build --config Release -j{cpu_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify build succeeded\n",
    "if build_result == 0 and os.path.exists(\"build/bin/llama-server\"):\n",
    "    print(\"‚úÖ BUILD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    !ls -lh build/bin/llama-server\n",
    "else:\n",
    "    print(\"‚ùå BUILD FAILED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Check the build output above for errors.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dcf23",
   "metadata": {},
   "source": [
    "## Step 6: Verify Built Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d961d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:39:11.218232Z",
     "iopub.status.busy": "2026-01-16T20:39:11.217294Z",
     "iopub.status.idle": "2026-01-16T20:39:12.085514Z",
     "shell.execute_reply": "2026-01-16T20:39:12.084621Z",
     "shell.execute_reply.started": "2026-01-16T20:39:11.218169Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built binaries:\n",
      "============================================================\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-batched\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-batched-bench\n",
      "-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-bench\n",
      "-rwxr-xr-x 1 root root 231M Jan 16 20:37 llama-cli\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-completion\n",
      "-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-convert-llama2c-to-ggml\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-cvector-generator\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-debug\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-diffusion-cli\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-embedding\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-eval-callback\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-export-lora\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-finetune\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-fit-params\n",
      "-rwxr-xr-x 1 root root  17K Jan 16 20:37 llama-gemma3-cli\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:37 llama-gen-docs\n",
      "-rwxr-xr-x 1 root root 683K Jan 16 20:36 llama-gguf\n",
      "-rwxr-xr-x 1 root root 749K Jan 16 20:36 llama-gguf-hash\n",
      "-rwxr-xr-x 1 root root 225M Jan 16 20:37 llama-gguf-split\n",
      "-rwxr-xr-x 1 root root 229M Jan 16 20:36 llama-idle\n",
      "\n",
      "Key binary sizes:\n",
      "232M\tllama-server\n",
      "231M\tllama-cli\n",
      "225M\tllama-quantize\n",
      "\n",
      "Checking CUDA support in llama-server:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Built binaries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh llama-* 2>/dev/null | head -20\n",
    "\n",
    "print(\"\\nKey binary sizes:\")\n",
    "!du -h llama-server llama-cli llama-quantize 2>/dev/null\n",
    "\n",
    "print(\"\\nChecking CUDA support in llama-server:\")\n",
    "!./llama-server --help 2>&1 | grep -i \"cuda\\|gpu\\|ngl\" | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6abd2f",
   "metadata": {},
   "source": [
    "## Step 7: Test Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "201584ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:39:26.684382Z",
     "iopub.status.busy": "2026-01-16T20:39:26.684022Z",
     "iopub.status.idle": "2026-01-16T20:39:27.360666Z",
     "shell.execute_reply": "2026-01-16T20:39:27.359931Z",
     "shell.execute_reply.started": "2026-01-16T20:39:26.684352Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multi-GPU CLI flags:\n",
      "============================================================\n",
      "\n",
      "üìå --tensor-split (VRAM distribution):\n",
      "\n",
      "üìå --split-mode (layer/row splitting):\n",
      "\n",
      "üìå --main-gpu (primary GPU selection):\n",
      "\n",
      "‚úÖ Multi-GPU support confirmed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Testing multi-GPU CLI flags:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for multi-GPU flags\n",
    "print(\"\\nüìå --tensor-split (VRAM distribution):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"tensor-split\"\n",
    "\n",
    "print(\"\\nüìå --split-mode (layer/row splitting):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"split-mode\"\n",
    "\n",
    "print(\"\\nüìå --main-gpu (primary GPU selection):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"main-gpu\"\n",
    "\n",
    "print(\"\\n‚úÖ Multi-GPU support confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0b99",
   "metadata": {},
   "source": [
    "## Step 8: Create llcuda v2.2.0 Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb728ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:39:44.359892Z",
     "iopub.status.busy": "2026-01-16T20:39:44.358897Z",
     "iopub.status.idle": "2026-01-16T20:39:45.853350Z",
     "shell.execute_reply": "2026-01-16T20:39:45.852743Z",
     "shell.execute_reply.started": "2026-01-16T20:39:44.359845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating package: llcuda-v2.2.0-cuda12-kaggle-t4x2\n",
      "============================================================\n",
      "  ‚úÖ llama-server\n",
      "  ‚úÖ llama-cli\n",
      "  ‚úÖ llama-quantize\n",
      "  ‚úÖ llama-gguf\n",
      "  ‚úÖ llama-gguf-hash\n",
      "  ‚úÖ llama-gguf-split\n",
      "  ‚úÖ llama-imatrix\n",
      "  ‚úÖ llama-export-lora\n",
      "  ‚úÖ llama-embedding\n",
      "  ‚úÖ llama-tokenize\n",
      "  ‚ö†Ô∏è  llama-infill (not found)\n",
      "  ‚úÖ llama-perplexity\n",
      "  ‚úÖ llama-bench\n",
      "  ‚úÖ llama-cvector-generator\n",
      "\n",
      "üì¶ Copied 13/14 binaries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Package info\n",
    "VERSION = \"2.2.0\"\n",
    "BUILD_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "PACKAGE_NAME = f\"llcuda-v{VERSION}-cuda12-kaggle-t4x2\"\n",
    "PACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n",
    "\n",
    "print(f\"Creating package: {PACKAGE_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/include\", exist_ok=True)\n",
    "\n",
    "# Binaries to include\n",
    "BUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\n",
    "binaries = [\n",
    "    # Core server\n",
    "    \"llama-server\",\n",
    "    \"llama-cli\",\n",
    "    # Quantization & conversion\n",
    "    \"llama-quantize\",\n",
    "    \"llama-gguf\",\n",
    "    \"llama-gguf-hash\",\n",
    "    \"llama-gguf-split\",\n",
    "    \"llama-imatrix\",\n",
    "    # LoRA & embedding\n",
    "    \"llama-export-lora\",\n",
    "    \"llama-embedding\",\n",
    "    # Utilities\n",
    "    \"llama-tokenize\",\n",
    "    \"llama-infill\",\n",
    "    \"llama-perplexity\",\n",
    "    \"llama-bench\",\n",
    "    \"llama-cvector-generator\",\n",
    "]\n",
    "\n",
    "# Copy binaries\n",
    "copied = []\n",
    "for binary in binaries:\n",
    "    src = f\"{BUILD_BIN}/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n",
    "        os.chmod(f\"{PACKAGE_DIR}/bin/{binary}\", 0o755)\n",
    "        copied.append(binary)\n",
    "        print(f\"  ‚úÖ {binary}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  {binary} (not found)\")\n",
    "\n",
    "print(f\"\\nüì¶ Copied {len(copied)}/{len(binaries)} binaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c7058",
   "metadata": {},
   "source": [
    "## Step 9: Create Package Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5f97fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:46:29.753224Z",
     "iopub.status.busy": "2026-01-16T20:46:29.752471Z",
     "iopub.status.idle": "2026-01-16T20:46:29.784518Z",
     "shell.execute_reply": "2026-01-16T20:46:29.783898Z",
     "shell.execute_reply.started": "2026-01-16T20:46:29.753165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Package Metadata:\n",
      "{\n",
      "  \"package\": \"llcuda\",\n",
      "  \"version\": \"2.2.0\",\n",
      "  \"build_date\": \"2026-01-16T20:46:29.781471\",\n",
      "  \"platform\": {\n",
      "    \"name\": \"kaggle\",\n",
      "    \"gpu_count\": 2,\n",
      "    \"gpu_model\": \"Tesla T4\",\n",
      "    \"vram_per_gpu_gb\": 15,\n",
      "    \"total_vram_gb\": 30,\n",
      "    \"compute_capability\": \"7.5\",\n",
      "    \"architecture\": \"Turing\"\n",
      "  },\n",
      "  \"cuda\": {\n",
      "    \"version\": \"12.5\",\n",
      "    \"architectures\": [\n",
      "      \"sm_75\"\n",
      "    ],\n",
      "    \"flash_attention\": true,\n",
      "    \"flash_attention_all_quants\": true\n",
      "  },\n",
      "  \"llama_cpp\": {\n",
      "    \"commit\": \"388ce822415f24c60fcf164a321455f1e008cafb\",\n",
      "    \"commit_date\": \"2026-01-16 16:59:56 +0200\",\n",
      "    \"commit_message\": \"ggml : extend ggml_pool_1d + metal (#16429)\",\n",
      "    \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
      "  },\n",
      "  \"multi_gpu\": {\n",
      "    \"supported\": true,\n",
      "    \"method\": \"native_cuda\",\n",
      "    \"modes\": {\n",
      "      \"tensor_split\": {\n",
      "        \"description\": \"Split model across both GPUs for larger models\",\n",
      "        \"flags\": [\n",
      "          \"--tensor-split 0.5,0.5\",\n",
      "          \"--split-mode layer\"\n",
      "        ],\n",
      "        \"use_case\": \"Large GGUF models (>15GB)\"\n",
      "      },\n",
      "      \"split_workload\": {\n",
      "        \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n",
      "        \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n",
      "        \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n",
      "      }\n",
      "    },\n",
      "    \"recommended_config\": {\n",
      "      \"tensor_split\": \"0.5,0.5\",\n",
      "      \"split_mode\": \"layer\",\n",
      "      \"n_gpu_layers\": -1\n",
      "    }\n",
      "  },\n",
      "  \"split_workload\": {\n",
      "    \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n",
      "    \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n",
      "    \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n",
      "    \"rapids_packages\": [\n",
      "      \"cudf-cu12\",\n",
      "      \"cuml-cu12\",\n",
      "      \"cugraph-cu12\"\n",
      "    ],\n",
      "    \"graphistry_packages\": [\n",
      "      \"graphistry[ai]\"\n",
      "    ],\n",
      "    \"usage\": {\n",
      "      \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n",
      "      \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n",
      "    }\n",
      "  },\n",
      "  \"binaries\": [\n",
      "    \"llama-server\",\n",
      "    \"llama-cli\",\n",
      "    \"llama-quantize\",\n",
      "    \"llama-gguf\",\n",
      "    \"llama-gguf-hash\",\n",
      "    \"llama-gguf-split\",\n",
      "    \"llama-imatrix\",\n",
      "    \"llama-export-lora\",\n",
      "    \"llama-embedding\",\n",
      "    \"llama-tokenize\",\n",
      "    \"llama-perplexity\",\n",
      "    \"llama-bench\",\n",
      "    \"llama-cvector-generator\"\n",
      "  ],\n",
      "  \"features\": [\n",
      "    \"multi-gpu-tensor-split\",\n",
      "    \"split-workload-architecture\",\n",
      "    \"flash-attention-all-quants\",\n",
      "    \"openai-compatible-api\",\n",
      "    \"anthropic-compatible-api\",\n",
      "    \"29-quantization-formats\",\n",
      "    \"lora-adapters\",\n",
      "    \"grammar-constraints\",\n",
      "    \"json-schema-output\",\n",
      "    \"embeddings-reranking\",\n",
      "    \"streaming-sse\",\n",
      "    \"kv-cache-slots\",\n",
      "    \"speculative-decoding\"\n",
      "  ],\n",
      "  \"unsloth_integration\": {\n",
      "    \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n",
      "    \"workflow\": \"Unsloth (training) \\u2192 GGUF (conversion) \\u2192 llcuda (inference)\",\n",
      "    \"supported_exports\": [\n",
      "      \"f16\",\n",
      "      \"q8_0\",\n",
      "      \"q4_k_m\",\n",
      "      \"q5_k_m\",\n",
      "      \"iq4_xs\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get llama.cpp info\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "commit_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n",
    "commit_date = subprocess.getoutput(\"git log -1 --format=%ci\")\n",
    "commit_msg = subprocess.getoutput(\"git log -1 --format=%s\")\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = subprocess.getoutput(\"nvcc --version | grep release | sed 's/.*release //' | cut -d, -f1\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"package\": \"llcuda\",\n",
    "    \"version\": VERSION,\n",
    "    \"build_date\": datetime.now().isoformat(),\n",
    "    \"platform\": {\n",
    "        \"name\": \"kaggle\",\n",
    "        \"gpu_count\": 2,\n",
    "        \"gpu_model\": \"Tesla T4\",\n",
    "        \"vram_per_gpu_gb\": 15,\n",
    "        \"total_vram_gb\": 30,\n",
    "        \"compute_capability\": \"7.5\",\n",
    "        \"architecture\": \"Turing\"\n",
    "    },\n",
    "    \"cuda\": {\n",
    "        \"version\": cuda_version,\n",
    "        \"architectures\": [\"sm_75\"],\n",
    "        \"flash_attention\": True,\n",
    "        \"flash_attention_all_quants\": True\n",
    "    },\n",
    "    \"llama_cpp\": {\n",
    "        \"commit\": commit_hash,\n",
    "        \"commit_date\": commit_date,\n",
    "        \"commit_message\": commit_msg,\n",
    "        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
    "    },\n",
    "    \"multi_gpu\": {\n",
    "        \"supported\": True,\n",
    "        \"method\": \"native_cuda\",\n",
    "        \"modes\": {\n",
    "            \"tensor_split\": {\n",
    "                \"description\": \"Split model across both GPUs for larger models\",\n",
    "                \"flags\": [\"--tensor-split 0.5,0.5\", \"--split-mode layer\"],\n",
    "                \"use_case\": \"Large GGUF models (>15GB)\"\n",
    "            },\n",
    "            \"split_workload\": {\n",
    "                \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n",
    "                \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n",
    "                \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n",
    "            }\n",
    "        },\n",
    "        \"recommended_config\": {\n",
    "            \"tensor_split\": \"0.5,0.5\",\n",
    "            \"split_mode\": \"layer\",\n",
    "            \"n_gpu_layers\": -1\n",
    "        }\n",
    "    },\n",
    "    \"split_workload\": {\n",
    "        \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n",
    "        \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n",
    "        \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n",
    "        \"rapids_packages\": [\"cudf-cu12\", \"cuml-cu12\", \"cugraph-cu12\"],\n",
    "        \"graphistry_packages\": [\"graphistry[ai]\"],\n",
    "        \"usage\": {\n",
    "            \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n",
    "            \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n",
    "        }\n",
    "    },\n",
    "    \"binaries\": copied,\n",
    "    \"features\": [\n",
    "        \"multi-gpu-tensor-split\",\n",
    "        \"split-workload-architecture\",\n",
    "        \"flash-attention-all-quants\",\n",
    "        \"openai-compatible-api\",\n",
    "        \"anthropic-compatible-api\",\n",
    "        \"29-quantization-formats\",\n",
    "        \"lora-adapters\",\n",
    "        \"grammar-constraints\",\n",
    "        \"json-schema-output\",\n",
    "        \"embeddings-reranking\",\n",
    "        \"streaming-sse\",\n",
    "        \"kv-cache-slots\",\n",
    "        \"speculative-decoding\"\n",
    "    ],\n",
    "    \"unsloth_integration\": {\n",
    "        \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n",
    "        \"workflow\": \"Unsloth (training) ‚Üí GGUF (conversion) ‚Üí llcuda (inference)\",\n",
    "        \"supported_exports\": [\"f16\", \"q8_0\", \"q4_k_m\", \"q5_k_m\", \"iq4_xs\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write metadata\n",
    "os.chdir(\"/kaggle/working\")\n",
    "with open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"üìã Package Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b1740",
   "metadata": {},
   "source": [
    "## Step 10: Create README and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6c3a2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:46:37.885783Z",
     "iopub.status.busy": "2026-01-16T20:46:37.884950Z",
     "iopub.status.idle": "2026-01-16T20:46:37.893058Z",
     "shell.execute_reply": "2026-01-16T20:46:37.892242Z",
     "shell.execute_reply.started": "2026-01-16T20:46:37.885746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ README.md created\n",
      "\n",
      "# llcuda v2.2.0 - Kaggle 2√ó Tesla T4 Build\n",
      "\n",
      "Pre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n",
      "\n",
      "## üéØ Unsloth Integration\n",
      "\n",
      "llcuda is the **CUDA 12 inference backend for Unsloth**:\n",
      "\n",
      "```\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n",
      "‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n",
      "‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "```\n",
      "\n",
      "## üöÄ Quick Start\n",
      "\n",
      "### 1. Extract Package\n",
      "```bash\n",
      "tar -xzf llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n",
      "cd llcuda-v2.2.0-cuda12-kaggle-t4x2\n",
      "chmod +x bin/*\n",
      "```\n",
      "\n",
      "### 2. Start Multi-GPU Server\n",
      "```bash\n",
      "./bin/llama-server \\\n",
      "    -m /path/to/model.gguf \\\n",
      "    -ngl 99 \\\n",
      "    --tensor-split 0.5,0.5 \\\n",
      "    --split-mode layer \\\n",
      "    -fa \\\n",
      "    --host 0.0.0.0 \\\n",
      "    --port 8080 \\\n",
      "    -c 8192\n",
      "```\n",
      "\n",
      "### 3. Use with Python\n",
      "```python\n",
      "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
      "\n",
      "# Get optimal config for Kaggle\n",
      "config = kaggle_t4_dual_config()\n",
      "print(config.to_cli_args())\n",
      "\n",
      "# Connect to server\n",
      "client = LlamaCppClient(\"http://localhost:8080\")\n",
      "\n",
      "# OpenAI-compatible chat\n",
      "response = client.chat.completions.create(\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
      "    max_tokens=100\n",
      ")\n",
      "print(response.choices[0].message.content)\n",
      "```\n",
      "\n",
      "## üìä Multi-GPU Flags\n",
      "\n",
      "| Flag | Description | Example |\n",
      "|------|-------------|--------|\n",
      "| `-ngl 99` | Offload all layers to GPU | Required |\n",
      "| `--tensor-split` | VRAM rat...\n"
     ]
    }
   ],
   "source": [
    "readme_content = f'''# llcuda v{VERSION} - Kaggle 2√ó Tesla T4 Build\n",
    "\n",
    "Pre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n",
    "\n",
    "## üéØ Unsloth Integration\n",
    "\n",
    "llcuda is the **CUDA 12 inference backend for Unsloth**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n",
    "‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n",
    "‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### 1. Extract Package\n",
    "```bash\n",
    "tar -xzf llcuda-v{VERSION}-cuda12-kaggle-t4x2.tar.gz\n",
    "cd llcuda-v{VERSION}-cuda12-kaggle-t4x2\n",
    "chmod +x bin/*\n",
    "```\n",
    "\n",
    "### 2. Start Multi-GPU Server\n",
    "```bash\n",
    "./bin/llama-server \\\\\n",
    "    -m /path/to/model.gguf \\\\\n",
    "    -ngl 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    -fa \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port 8080 \\\\\n",
    "    -c 8192\n",
    "```\n",
    "\n",
    "### 3. Use with Python\n",
    "```python\n",
    "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
    "\n",
    "# Get optimal config for Kaggle\n",
    "config = kaggle_t4_dual_config()\n",
    "print(config.to_cli_args())\n",
    "\n",
    "# Connect to server\n",
    "client = LlamaCppClient(\"http://localhost:8080\")\n",
    "\n",
    "# OpenAI-compatible chat\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "## üìä Multi-GPU Flags\n",
    "\n",
    "| Flag | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| `-ngl 99` | Offload all layers to GPU | Required |\n",
    "| `--tensor-split` | VRAM ratio per GPU | `0.5,0.5` |\n",
    "| `--split-mode` | Split strategy | `layer` or `row` |\n",
    "| `--main-gpu` | Primary GPU ID | `0` |\n",
    "| `-fa` | FlashAttention | Recommended |\n",
    "\n",
    "## üì¶ Recommended Models for 30GB VRAM\n",
    "\n",
    "| Model | Quant | Size | Context | Fits? |\n",
    "|-------|-------|------|---------|-------|\n",
    "| Llama 3.1 70B | IQ3_XS | ~25GB | 4K | ‚úÖ |\n",
    "| Qwen2.5 32B | Q4_K_M | ~19GB | 8K | ‚úÖ |\n",
    "| Gemma 2 27B | Q4_K_M | ~16GB | 8K | ‚úÖ |\n",
    "| Llama 3.1 8B | Q8_0 | ~9GB | 16K | ‚úÖ |\n",
    "| Mistral 7B | Q8_0 | ~8GB | 32K | ‚úÖ |\n",
    "\n",
    "## üîß Unsloth ‚Üí llcuda Workflow\n",
    "\n",
    "```python\n",
    "# 1. Fine-tune with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "# ... training ...\n",
    "\n",
    "# 2. Export to GGUF (Unsloth built-in)\n",
    "model.save_pretrained_gguf(\"my_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# 3. Run with llcuda\n",
    "# ./bin/llama-server -m my_model-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5\n",
    "```\n",
    "\n",
    "## üìã Build Info\n",
    "\n",
    "- **llcuda Version:** {VERSION}\n",
    "- **CUDA Version:** 12.4\n",
    "- **Target GPU:** Tesla T4 √ó 2\n",
    "- **Compute Capability:** SM 7.5 (Turing)\n",
    "- **FlashAttention:** All quantization types\n",
    "- **Build Date:** {BUILD_DATE}\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [llcuda GitHub](https://github.com/llcuda/llcuda)\n",
    "- [Unsloth](https://github.com/unslothai/unsloth)\n",
    "- [llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README.md created\")\n",
    "print(f\"\\n{readme_content[:1500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e557cd",
   "metadata": {},
   "source": [
    "## Step 11: Create Helper Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abbad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:46:45.053096Z",
     "iopub.status.busy": "2026-01-16T20:46:45.052817Z",
     "iopub.status.idle": "2026-01-16T20:46:45.059851Z",
     "shell.execute_reply": "2026-01-16T20:46:45.059061Z",
     "shell.execute_reply.started": "2026-01-16T20:46:45.053069Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper scripts created:\n",
      "   - start-server.sh\n",
      "   - quantize.sh\n"
     ]
    }
   ],
   "source": [
    "# Create start-server.sh helper script\n",
    "start_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Start Multi-GPU Server\n",
    "# Usage: ./start-server.sh <model.gguf> [port]\n",
    "\n",
    "MODEL=\"$1\"\n",
    "PORT=\"${2:-8080}\"\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"Usage: $0 <model.gguf> [port]\"\n",
    "    echo \"Example: $0 qwen2.5-7b-Q4_K_M.gguf 8080\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Starting llama-server with dual T4 config...\"\n",
    "echo \"Model: $MODEL\"\n",
    "echo \"Port: $PORT\"\n",
    "echo \"\"\n",
    "\n",
    "\"$SCRIPT_DIR/bin/llama-server\" \\\\\n",
    "    --model \"$MODEL\" \\\\\n",
    "    --n-gpu-layers 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    --flash-attn \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port \"$PORT\" \\\\\n",
    "    --ctx-size 8192 \\\\\n",
    "    --batch-size 2048 \\\\\n",
    "    --ubatch-size 512 \\\\\n",
    "    --parallel 4\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/start-server.sh\", \"w\") as f:\n",
    "    f.write(start_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/start-server.sh\", 0o755)\n",
    "\n",
    "# Create quantize.sh helper script\n",
    "quantize_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Quantize Model\n",
    "# Usage: ./quantize.sh <input.gguf> <output.gguf> [quant_type]\n",
    "\n",
    "INPUT=\"$1\"\n",
    "OUTPUT=\"$2\"\n",
    "QUANT=\"${3:-Q4_K_M}\"\n",
    "\n",
    "if [ -z \"$INPUT\" ] || [ -z \"$OUTPUT\" ]; then\n",
    "    echo \"Usage: $0 <input.gguf> <output.gguf> [quant_type]\"\n",
    "    echo \"Quant types: Q4_K_M (default), Q8_0, Q5_K_M, IQ4_XS, etc.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Quantizing: $INPUT ‚Üí $OUTPUT ($QUANT)\"\n",
    "\"$SCRIPT_DIR/bin/llama-quantize\" \"$INPUT\" \"$OUTPUT\" \"$QUANT\"\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/quantize.sh\", \"w\") as f:\n",
    "    f.write(quantize_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/quantize.sh\", 0o755)\n",
    "\n",
    "print(\"‚úÖ Helper scripts created:\")\n",
    "print(\"   - start-server.sh\")\n",
    "print(\"   - quantize.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e467ab",
   "metadata": {},
   "source": [
    "## Step 12: Create Distribution Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a176493f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:46:48.423959Z",
     "iopub.status.busy": "2026-01-16T20:46:48.423470Z",
     "iopub.status.idle": "2026-01-16T20:49:23.248319Z",
     "shell.execute_reply": "2026-01-16T20:49:23.247586Z",
     "shell.execute_reply.started": "2026-01-16T20:46:48.423931Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating distribution archive: llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n",
      "============================================================\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/quantize.sh\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-perplexity\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-quantize\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-server\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-cvector-generator\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-imatrix\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-embedding\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-bench\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-export-lora\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-cli\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf-split\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-tokenize\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-gguf-hash\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/README.md\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/start-server.sh\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/lib/\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/metadata.json\n",
      "llcuda-v2.2.0-cuda12-kaggle-t4x2/include/\n",
      "\n",
      "============================================================\n",
      "üì¶ DISTRIBUTION PACKAGE READY\n",
      "============================================================\n",
      "-rw-r--r-- 1 root root 961M Jan 16 20:49 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n",
      "-rw-r--r-- 1 root root  106 Jan 16 20:49 llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz.sha256\n",
      "\n",
      "SHA256: 489f3df54bac24d3801af3149c346402bea099d2bd8793897aa606c5c8af0025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "TARBALL = f\"{PACKAGE_NAME}.tar.gz\"\n",
    "\n",
    "print(f\"Creating distribution archive: {TARBALL}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tarball\n",
    "!tar -czvf {TARBALL} {PACKAGE_NAME}\n",
    "\n",
    "# Calculate SHA256\n",
    "with open(TARBALL, \"rb\") as f:\n",
    "    sha256 = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Write checksum file\n",
    "with open(f\"{TARBALL}.sha256\", \"w\") as f:\n",
    "    f.write(f\"{sha256}  {TARBALL}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ DISTRIBUTION PACKAGE READY\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh {TARBALL}*\n",
    "print(f\"\\nSHA256: {sha256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214c3f",
   "metadata": {},
   "source": [
    "## Step 13: Test Multi-GPU Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980daa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:52:18.166608Z",
     "iopub.status.busy": "2026-01-16T20:52:18.165901Z",
     "iopub.status.idle": "2026-01-16T20:53:18.478478Z",
     "shell.execute_reply": "2026-01-16T20:53:18.477720Z",
     "shell.execute_reply.started": "2026-01-16T20:52:18.166572Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading small test model...\n",
      "‚úÖ Model: /kaggle/working/models/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q4_K_M.gguf\n",
      "\n",
      "Starting llama-server with dual T4 config...\n",
      "Command: /kaggle/working/llcuda-v2.2.0-cuda12-kaggle-t4x2/bin/llama-server -m /kaggle/working/models/models--lmstudio-community--gemma-2-2b-it-GGUF/snapshots/6aa72da804ad76c5dc862867bfba6256de9172c7/gemma-2-2b-it-Q4_K_M.gguf -ngl 20 --tensor-split 0.5,0.5 --split-mode layer -fa --host 127.0.0.1 --port 8080 -c 4096\n",
      "\n",
      "Waiting for server to start...\n",
      "‚ö†Ô∏è Server startup timeout\n",
      "\n",
      "üìä GPU Memory Usage:\n",
      "index, memory.used [MiB], memory.total [MiB]\n",
      "0, 3 MiB, 15360 MiB\n",
      "1, 3 MiB, 15360 MiB\n"
     ]
    }
   ],
   "source": [
    "# Download a small test model and verify multi-GPU works\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import signal\n",
    "\n",
    "print(\"Downloading small test model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"‚úÖ Model: {model_path}\")\n",
    "\n",
    "# Kill any existing server on port 8080\n",
    "print(\"\\nüîß Cleaning up any existing server...\")\n",
    "os.system(\"pkill -f 'llama-server.*8080' 2>/dev/null || true\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Start server with multi-GPU - use -ngl 99 to offload ALL layers\n",
    "print(\"\\nStarting llama-server with dual T4 config...\")\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",  # Offload ALL layers to GPU\n",
    "    \"--tensor-split\", \"0.5,0.5\",\n",
    "    \"--split-mode\", \"layer\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\",\n",
    "    \"--log-disable\"  # Reduce log noise\n",
    "]\n",
    "\n",
    "print(f\"Command: {' '.join(server_cmd)}\")\n",
    "\n",
    "# Start server with stderr visible for debugging\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait for server with better feedback\n",
    "print(\"\\nWaiting for server to start (may take 30-60s for model loading)...\")\n",
    "server_ready = False\n",
    "for i in range(90):  # Increase timeout to 90 seconds\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"‚úÖ Server ready in {i+1}s!\")\n",
    "            server_ready = True\n",
    "            break\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        if i % 10 == 9:\n",
    "            print(f\"   Still waiting... ({i+1}s)\")\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        print(f\"   Unexpected error: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "if not server_ready:\n",
    "    print(\"‚ö†Ô∏è Server startup timeout!\")\n",
    "    print(\"\\nüìã Server stderr (last 2000 chars):\")\n",
    "    try:\n",
    "        # Check if server is still running\n",
    "        if server.poll() is not None:\n",
    "            print(f\"   Server exited with code: {server.returncode}\")\n",
    "        stderr_output = server.stderr.read(4000).decode('utf-8', errors='ignore')\n",
    "        print(stderr_output[-2000:] if len(stderr_output) > 2000 else stderr_output)\n",
    "    except Exception as e:\n",
    "        print(f\"   Could not read stderr: {e}\")\n",
    "\n",
    "# Check GPU usage\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ec572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T20:50:55.250128Z",
     "iopub.status.busy": "2026-01-16T20:50:55.249783Z",
     "iopub.status.idle": "2026-01-16T20:50:55.284100Z",
     "shell.execute_reply": "2026-01-16T20:50:55.283061Z",
     "shell.execute_reply.started": "2026-01-16T20:50:55.250093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multi-GPU inference...\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8080): Failed to establish a new connection: [Errno 111] Connection refused\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             conn.request(\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tunnel_host\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             raise NewConnectionError(\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Failed to establish a new connection: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: HTTPConnection(host='127.0.0.1', port=8080): Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    842\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8080): Failed to establish a new connection: [Errno 111] Connection refused\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/3215935741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m response = requests.post(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m\"http://127.0.0.1:8080/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     json={\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \"\"\"\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    675\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /v1/chat/completions (Caused by NewConnectionError(\"HTTPConnection(host='127.0.0.1', port=8080): Failed to establish a new connection: [Errno 111] Connection refused\"))"
     ]
    }
   ],
   "source": [
    "# Test inference (only if server is running)\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"Testing multi-GPU inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First check if server is healthy\n",
    "try:\n",
    "    health = requests.get(\"http://127.0.0.1:8080/health\", timeout=5)\n",
    "    if health.status_code != 200:\n",
    "        print(\"‚ùå Server is not healthy. Skipping inference test.\")\n",
    "        print(\"   Please re-run the previous cell or check server logs.\")\n",
    "        raise SystemExit(0)\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"‚ùå Server is not running. Skipping inference test.\")\n",
    "    print(\"   Please re-run the previous cell to start the server.\")\n",
    "    raise SystemExit(0)\n",
    "\n",
    "print(\"‚úÖ Server is healthy, running inference test...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "try:\n",
    "    response = requests.post(\n",
    "        \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "        json={\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "            \"max_tokens\": 100,\n",
    "            \"temperature\": 0.7\n",
    "        },\n",
    "        timeout=120  # Increase timeout for inference\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        usage = result.get(\"usage\", {})\n",
    "        \n",
    "        print(f\"‚úÖ Response ({elapsed:.2f}s):\")\n",
    "        print(f\"   {content}\")\n",
    "        print(f\"\\nüìä Tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "        if usage.get('completion_tokens'):\n",
    "            tps = usage['completion_tokens'] / elapsed\n",
    "            print(f\"üìä Speed: {tps:.1f} tokens/sec\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: {response.status_code}\")\n",
    "        print(response.text[:500])\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"‚ùå Inference request timed out (120s)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - stop server\n",
    "import os\n",
    "\n",
    "print(\"Stopping server...\")\n",
    "try:\n",
    "    server.terminate()\n",
    "    server.wait(timeout=10)\n",
    "    print(\"‚úÖ Server stopped gracefully\")\n",
    "except NameError:\n",
    "    print(\"‚ö†Ô∏è Server variable not defined (server may not have started)\")\n",
    "    os.system(\"pkill -f 'llama-server.*8080' 2>/dev/null || true\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error stopping server: {e}\")\n",
    "    os.system(\"pkill -f 'llama-server.*8080' 2>/dev/null || true\")\n",
    "\n",
    "# Show final GPU state\n",
    "print(\"\\nüìä Final GPU State:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f0e65",
   "metadata": {},
   "source": [
    "## Step 13b: Test Split-GPU Architecture (LLM + Graphistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split-GPU Architecture Demo:\n",
    "- GPU 0: llama-server (LLM inference)\n",
    "- GPU 1: RAPIDS/Graphistry (graph simulation)\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLIT-GPU ARCHITECTURE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 0: Start llama-server (LLM)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 0: Starting llama-server...\")\n",
    "\n",
    "# Force llama-server to use GPU 0 only\n",
    "llama_env = os.environ.copy()\n",
    "llama_env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env=llama_env\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"   ‚úÖ llama-server ready on GPU 0 ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Server timeout\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 1: RAPIDS/Graphistry graph operations\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 1: Running RAPIDS graph simulation...\")\n",
    "\n",
    "# Force RAPIDS to use GPU 1 only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "# Create sample graph data (simulating knowledge graph from LLM)\n",
    "edges = cudf.DataFrame({\n",
    "    \"src\": [0, 1, 2, 3, 4, 0, 1, 2],\n",
    "    \"dst\": [1, 2, 3, 4, 0, 2, 3, 4],\n",
    "    \"weight\": [1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 1.0, 0.8]\n",
    "})\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "\n",
    "print(f\"   Graph: {G.number_of_vertices()} vertices, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Run PageRank on GPU 1\n",
    "pagerank = cugraph.pagerank(G)\n",
    "print(f\"   PageRank computed: {len(pagerank)} nodes\")\n",
    "print(f\"   Top node: {pagerank.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Combined workflow: LLM query ‚Üí Graph update\n",
    "# ============================================================================\n",
    "print(\"\\nüîó Combined LLM + Graph workflow...\")\n",
    "\n",
    "# Reset CUDA_VISIBLE_DEVICES for requests\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Query LLM on GPU 0\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"List 3 related concepts to 'machine learning'\"}],\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"   LLM (GPU 0): {llm_output[:100]}...\")\n",
    "    \n",
    "    # Simulate adding LLM-derived edges to graph\n",
    "    new_edges = cudf.DataFrame({\n",
    "        \"src\": [5, 5, 5],\n",
    "        \"dst\": [0, 1, 2],\n",
    "        \"weight\": [1.0, 1.0, 1.0]\n",
    "    })\n",
    "    all_edges = cudf.concat([edges, new_edges])\n",
    "    G2 = cugraph.Graph()\n",
    "    G2.from_cudf_edgelist(all_edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "    print(f\"   Graph (GPU 1): Updated to {G2.number_of_vertices()} vertices\")\n",
    "\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"\\n‚úÖ Split-GPU test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bb500",
   "metadata": {},
   "source": [
    "## Step 13b: llcuda v2.2.0 Module Integration Demo\n",
    "\n",
    "Demonstrate the new Graphistry and Louie.AI modules from llcuda v2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# llcuda v2.2.0 Module Integration Demo\n",
    "# ============================================================================\n",
    "# This demonstrates the new Graphistry and Louie.AI modules\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"llcuda v2.2.0 MODULE INTEGRATION DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install llcuda from GitHub (use main branch or specific version)\n",
    "!pip install -q git+https://github.com/llcuda/llcuda.git\n",
    "\n",
    "import llcuda\n",
    "\n",
    "print(f\"\\nüì¶ llcuda version: {llcuda.__version__}\")\n",
    "print(f\"\\nüìã Available exports:\")\n",
    "print(f\"   {llcuda.__all__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SplitGPUConfig - Configure Split-GPU Workloads\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. SplitGPUConfig Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config = llcuda.SplitGPUConfig(llm_gpu=0, graph_gpu=1)\n",
    "print(f\"   LLM GPU: {config.llm_gpu}\")\n",
    "print(f\"   Graph GPU: {config.graph_gpu}\")\n",
    "\n",
    "# Get environment variables for each GPU\n",
    "print(f\"\\n   LLM env: {config.llm_env()}\")\n",
    "print(f\"   Graph env: {config.graph_env()}\")\n",
    "\n",
    "# Generate llama-server command\n",
    "model_path = f\"/kaggle/working/{PACKAGE_NAME}/models/gemma-3-1b-Q4_K_M.gguf\"\n",
    "cmd = config.llama_server_cmd(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=99,\n",
    "    flash_attention=True,\n",
    "    port=8080\n",
    ")\n",
    "print(f\"\\n   Server command:\\n   {' '.join(cmd)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Graphistry Module - Graph Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. Graphistry Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.graphistry import GraphWorkload, RAPIDSBackend, check_rapids_available\n",
    "\n",
    "# Check RAPIDS availability\n",
    "rapids_status = check_rapids_available()\n",
    "print(f\"   RAPIDS status: {rapids_status}\")\n",
    "\n",
    "# Create GraphWorkload on GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "workload = GraphWorkload(gpu_id=1)\n",
    "\n",
    "# Sample entities and relationships (simulating LLM-extracted knowledge)\n",
    "entities = [\n",
    "    {\"id\": \"Machine Learning\", \"type\": \"field\", \"properties\": {\"year\": 1959}},\n",
    "    {\"id\": \"Deep Learning\", \"type\": \"field\", \"properties\": {\"year\": 2006}},\n",
    "    {\"id\": \"Neural Networks\", \"type\": \"concept\"},\n",
    "    {\"id\": \"Transformers\", \"type\": \"architecture\", \"properties\": {\"year\": 2017}},\n",
    "    {\"id\": \"GPT\", \"type\": \"model\"},\n",
    "    {\"id\": \"BERT\", \"type\": \"model\"},\n",
    "    {\"id\": \"CNN\", \"type\": \"architecture\"},\n",
    "]\n",
    "\n",
    "relationships = [\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Deep Learning\", \"type\": \"contains\", \"weight\": 0.9},\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Neural Networks\", \"type\": \"uses\", \"weight\": 0.85},\n",
    "    {\"source\": \"Deep Learning\", \"target\": \"Transformers\", \"type\": \"includes\", \"weight\": 0.95},\n",
    "    {\"source\": \"Transformers\", \"target\": \"GPT\", \"type\": \"basis_for\", \"weight\": 0.9},\n",
    "    {\"source\": \"Transformers\", \"target\": \"BERT\", \"type\": \"basis_for\", \"weight\": 0.88},\n",
    "    {\"source\": \"Neural Networks\", \"target\": \"CNN\", \"type\": \"type_of\", \"weight\": 0.8},\n",
    "]\n",
    "\n",
    "# Create knowledge graph using the correct API\n",
    "g = workload.create_knowledge_graph(entities, relationships)\n",
    "print(f\"   Knowledge graph created with Graphistry\")\n",
    "\n",
    "# Run PageRank using edges DataFrame (correct API)\n",
    "import pandas as pd\n",
    "edges_df = pd.DataFrame([\n",
    "    {\"src\": r[\"source\"], \"dst\": r[\"target\"], \"weight\": r.get(\"weight\", 1.0)}\n",
    "    for r in relationships\n",
    "])\n",
    "pagerank_result = workload.run_pagerank(edges_df)\n",
    "print(f\"   PageRank: top node = {pagerank_result.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Louie Module - Natural Language Graph Queries\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. Louie Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.louie import LouieClient, KnowledgeExtractor\n",
    "\n",
    "# Initialize Louie client (connected to llama-server on GPU 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "louie = LouieClient(llm_endpoint=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Knowledge extraction example\n",
    "text = \"\"\"\n",
    "NVIDIA develops GPUs for deep learning. The Tesla T4 is optimized for inference.\n",
    "llcuda v2.2.0 runs on Tesla T4 with FlashAttention enabled.\n",
    "cuGraph provides GPU-accelerated graph analytics.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"   Input text: {text[:60]}...\")\n",
    "\n",
    "# Extract entities (requires running LLM server)\n",
    "try:\n",
    "    entities = louie.extract_entities(text)\n",
    "    print(f\"   Extracted entities: {entities[:3]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   (LLM server required for entity extraction)\")\n",
    "    # Simulated output\n",
    "    entities = [\n",
    "        {\"name\": \"NVIDIA\", \"type\": \"ORG\"},\n",
    "        {\"name\": \"Tesla T4\", \"type\": \"PRODUCT\"},\n",
    "        {\"name\": \"llcuda\", \"type\": \"SOFTWARE\"},\n",
    "        {\"name\": \"cuGraph\", \"type\": \"SOFTWARE\"}\n",
    "    ]\n",
    "    print(f\"   Demo entities: {entities}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RAPIDS Backend Direct Access\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. RAPIDS Backend Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "backend = RAPIDSBackend()\n",
    "\n",
    "# Create a cuDF DataFrame\n",
    "import cudf\n",
    "gpu_edges = cudf.DataFrame({\n",
    "    \"source\": [0, 1, 2, 3, 4, 0, 1],\n",
    "    \"target\": [1, 2, 3, 4, 0, 2, 3],\n",
    "    \"weight\": [1.0, 0.8, 0.9, 0.7, 1.0, 0.6, 0.85]\n",
    "})\n",
    "\n",
    "# Run graph algorithms\n",
    "import cugraph\n",
    "G_rapids = cugraph.Graph()\n",
    "G_rapids.from_cudf_edgelist(gpu_edges, source=\"source\", destination=\"target\")\n",
    "\n",
    "# Louvain community detection\n",
    "louvain = cugraph.louvain(G_rapids)\n",
    "print(f\"   Louvain communities: {louvain['partition'].nunique()} detected\")\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness = cugraph.betweenness_centrality(G_rapids)\n",
    "top_node = betweenness.nlargest(1, 'betweenness_centrality')['vertex'].values[0]\n",
    "print(f\"   Highest betweenness: node {top_node}\")\n",
    "\n",
    "print(\"\\n‚úÖ llcuda v2.2.0 module integration complete!\")\n",
    "print(\"   All new APIs functional on Kaggle 2√ó T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aebf7",
   "metadata": {},
   "source": [
    "## Step 14: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c32db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéâ llcuda v2.2.0 BUILD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Distribution Package:\")\n",
    "!ls -lh {PACKAGE_NAME}.tar.gz\n",
    "\n",
    "print(f\"\\nüìÅ Package Contents:\")\n",
    "!ls -la {PACKAGE_NAME}/\n",
    "\n",
    "print(f\"\\nüîß Binaries:\")\n",
    "!ls -lh {PACKAGE_NAME}/bin/ | head -10\n",
    "\n",
    "print(f\"\\nüìã Metadata Summary:\")\n",
    "print(f\"   Version: {VERSION}\")\n",
    "print(f\"   Platform: Kaggle 2√ó Tesla T4\")\n",
    "print(f\"   CUDA: {cuda_version}\")\n",
    "print(f\"   Compute: SM 7.5 (Turing)\")\n",
    "print(f\"   FlashAttention: ‚úÖ All quants\")\n",
    "print(f\"   Multi-GPU: ‚úÖ Native CUDA\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Download: {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   2. Extract: tar -xzf {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   3. Run: ./start-server.sh model.gguf 8080\")\n",
    "\n",
    "print(f\"\\nüì• Download from Kaggle Output tab\")\n",
    "print(f\"   or copy to output: !cp {PACKAGE_NAME}.tar.gz /kaggle/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Kaggle output for download\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"/kaggle/output\", exist_ok=True)\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\", \"/kaggle/output/\")\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz.sha256\", \"/kaggle/output/\")\n",
    "\n",
    "print(\"‚úÖ Package copied to /kaggle/output/ for download\")\n",
    "!ls -lh /kaggle/output/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
