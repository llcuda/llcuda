{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c03abd5",
   "metadata": {},
   "source": [
    "# Build llcuda v2.0 for Tesla T4 (Google Colab)\n",
    "\n",
    "**Purpose**: Build complete CUDA 12 binaries for llcuda v2.0 on Google Colab Tesla T4 GPU\n",
    "\n",
    "**Output**:\n",
    "1. llama.cpp binaries (264 MB) - HTTP server mode\n",
    "2. llcuda_cpp.so (native extension) - v2.0 Tensor API\n",
    "\n",
    "**Requirements**:\n",
    "- Google Colab with T4 GPU\n",
    "- CUDA 12.x\n",
    "- Python 3.11+\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810002c0",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99522de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1964739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783bc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Expected: 3.10+ (Colab default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e74e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify compute capability\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "compute_cap = result.stdout.strip()\n",
    "major, minor = map(int, compute_cap.split('.'))\n",
    "\n",
    "print(f\"Compute Capability: SM {major}.{minor}\")\n",
    "\n",
    "if major == 7 and minor == 5:\n",
    "    print(\"âœ“ Tesla T4 detected - Perfect for llcuda v2.0!\")\n",
    "elif major >= 7 and minor >= 5:\n",
    "    print(f\"âœ“ SM {major}.{minor} detected - Compatible with llcuda v2.0\")\n",
    "else:\n",
    "    print(f\"âš  WARNING: SM {major}.{minor} is below SM 7.5 (T4)\")\n",
    "    print(\"llcuda v2.0 requires SM 7.5+ for Tensor Cores and FlashAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5ae45",
   "metadata": {},
   "source": [
    "## Step 2: Clone llcuda v2.0 Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone llcuda v2.0\n",
    "!git clone https://github.com/waqasm86/llcuda.git\n",
    "%cd llcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8888ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have llcuda v2.0 structure\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking llcuda v2.0 repository structure...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Required files for llcuda v2.0\n",
    "required_files = [\n",
    "    'CMakeLists.txt',\n",
    "    'csrc/core/device.h',\n",
    "    'csrc/core/device.cu',\n",
    "    'csrc/core/tensor.h',\n",
    "    'csrc/core/tensor.cu',\n",
    "    'csrc/bindings.cpp',\n",
    "    'csrc/ops/matmul.h',\n",
    "    'csrc/ops/matmul.cu',\n",
    "    'llcuda/__init__.py',\n",
    "    'llcuda/_internal/bootstrap.py',\n",
    "    'pyproject.toml',\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "found_files = []\n",
    "\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"âœ“ {file}\")\n",
    "        found_files.append(file)\n",
    "    else:\n",
    "        print(f\"âœ— MISSING: {file}\")\n",
    "        missing_files.append(file)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFound: {len(found_files)}/{len(required_files)} files\")\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\nâŒ ERROR: {len(missing_files)} required files are missing!\")\n",
    "    print(\"\\nMissing files:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"  - {file}\")\n",
    "    print(\"\\nPossible causes:\")\n",
    "    print(\"  1. Repository clone incomplete\")\n",
    "    print(\"  2. Wrong branch (need 'main' branch)\")\n",
    "    print(\"  3. Files not yet pushed to GitHub\")\n",
    "    print(\"\\nSolution:\")\n",
    "    print(\"  1. Delete the llcuda directory: !rm -rf /content/llcuda\")\n",
    "    print(\"  2. Re-clone: !git clone https://github.com/waqasm86/llcuda.git\")\n",
    "    print(\"  3. Ensure you're on main branch: !git checkout main\")\n",
    "    raise FileNotFoundError(f\"Required llcuda v2.0 files not found: {', '.join(missing_files)}\")\n",
    "\n",
    "print(\"\\nâœ… All required files present - Ready to build!\")\n",
    "\n",
    "# Show directory structure\n",
    "print(\"\\nDirectory structure:\")\n",
    "!ls -la csrc/\n",
    "!ls -la csrc/core/\n",
    "!ls -la csrc/ops/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08d133",
   "metadata": {},
   "source": [
    "## Step 3: Build llama.cpp Binaries (HTTP Server Mode)\n",
    "\n",
    "These binaries power the v1.x HTTP server mode and GGUF model support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac616ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone llama.cpp\n",
    "%cd /content\n",
    "!git clone https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d720cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure llama.cpp for Tesla T4 with FlashAttention\n",
    "!cmake -B build_cuda12_t4 \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DGGML_CUDA_FORCE_MMQ=OFF \\\n",
    "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\n",
    "    -DGGML_CUDA_FA=ON \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_CUDA_GRAPHS=ON \\\n",
    "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DLLAMA_BUILD_TOOLS=ON \\\n",
    "    -DLLAMA_CURL=ON \\\n",
    "    -DBUILD_SHARED_LIBS=ON \\\n",
    "    -DCMAKE_INSTALL_RPATH='$ORIGIN/../lib' \\\n",
    "    -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ef413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (this takes ~10 minutes)\n",
    "import time\n",
    "\n",
    "print(\"Building llama.cpp... (estimated 10 minutes)\")\n",
    "start_time = time.time()\n",
    "\n",
    "!cmake --build build_cuda12_t4 --config Release -j$(nproc)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ“ Build completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify binaries were built\n",
    "!ls -lh build_cuda12_t4/bin/llama-server\n",
    "!ls -lh build_cuda12_t4/bin/*.so* | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test llama-server (fix libcuda.so.1 issue)\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Find libcuda.so.1\n",
    "!find /usr -name \"libcuda.so.1\" 2>/dev/null | head -1\n",
    "\n",
    "# Set LD_LIBRARY_PATH and test\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/targets/x86_64-linux/lib:/content/llama.cpp/build_cuda12_t4/bin'\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['/content/llama.cpp/build_cuda12_t4/bin/llama-server', '--version'],\n",
    "    env=os.environ,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ“ llama-server works!\")\n",
    "else:\n",
    "    print(f\"\\nâœ— Error: Return code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package llama.cpp binaries\n",
    "%cd /content\n",
    "\n",
    "!mkdir -p package_t4/bin\n",
    "!mkdir -p package_t4/lib\n",
    "\n",
    "# Copy essential binaries\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-server package_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-cli package_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-quantize package_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-embedding package_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-bench package_t4/bin/\n",
    "\n",
    "# Copy all shared libraries\n",
    "!cp llama.cpp/build_cuda12_t4/bin/*.so* package_t4/lib/\n",
    "\n",
    "print(\"\\n=== Package Contents ===\")\n",
    "!du -sh package_t4\n",
    "!du -sh package_t4/bin\n",
    "!du -sh package_t4/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2bf5c",
   "metadata": {},
   "source": [
    "## Step 4: Build llcuda v2.0 Native Extension (Tensor API)\n",
    "\n",
    "This is the NEW v2.0 PyTorch-style tensor API with custom CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c96ff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pybind11 cmake ninja\n",
    "!pip install -q numpy torch --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c23dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llcuda v2.0 native extension\n",
    "%cd /content/llcuda\n",
    "\n",
    "# Clean previous builds\n",
    "!rm -rf build/native_t4\n",
    "!rm -f llcuda_cpp*.so\n",
    "\n",
    "# Create build directory\n",
    "!mkdir -p build/native_t4\n",
    "%cd build/native_t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba0af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure CMake for T4\n",
    "!cmake ../.. \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DPython3_EXECUTABLE=$(which python3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build (this takes ~5 minutes)\n",
    "print(\"Building llcuda v2.0 native extension... (estimated 5 minutes)\")\n",
    "start_time = time.time()\n",
    "\n",
    "!make -j$(nproc)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ“ Build completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the extension was built\n",
    "!ls -lh llcuda_cpp*.so\n",
    "!file llcuda_cpp*.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy extension to package root\n",
    "!cp llcuda_cpp*.so /content/llcuda/\n",
    "\n",
    "print(\"\\n=== Extension Info ===\")\n",
    "!ls -lh /content/llcuda/llcuda_cpp*.so\n",
    "!du -sh /content/llcuda/llcuda_cpp*.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222f1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the native extension\n",
    "%cd /content/llcuda\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/llcuda')\n",
    "\n",
    "try:\n",
    "    import llcuda_cpp\n",
    "    \n",
    "    # Test device detection\n",
    "    device_count = llcuda_cpp.Device.get_device_count()\n",
    "    print(f\"âœ“ Devices found: {device_count}\")\n",
    "    \n",
    "    # Test device properties\n",
    "    props = llcuda_cpp.Device.get_device_properties(0)\n",
    "    print(f\"âœ“ Device: {props.name}\")\n",
    "    print(f\"âœ“ Compute: SM {props.compute_capability_major}.{props.compute_capability_minor}\")\n",
    "    print(f\"âœ“ Memory: {props.total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Test tensor creation\n",
    "    tensor = llcuda_cpp.Tensor([10, 10], llcuda_cpp.DType.Float32, 0)\n",
    "    print(f\"âœ“ Tensor created: {tensor.shape()}\")\n",
    "    \n",
    "    # Test matrix multiplication\n",
    "    A = llcuda_cpp.Tensor.zeros([64, 64], llcuda_cpp.DType.Float32, 0)\n",
    "    B = llcuda_cpp.Tensor.zeros([64, 64], llcuda_cpp.DType.Float32, 0)\n",
    "    C = llcuda_cpp.ops.matmul(A, B)\n",
    "    print(f\"âœ“ MatMul works: {C.shape()}\")\n",
    "    \n",
    "    print(\"\\nâœ“ llcuda v2.0 native extension works perfectly!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâœ— Error testing extension: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aae2af",
   "metadata": {},
   "source": [
    "## Step 5: Create Release Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b98ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package 1: llama.cpp binaries (HTTP server mode)\n",
    "%cd /content\n",
    "\n",
    "!tar -czf llcuda-binaries-cuda12-t4.tar.gz package_t4/\n",
    "\n",
    "print(\"\\n=== Package 1: llama.cpp Binaries ===\")\n",
    "!ls -lh llcuda-binaries-cuda12-t4.tar.gz\n",
    "!du -h llcuda-binaries-cuda12-t4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b29b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package 2: llcuda v2.0 native extension\n",
    "%cd /content/llcuda\n",
    "\n",
    "# Create package directory\n",
    "!mkdir -p native_extension_t4\n",
    "!cp llcuda_cpp*.so native_extension_t4/\n",
    "!cp CMakeLists.txt native_extension_t4/\n",
    "!cp build_native.sh native_extension_t4/\n",
    "\n",
    "# Create metadata\n",
    "!echo 'Tesla T4 (SM 7.5)' > native_extension_t4/GPU_TARGET.txt\n",
    "!echo 'CUDA 12.x' >> native_extension_t4/GPU_TARGET.txt\n",
    "!echo 'Built on Google Colab' >> native_extension_t4/GPU_TARGET.txt\n",
    "!date >> native_extension_t4/GPU_TARGET.txt\n",
    "\n",
    "!tar -czf llcuda-v2-native-t4.tar.gz native_extension_t4/\n",
    "\n",
    "print(\"\\n=== Package 2: llcuda v2.0 Native Extension ===\")\n",
    "!ls -lh llcuda-v2-native-t4.tar.gz\n",
    "!du -h llcuda-v2-native-t4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package 3: Combined package (both binaries + extension)\n",
    "%cd /content\n",
    "\n",
    "!mkdir -p llcuda_v2_complete_t4/binaries\n",
    "!mkdir -p llcuda_v2_complete_t4/native\n",
    "\n",
    "# Copy llama.cpp binaries\n",
    "!cp -r package_t4/* llcuda_v2_complete_t4/binaries/\n",
    "\n",
    "# Copy native extension\n",
    "!cp llcuda/llcuda_cpp*.so llcuda_v2_complete_t4/native/\n",
    "\n",
    "# Create README\n",
    "cat > llcuda_v2_complete_t4/README.md << 'EOF'\n",
    "# llcuda v2.0 Complete Package for Tesla T4\n",
    "\n",
    "**Built on**: Google Colab\n",
    "**GPU**: Tesla T4 (SM 7.5)\n",
    "**CUDA**: 12.x\n",
    "\n",
    "## Contents\n",
    "\n",
    "### binaries/\n",
    "llama.cpp binaries for HTTP server mode:\n",
    "- `bin/llama-server` - HTTP inference server\n",
    "- `bin/llama-cli` - Command-line interface\n",
    "- `bin/llama-quantize` - Model quantization tool\n",
    "- `lib/*.so` - Shared libraries (libggml-cuda.so with FlashAttention)\n",
    "\n",
    "### native/\n",
    "llcuda v2.0 native extension:\n",
    "- `llcuda_cpp.cpython-*.so` - PyTorch-style Tensor API\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "# Extract\n",
    "tar -xzf llcuda_v2_complete_t4.tar.gz\n",
    "\n",
    "# Copy to llcuda package\n",
    "cp -r llcuda_v2_complete_t4/binaries/* ~/llcuda/binaries/cuda12/\n",
    "cp llcuda_v2_complete_t4/native/*.so ~/llcuda/\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "- âœ… FlashAttention enabled\n",
    "- âœ… Tensor Core optimizations\n",
    "- âœ… CUDA Graphs support\n",
    "- âœ… All quantization formats (Q4_K_M, Q8_0, NF4, etc.)\n",
    "- âœ… Optimized for SM 7.5+ GPUs\n",
    "\n",
    "EOF\n",
    "\n",
    "!tar -czf llcuda-v2-complete-t4.tar.gz llcuda_v2_complete_t4/\n",
    "\n",
    "print(\"\\n=== Package 3: Complete Bundle ===\")\n",
    "!ls -lh llcuda-v2-complete-t4.tar.gz\n",
    "!du -h llcuda-v2-complete-t4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf62e5",
   "metadata": {},
   "source": [
    "## Step 6: Summary and Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8509695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all packages\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BUILD COMPLETE - llcuda v2.0 for Tesla T4\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nðŸ“¦ Available Packages:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "packages = [\n",
    "    \"/content/llcuda-binaries-cuda12-t4.tar.gz\",\n",
    "    \"/content/llcuda/llcuda-v2-native-t4.tar.gz\",\n",
    "    \"/content/llcuda-v2-complete-t4.tar.gz\"\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    !ls -lh {pkg}\n",
    "\n",
    "print(\"\\nâœ… All packages built successfully!\")\n",
    "print(\"\\nðŸ“¥ Download instructions:\")\n",
    "print(\"   1. Click the folder icon on the left\")\n",
    "print(\"   2. Right-click on the .tar.gz files\")\n",
    "print(\"   3. Select 'Download'\")\n",
    "print(\"\\n   Or run the cell below to auto-download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-download all packages\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading Package 1: llama.cpp binaries (264 MB)...\")\n",
    "files.download('/content/llcuda-binaries-cuda12-t4.tar.gz')\n",
    "\n",
    "print(\"\\nDownloading Package 2: llcuda v2.0 native extension...\")\n",
    "files.download('/content/llcuda/llcuda-v2-native-t4.tar.gz')\n",
    "\n",
    "print(\"\\nDownloading Package 3: Complete bundle...\")\n",
    "files.download('/content/llcuda-v2-complete-t4.tar.gz')\n",
    "\n",
    "print(\"\\nâœ“ All downloads started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0eef56",
   "metadata": {},
   "source": [
    "## Step 7: Upload to GitHub Releases (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a7b07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install GitHub CLI (if you want to upload directly)\n",
    "!curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg\n",
    "!echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null\n",
    "!sudo apt update\n",
    "!sudo apt install gh -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82824299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with GitHub (manual step)\n",
    "print(\"Run this command and follow the prompts:\")\n",
    "print(\"  gh auth login\")\n",
    "print(\"\\nThen run the upload commands below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d301e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to GitHub releases (after authentication)\n",
    "# Uncomment and modify these commands:\n",
    "\n",
    "# !gh release create v2.0.0 \\\n",
    "#     --repo waqasm86/llcuda \\\n",
    "#     --title \"llcuda v2.0.0 - Tesla T4 Release\" \\\n",
    "#     --notes \"Complete CUDA 12 build for Tesla T4 (SM 7.5)\" \\\n",
    "#     /content/llcuda-binaries-cuda12-t4.tar.gz \\\n",
    "#     /content/llcuda/llcuda-v2-native-t4.tar.gz \\\n",
    "#     /content/llcuda-v2-complete-t4.tar.gz\n",
    "\n",
    "print(\"Upload commands ready (uncomment to use)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1239e",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Build Complete!\n",
    "\n",
    "You now have:\n",
    "\n",
    "1. **llcuda-binaries-cuda12-t4.tar.gz** (264 MB)\n",
    "   - llama.cpp server with FlashAttention\n",
    "   - For HTTP server mode (v1.x compatibility)\n",
    "\n",
    "2. **llcuda-v2-native-t4.tar.gz** (~100 MB)\n",
    "   - llcuda v2.0 native extension\n",
    "   - PyTorch-style Tensor API\n",
    "\n",
    "3. **llcuda-v2-complete-t4.tar.gz** (~350 MB)\n",
    "   - Everything bundled together\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Download packages to your local machine\n",
    "2. Upload to GitHub releases:\n",
    "   ```bash\n",
    "   gh release create v2.0.0 \\\n",
    "       --repo waqasm86/llcuda \\\n",
    "       --title \"llcuda v2.0.0 - Tesla T4 Release\" \\\n",
    "       llcuda-*.tar.gz\n",
    "   ```\n",
    "\n",
    "3. Update bootstrap.py to download from releases\n",
    "\n",
    "4. Test on fresh Colab instance\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: Google Colab Tesla T4 | CUDA 12 | Python 3.10+\n",
    "**For**: llcuda v2.0 - Unsloth Integration\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
