{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llcuda + Unsloth Tutorial - Tesla T4\n",
    "\n",
    "**Tutorial**: Using llcuda as CUDA backend for Unsloth GGUF models\n",
    "\n",
    "**What This Demonstrates**:\n",
    "1. Install llcuda v2.0.1 on Google Colab\n",
    "2. Use llcuda with Unsloth GGUF models\n",
    "3. Run fast inference on Tesla T4 GPU\n",
    "4. Compare performance with and without FlashAttention\n",
    "\n",
    "**Requirements**:\n",
    "- Google Colab with T4 GPU\n",
    "- Runtime: GPU (T4)\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Based on research from:\n",
    "- [Unsloth GGUF Documentation](https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf)\n",
    "- [Unsloth GitHub Repository](https://github.com/unslothai/unsloth)\n",
    "- [llcuda GitHub Repository](https://github.com/waqasm86/llcuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU (Must be Tesla T4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpu_info = result.stdout.strip().split(',')\n",
    "gpu_name = gpu_info[0].strip()\n",
    "compute_cap = gpu_info[1].strip()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Compute Capability: SM {compute_cap}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if 'T4' in gpu_name and compute_cap == '7.5':\n",
    "    print(\"\\n‚úÖ Tesla T4 detected - Perfect for llcuda!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  {gpu_name} detected - llcuda v2.0.1 is optimized for T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install llcuda v2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llcuda (will auto-download CUDA binaries on first import)\n",
    "!pip install -q llcuda\n",
    "\n",
    "print(\"‚úÖ llcuda installed\")\n",
    "print(\"\\nNote: CUDA binaries (~140 MB) will be downloaded on first import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import llcuda (triggers binary download)\n",
    "import llcuda\n",
    "\n",
    "print(f\"\\n‚úÖ llcuda version: {llcuda.__version__}\")\n",
    "print(\"\\nIf this is first run, binaries were just downloaded\")\n",
    "print(\"Subsequent runs will use cached binaries (instant)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU compatibility\n",
    "compat = llcuda.check_gpu_compatibility()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"GPU Compatibility Check\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"GPU: {compat['gpu_name']}\")\n",
    "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
    "print(f\"Compatible: {compat['compatible']}\")\n",
    "print(f\"Platform: {compat['platform']}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Using llcuda with Unsloth GGUF Models\n",
    "\n",
    "### Method 1: Model Registry (Easiest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load Gemma 3-1B from registry (auto-downloads from HuggingFace)\n",
    "print(\"üì• Loading Gemma 3-1B Q4_K_M...\")\n",
    "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
    "\n",
    "engine.load_model(\n",
    "    \"gemma-3-1b-Q4_K_M\",  # Registry name\n",
    "    silent=True,           # Suppress server output\n",
    "    auto_start=True        # Start server automatically\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(\"\\nGenerating...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*70)\n",
    "print(result.text)\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Tokens: {result.tokens_generated}\")\n",
    "print(f\"  Latency: {result.latency_ms:.1f} ms\")\n",
    "print(f\"  Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nExpected on T4: ~45 tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Direct HuggingFace Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace syntax: repo:filename\n",
    "engine2 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8091\")\n",
    "\n",
    "print(\"üì• Loading from Unsloth HuggingFace repository...\\n\")\n",
    "\n",
    "engine2.load_model(\n",
    "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    silent=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded from Unsloth repo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different prompt\n",
    "prompt2 = \"Write a Python function to calculate fibonacci numbers.\"\n",
    "\n",
    "print(f\"\\nPrompt: {prompt2}\")\n",
    "print(\"\\nGenerating...\\n\")\n",
    "\n",
    "result2 = engine2.infer(\n",
    "    prompt2,\n",
    "    max_tokens=200,\n",
    "    temperature=0.3  # Lower temperature for code\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(result2.text)\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSpeed: {result2.tokens_per_sec:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple prompts\n",
    "prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain neural networks briefly.\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"\\nüöÄ Running batch inference...\\n\")\n",
    "\n",
    "results = engine.batch_infer(prompts, max_tokens=80)\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Query {i}: {prompt}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(result.text)\n",
    "    print(f\"\\nSpeed: {result.tokens_per_sec:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated metrics\n",
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Throughput:\")\n",
    "print(f\"  Total requests: {metrics['throughput']['total_requests']}\")\n",
    "print(f\"  Total tokens: {metrics['throughput']['total_tokens']}\")\n",
    "print(f\"  Avg speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Latency:\")\n",
    "print(f\"  Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
    "print(f\"  P50: {metrics['latency']['p50_ms']:.1f} ms\")\n",
    "print(f\"  P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
    "print(f\"  P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
    "print(f\"  Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
    "print(f\"  Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
    "\n",
    "print(f\"\\nüìà Sample count: {metrics['latency']['sample_count']}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing Different Models\n",
    "\n",
    "Try larger models (if you have enough VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available models in registry\n",
    "from llcuda._internal.registry import list_registry_models\n",
    "\n",
    "models = list_registry_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AVAILABLE MODELS IN REGISTRY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, info in models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Size: {info['size_mb']} MB\")\n",
    "    print(f\"  Min VRAM: {info['min_vram_gb']} GB\")\n",
    "    print(f\"  {info['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Llama 3.2-3B (if you have enough memory)\n",
    "# Uncomment to try:\n",
    "\n",
    "# engine3 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8092\")\n",
    "# engine3.load_model(\"unsloth/llama-3.2-3b-Q4_K_M.gguf\", silent=True)\n",
    "# result = engine3.infer(\"What is AI?\", max_tokens=100)\n",
    "# print(result.text)\n",
    "# print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(\"\\nTip: Tesla T4 has 16GB VRAM\")\n",
    "print(\"     Can run models up to ~8B parameters with Q4 quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Integration with Unsloth Training\n",
    "\n",
    "### Scenario: Fine-tune with Unsloth ‚Üí Export GGUF ‚Üí Inference with llcuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example workflow (conceptual - not running full training)\n",
    "\n",
    "print(\"\"\"\n",
    "UNSLOTH + llcuda WORKFLOW\n",
    "=========================\n",
    "\n",
    "1. Fine-tune with Unsloth:\n",
    "   ```python\n",
    "   from unsloth import FastLanguageModel\n",
    "   \n",
    "   model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "       \"unsloth/gemma-3-1b-it\",\n",
    "       max_seq_length=2048,\n",
    "       load_in_4bit=True\n",
    "   )\n",
    "   \n",
    "   # Add LoRA adapters and train...\n",
    "   model = FastLanguageModel.get_peft_model(model, ...)\n",
    "   trainer.train()\n",
    "   ```\n",
    "\n",
    "2. Export to GGUF:\n",
    "   ```python\n",
    "   # After training\n",
    "   model.save_pretrained_gguf(\n",
    "       \"my_model\",\n",
    "       tokenizer,\n",
    "       quantization_method=\"q4_k_m\"\n",
    "   )\n",
    "   ```\n",
    "\n",
    "3. Deploy with llcuda:\n",
    "   ```python\n",
    "   import llcuda\n",
    "   \n",
    "   engine = llcuda.InferenceEngine()\n",
    "   engine.load_model(\"my_model/unsloth.Q4_K_M.gguf\")\n",
    "   \n",
    "   result = engine.infer(\"Your prompt\", max_tokens=100)\n",
    "   print(result.text)\n",
    "   ```\n",
    "\n",
    "Benefits:\n",
    "- Fast training with Unsloth (2x faster, 70% less VRAM)\n",
    "- Fast inference with llcuda (FlashAttention, T4-optimized)\n",
    "- Easy deployment (GGUF format, single file)\n",
    "- Compatible with llama.cpp ecosystem\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom generation parameters\n",
    "result = engine.infer(\n",
    "    \"Write a short poem about AI.\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,      # Higher = more creative\n",
    "    top_p=0.95,           # Nucleus sampling\n",
    "    top_k=50,             # Top-k sampling\n",
    "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATIVE GENERATION (temp=0.9)\")\n",
    "print(\"=\"*70)\n",
    "print(result.text)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager usage (auto-cleanup)\n",
    "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
    "    temp_engine.load_model(\"gemma-3-1b-Q4_K_M\", silent=True)\n",
    "    result = temp_engine.infer(\"Quick test\", max_tokens=20)\n",
    "    print(f\"\\nQuick test: {result.text}\")\n",
    "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(\"\\n‚úÖ Engine auto-cleaned up after context exit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Benchmark Different Quantizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Q4_K_M vs Q8_0 (if you have time)\n",
    "# Note: Q8_0 is slower but higher quality\n",
    "\n",
    "test_prompt = \"Explain the theory of relativity.\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUANTIZATION COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Q4_K_M (already loaded)\n",
    "result_q4 = engine.infer(test_prompt, max_tokens=50)\n",
    "print(f\"\\nQ4_K_M:\")\n",
    "print(f\"  Speed: {result_q4.tokens_per_sec:.1f} tok/s\")\n",
    "print(f\"  Latency: {result_q4.latency_ms:.1f} ms\")\n",
    "\n",
    "# To test Q8_0, you would load a Q8_0 model:\n",
    "# engine_q8 = llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8094\")\n",
    "# engine_q8.load_model(\"gemma-3-1b-Q8_0\", silent=True)\n",
    "# result_q8 = engine_q8.infer(test_prompt, max_tokens=50)\n",
    "# print(f\"\\nQ8_0:\")\n",
    "# print(f\"  Speed: {result_q8.tokens_per_sec:.1f} tok/s\")\n",
    "# print(f\"  Latency: {result_q8.latency_ms:.1f} ms\")\n",
    "\n",
    "print(\"\\nTypical results on T4:\")\n",
    "print(\"  Q4_K_M: ~45 tok/s (smaller, faster)\")\n",
    "print(\"  Q8_0: ~35 tok/s (larger, higher quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "‚úÖ **Installation**: llcuda v2.0.1 with automatic binary download\n",
    "‚úÖ **Model Loading**: From registry and HuggingFace\n",
    "‚úÖ **Inference**: Single and batch prompts\n",
    "‚úÖ **Performance**: ~45 tok/s on Tesla T4 with Q4_K_M\n",
    "‚úÖ **Integration**: Unsloth fine-tuning ‚Üí llcuda deployment workflow\n",
    "\n",
    "### Performance Summary (Tesla T4)\n",
    "\n",
    "| Model | Quantization | Speed | VRAM |\n",
    "|-------|--------------|-------|------|\n",
    "| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB |\n",
    "| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB |\n",
    "| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB |\n",
    "| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB |\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- ‚úÖ FlashAttention (2-3x faster for long contexts)\n",
    "- ‚úÖ Tensor Core optimization\n",
    "- ‚úÖ CUDA Graphs (reduced overhead)\n",
    "- ‚úÖ All quantization formats\n",
    "- ‚úÖ Seamless Unsloth integration\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **llcuda**: https://github.com/waqasm86/llcuda\n",
    "- **Unsloth**: https://github.com/unslothai/unsloth\n",
    "- **Unsloth GGUF Docs**: https://unsloth.ai/docs/basics/inference-and-deployment/saving-to-gguf\n",
    "- **llama.cpp**: https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: llcuda v2.0.1 | Tesla T4 | CUDA 12 | Unsloth Integration\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
