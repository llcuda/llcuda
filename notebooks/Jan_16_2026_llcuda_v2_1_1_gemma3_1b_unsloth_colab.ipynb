{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f421925d",
   "metadata": {},
   "source": [
    "# llcuda v2.1.1 + Gemma 3-1B-IT on Google Colab (Tesla T4)\n",
    "\n",
    "**Date**: January 16, 2026  \n",
    "**llcuda Version**: v2.1.1 (GitHub-only distribution)  \n",
    "**GPU**: Tesla T4 (SM 7.5)  \n",
    "**Model**: Gemma 3-1B-IT Instruct (Q4_K_M quantization)  \n",
    "**Framework**: Unsloth HuggingFace Integration\n",
    "\n",
    "**Key Improvements in v2.1.1**:\n",
    "- âœ… Fixed circular import bug (RuntimeWarning eliminated)\n",
    "- âœ… Reliable binary discovery and fallback mechanism\n",
    "- âœ… T4-optimized CUDA 12 binaries with FlashAttention\n",
    "- âœ… Seamless Unsloth model loading from HuggingFace\n",
    "\n",
    "**Note**: This notebook includes cache clearing for clean installations. Skip cell 1 if running on a fresh Colab session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cf62e",
   "metadata": {},
   "source": [
    "## Step 0: Clean Installation (Optional - For Fresh Start)\n",
    "\n",
    "**Use this cell if you:**\n",
    "- Previously installed llcuda and want a completely clean start\n",
    "- Encountered the circular import RuntimeWarning\n",
    "- Have cached old binaries from v2.0.6 or earlier v2.1.0\n",
    "\n",
    "**Skip this cell if:**\n",
    "- Using a fresh Colab notebook (first time)\n",
    "- Just restarted the kernel\n",
    "- Don't have old llcuda installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Clean installation for fresh start\n",
    "# Skip this cell if using a fresh Colab notebook\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning old llcuda installations for Google Colab (Tesla T4)...\\n\")\n",
    "\n",
    "# Step 1: Uninstall old llcuda package\n",
    "print(\"Step 1: Uninstalling old llcuda package...\")\n",
    "!pip uninstall -y llcuda >/dev/null 2>&1\n",
    "print(\"âœ… Old package removed\\n\")\n",
    "\n",
    "# Step 2: Clear pip cache\n",
    "print(\"Step 2: Clearing pip package cache...\")\n",
    "!pip cache purge >/dev/null 2>&1\n",
    "print(\"âœ… Pip cache cleared\\n\")\n",
    "\n",
    "# Step 3: Remove cached CUDA binaries\n",
    "print(\"Step 3: Removing cached CUDA binaries...\")\n",
    "cache_dir = os.path.expanduser(\"~/.cache/llcuda\")\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(f\"âœ… Cleared {cache_dir}/\\n\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  No cached binaries found (first installation)\\n\")\n",
    "\n",
    "# Step 4: Display clean state\n",
    "print(\"=\"*70)\n",
    "print(\"âœ… CLEAN INSTALLATION READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“ What was cleaned:\")\n",
    "print(\"   â€¢ Uninstalled old llcuda package\")\n",
    "print(\"   â€¢ Cleared pip package cache\")\n",
    "print(\"   â€¢ Removed ~/.cache/llcuda/ binaries\")\n",
    "print(\"\\nâš¡ Next steps:\")\n",
    "print(\"   1. Run Step 1 to verify Tesla T4 GPU\")\n",
    "print(\"   2. Run Step 2 to install fresh llcuda v2.1.1 from GitHub\")\n",
    "print(\"   3. Subsequent steps will download v2.1.1 binaries\")\n",
    "print(\"   4. No RuntimeWarning about circular imports!\")\n",
    "print(\"\\nğŸ’¡ Colab Note: If errors persist after cleanup:\")\n",
    "print(\"   â€¢ Menu â†’ Kernel â†’ Restart kernel (then run again)\")\n",
    "print(\"   â€¢ This clears Python's module cache\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed4518",
   "metadata": {},
   "source": [
    "## Step 1: Verify Tesla T4 GPU\n",
    "\n",
    "llcuda v2.1.1 ships with Tesla T4-optimized CUDA 12 binaries (v2.1.1 bundle) and automatically falls back to the proven v2.0.6 bundle only if the latest archive is unavailable. Fixed llama-server discovery means fallback download works seamlessly (no more NameError)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU configuration\n",
    "!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n",
    "\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpu_info = result.stdout.strip().split(',')\n",
    "gpu_name = gpu_info[0].strip()\n",
    "compute_cap = gpu_info[1].strip()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Compute Capability: SM {compute_cap}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if 'T4' in gpu_name and compute_cap == '7.5':\n",
    "    print(\"\\nâœ… Tesla T4 detected - Perfect for llcuda v2.1.1!\")\n",
    "    print(\"   Binaries (v2.1.1) include FlashAttention and Tensor Core optimization\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  {gpu_name} detected\")\n",
    "    print(\"   llcuda v2.1.1 is optimized for Tesla T4\")\n",
    "    print(\"   Performance may vary on other GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e92df46",
   "metadata": {},
   "source": [
    "## Step 2: Install llcuda v2.1.1 from GitHub\n",
    "\n",
    "**Note**: llcuda is now **GitHub-only** (no longer on PyPI)\n",
    "\n",
    "The primary CUDA binary bundle (v2.1.1, ~267 MB) auto-downloads from GitHub Releases on first import. If that archive is temporarily unavailable, the bootstrapper transparently falls back to the v2.0.6 bundle that remains 100% compatible with the v2.1.1 Python APIs. **Fixed in v2.1.1**: llama-server discovery now works correctly with no NameError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b594f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llcuda v2.1.1 from GitHub\n",
    "print(\"ğŸ“¥ Installing llcuda v2.1.1 from GitHub...\\n\")\n",
    "\n",
    "!pip install -q \"git+https://github.com/llcuda/llcuda.git@v2.1.1\"\n",
    "\n",
    "print(\"\\nâœ… llcuda v2.1.1 installed successfully!\")\n",
    "print(\"\\nâ„¹ï¸  CUDA binaries (v2.1.1, ~267 MB) will auto-download on first import\")\n",
    "print(\"   These binaries are fully compatible with v2.1.1 Python APIs\")\n",
    "print(\"   Download happens once - subsequent runs use cached binaries\")\n",
    "print(\"   âœ¨ New in v2.1.1: Fixed llama-server fallback mechanism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d13212",
   "metadata": {},
   "source": [
    "## Step 3: Import llcuda (Triggers Binary Download)\n",
    "\n",
    "First import downloads CUDA binaries from:\n",
    "https://github.com/llcuda/llcuda/releases/download/v2.1.1/llcuda-binaries-cuda12-t4-v2.1.1.tar.gz\n",
    "\n",
    "**Note**: If the v2.1.1 archive is unavailable, llcuda automatically falls back to the v2.0.6 bundle so the notebook can continue running without manual intervention. This fallback mechanism has been fixed in v2.1.1 to work reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llcuda\n",
    "import time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"llcuda version: {llcuda.__version__}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nâœ… llcuda imported successfully!\")\n",
    "print(\"\\nâ„¹ï¸  If this was the first run:\")\n",
    "print(\"   - CUDA binaries were downloaded to ~/.cache/llcuda/\")\n",
    "print(\"   - Includes: llama-server, libggml-cuda.so, and supporting libs\")\n",
    "print(\"   - Next imports will be instant (binaries cached)\")\n",
    "print(\"   - v2.1.1 feature: Reliable llama-server discovery and fallback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab6fd0",
   "metadata": {},
   "source": [
    "## Step 4: Verify GPU Compatibility\n",
    "\n",
    "Check if llcuda can detect and use the GPU properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eea891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU compatibility with llcuda\n",
    "compat = llcuda.check_gpu_compatibility()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GPU COMPATIBILITY CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"GPU Name: {compat['gpu_name']}\")\n",
    "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
    "print(f\"Platform: {compat['platform']}\")\n",
    "print(f\"Compatible: {compat['compatible']}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if compat['compatible']:\n",
    "    print(\"\\nâœ… GPU is compatible with llcuda!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  GPU compatibility issue detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74fede8",
   "metadata": {},
   "source": [
    "## Step 5: Load Gemma 3-1B-IT from Unsloth\n",
    "\n",
    "**Three methods to load models:**\n",
    "\n",
    "1. **HuggingFace Hub** (recommended): Direct from Unsloth's repo\n",
    "2. **Model Registry**: Pre-configured model names\n",
    "3. **Local Path**: From downloaded GGUF file\n",
    "\n",
    "We'll use Method 1: Direct from Unsloth HuggingFace repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\")\n",
    "print(\"   Repository: unsloth/gemma-3-1b-it-GGUF\")\n",
    "print(\"   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\")\n",
    "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
    "\n",
    "# Load model from Unsloth HuggingFace repository\n",
    "# Format: repo_id:filename\n",
    "start_time = time.time()\n",
    "\n",
    "engine.load_model(\n",
    "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    silent=True,        # Suppress llama-server output\n",
    "    auto_start=True     # Start server automatically\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Model loaded successfully in {load_time:.1f}s!\")\n",
    "print(\"\\nğŸš€ Ready for inference!\")\n",
    "print(\"   v2.1.1 now features improved llama-server discovery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff753d0f",
   "metadata": {},
   "source": [
    "## Step 6: First Inference - General Knowledge\n",
    "\n",
    "Test the model with a general knowledge question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc42542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a general knowledge prompt\n",
    "prompt = \"Explain quantum computing in simple terms that a beginner can understand.\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROMPT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(prompt)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nğŸ¤– Generating response...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"RESPONSE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result.text)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance:\")\n",
    "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
    "print(f\"   Latency: {result.latency_ms:.1f} ms\")\n",
    "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nğŸ’¡ Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40db58",
   "metadata": {},
   "source": [
    "## Step 7: Code Generation\n",
    "\n",
    "Test the model's ability to generate Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code generation\n",
    "code_prompt = \"\"\"Write a Python function to calculate the fibonacci sequence using dynamic programming.\n",
    "Include docstring and example usage.\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CODE GENERATION TEST\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prompt: {code_prompt}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nğŸ¤– Generating code...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    code_prompt,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3,  # Lower temperature for more deterministic code\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(result.text)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce1d0e",
   "metadata": {},
   "source": [
    "## Step 8: Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently with batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a05f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare multiple prompts\n",
    "prompts = [\n",
    "    \"What is machine learning in one sentence?\",\n",
    "    \"Explain neural networks briefly.\",\n",
    "    \"What is the difference between AI and ML?\",\n",
    "    \"Define deep learning concisely.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BATCH INFERENCE - Processing 4 prompts\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"Query {i}: {prompt}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(result.text)\n",
    "    print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tok/s | Latency: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total batch time: {total_time:.1f}s for {len(prompts)} prompts\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58136558",
   "metadata": {},
   "source": [
    "## Step 9: Performance Metrics\n",
    "\n",
    "Analyze aggregated performance metrics across all requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c50651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive metrics\n",
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERFORMANCE METRICS SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Throughput:\")\n",
    "print(f\"   Total requests: {metrics['throughput']['total_requests']}\")\n",
    "print(f\"   Total tokens: {metrics['throughput']['total_tokens']}\")\n",
    "print(f\"   Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  Latency Distribution:\")\n",
    "print(f\"   Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
    "print(f\"   Median (P50): {metrics['latency']['p50_ms']:.1f} ms\")\n",
    "print(f\"   P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
    "print(f\"   P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
    "print(f\"   Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
    "print(f\"   Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Sample count: {metrics['latency']['sample_count']}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e856de",
   "metadata": {},
   "source": [
    "## Step 10: Advanced Generation Parameters\n",
    "\n",
    "Explore different generation strategies and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a66650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test creative writing with higher temperature\n",
    "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CREATIVE GENERATION (High Temperature)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prompt: {creative_prompt}\")\n",
    "print(f\"Parameters: temperature=0.9, top_p=0.95, top_k=50\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    creative_prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,     # High creativity\n",
    "    top_p=0.95,          # Nucleus sampling\n",
    "    top_k=50,            # Top-k sampling\n",
    "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
    ")\n",
    "\n",
    "print(result.text)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b55bc3",
   "metadata": {},
   "source": [
    "## Step 11: Alternative Model Loading Methods\n",
    "\n",
    "Demonstrate other ways to load models with llcuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef5eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALTERNATIVE MODEL LOADING METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£  HuggingFace Hub (Current method - RECOMMENDED):\")\n",
    "print(\"   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\")\n",
    "print(\"   âœ… Direct from Unsloth repository\")\n",
    "print(\"   âœ… Auto-downloads and caches\")\n",
    "print(\"   âœ… Always up-to-date\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Model Registry (Pre-configured shortcuts):\")\n",
    "print(\"   engine.load_model('gemma-3-1b-Q4_K_M')\")\n",
    "print(\"   âœ… Simple one-word names\")\n",
    "print(\"   âœ… Curated model list\")\n",
    "print(\"   âš ï¸  May not include all Unsloth models\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  Local Path (Pre-downloaded GGUF):\")\n",
    "print(\"   engine.load_model('/path/to/model.gguf')\")\n",
    "print(\"   âœ… Full control over model files\")\n",
    "print(\"   âœ… No network dependency after download\")\n",
    "print(\"   âš ï¸  Manual model management\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: For Unsloth models, use method 1 (HuggingFace Hub)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecd236",
   "metadata": {},
   "source": [
    "## Step 12: Unsloth Fine-tuning â†’ llcuda Inference Workflow\n",
    "\n",
    "Example workflow showing how to integrate llcuda with Unsloth fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca883cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘          UNSLOTH FINE-TUNING â†’ llcuda INFERENCE WORKFLOW             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "STEP 1: Fine-tune with Unsloth\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"unsloth/gemma-3-1b-it\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "# Fine-tune on your dataset\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    ...\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "STEP 2: Export to GGUF\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Save fine-tuned model as GGUF (Q4_K_M quantization)\n",
    "model.save_pretrained_gguf(\n",
    "    \"my_finetuned_model\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\"\n",
    ")\n",
    "\n",
    "# This creates: my_finetuned_model/unsloth.Q4_K_M.gguf\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "STEP 3: Deploy with llcuda v2.1.1\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import llcuda\n",
    "\n",
    "# Load your fine-tuned GGUF model\n",
    "engine = llcuda.InferenceEngine()\n",
    "engine.load_model(\"my_finetuned_model/unsloth.Q4_K_M.gguf\")\n",
    "\n",
    "# Run inference\n",
    "result = engine.infer(\"Your task-specific prompt\", max_tokens=200)\n",
    "print(result.text)\n",
    "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "âœ… BENEFITS:\n",
    "   â€¢ Fast training with Unsloth (2x faster, 70% less VRAM)\n",
    "   â€¢ Fast inference with llcuda v2.1.1 (FlashAttention, T4-optimized)\n",
    "   â€¢ Easy deployment (GGUF = single portable file)\n",
    "   â€¢ Full llama.cpp ecosystem compatibility\n",
    "   â€¢ Production-ready inference server\n",
    "   â€¢ Reliable binary discovery (fixed in v2.1.1)\n",
    "\n",
    "ğŸ“Š EXPECTED PERFORMANCE (Tesla T4 with v2.1.1):\n",
    "   â€¢ Gemma 3-1B Q4_K_M: ~45 tok/s\n",
    "   â€¢ Llama 3.2-3B Q4_K_M: ~30 tok/s\n",
    "   â€¢ Qwen 2.5-7B Q4_K_M: ~18 tok/s\n",
    "   â€¢ Llama 3.1-8B Q4_K_M: ~15 tok/s\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4b821",
   "metadata": {},
   "source": [
    "## Step 13: Context Manager Usage\n",
    "\n",
    "Use llcuda with Python context managers for automatic cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e03cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager automatically cleans up resources\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXT MANAGER DEMO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
    "    print(\"ğŸ“¥ Loading model in context...\")\n",
    "    temp_engine.load_model(\n",
    "        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ¤– Running quick test...\\n\")\n",
    "    result = temp_engine.infer(\n",
    "        \"What is the capital of France?\",\n",
    "        max_tokens=30\n",
    "    )\n",
    "\n",
    "    print(f\"Response: {result.text}\")\n",
    "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(\"\\nâœ… Context exited - engine automatically cleaned up\")\n",
    "print(\"   Server stopped, resources released\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c71185",
   "metadata": {},
   "source": [
    "## Step 14: Available Unsloth Models\n",
    "\n",
    "Browse popular Unsloth GGUF models compatible with llcuda v2.1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘              POPULAR UNSLOTH GGUF MODELS FOR llcuda v2.1.1          â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ”¹ SMALL MODELS (1-3B) - Best for T4\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. Gemma 3-1B Instruct\n",
    "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
    "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
    "   Speed on T4: ~45 tok/s | VRAM: ~1.2 GB\n",
    "   Use case: General chat, Q&A, reasoning\n",
    "\n",
    "2. Llama 3.2-3B Instruct\n",
    "   Repository: unsloth/Llama-3.2-3B-Instruct-GGUF\n",
    "   File: Llama-3.2-3B-Instruct-Q4_K_M.gguf (~2 GB)\n",
    "   Speed on T4: ~30 tok/s | VRAM: ~2.0 GB\n",
    "   Use case: Instruction following, chat\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ğŸ”¹ MEDIUM MODELS (7-8B) - Fits on T4\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "3. Qwen 2.5-7B Instruct\n",
    "   Repository: unsloth/Qwen2.5-7B-Instruct-GGUF\n",
    "   File: Qwen2.5-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
    "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
    "   Use case: Multilingual, coding, math\n",
    "\n",
    "4. Llama 3.1-8B Instruct\n",
    "   Repository: unsloth/Meta-Llama-3.1-8B-Instruct-GGUF\n",
    "   File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB)\n",
    "   Speed on T4: ~15 tok/s | VRAM: ~5.5 GB\n",
    "   Use case: Advanced reasoning, long context\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ğŸ”¹ CODE MODELS\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "5. Qwen 2.5-Coder-7B\n",
    "   Repository: unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n",
    "   File: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
    "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
    "   Use case: Code generation, debugging\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ğŸ’¡ LOADING SYNTAX:\n",
    "\n",
    "engine.load_model(\"unsloth/REPO-NAME:filename.gguf\")\n",
    "\n",
    "Example:\n",
    "engine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\")\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "ğŸ“¦ QUANTIZATION GUIDE:\n",
    "   â€¢ Q4_K_M: Best balance (recommended for T4)\n",
    "   â€¢ Q5_K_M: Higher quality, slower\n",
    "   â€¢ Q8_0: Near full precision, much slower\n",
    "   â€¢ Q2_K: Smallest, lowest quality\n",
    "\n",
    "ğŸ¯ T4 GPU LIMITS:\n",
    "   â€¢ Total VRAM: 16 GB\n",
    "   â€¢ Recommended max model: 8B parameters (Q4_K_M)\n",
    "   â€¢ Leave ~2-3 GB for context and processing\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88619b",
   "metadata": {},
   "source": [
    "## ğŸ“Š Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "âœ… **Installed llcuda v2.1.1** from GitHub (GitHub-only distribution)\n",
    "âœ… **Auto-downloaded binaries** from GitHub Releases (v2.1.1 primary bundle with seamless v2.0.6 fallback)\n",
    "âœ… **Fixed llama-server discovery** in v2.1.1 (no more NameError on fallback)\n",
    "âœ… **Loaded Gemma 3-1B-IT** GGUF from Unsloth HuggingFace\n",
    "âœ… **Ran inference** with ~45 tok/s on Tesla T4\n",
    "âœ… **Batch processing** multiple prompts efficiently\n",
    "âœ… **Performance analysis** with detailed metrics\n",
    "âœ… **Demonstrated workflow** from Unsloth fine-tuning to llcuda deployment\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Results (Tesla T4 with v2.1.1)\n",
    "\n",
    "| Model | Quantization | Speed | VRAM | Context |\n",
    "|-------|--------------|-------|------|----------|\n",
    "| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB | 2048 |\n",
    "| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB | 4096 |\n",
    "| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB | 8192 |\n",
    "| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB | 8192 |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Features of llcuda v2.1.1\n",
    "\n",
    "ğŸš€ **GitHub-Only Distribution**\n",
    "- No PyPI dependency\n",
    "- Install: `pip install \"git+https://github.com/llcuda/llcuda.git@v2.1.1\"`\n",
    "- Binaries auto-download from GitHub Releases (v2.1.1 primary archive; automatic fallback to v2.0.6)\n",
    "\n",
    "âš¡ **Performance Optimizations**\n",
    "- FlashAttention support (2-3x faster for long contexts)\n",
    "- Tensor Core optimization for SM 7.5 (Tesla T4)\n",
    "- CUDA Graphs for reduced overhead\n",
    "- All quantization formats supported\n",
    "\n",
    "ğŸ”„ **Seamless Unsloth Integration**\n",
    "- Direct loading from Unsloth HuggingFace repos\n",
    "- Compatible with Unsloth fine-tuned GGUF exports\n",
    "- Full llama.cpp ecosystem support\n",
    "\n",
    "âœ¨ **v2.1.1 Improvements**\n",
    "- Fixed llama-server discovery mechanism\n",
    "- Reliable fallback to v2.0.6 binaries (no NameError)\n",
    "- Enhanced Colab compatibility\n",
    "- Consistent versioning across entire project\n",
    "\n",
    "---\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **llcuda GitHub**: https://github.com/llcuda/llcuda\n",
    "- **Installation Guide**: https://github.com/llcuda/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
    "- **Releases**: https://github.com/llcuda/llcuda/releases\n",
    "- **Unsloth**: https://github.com/unslothai/unsloth\n",
    "- **Unsloth Models**: https://huggingface.co/unsloth\n",
    "- **Unsloth GGUF Docs**: https://docs.unsloth.ai/basics/saving-to-gguf\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try different models**: Experiment with larger models from Unsloth\n",
    "2. **Fine-tune with Unsloth**: Train on your custom dataset\n",
    "3. **Export to GGUF**: Use Unsloth's export functionality\n",
    "4. **Deploy with llcuda**: Fast inference on your fine-tuned models\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: llcuda v2.1.1 | Tesla T4 | CUDA 12 | Unsloth Integration | FlashAttention\n",
    "\n",
    "**Author**: Waqas Muhammad (waqasm86@gmail.com)  \n",
    "**License**: MIT"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
