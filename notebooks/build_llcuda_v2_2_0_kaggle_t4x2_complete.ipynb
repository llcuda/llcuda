{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a36ca5",
   "metadata": {},
   "source": [
    "# llcuda v2.2.0 - Kaggle 2√ó T4 Build Notebook\n",
    "\n",
    "## Architecture: Split-GPU Workload\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         GPU 0             ‚îÇ            GPU 1              ‚îÇ\n",
    "‚îÇ  llama-server (GGUF)      ‚îÇ  RAPIDS + Graphistry          ‚îÇ\n",
    "‚îÇ  LLM Inference            ‚îÇ  Graph Visualization (cuGraph)‚îÇ\n",
    "‚îÇ  15GB VRAM                ‚îÇ  15GB VRAM                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "This notebook builds llcuda binaries for **split-GPU** operation:\n",
    "- **GPU 0**: llama-server with GGUF model (LLM inference)\n",
    "- **GPU 1**: RAPIDS/Graphistry with cuDF/cuGraph (graph simulation)\n",
    "\n",
    "## Step 1: Verify Kaggle GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b107f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have 2√ó T4 GPUs\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check nvidia-smi\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "gpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\n",
    "print(f\"\\nüìä Detected GPUs: {len(gpu_lines)}\")\n",
    "for line in gpu_lines:\n",
    "    print(f\"   {line}\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(\"\\nüìä CUDA Version:\")\n",
    "!nvcc --version | grep release\n",
    "\n",
    "# Check total VRAM\n",
    "print(\"\\nüìä VRAM Summary:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# Verify we have 2 GPUs\n",
    "if len(gpu_lines) >= 2:\n",
    "    print(\"\\n‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Less than 2 GPUs detected!\")\n",
    "    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334985f0",
   "metadata": {},
   "source": [
    "## Step 2: Install Build Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ce1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install CMake, Ninja, and other build tools\n",
    "print(\"Installing build dependencies...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq cmake ninja-build ccache build-essential\n",
    "!pip install -q huggingface_hub tqdm requests sseclient-py\n",
    "\n",
    "print(\"\\n‚úÖ Build dependencies installed\")\n",
    "!cmake --version | head -1\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eee12f",
   "metadata": {},
   "source": [
    "## Step 2b: Install RAPIDS + Graphistry (GPU 1 Workload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097adecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install RAPIDS (cuDF, cuGraph, cuML) for GPU 1 graph workload\n",
    "# This runs alongside llama-server on GPU 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSTALLING RAPIDS + GRAPHISTRY FOR GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install RAPIDS via pip (CUDA 12)\n",
    "!pip install -q \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cudf-cu12 cuml-cu12 cugraph-cu12 pylibraft-cu12 raft-dask-cu12\n",
    "\n",
    "# Install Graphistry with GPU support\n",
    "!pip install -q graphistry[ai]\n",
    "\n",
    "# Verify installation\n",
    "print(\"\\nüì¶ Installed packages:\")\n",
    "import cudf\n",
    "import cugraph\n",
    "print(f\"   cuDF version: {cudf.__version__}\")\n",
    "print(f\"   cuGraph version: {cugraph.__version__}\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"   Graphistry version: {graphistry.__version__}\")\n",
    "except:\n",
    "    print(\"   Graphistry: available (no __version__)\")\n",
    "\n",
    "print(\"\\n‚úÖ RAPIDS + Graphistry installed for GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69ed34",
   "metadata": {},
   "source": [
    "## Step 3: Clone llama.cpp (Latest Stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fa6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Clean any previous build\n",
    "!rm -rf llama.cpp\n",
    "\n",
    "# Clone llama.cpp\n",
    "print(\"Cloning llama.cpp...\")\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
    "\n",
    "os.chdir(\"llama.cpp\")\n",
    "\n",
    "# Get commit info\n",
    "print(\"\\nüì¶ llama.cpp Version:\")\n",
    "!git log -1 --oneline\n",
    "!git describe --tags --always 2>/dev/null || echo \"(no tag)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa16d20",
   "metadata": {},
   "source": [
    "## Step 4: Configure CMake for Dual T4 (SM 7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf67503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Clean previous build\n",
    "!rm -rf build\n",
    "\n",
    "print(\"Configuring CMake for Kaggle 2√ó Tesla T4...\")\n",
    "print(\"\")\n",
    "\n",
    "# CMake configuration for dual T4\n",
    "# Key flags:\n",
    "# - GGML_CUDA=ON: Enable CUDA backend\n",
    "# - CMAKE_CUDA_ARCHITECTURES=75: Tesla T4 (Turing, SM 7.5)\n",
    "# - GGML_CUDA_FA_ALL_QUANTS=ON: FlashAttention for ALL quantization types\n",
    "# - BUILD_SHARED_LIBS=OFF: Static linking for portability\n",
    "# - GGML_NATIVE=OFF: Don't optimize for build machine (for portability)\n",
    "\n",
    "cmake_cmd = \"\"\"\n",
    "cmake -B build -G Ninja \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DBUILD_SHARED_LIBS=OFF \\\n",
    "    -DLLAMA_BUILD_EXAMPLES=ON \\\n",
    "    -DLLAMA_BUILD_TESTS=OFF \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DCMAKE_C_COMPILER=gcc \\\n",
    "    -DCMAKE_CXX_COMPILER=g++\n",
    "\"\"\"\n",
    "\n",
    "!{cmake_cmd}\n",
    "\n",
    "print(\"\\n‚úÖ CMake configuration complete!\")\n",
    "print(\"   Target: SM 7.5 (Tesla T4)\")\n",
    "print(\"   FlashAttention: All quantization types\")\n",
    "print(\"   Static linking: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267abd9c",
   "metadata": {},
   "source": [
    "## Step 5: Build llama.cpp (This takes ~8-12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Get CPU count for parallel build\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Building with {cpu_count} parallel jobs...\")\n",
    "print(\"This will take approximately 8-12 minutes.\\n\")\n",
    "\n",
    "# Build\n",
    "build_result = os.system(f\"cmake --build build --config Release -j{cpu_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify build succeeded\n",
    "if build_result == 0 and os.path.exists(\"build/bin/llama-server\"):\n",
    "    print(\"‚úÖ BUILD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    !ls -lh build/bin/llama-server\n",
    "else:\n",
    "    print(\"‚ùå BUILD FAILED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Check the build output above for errors.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dcf23",
   "metadata": {},
   "source": [
    "## Step 6: Verify Built Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d961d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Built binaries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh llama-* 2>/dev/null | head -20\n",
    "\n",
    "print(\"\\nKey binary sizes:\")\n",
    "!du -h llama-server llama-cli llama-quantize 2>/dev/null\n",
    "\n",
    "print(\"\\nChecking CUDA support in llama-server:\")\n",
    "!./llama-server --help 2>&1 | grep -i \"cuda\\|gpu\\|ngl\" | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6abd2f",
   "metadata": {},
   "source": [
    "## Step 7: Test Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Testing multi-GPU CLI flags:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for multi-GPU flags\n",
    "print(\"\\nüìå --tensor-split (VRAM distribution):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"tensor-split\"\n",
    "\n",
    "print(\"\\nüìå --split-mode (layer/row splitting):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"split-mode\"\n",
    "\n",
    "print(\"\\nüìå --main-gpu (primary GPU selection):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"main-gpu\"\n",
    "\n",
    "print(\"\\n‚úÖ Multi-GPU support confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0b99",
   "metadata": {},
   "source": [
    "## Step 8: Create llcuda v2.2.0 Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb728ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Package info\n",
    "VERSION = \"2.2.0\"\n",
    "BUILD_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "PACKAGE_NAME = f\"llcuda-v{VERSION}-cuda12-kaggle-t4x2\"\n",
    "PACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n",
    "\n",
    "print(f\"Creating package: {PACKAGE_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/include\", exist_ok=True)\n",
    "\n",
    "# Binaries to include\n",
    "BUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\n",
    "binaries = [\n",
    "    # Core server\n",
    "    \"llama-server\",\n",
    "    \"llama-cli\",\n",
    "    # Quantization & conversion\n",
    "    \"llama-quantize\",\n",
    "    \"llama-gguf\",\n",
    "    \"llama-gguf-hash\",\n",
    "    \"llama-gguf-split\",\n",
    "    \"llama-imatrix\",\n",
    "    # LoRA & embedding\n",
    "    \"llama-export-lora\",\n",
    "    \"llama-embedding\",\n",
    "    # Utilities\n",
    "    \"llama-tokenize\",\n",
    "    \"llama-infill\",\n",
    "    \"llama-perplexity\",\n",
    "    \"llama-bench\",\n",
    "    \"llama-cvector-generator\",\n",
    "]\n",
    "\n",
    "# Copy binaries\n",
    "copied = []\n",
    "for binary in binaries:\n",
    "    src = f\"{BUILD_BIN}/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n",
    "        os.chmod(f\"{PACKAGE_DIR}/bin/{binary}\", 0o755)\n",
    "        copied.append(binary)\n",
    "        print(f\"  ‚úÖ {binary}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  {binary} (not found)\")\n",
    "\n",
    "print(f\"\\nüì¶ Copied {len(copied)}/{len(binaries)} binaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c7058",
   "metadata": {},
   "source": [
    "## Step 9: Create Package Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get llama.cpp info\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "commit_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n",
    "commit_date = subprocess.getoutput(\"git log -1 --format=%ci\")\n",
    "commit_msg = subprocess.getoutput(\"git log -1 --format=%s\")\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = subprocess.getoutput(\"nvcc --version | grep release | sed 's/.*release //' | cut -d, -f1\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"package\": \"llcuda\",\n",
    "    \"version\": VERSION,\n",
    "    \"build_date\": datetime.now().isoformat(),\n",
    "    \"platform\": {\n",
    "        \"name\": \"kaggle\",\n",
    "        \"gpu_count\": 2,\n",
    "        \"gpu_model\": \"Tesla T4\",\n",
    "        \"vram_per_gpu_gb\": 15,\n",
    "        \"total_vram_gb\": 30,\n",
    "        \"compute_capability\": \"7.5\",\n",
    "        \"architecture\": \"Turing\"\n",
    "    },\n",
    "    \"cuda\": {\n",
    "        \"version\": cuda_version,\n",
    "        \"architectures\": [\"sm_75\"],\n",
    "        \"flash_attention\": True,\n",
    "        \"flash_attention_all_quants\": True\n",
    "    },\n",
    "    \"llama_cpp\": {\n",
    "        \"commit\": commit_hash,\n",
    "        \"commit_date\": commit_date,\n",
    "        \"commit_message\": commit_msg,\n",
    "        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
    "    },\n",
    "    \"multi_gpu\": {\n",
    "        \"supported\": True,\n",
    "        \"method\": \"native_cuda\",\n",
    "        \"modes\": {\n",
    "            \"tensor_split\": {\n",
    "                \"description\": \"Split model across both GPUs for larger models\",\n",
    "                \"flags\": [\"--tensor-split 0.5,0.5\", \"--split-mode layer\"],\n",
    "                \"use_case\": \"Large GGUF models (>15GB)\"\n",
    "            },\n",
    "            \"split_workload\": {\n",
    "                \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n",
    "                \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n",
    "                \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n",
    "            }\n",
    "        },\n",
    "        \"recommended_config\": {\n",
    "            \"tensor_split\": \"0.5,0.5\",\n",
    "            \"split_mode\": \"layer\",\n",
    "            \"n_gpu_layers\": -1\n",
    "        }\n",
    "    },\n",
    "    \"split_workload\": {\n",
    "        \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n",
    "        \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n",
    "        \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n",
    "        \"rapids_packages\": [\"cudf-cu12\", \"cuml-cu12\", \"cugraph-cu12\"],\n",
    "        \"graphistry_packages\": [\"graphistry[ai]\"],\n",
    "        \"usage\": {\n",
    "            \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n",
    "            \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n",
    "        }\n",
    "    },\n",
    "    \"binaries\": copied,\n",
    "    \"features\": [\n",
    "        \"multi-gpu-tensor-split\",\n",
    "        \"split-workload-architecture\",\n",
    "        \"flash-attention-all-quants\",\n",
    "        \"openai-compatible-api\",\n",
    "        \"anthropic-compatible-api\",\n",
    "        \"29-quantization-formats\",\n",
    "        \"lora-adapters\",\n",
    "        \"grammar-constraints\",\n",
    "        \"json-schema-output\",\n",
    "        \"embeddings-reranking\",\n",
    "        \"streaming-sse\",\n",
    "        \"kv-cache-slots\",\n",
    "        \"speculative-decoding\"\n",
    "    ],\n",
    "    \"unsloth_integration\": {\n",
    "        \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n",
    "        \"workflow\": \"Unsloth (training) ‚Üí GGUF (conversion) ‚Üí llcuda (inference)\",\n",
    "        \"supported_exports\": [\"f16\", \"q8_0\", \"q4_k_m\", \"q5_k_m\", \"iq4_xs\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write metadata\n",
    "os.chdir(\"/kaggle/working\")\n",
    "with open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"üìã Package Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b1740",
   "metadata": {},
   "source": [
    "## Step 10: Create README and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_content = f'''# llcuda v{VERSION} - Kaggle 2√ó Tesla T4 Build\n",
    "\n",
    "Pre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n",
    "\n",
    "## üéØ Unsloth Integration\n",
    "\n",
    "llcuda is the **CUDA 12 inference backend for Unsloth**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n",
    "‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n",
    "‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### 1. Extract Package\n",
    "```bash\n",
    "tar -xzf llcuda-v{VERSION}-cuda12-kaggle-t4x2.tar.gz\n",
    "cd llcuda-v{VERSION}-cuda12-kaggle-t4x2\n",
    "chmod +x bin/*\n",
    "```\n",
    "\n",
    "### 2. Start Multi-GPU Server\n",
    "```bash\n",
    "./bin/llama-server \\\\\n",
    "    -m /path/to/model.gguf \\\\\n",
    "    -ngl 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    -fa \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port 8080 \\\\\n",
    "    -c 8192\n",
    "```\n",
    "\n",
    "### 3. Use with Python\n",
    "```python\n",
    "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
    "\n",
    "# Get optimal config for Kaggle\n",
    "config = kaggle_t4_dual_config()\n",
    "print(config.to_cli_args())\n",
    "\n",
    "# Connect to server\n",
    "client = LlamaCppClient(\"http://localhost:8080\")\n",
    "\n",
    "# OpenAI-compatible chat\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "## üìä Multi-GPU Flags\n",
    "\n",
    "| Flag | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| `-ngl 99` | Offload all layers to GPU | Required |\n",
    "| `--tensor-split` | VRAM ratio per GPU | `0.5,0.5` |\n",
    "| `--split-mode` | Split strategy | `layer` or `row` |\n",
    "| `--main-gpu` | Primary GPU ID | `0` |\n",
    "| `-fa` | FlashAttention | Recommended |\n",
    "\n",
    "## üì¶ Recommended Models for 30GB VRAM\n",
    "\n",
    "| Model | Quant | Size | Context | Fits? |\n",
    "|-------|-------|------|---------|-------|\n",
    "| Llama 3.1 70B | IQ3_XS | ~25GB | 4K | ‚úÖ |\n",
    "| Qwen2.5 32B | Q4_K_M | ~19GB | 8K | ‚úÖ |\n",
    "| Gemma 2 27B | Q4_K_M | ~16GB | 8K | ‚úÖ |\n",
    "| Llama 3.1 8B | Q8_0 | ~9GB | 16K | ‚úÖ |\n",
    "| Mistral 7B | Q8_0 | ~8GB | 32K | ‚úÖ |\n",
    "\n",
    "## üîß Unsloth ‚Üí llcuda Workflow\n",
    "\n",
    "```python\n",
    "# 1. Fine-tune with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "# ... training ...\n",
    "\n",
    "# 2. Export to GGUF (Unsloth built-in)\n",
    "model.save_pretrained_gguf(\"my_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# 3. Run with llcuda\n",
    "# ./bin/llama-server -m my_model-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5\n",
    "```\n",
    "\n",
    "## üìã Build Info\n",
    "\n",
    "- **llcuda Version:** {VERSION}\n",
    "- **CUDA Version:** 12.4\n",
    "- **Target GPU:** Tesla T4 √ó 2\n",
    "- **Compute Capability:** SM 7.5 (Turing)\n",
    "- **FlashAttention:** All quantization types\n",
    "- **Build Date:** {BUILD_DATE}\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [llcuda GitHub](https://github.com/llcuda/llcuda)\n",
    "- [Unsloth](https://github.com/unslothai/unsloth)\n",
    "- [llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README.md created\")\n",
    "print(f\"\\n{readme_content[:1500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e557cd",
   "metadata": {},
   "source": [
    "## Step 11: Create Helper Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create start-server.sh helper script\n",
    "start_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Start Multi-GPU Server\n",
    "# Usage: ./start-server.sh <model.gguf> [port]\n",
    "\n",
    "MODEL=\"$1\"\n",
    "PORT=\"${2:-8080}\"\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"Usage: $0 <model.gguf> [port]\"\n",
    "    echo \"Example: $0 qwen2.5-7b-Q4_K_M.gguf 8080\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Starting llama-server with dual T4 config...\"\n",
    "echo \"Model: $MODEL\"\n",
    "echo \"Port: $PORT\"\n",
    "echo \"\"\n",
    "\n",
    "\"$SCRIPT_DIR/bin/llama-server\" \\\\\n",
    "    --model \"$MODEL\" \\\\\n",
    "    --n-gpu-layers 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    --flash-attn \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port \"$PORT\" \\\\\n",
    "    --ctx-size 8192 \\\\\n",
    "    --batch-size 2048 \\\\\n",
    "    --ubatch-size 512 \\\\\n",
    "    --parallel 4\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/start-server.sh\", \"w\") as f:\n",
    "    f.write(start_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/start-server.sh\", 0o755)\n",
    "\n",
    "# Create quantize.sh helper script\n",
    "quantize_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Quantize Model\n",
    "# Usage: ./quantize.sh <input.gguf> <output.gguf> [quant_type]\n",
    "\n",
    "INPUT=\"$1\"\n",
    "OUTPUT=\"$2\"\n",
    "QUANT=\"${3:-Q4_K_M}\"\n",
    "\n",
    "if [ -z \"$INPUT\" ] || [ -z \"$OUTPUT\" ]; then\n",
    "    echo \"Usage: $0 <input.gguf> <output.gguf> [quant_type]\"\n",
    "    echo \"Quant types: Q4_K_M (default), Q8_0, Q5_K_M, IQ4_XS, etc.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Quantizing: $INPUT ‚Üí $OUTPUT ($QUANT)\"\n",
    "\"$SCRIPT_DIR/bin/llama-quantize\" \"$INPUT\" \"$OUTPUT\" \"$QUANT\"\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/quantize.sh\", \"w\") as f:\n",
    "    f.write(quantize_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/quantize.sh\", 0o755)\n",
    "\n",
    "print(\"‚úÖ Helper scripts created:\")\n",
    "print(\"   - start-server.sh\")\n",
    "print(\"   - quantize.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e467ab",
   "metadata": {},
   "source": [
    "## Step 12: Create Distribution Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "TARBALL = f\"{PACKAGE_NAME}.tar.gz\"\n",
    "\n",
    "print(f\"Creating distribution archive: {TARBALL}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tarball\n",
    "!tar -czvf {TARBALL} {PACKAGE_NAME}\n",
    "\n",
    "# Calculate SHA256\n",
    "with open(TARBALL, \"rb\") as f:\n",
    "    sha256 = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Write checksum file\n",
    "with open(f\"{TARBALL}.sha256\", \"w\") as f:\n",
    "    f.write(f\"{sha256}  {TARBALL}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ DISTRIBUTION PACKAGE READY\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh {TARBALL}*\n",
    "print(f\"\\nSHA256: {sha256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214c3f",
   "metadata": {},
   "source": [
    "## Step 13: Test Multi-GPU Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small test model and verify multi-GPU works\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"Downloading small test model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"‚úÖ Model: {model_path}\")\n",
    "\n",
    "# Start server with multi-GPU\n",
    "print(\"\\nStarting llama-server with dual T4 config...\")\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"--tensor-split\", \"0.5,0.5\",\n",
    "    \"--split-mode\", \"layer\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "print(f\"Command: {' '.join(server_cmd)}\")\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "print(\"\\nWaiting for server to start...\")\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"‚úÖ Server ready in {i+1}s!\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Server startup timeout\")\n",
    "\n",
    "# Check GPU usage\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ec572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"Testing multi-GPU inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start = time.time()\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    timeout=60\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    usage = result.get(\"usage\", {})\n",
    "    \n",
    "    print(f\"‚úÖ Response ({elapsed:.2f}s):\")\n",
    "    print(f\"   {content}\")\n",
    "    print(f\"\\nüìä Tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "    if usage.get('completion_tokens'):\n",
    "        tps = usage['completion_tokens'] / elapsed\n",
    "        print(f\"üìä Speed: {tps:.1f} tokens/sec\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - stop server\n",
    "print(\"Stopping server...\")\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"‚úÖ Server stopped\")\n",
    "\n",
    "# Show final GPU state\n",
    "print(\"\\nüìä Final GPU State:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f0e65",
   "metadata": {},
   "source": [
    "## Step 13b: Test Split-GPU Architecture (LLM + Graphistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split-GPU Architecture Demo:\n",
    "- GPU 0: llama-server (LLM inference)\n",
    "- GPU 1: RAPIDS/Graphistry (graph simulation)\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLIT-GPU ARCHITECTURE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 0: Start llama-server (LLM)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 0: Starting llama-server...\")\n",
    "\n",
    "# Force llama-server to use GPU 0 only\n",
    "llama_env = os.environ.copy()\n",
    "llama_env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env=llama_env\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"   ‚úÖ llama-server ready on GPU 0 ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Server timeout\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 1: RAPIDS/Graphistry graph operations\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 1: Running RAPIDS graph simulation...\")\n",
    "\n",
    "# Force RAPIDS to use GPU 1 only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "# Create sample graph data (simulating knowledge graph from LLM)\n",
    "edges = cudf.DataFrame({\n",
    "    \"src\": [0, 1, 2, 3, 4, 0, 1, 2],\n",
    "    \"dst\": [1, 2, 3, 4, 0, 2, 3, 4],\n",
    "    \"weight\": [1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 1.0, 0.8]\n",
    "})\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "\n",
    "print(f\"   Graph: {G.number_of_vertices()} vertices, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Run PageRank on GPU 1\n",
    "pagerank = cugraph.pagerank(G)\n",
    "print(f\"   PageRank computed: {len(pagerank)} nodes\")\n",
    "print(f\"   Top node: {pagerank.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Combined workflow: LLM query ‚Üí Graph update\n",
    "# ============================================================================\n",
    "print(\"\\nüîó Combined LLM + Graph workflow...\")\n",
    "\n",
    "# Reset CUDA_VISIBLE_DEVICES for requests\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Query LLM on GPU 0\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"List 3 related concepts to 'machine learning'\"}],\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"   LLM (GPU 0): {llm_output[:100]}...\")\n",
    "    \n",
    "    # Simulate adding LLM-derived edges to graph\n",
    "    new_edges = cudf.DataFrame({\n",
    "        \"src\": [5, 5, 5],\n",
    "        \"dst\": [0, 1, 2],\n",
    "        \"weight\": [1.0, 1.0, 1.0]\n",
    "    })\n",
    "    all_edges = cudf.concat([edges, new_edges])\n",
    "    G2 = cugraph.Graph()\n",
    "    G2.from_cudf_edgelist(all_edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "    print(f\"   Graph (GPU 1): Updated to {G2.number_of_vertices()} vertices\")\n",
    "\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"\\n‚úÖ Split-GPU test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aebf7",
   "metadata": {},
   "source": [
    "## Step 14: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c32db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéâ llcuda v2.2.0 BUILD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Distribution Package:\")\n",
    "!ls -lh {PACKAGE_NAME}.tar.gz\n",
    "\n",
    "print(f\"\\nüìÅ Package Contents:\")\n",
    "!ls -la {PACKAGE_NAME}/\n",
    "\n",
    "print(f\"\\nüîß Binaries:\")\n",
    "!ls -lh {PACKAGE_NAME}/bin/ | head -10\n",
    "\n",
    "print(f\"\\nüìã Metadata Summary:\")\n",
    "print(f\"   Version: {VERSION}\")\n",
    "print(f\"   Platform: Kaggle 2√ó Tesla T4\")\n",
    "print(f\"   CUDA: {cuda_version}\")\n",
    "print(f\"   Compute: SM 7.5 (Turing)\")\n",
    "print(f\"   FlashAttention: ‚úÖ All quants\")\n",
    "print(f\"   Multi-GPU: ‚úÖ Native CUDA\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Download: {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   2. Extract: tar -xzf {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   3. Run: ./start-server.sh model.gguf 8080\")\n",
    "\n",
    "print(f\"\\nüì• Download from Kaggle Output tab\")\n",
    "print(f\"   or copy to output: !cp {PACKAGE_NAME}.tar.gz /kaggle/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Kaggle output for download\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"/kaggle/output\", exist_ok=True)\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\", \"/kaggle/output/\")\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz.sha256\", \"/kaggle/output/\")\n",
    "\n",
    "print(\"‚úÖ Package copied to /kaggle/output/ for download\")\n",
    "!ls -lh /kaggle/output/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
