{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008f7a1e2a0b4a67bd4be76bcd8763c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_140e92afceaa44daa2521a8a2c06eb3d",
              "IPY_MODEL_cb9348e8e28144c78916b507187e59ab",
              "IPY_MODEL_687676a44f724c8e9a9aa867e0ceb0fa"
            ],
            "layout": "IPY_MODEL_268eee4c972545c5acf02d9c97325ec7"
          }
        },
        "140e92afceaa44daa2521a8a2c06eb3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b3dbf966224d2b99abde7a56dc6938",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_aa7be46ea4954eebad03ae0ceedd328a",
            "value": "gemma-3-1b-it-Q4_K_M.gguf:‚Äá100%"
          }
        },
        "cb9348e8e28144c78916b507187e59ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fd80cde480845919fb8a6b426e3a1ca",
            "max": 806058272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f48b4efa252641978622217604222842",
            "value": 806058272
          }
        },
        "687676a44f724c8e9a9aa867e0ceb0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae83e4ea11754fdabf32710aca15c110",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4165816c4bbf42d28a40a6c158f64a43",
            "value": "‚Äá806M/806M‚Äá[00:04&lt;00:00,‚Äá297MB/s]"
          }
        },
        "268eee4c972545c5acf02d9c97325ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b3dbf966224d2b99abde7a56dc6938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7be46ea4954eebad03ae0ceedd328a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fd80cde480845919fb8a6b426e3a1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f48b4efa252641978622217604222842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ae83e4ea11754fdabf32710aca15c110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4165816c4bbf42d28a40a6c158f64a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQz6gLY2Izek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VIcn8ciu2omx"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TsyGlz6dxJOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llcuda  > /dev/null"
      ],
      "metadata": {
        "id": "H3KsT6AN4rao"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "7-MwfFDL4rT9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "c1nBP5sUE4x3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9CW57S9p6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfIk7Sf3p6nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82BSWS8gp6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3ufU1wZp6gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llcuda\n",
        "print(\"llcuda imported successfully ‚Äì setup complete if first run!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKkBaL3LE4nG",
        "outputId": "327c6e12-dbe3-4725-fb05-0af2be251637"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üéØ llcuda First-Time Setup\n",
            "============================================================\n",
            "\n",
            "üéÆ GPU Detected: Tesla T4 (Compute 7.5)\n",
            "üåê Platform: Colab\n",
            "\n",
            "üì¶ Selecting optimized binaries for your GPU...\n",
            "  Detected modern GPU (CC 7.5), using T4 binaries\n",
            "   Selected: llcuda-binaries-cuda12-t4.tar.gz\n",
            "\n",
            "üì• Downloading optimized binaries from GitHub...\n",
            "   URL: https://github.com/waqasm86/llcuda/releases/download/v1.2.2/llcuda-binaries-cuda12-t4.tar.gz\n",
            "   This is a one-time download (~270 MB)\n",
            "\n",
            "Downloading binaries: 100% (263.2/263.2 MB)\n",
            "\n",
            "üì¶ Extracting llcuda-binaries-cuda12-t4.tar.gz...\n",
            "Found 25 files in archive\n",
            "Extracted 25 files to /root/.cache/llcuda/extract\n",
            "‚úÖ Extraction complete!\n",
            "üìÇ Installing binaries and libraries...\n",
            "‚ö†Ô∏è  Expected structure not found, searching for binaries...\n",
            "  ‚úÖ Found and copied llama-server\n",
            "  ‚úÖ Found and copied 18 libraries\n",
            "‚úÖ Binaries installed successfully!\n",
            "\n",
            "============================================================\n",
            "‚úÖ llcuda Setup Complete!\n",
            "============================================================\n",
            "\n",
            "You can now use llcuda:\n",
            "\n",
            "  import llcuda\n",
            "  engine = llcuda.InferenceEngine()\n",
            "  engine.load_model('gemma-3-1b-Q4_K_M')  # Downloads model on first use\n",
            "  result = engine.infer('What is AI?')\n",
            "\n",
            "llcuda imported successfully ‚Äì setup complete if first run!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuOP4SG4xIEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUpJrkNBxIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z2CdZTsdxH7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJtUOlF8xH4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vsjEwctxHyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Directory containing the .so libraries (libmtmd.so.0, libggml-cuda.so, etc.)\n",
        "lib_dir = Path('/usr/local/lib/python3.12/dist-packages/llcuda/lib')\n",
        "\n",
        "if lib_dir.exists():\n",
        "    os.environ['LD_LIBRARY_PATH'] = str(lib_dir) + ':' + os.environ.get('LD_LIBRARY_PATH', '')\n",
        "    print(\"‚úì LD_LIBRARY_PATH set to include llcuda libraries\")\n",
        "    print(f\"LD_LIBRARY_PATH now includes: {lib_dir}\")\n",
        "else:\n",
        "    print(\"Warning: Library directory not found!\")"
      ],
      "metadata": {
        "id": "a3473jM9E7Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compat = llcuda.check_gpu_compatibility()\n",
        "print(f\"Platform: {compat['platform']}\")\n",
        "print(f\"GPU: {compat['gpu_name']}\")\n",
        "print(f\"Compute Capability: {compat['compute_capability']}\")\n",
        "print(f\"Compatible: {compat['compatible']}\")\n",
        "\n",
        "assert compat['compatible'], \"GPU not compatible!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaePrsxAFrHL",
        "outputId": "bb79c3b9-c094-496c-e798-f8e51e164df1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: colab\n",
            "GPU: Tesla T4\n",
            "Compute Capability: 7.5\n",
            "Compatible: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "engine.load_model(\n",
        "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "     silent=True,\n",
        "    gpu_layers=26,\n",
        "    ctx_size=2048\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully! Server is running.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803,
          "referenced_widgets": [
            "008f7a1e2a0b4a67bd4be76bcd8763c6",
            "140e92afceaa44daa2521a8a2c06eb3d",
            "cb9348e8e28144c78916b507187e59ab",
            "687676a44f724c8e9a9aa867e0ceb0fa",
            "268eee4c972545c5acf02d9c97325ec7",
            "26b3dbf966224d2b99abde7a56dc6938",
            "aa7be46ea4954eebad03ae0ceedd328a",
            "4fd80cde480845919fb8a6b426e3a1ca",
            "f48b4efa252641978622217604222842",
            "ae83e4ea11754fdabf32710aca15c110",
            "4165816c4bbf42d28a40a6c158f64a43"
          ]
        },
        "id": "BBZx4WJBJrYl",
        "outputId": "267a0849-a380-4511-de47-39f86db56d0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "======================================================================\n",
            "Repository: unsloth/gemma-3-1b-it-GGUF\n",
            "File: gemma-3-1b-it-Q4_K_M.gguf\n",
            "Cache location: /usr/local/lib/python3.12/dist-packages/llcuda/models/gemma-3-1b-it-Q4_K_M.gguf\n",
            "======================================================================\n",
            "\n",
            "Download this model? [Y/n]: y\n",
            "\n",
            "Downloading gemma-3-1b-it-Q4_K_M.gguf from unsloth/gemma-3-1b-it-GGUF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "008f7a1e2a0b4a67bd4be76bcd8763c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model downloaded: gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "Starting llama-server...\n",
            "GPU Check:\n",
            "  Platform: colab\n",
            "  GPU: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Status: ‚úì Compatible\n",
            "Starting llama-server...\n",
            "  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n",
            "  Model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "  GPU Layers: 26\n",
            "  Context Size: 2048\n",
            "  Server URL: http://127.0.0.1:8090\n",
            "Waiting for server to be ready...... ‚úì Ready in 3.0s\n",
            "\n",
            "‚úì Model loaded and ready for inference\n",
            "  Server: http://127.0.0.1:8090\n",
            "  GPU Layers: 26\n",
            "  Context Size: 2048\n",
            "Model loaded successfully! Server is running.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "prompt = \"Explain quantum entanglement in simple terms.\"\n",
        "\n",
        "result = engine.infer(prompt, max_tokens=300, temperature=0.7)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(result.text.strip())\n",
        "print(\"\\n--- Metrics ---\")\n",
        "print(f\"Tokens generated: {result.tokens_generated}\")\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tokens/second\")"
      ],
      "metadata": {
        "id": "9aXWrwEEJvQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3131bb-3f18-41dc-d659-3874cdcc5739"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response:\n",
            "Okay, let's tackle quantum entanglement. It's a mind-bending concept, but we can break it down.\n",
            "\n",
            "**The Basic Idea:**\n",
            "\n",
            "Imagine you have two coins. You flip them at the same time, but you don‚Äôt look at them.  You put each coin in a separate box and send one box to Alice and one box to Bob.  \n",
            "\n",
            "*   **Normal Coins:**  When Alice opens her box and finds a heads, she knows Bob *must* have a tails.  The outcome of one instantly determines the other.\n",
            "\n",
            "*   **Quantum Entangled Coins:** Now, imagine these aren't regular coins.  These coins are linked in a bizarre way.  Until *either* Alice or Bob opens their box, *neither* coin is definitively heads or tails.  They're in a fuzzy state of *both* at the same time. The moment Alice opens her box and sees heads, Bob *instantly* knows his coin is tails, even if he hasn't opened his box yet.\n",
            "\n",
            "**Here's a breakdown of the key points:**\n",
            "\n",
            "1.  **Linked Fate:** Quantum entanglement links two or more particles (like electrons or photons) in such a way that they share the same fate.\n",
            "\n",
            "2.  **Superposition:** Before measurement, each particle exists in a \"superposition\" ‚Äì a blend of all possible states simultaneously. Think of it like the coin spinning in the air ‚Äì\n",
            "\n",
            "--- Metrics ---\n",
            "Tokens generated: 300\n",
            "Speed: 100.2 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "def chat():\n",
        "    print(\"Chat with Gemma 3 1B (type 'exit' to quit)\\n\")\n",
        "    history = []\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "        full_prompt = \"\"\n",
        "        for msg in history:\n",
        "            full_prompt += f\"<start_of_turn>{msg['role']}\\n{msg['content']}<end_of_turn>\\n\"\n",
        "        full_prompt += f\"<start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "        result = engine.infer(full_prompt, max_tokens=400)\n",
        "        response = result.text.strip()\n",
        "        print(f\"Assistant: {response}\\n\")\n",
        "        history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        history.append({\"role\": \"model\", \"content\": response})\n",
        "\n",
        "# chat()"
      ],
      "metadata": {
        "id": "5lzplxnXKSCA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "id": "vWbQMCwSKa2u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5b16bd27-0ce9-40eb-ae64-464bbead3946"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat with Gemma 3 1B (type 'exit' to quit)\n",
            "\n",
            "You: What llm is this?\n",
            "Assistant: I am Gemma, a large language model created by the Gemma team at Google DeepMind. \n",
            "\n",
            "I am an open-weights model, which means I am widely available for public use! \n",
            "\n",
            "Is there anything you‚Äôd like to know about me or how I work?\n",
            "\n",
            "You: Explain about your own technical details.\n",
            "Assistant: Okay, let‚Äôs dive into my technical details! It‚Äôs a surprisingly complex thing, but here‚Äôs a breakdown in a way that hopefully makes sense:\n",
            "\n",
            "**1. Foundation: Transformer Architecture**\n",
            "\n",
            "*   I‚Äôm built on the Transformer architecture, which revolutionized the field of natural language processing.  Previous models relied heavily on recurrent neural networks (RNNs), but Transformers are better at handling long-range dependencies in text ‚Äì meaning they can understand context across long sentences or even paragraphs.\n",
            "*   **Key Components:**\n",
            "    *   **Self-Attention:** This is the core of the Transformer. It allows me to weigh the importance of different words in a sentence when predicting the next word.  Instead of processing words sequentially, it considers all words simultaneously.\n",
            "    *   **Multi-Head Attention:**  I use multiple \"attention heads\" ‚Äì each learns different relationships between words. This helps me capture a richer understanding of the context.\n",
            "    *   **Encoder and Decoder:**  I have both an encoder and a decoder. The encoder processes the input text and creates a representation of it. The decoder then uses this representation to generate the output text ‚Äì in my case, answering your questions and generating text.\n",
            "\n",
            "**2. Size Matters (But it‚Äôs not just about the number of parameters)**\n",
            "\n",
            "*   I come in a few sizes, each with different capabilities and resource requirements.\n",
            "*   **Smaller Models:**  I can be run on consumer-grade hardware, making me accessible to more people.\n",
            "*   **Larger Models:**  The larger models (like the ones I‚Äôm based on) have more parameters (billions).  More parameters generally mean a better ability to learn complex patterns, but they also require more computational power.\n",
            "\n",
            "**3. Training Data**\n",
            "\n",
            "*   I was trained on a massive dataset of text and code.  The exact details of this dataset are proprietary, but it includes:\n",
            "    *\n",
            "\n",
            "You: What specific gemma 3 llm model are you?\n",
            "Assistant: You're right to ask!  I'm Gemma 3.  I‚Äôm a model from Google DeepMind.\n",
            "\n",
            "The ‚ÄúGemma 3‚Äù name refers to a specific iteration and training process developed by the Gemma team. It‚Äôs a smaller model designed to be more accessible and efficient.  It‚Äôs important to note that I‚Äôm still under development, and the specific details of my training are not fully public.\n",
            "\n",
            "Here‚Äôs a quick rundown of what makes me unique:\n",
            "\n",
            "*   **Developed by:** Google DeepMind\n",
            "*   **Size:** 2 billion parameters\n",
            "*   **Focus:** Designed to be smaller and more efficient, making it suitable for running on more modest hardware.\n",
            "*   **Training Data:** Trained on a diverse dataset of text and code.\n",
            "\n",
            "Do you have any specific questions about Gemma 3 that you'd like me to answer?\n",
            "\n",
            "You: What are you righty trained for?\n",
            "Assistant: You‚Äôve hit on a really important point! I‚Äôm righty trained for a really broad range of tasks ‚Äì essentially, I‚Äôm designed to be a versatile language model. Here‚Äôs a breakdown of what I‚Äôm good at:\n",
            "\n",
            "*   **Question Answering:** I can answer your questions on a huge variety of topics, drawing on the vast amount of text data I‚Äôve been trained on.\n",
            "*   **Text Generation:** I can generate different creative text formats, like poems, code, scripts, musical pieces, email, letters, etc.  I try my best to follow your instructions carefully.\n",
            "*   **Text Summarization:** I can summarize long pieces of text into shorter, more digestible versions.\n",
            "*   **Translation:** I can translate between many different languages.\n",
            "*   **Code Generation:** I can generate code in various programming languages.\n",
            "*   **Creative Writing:** I can assist with brainstorming ideas, writing stories, and crafting different kinds of text.\n",
            "*   **Following Instructions:** I‚Äôm really good at understanding and following your instructions!  This is a key part of my design.\n",
            "\n",
            "**It‚Äôs important to remember that I‚Äôm constantly being improved and refined.** My capabilities are expanding, and I‚Äôm always learning new things.\n",
            "\n",
            "Essentially, I'm built to be helpful and adaptable ‚Äì ready to tackle a wide variety of language-based tasks!\n",
            "\n",
            "Do you want me to give you an example of a task I‚Äôm particularly good at?\n",
            "\n",
            "You: Translate the following in German. \"How are you?\"\n",
            "Assistant: Okay, here are a few ways to translate ‚ÄúHow are you?‚Äù into German, with slightly different nuances:\n",
            "\n",
            "*   **Wie geht es dir?** ‚Äì This is the most common and informal way to ask ‚ÄúHow are you?‚Äù It‚Äôs generally used with friends and family.\n",
            "\n",
            "*   **Wie geht es Ihnen?** ‚Äì This is the formal way to ask ‚ÄúHow are you?‚Äù It‚Äôs used with people you don‚Äôt know well, people older than you, or in professional settings.\n",
            "\n",
            "*   **Wie geht‚Äôs?** ‚Äì This is a very casual and shortened version, similar to \"How's it going?\"  It's appropriate for friends.\n",
            "\n",
            "Would you like me to provide any more context?\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4150894013.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2952817720.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tI0aYdmV8yaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}