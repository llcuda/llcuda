{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ llcuda Quickstart Tutorial - Gemma 3-1B Inference on Tesla T4\n",
    "\n",
    "**Welcome to llcuda!** This notebook shows you how to run fast LLM inference on Google Colab's free Tesla T4 GPU.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "‚úÖ Install llcuda (CUDA 12-first inference backend)\n",
    "‚úÖ Verify Tesla T4 GPU and CUDA 12 compatibility\n",
    "‚úÖ Load Unsloth's Gemma 3-1B GGUF model\n",
    "‚úÖ Run fast inference (~45 tokens/second)\n",
    "‚úÖ Use chat mode for conversations\n",
    "‚úÖ Benchmark performance\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **GPU**: Tesla T4 (Google Colab free tier)\n",
    "- **Runtime**: Python 3.10+\n",
    "- **CUDA**: 12.x (pre-installed on Colab)\n",
    "\n",
    "---\n",
    "\n",
    "**Let's get started!** üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Verify GPU and Environment\n",
    "\n",
    "First, let's make sure you have a Tesla T4 GPU and CUDA 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpu_info = result.stdout.strip().split(',')\n",
    "gpu_name = gpu_info[0].strip()\n",
    "compute_cap = gpu_info[1].strip()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Compute Capability: SM {compute_cap}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if 'T4' in gpu_name and compute_cap == '7.5':\n",
    "    print(\"\\n‚úÖ Perfect! Tesla T4 (SM 7.5) detected\")\n",
    "    print(\"   llcuda is optimized for your GPU\")\n",
    "elif compute_cap == '7.5':\n",
    "    print(f\"\\n‚ö†Ô∏è  {gpu_name} (SM 7.5) - Should work\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå WARNING: SM {compute_cap} detected\")\n",
    "    print(\"   This tutorial is optimized for SM 7.5 (Tesla T4)\")\n",
    "    print(\"   Performance may vary on your GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_cuda"
   },
   "outputs": [],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version | grep \"release\"\n",
    "\n",
    "import sys\n",
    "print(f\"\\nPython version: {sys.version.split()[0]}\")\n",
    "print(f\"Expected: 3.10+ (Colab default)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Install llcuda\n",
    "\n",
    "Installing llcuda is simple - just one pip command!\n",
    "\n",
    "**What happens:**\n",
    "1. Installs lightweight Python package (~70 KB)\n",
    "2. On first import, downloads CUDA 12 binaries (~267 MB, one-time)\n",
    "3. Auto-configures environment for Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_llcuda"
   },
   "outputs": [],
   "source": [
    "!pip install -q llcuda\n",
    "\n",
    "print(\"‚úÖ llcuda installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Import llcuda and Download Binaries\n",
    "\n",
    "First import triggers automatic download of T4-optimized CUDA binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_llcuda"
   },
   "outputs": [],
   "source": [
    "import llcuda\n",
    "\n",
    "print(f\"\\n‚úÖ llcuda version: {llcuda.__version__}\")\n",
    "print(\"\\nBinaries downloaded and configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Verify CUDA Availability\n",
    "\n",
    "Let's check that llcuda can see your Tesla T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_llcuda_cuda"
   },
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_info = llcuda.detect_cuda()\n",
    "\n",
    "print(f\"CUDA available: {cuda_info['available']}\")\n",
    "\n",
    "if cuda_info['available']:\n",
    "    gpu = cuda_info['gpus'][0]\n",
    "    print(f\"GPU: {gpu['name']}\")\n",
    "    print(f\"Compute Capability: SM {gpu['compute_capability']}\")\n",
    "    print(f\"Memory: {gpu['memory_total_mb']} MB\")\n",
    "    print(\"\\n‚úÖ llcuda is ready for inference!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CUDA not detected. Please check your GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "## Step 5: Load Gemma 3-1B GGUF Model\n",
    "\n",
    "We'll use the **Unsloth Gemma 3-1B Instruct** model in GGUF format, quantized with Q4_K_M for optimal balance between speed and quality.\n",
    "\n",
    "**Model Details:**\n",
    "- **Name**: unsloth/gemma-3-1b-it-GGUF\n",
    "- **Quantization**: Q4_K_M (4-bit with K-means)\n",
    "- **Size**: ~700 MB\n",
    "- **VRAM**: ~1.2 GB\n",
    "- **Speed**: ~45 tokens/second on Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_engine"
   },
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "print(\"‚úÖ Inference engine created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Load Gemma 3-1B GGUF model from Unsloth\n",
    "print(\"üì• Loading Gemma 3-1B Instruct (Q4_K_M)...\")\n",
    "print(\"   This will download ~700 MB on first run\\n\")\n",
    "\n",
    "engine.load_model(\n",
    "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    n_gpu_layers=99,  # Offload all layers to GPU\n",
    "    ctx_size=2048,    # Context window\n",
    "    n_threads=4       # CPU threads for host operations\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(\"   Ready for inference on Tesla T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "## Step 6: Your First Inference\n",
    "\n",
    "Let's run a simple test to see llcuda in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "first_inference"
   },
   "outputs": [],
   "source": [
    "# Simple inference test\n",
    "prompt = \"What is artificial intelligence?\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Generating response...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    prompt,\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESPONSE:\")\n",
    "print(\"=\"*60)\n",
    "print(result.text)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nüìä Performance:\")\n",
    "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
    "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/second\")\n",
    "print(f\"   Latency: {result.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7"
   },
   "source": [
    "## Step 7: Interactive Chat Mode\n",
    "\n",
    "Let's use llcuda's chat engine for multi-turn conversations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chat_engine"
   },
   "outputs": [],
   "source": [
    "# Create chat engine\n",
    "chat = llcuda.ChatEngine(engine)\n",
    "\n",
    "print(\"‚úÖ Chat engine ready!\")\n",
    "print(\"   Multi-turn conversation enabled\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chat_example"
   },
   "outputs": [],
   "source": [
    "# Example conversation\n",
    "messages = [\n",
    "    \"Hello! Can you explain what large language models are?\",\n",
    "    \"How are they trained?\",\n",
    "    \"What are some practical applications?\"\n",
    "]\n",
    "\n",
    "for i, user_msg in enumerate(messages, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Turn {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üë§ User: {user_msg}\")\n",
    "    \n",
    "    response = chat.send(user_msg, max_tokens=100)\n",
    "    \n",
    "    print(f\"ü§ñ Gemma: {response.text}\")\n",
    "    print(f\"‚ö° Speed: {response.tokens_per_sec:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8"
   },
   "source": [
    "## Step 8: Benchmark Performance\n",
    "\n",
    "Let's measure inference speed across different prompt lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test prompts of varying lengths\n",
    "test_cases = [\n",
    "    (\"Short\", \"What is AI?\", 50),\n",
    "    (\"Medium\", \"Explain the concept of neural networks and how they work.\", 100),\n",
    "    (\"Long\", \"Write a detailed explanation of transformer architecture in machine learning.\", 200)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PERFORMANCE BENCHMARK - Gemma 3-1B Q4_K_M on Tesla T4\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, prompt, max_tok in test_cases:\n",
    "    print(f\"\\nüìä Test: {name} prompt ({len(prompt)} chars, {max_tok} max tokens)\")\n",
    "    \n",
    "    result = engine.infer(prompt, max_tokens=max_tok, temperature=0.7)\n",
    "    \n",
    "    results.append({\n",
    "        'name': name,\n",
    "        'tokens': result.tokens_generated,\n",
    "        'speed': result.tokens_per_sec,\n",
    "        'latency': result.latency_ms\n",
    "    })\n",
    "    \n",
    "    print(f\"   Tokens: {result.tokens_generated}\")\n",
    "    print(f\"   Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "    print(f\"   Latency: {result.latency_ms:.0f} ms\")\n",
    "\n",
    "# Summary\n",
    "avg_speed = sum(r['speed'] for r in results) / len(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Average Speed: {avg_speed:.1f} tokens/second\")\n",
    "print(f\"Expected: ~45 tok/s for Gemma 3-1B Q4_K_M on Tesla T4\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9"
   },
   "source": [
    "## Step 9: Try Your Own Prompts!\n",
    "\n",
    "Now it's your turn! Modify the prompt below and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_prompt"
   },
   "outputs": [],
   "source": [
    "# ‚úèÔ∏è Edit this prompt to try your own!\n",
    "your_prompt = \"Write a haiku about machine learning.\"\n",
    "\n",
    "print(f\"Your prompt: {your_prompt}\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    your_prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.8,  # Higher = more creative\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(result.text)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n‚ö° Generated at {result.tokens_per_sec:.1f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step10"
   },
   "source": [
    "## Step 10: Advanced Features\n",
    "\n",
    "### Streaming Inference\n",
    "\n",
    "Get tokens as they're generated (like ChatGPT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "streaming"
   },
   "outputs": [],
   "source": [
    "from llcuda.jupyter import stream_response\n",
    "\n",
    "prompt = \"Tell me a short story about a robot learning to paint.\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(\"Response (streaming):\\n\")\n",
    "\n",
    "# Stream the response\n",
    "for chunk in stream_response(engine, prompt, max_tokens=200):\n",
    "    print(chunk, end='', flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Streaming complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step11"
   },
   "source": [
    "### Model Information\n",
    "\n",
    "Get details about the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "model_info"
   },
   "outputs": [],
   "source": [
    "# Get model information\n",
    "info = engine.get_model_info()\n",
    "\n",
    "print(\"üìã Model Information:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Name: {info.get('name', 'N/A')}\")\n",
    "print(f\"Architecture: {info.get('architecture', 'N/A')}\")\n",
    "print(f\"Quantization: {info.get('quantization', 'N/A')}\")\n",
    "print(f\"Context Size: {info.get('context_size', 'N/A')}\")\n",
    "print(f\"Vocab Size: {info.get('vocab_size', 'N/A')}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step12"
   },
   "source": [
    "### Engine Statistics\n",
    "\n",
    "View performance metrics collected during this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "engine_stats"
   },
   "outputs": [],
   "source": [
    "# Get engine statistics\n",
    "stats = engine.get_stats()\n",
    "\n",
    "print(\"üìä Session Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total requests: {stats.get('total_requests', 0)}\")\n",
    "print(f\"Total tokens generated: {stats.get('total_tokens', 0)}\")\n",
    "print(f\"Average speed: {stats.get('avg_tokens_per_sec', 0):.1f} tok/s\")\n",
    "print(f\"Average latency: {stats.get('avg_latency_ms', 0):.0f} ms\")\n",
    "print(f\"Median latency (p50): {stats.get('p50_latency_ms', 0):.0f} ms\")\n",
    "print(f\"95th percentile (p95): {stats.get('p95_latency_ms', 0):.0f} ms\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "Stop the inference server to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stop_server"
   },
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "engine.stop()\n",
    "\n",
    "print(\"‚úÖ Inference server stopped\")\n",
    "print(\"   GPU memory freed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Congratulations! You've completed the llcuda quickstart tutorial. Here's what you can try next:\n",
    "\n",
    "### 1. Try Different Models\n",
    "\n",
    "llcuda supports any GGUF model from Unsloth or llama.cpp:\n",
    "\n",
    "```python\n",
    "# Llama 3.2-3B\n",
    "engine.load_model(\"unsloth/Llama-3.2-3B-Instruct-GGUF:Llama-3.2-3B-Instruct-Q4_K_M.gguf\")\n",
    "\n",
    "# Qwen 2.5-7B\n",
    "engine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:qwen2.5-7b-instruct-q4_k_m.gguf\")\n",
    "\n",
    "# Your own fine-tuned model\n",
    "engine.load_model(\"/path/to/your/model.gguf\")\n",
    "```\n",
    "\n",
    "### 2. Explore Quantization Types\n",
    "\n",
    "Trade speed vs quality:\n",
    "\n",
    "- **Q4_K_M**: Best balance (default)\n",
    "- **Q5_K_M**: Higher quality, slower\n",
    "- **Q8_0**: Near-original quality, slower\n",
    "- **Q2_K**: Fastest, lower quality\n",
    "\n",
    "### 3. Integrate with Unsloth Fine-tuning\n",
    "\n",
    "```python\n",
    "# After fine-tuning with Unsloth:\n",
    "# 1. Export to GGUF\n",
    "model.save_pretrained_gguf(\"my_model\", quantization_method=\"q4_k_m\")\n",
    "\n",
    "# 2. Load with llcuda\n",
    "engine.load_model(\"my_model/my_model-Q4_K_M.gguf\")\n",
    "```\n",
    "\n",
    "### 4. Production Deployment\n",
    "\n",
    "Use llcuda's HTTP server API:\n",
    "\n",
    "```python\n",
    "from llcuda import ServerManager\n",
    "\n",
    "server = ServerManager()\n",
    "server.start(\n",
    "    model_path=\"model.gguf\",\n",
    "    n_gpu_layers=99,\n",
    "    port=8090\n",
    ")\n",
    "\n",
    "# Access via HTTP at localhost:8090\n",
    "```\n",
    "\n",
    "### 5. Native Tensor Operations (Advanced)\n",
    "\n",
    "```python\n",
    "from llcuda.core import Tensor, DType, matmul\n",
    "\n",
    "# Create tensors on GPU\n",
    "A = Tensor.zeros([2048, 2048], dtype=DType.Float16, device=0)\n",
    "B = Tensor.zeros([2048, 2048], dtype=DType.Float16, device=0)\n",
    "\n",
    "# Matrix multiplication with Tensor Cores\n",
    "C = A @ B\n",
    "```\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **GitHub**: https://github.com/waqasm86/llcuda\n",
    "- **PyPI**: https://pypi.org/project/llcuda/\n",
    "- **Documentation**: https://github.com/waqasm86/llcuda#readme\n",
    "- **Unsloth**: https://github.com/unslothai/unsloth\n",
    "- **llama.cpp**: https://github.com/ggerganov/llama.cpp\n",
    "\n",
    "## üí¨ Support\n",
    "\n",
    "- **Issues**: https://github.com/waqasm86/llcuda/issues\n",
    "- **Discussions**: https://github.com/waqasm86/llcuda/discussions\n",
    "\n",
    "---\n",
    "\n",
    "**Happy inferencing!** üöÄ\n",
    "\n",
    "*Built with llcuda v2.0.1 - CUDA 12-first inference backend for Unsloth on Tesla T4*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
