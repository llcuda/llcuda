{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e98e189",
   "metadata": {},
   "source": [
    "## Step 1: Model Size Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š MODEL SIZE REFERENCE FOR DUAL T4 (30GB)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_table = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model Size       â”‚ Quantization   â”‚ VRAM Needed      â”‚ Fits Dual T4?    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1-3B             â”‚ Q4_K_M         â”‚ ~2-3 GB          â”‚ âœ… Single GPU    â”‚\n",
    "â”‚ 7-8B             â”‚ Q4_K_M         â”‚ ~5-6 GB          â”‚ âœ… Single GPU    â”‚\n",
    "â”‚ 7-8B             â”‚ F16            â”‚ ~15 GB           â”‚ âœ… Single GPU    â”‚\n",
    "â”‚ 13B              â”‚ Q4_K_M         â”‚ ~8-9 GB          â”‚ âœ… Single GPU    â”‚\n",
    "â”‚ 13B              â”‚ Q8_0           â”‚ ~14-15 GB        â”‚ âœ… Single GPU    â”‚\n",
    "â”‚ 32-34B           â”‚ Q4_K_M         â”‚ ~20-22 GB        â”‚ âœ… Dual GPU      â”‚\n",
    "â”‚ 70B              â”‚ Q4_K_M         â”‚ ~40-42 GB        â”‚ âŒ Too Large     â”‚\n",
    "â”‚ 70B              â”‚ IQ3_XS         â”‚ ~25-27 GB        â”‚ âœ… Dual GPU      â”‚\n",
    "â”‚ 70B              â”‚ IQ2_XXS        â”‚ ~20-22 GB        â”‚ âœ… Dual GPU      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“ Recommended Models for Kaggle:\n",
    "â€¢ Best Quality (Single T4):  7-8B Q8_0 or F16\n",
    "â€¢ Best Quality (Dual T4):    32B Q4_K_M\n",
    "â€¢ Largest Possible:          70B IQ3_XS (aggressive quantization)\n",
    "\"\"\"\n",
    "print(models_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d4c66",
   "metadata": {},
   "source": [
    "## Step 2: I-Quant Types for Large Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ”¢ I-QUANT TYPES (FOR 70B+ MODELS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "iquant_table = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Quantization     â”‚ Bits/Weight    â”‚ 70B VRAM         â”‚ Quality          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ IQ4_XS           â”‚ ~4.25 bits     â”‚ ~35 GB           â”‚ â­â­â­â­         â”‚\n",
    "â”‚ IQ3_M            â”‚ ~3.4 bits      â”‚ ~28 GB           â”‚ â­â­â­â­         â”‚\n",
    "â”‚ IQ3_S            â”‚ ~3.0 bits      â”‚ ~26 GB           â”‚ â­â­â­           â”‚\n",
    "â”‚ IQ3_XS           â”‚ ~3.0 bits      â”‚ ~25 GB âœ“         â”‚ â­â­â­           â”‚\n",
    "â”‚ IQ3_XXS          â”‚ ~2.8 bits      â”‚ ~24 GB âœ“         â”‚ â­â­â­           â”‚\n",
    "â”‚ IQ2_M            â”‚ ~2.7 bits      â”‚ ~23 GB âœ“         â”‚ â­â­             â”‚\n",
    "â”‚ IQ2_S            â”‚ ~2.5 bits      â”‚ ~21 GB âœ“         â”‚ â­â­             â”‚\n",
    "â”‚ IQ2_XS           â”‚ ~2.3 bits      â”‚ ~20 GB âœ“         â”‚ â­â­             â”‚\n",
    "â”‚ IQ2_XXS          â”‚ ~2.0 bits      â”‚ ~18 GB âœ“         â”‚ â­               â”‚\n",
    "â”‚ IQ1_M            â”‚ ~1.75 bits     â”‚ ~16 GB âœ“         â”‚ â­               â”‚\n",
    "â”‚ IQ1_S            â”‚ ~1.5 bits      â”‚ ~14 GB âœ“         â”‚ Experimental     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“ Recommendation for 70B on Dual T4:\n",
    "   âœ“ IQ3_XS  - Best balance of quality and size\n",
    "   âœ“ IQ2_XXS - Fits comfortably, lower quality\n",
    "\"\"\"\n",
    "print(iquant_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67cb53",
   "metadata": {},
   "source": [
    "## Step 3: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76346ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
    "!pip install -q huggingface_hub\n",
    "\n",
    "import llcuda\n",
    "print(f\"âœ… llcuda {llcuda.__version__} installed\")\n",
    "\n",
    "# Verify GPUs\n",
    "!nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36467754",
   "metadata": {},
   "source": [
    "## Step 4: Download 70B IQ3_XS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“¥ DOWNLOADING 70B MODEL (IQ3_XS)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Example: Llama 2 70B IQ3_XS\n",
    "# Note: Replace with actual model repo as needed\n",
    "MODEL_REPO = \"TheBloke/Llama-2-70B-GGUF\"  # Example repo\n",
    "MODEL_FILE = \"llama-2-70b.IQ3_XS.gguf\"    # ~25GB file\n",
    "\n",
    "print(f\"ğŸ“¦ Downloading {MODEL_FILE}...\")\n",
    "print(\"â³ This may take 15-30 minutes on Kaggle...\\n\")\n",
    "\n",
    "try:\n",
    "    model_path = hf_hub_download(\n",
    "        repo_id=MODEL_REPO,\n",
    "        filename=MODEL_FILE,\n",
    "        local_dir=\"/kaggle/working/models\",\n",
    "        resume_download=True  # Resume if interrupted\n",
    "    )\n",
    "    \n",
    "    file_size = os.path.getsize(model_path) / 1024**3\n",
    "    print(f\"\\nâœ… Downloaded: {model_path}\")\n",
    "    print(f\"ğŸ“Š Size: {file_size:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Download failed: {e}\")\n",
    "    print(\"\\nğŸ“ Alternative: Use a smaller 70B quant or different model\")\n",
    "    print(\"   Example repos with 70B IQ3_XS:\")\n",
    "    print(\"   â€¢ bartowski/Llama-3.1-70B-Instruct-GGUF\")\n",
    "    print(\"   â€¢ bartowski/Qwen2.5-72B-Instruct-GGUF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784b32d7",
   "metadata": {},
   "source": [
    "## Step 5: Calculate Optimal Tensor Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"âš™ï¸ CALCULATING TENSOR SPLIT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Get available memory on each GPU\n",
    "gpu0_free = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "gpu1_free = torch.cuda.get_device_properties(1).total_memory / 1024**3\n",
    "\n",
    "print(f\"\\nğŸ“Š GPU 0: {gpu0_free:.1f} GB total\")\n",
    "print(f\"ğŸ“Š GPU 1: {gpu1_free:.1f} GB total\")\n",
    "\n",
    "# For identical GPUs, use 50/50 split\n",
    "total = gpu0_free + gpu1_free\n",
    "split0 = gpu0_free / total\n",
    "split1 = gpu1_free / total\n",
    "\n",
    "print(f\"\\nğŸ“Š Calculated tensor split: {split0:.2f},{split1:.2f}\")\n",
    "\n",
    "# Leave headroom for KV cache\n",
    "TENSOR_SPLIT = \"0.48,0.48\"  # Leave ~1GB per GPU for KV cache\n",
    "print(f\"ğŸ“Š Recommended split (with headroom): {TENSOR_SPLIT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a23a811",
   "metadata": {},
   "source": [
    "## Step 6: Start Server with 70B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from llcuda.server import ServerManager\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸš€ STARTING 70B MODEL ON DUAL T4\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration for 70B on dual T4\n",
    "print(\"\\nğŸ“Š Server Configuration:\")\n",
    "print(f\"   Model: 70B IQ3_XS\")\n",
    "print(f\"   Tensor Split: {TENSOR_SPLIT}\")\n",
    "print(f\"   Context: 2048\")\n",
    "print(f\"   Flash Attention: True\")\n",
    "\n",
    "server = ServerManager()\n",
    "server.start_server(\n",
    "    model_path=model_path,  # From download step\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8080,\n",
    "    \n",
    "    # GPU Configuration\n",
    "    gpu_layers=999,           # All layers on GPU\n",
    "    tensor_split=TENSOR_SPLIT,  # Distribute across both T4s\n",
    "    \n",
    "    # Memory Optimization\n",
    "    ctx_size=2048,          # Smaller context for 70B\n",
    "    flash_attention=True,            # Enable flash attention\n",
    "    \n",
    "    # Performance\n",
    "    batch_size=128,                # Smaller batch for memory\n",
    ")\n",
    "\n",
    "print(\"\\nâ³ Loading model (may take 2-5 minutes for 70B)...\")\n",
    "if server.check_server_health(timeout=300):  # 5 minute timeout\n",
    "    print(\"\\nâœ… 70B model loaded on dual T4!\")\n",
    "else:\n",
    "    print(\"\\nâŒ Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacdc736",
   "metadata": {},
   "source": [
    "## Step 7: Verify GPU Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fe52aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¾ GPU MEMORY USAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nğŸ“Š Expected memory distribution:\")\n",
    "print(\"   GPU 0: ~12-13 GB (model layers 0-39)\")\n",
    "print(\"   GPU 1: ~12-13 GB (model layers 40-79)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b4a02",
   "metadata": {},
   "source": [
    "## Step 8: Test 70B Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda.api.client import LlamaCppClient\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ§ª TESTING 70B INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "print(f\"\\nğŸ’¬ Prompt: {prompt}\")\n",
    "print(\"\\nğŸ“ Response:\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "tokens = response.usage.completion_tokens\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance:\")\n",
    "print(f\"   Time: {elapsed:.2f}s\")\n",
    "print(f\"   Tokens: {tokens}\")\n",
    "print(f\"   Speed: {tokens/elapsed:.1f} tok/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7744f2",
   "metadata": {},
   "source": [
    "## Step 9: Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086512e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š PERFORMANCE BENCHMARKS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prompts = [\n",
    "    \"Hello\",  # Very short\n",
    "    \"What is the capital of France?\",  # Short\n",
    "    \"Write a paragraph about machine learning.\",  # Medium\n",
    "    \"Explain the differences between Python and JavaScript with examples.\",  # Long\n",
    "]\n",
    "\n",
    "print(\"\\nğŸƒ Running benchmarks...\\n\")\n",
    "\n",
    "for prompt in prompts:\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    tokens = response.usage.completion_tokens\n",
    "    \n",
    "    print(f\"   Prompt: \\\"{prompt[:40]}...\\\"\")\n",
    "    print(f\"   Time: {elapsed:.2f}s | Tokens: {tokens} | Speed: {tokens/elapsed:.1f} tok/s\")\n",
    "    print()\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ Expected Performance (70B on Dual T4):\n",
    "   â€¢ IQ3_XS: ~5-10 tok/s\n",
    "   â€¢ IQ2_XXS: ~8-15 tok/s (lower quality)\n",
    "   \n",
    "   Compared to:\n",
    "   â€¢ 7B Q4_K_M on single T4: ~40-60 tok/s\n",
    "   â€¢ 13B Q4_K_M on single T4: ~25-35 tok/s\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e751b4",
   "metadata": {},
   "source": [
    "## Step 10: Alternative - 32B Model (Better Speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5160982",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"âš¡ ALTERNATIVE: 32B MODEL FOR BETTER SPEED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“ If 70B is too slow, consider 32B models:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model                      â”‚ Quantization â”‚ VRAM     â”‚ Speed        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Qwen2.5-32B-Instruct      â”‚ Q4_K_M       â”‚ ~20 GB   â”‚ ~15-25 tok/s â”‚\n",
    "â”‚ DeepSeek-V2-Lite-33B      â”‚ Q4_K_M       â”‚ ~20 GB   â”‚ ~15-25 tok/s â”‚\n",
    "â”‚ Yi-34B                     â”‚ Q4_K_M       â”‚ ~21 GB   â”‚ ~15-25 tok/s â”‚\n",
    "â”‚ Mixtral-8x7B (MoE)        â”‚ Q4_K_M       â”‚ ~26 GB   â”‚ ~20-30 tok/s â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ“¦ Example download:\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Qwen2.5-32B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-32B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d6f3b",
   "metadata": {},
   "source": [
    "## Step 11: Streaming for Better UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaae6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸŒŠ STREAMING WITH LARGE MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ’¬ Streaming response (feels faster!):\\n\")\n",
    "\n",
    "start = time.time()\n",
    "token_count = 0\n",
    "\n",
    "for chunk in client.chat_completion_stream(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What are the benefits of streaming?\"}],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "):\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        print(content, end=\"\", flush=True)\n",
    "        token_count += 1\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\n\\nğŸ“Š Streamed {token_count} tokens in {elapsed:.2f}s\")\n",
    "print(\"\\nâœ… Streaming provides immediate feedback even with slower models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4eb35",
   "metadata": {},
   "source": [
    "## Step 12: Memory Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ’¡ MEMORY OPTIMIZATION TIPS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ”¹ Context Size Impact:\n",
    "\n",
    "   Context   â”‚ KV Cache per GPU â”‚ Recommended for 70B\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   512       â”‚ ~0.5 GB          â”‚ âœ… Safe\n",
    "   2048      â”‚ ~1.5 GB          â”‚ âœ… Default\n",
    "   4096      â”‚ ~3 GB            â”‚ âš ï¸ Tight fit\n",
    "   8192      â”‚ ~6 GB            â”‚ âŒ May OOM\n",
    "\n",
    "ğŸ”¹ Batch Size Impact:\n",
    "\n",
    "   n_batch â”‚ Memory â”‚ Speed\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€\n",
    "   64      â”‚ Lower  â”‚ Slower\n",
    "   128     â”‚ Medium â”‚ Medium (recommended)\n",
    "   256     â”‚ Higher â”‚ Faster\n",
    "   512     â”‚ High   â”‚ Fastest (may OOM)\n",
    "\n",
    "ğŸ”¹ If Running Out of Memory:\n",
    "\n",
    "   1. Reduce context_size (2048 â†’ 1024)\n",
    "   2. Reduce n_batch (128 â†’ 64)\n",
    "   3. Use smaller quantization (IQ3_XS â†’ IQ2_XXS)\n",
    "   4. Keep some layers on CPU (n_gpu_layers=60)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae53d5b",
   "metadata": {},
   "source": [
    "## Step 13: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca050623",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ›‘ Stopping server...\")\n",
    "server.stop_server()\n",
    "print(\"âœ… Server stopped\")\n",
    "\n",
    "# Check memory freed\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44124f93",
   "metadata": {},
   "source": [
    "## ğŸ“š Summary\n",
    "\n",
    "### Model Recommendations for Kaggle Dual T4:\n",
    "\n",
    "| Goal | Model | Quantization | VRAM | Speed |\n",
    "|------|-------|-------------|------|-------|\n",
    "| Best Speed | 7-8B | Q4_K_M | 5GB | 50+ tok/s |\n",
    "| Best Balance | 13B | Q4_K_M | 8GB | 30+ tok/s |\n",
    "| Best Quality | 32B | Q4_K_M | 20GB | 20+ tok/s |\n",
    "| Largest | 70B | IQ3_XS | 25GB | 8-10 tok/s |\n",
    "\n",
    "### Key Configuration for 70B:\n",
    "```python\n",
    "ServerConfig(\n",
    "    n_gpu_layers=999,\n",
    "    tensor_split=\"0.48,0.48\",\n",
    "    context_size=2048,  # Keep small\n",
    "    flash_attn=True,\n",
    "    n_batch=128,        # Keep moderate\n",
    ")\n",
    "```\n",
    "\n",
    "### I-Quant Selection:\n",
    "- **IQ3_XS** - Best quality that fits dual T4\n",
    "- **IQ2_XXS** - More headroom, lower quality\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [10-complete-workflow](10-complete-workflow-llcuda-v2.2.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
