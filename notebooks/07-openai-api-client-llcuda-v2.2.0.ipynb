{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e84e05",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62721d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Install dependencies\n",
    "!pip install -q git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
    "!pip install -q huggingface_hub sseclient-py openai\n",
    "\n",
    "import llcuda\n",
    "print(f\"‚úÖ llcuda {llcuda.__version__} installed\")\n",
    "\n",
    "# GPU check\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f075cc",
   "metadata": {},
   "source": [
    "## Step 2: Download Model and Start Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fc5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llcuda.server import ServerManager, ServerConfig\n",
    "import os\n",
    "\n",
    "# Download model\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"unsloth/gemma-3-1b-it-GGUF\",\n",
    "    filename=\"gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"‚úÖ Model: {model_path}\")\n",
    "\n",
    "# Start server with embeddings enabled\n",
    "config = ServerConfig(\n",
    "    model_path=model_path,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8080,\n",
    "    n_gpu_layers=99,\n",
    "    context_size=4096,\n",
    "    flash_attn=True,\n",
    "    embeddings=True,  # Enable embeddings endpoint\n",
    ")\n",
    "\n",
    "server = ServerManager()\n",
    "server.start_with_config(config)\n",
    "\n",
    "if server.wait_until_ready(timeout=60):\n",
    "    print(\"\\n‚úÖ Server ready at http://127.0.0.1:8080\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d6a997",
   "metadata": {},
   "source": [
    "## Step 3: Using llcuda Native Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75641e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda.api.client import LlamaCppClient\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîå LLCUDA NATIVE CLIENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Check server health\n",
    "health = client.health()\n",
    "print(f\"\\nüìä Server Health: {health}\")\n",
    "\n",
    "# Get model info\n",
    "models = client.list_models()\n",
    "print(f\"\\nüìä Available Models:\")\n",
    "for model in models.data:\n",
    "    print(f\"   - {model.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da10fb",
   "metadata": {},
   "source": [
    "## Step 4: Chat Completion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b90506",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üí¨ CHAT COMPLETION API\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Basic chat completion\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is Python's list comprehension?\"}\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Response:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(f\"\\nüìä Usage:\")\n",
    "print(f\"   Model: {response.model}\")\n",
    "print(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"   Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"   Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf0c85",
   "metadata": {},
   "source": [
    "## Step 5: Streaming Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üåä STREAMING CHAT COMPLETION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí¨ Streaming response:\\n\")\n",
    "\n",
    "for chunk in client.chat_completion_stream(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Write a short poem about CUDA programming.\"}\n",
    "    ],\n",
    "    max_tokens=150,\n",
    "    temperature=0.8,\n",
    "):\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ Stream complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d56382",
   "metadata": {},
   "source": [
    "## Step 6: Text Completion API (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28305db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìù TEXT COMPLETION API\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Text completion (not chat format)\n",
    "response = client.completion(\n",
    "    prompt=\"The benefits of GPU computing are:\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.5,\n",
    "    stop=[\"\\n\\n\"],  # Stop at double newline\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Completion:\")\n",
    "print(f\"The benefits of GPU computing are:{response.choices[0].text}\")\n",
    "\n",
    "print(f\"\\nüìä Finish reason: {response.choices[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66ad9a",
   "metadata": {},
   "source": [
    "## Step 7: Advanced Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0cf221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"‚öôÔ∏è ADVANCED PARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Available Parameters:\n",
    "\n",
    "üîπ Sampling Parameters:\n",
    "   temperature     - Randomness (0.0-2.0, default 0.8)\n",
    "   top_p           - Nucleus sampling (0.0-1.0, default 0.95)\n",
    "   top_k           - Top-k sampling (1-100, default 40)\n",
    "   min_p           - Minimum probability (0.0-1.0)\n",
    "   repeat_penalty  - Repetition penalty (1.0-2.0)\n",
    "   presence_penalty- OpenAI-style (-2.0 to 2.0)\n",
    "   frequency_penalty- OpenAI-style (-2.0 to 2.0)\n",
    "\n",
    "üîπ Generation Parameters:\n",
    "   max_tokens      - Maximum tokens to generate\n",
    "   stop            - Stop sequences (list of strings)\n",
    "   seed            - Random seed for reproducibility\n",
    "   n               - Number of completions to generate\n",
    "\n",
    "üîπ Response Format:\n",
    "   stream          - Stream response (True/False)\n",
    "   logprobs        - Return log probabilities\n",
    "\"\"\")\n",
    "\n",
    "# Example with advanced parameters\n",
    "response = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Name 3 programming languages.\"}],\n",
    "    max_tokens=50,\n",
    "    temperature=0.3,       # Low temperature for focused output\n",
    "    top_p=0.9,            # Nucleus sampling\n",
    "    repeat_penalty=1.1,   # Slight repetition penalty\n",
    "    seed=42,              # Reproducible output\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Response (with advanced params):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aaf36e",
   "metadata": {},
   "source": [
    "## Step 8: Tokenization API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî§ TOKENIZATION API\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Tokenize text\n",
    "text = \"Hello, CUDA programming!\"\n",
    "tokens = client.tokenize(text)\n",
    "\n",
    "print(f\"\\nüìù Text: '{text}'\")\n",
    "print(f\"üìä Tokens: {tokens.tokens}\")\n",
    "print(f\"üìä Token count: {len(tokens.tokens)}\")\n",
    "\n",
    "# Detokenize back\n",
    "decoded = client.detokenize(tokens.tokens)\n",
    "print(f\"üìù Decoded: '{decoded.content}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d75ea1",
   "metadata": {},
   "source": [
    "## Step 9: Embeddings API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî¢ EMBEDDINGS API\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Generate embeddings\n",
    "    texts = [\n",
    "        \"CUDA is a parallel computing platform.\",\n",
    "        \"Python is a programming language.\",\n",
    "        \"GPU acceleration speeds up computations.\"\n",
    "    ]\n",
    "    \n",
    "    embeddings = client.embeddings(texts)\n",
    "    \n",
    "    print(f\"\\nüìä Generated {len(embeddings.data)} embeddings\")\n",
    "    print(f\"üìä Embedding dimension: {len(embeddings.data[0].embedding)}\")\n",
    "    \n",
    "    # Show first few values\n",
    "    print(f\"\\nüìä First embedding (first 5 values):\")\n",
    "    print(f\"   {embeddings.data[0].embedding[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Embeddings not available: {e}\")\n",
    "    print(\"   Note: Start server with --embeddings flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5bc17c",
   "metadata": {},
   "source": [
    "## Step 10: Using Official OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîå USING OPENAI PYTHON CLIENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create OpenAI client pointing to llama-server\n",
    "openai_client = OpenAI(\n",
    "    base_url=\"http://127.0.0.1:8080/v1\",\n",
    "    api_key=\"not-needed\"  # llama-server doesn't require API key\n",
    ")\n",
    "\n",
    "# Use exactly like OpenAI API!\n",
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gemma-3-1b-it\",  # Model name (can be anything)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is llama.cpp?\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù Response via OpenAI client:\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(f\"\\n‚úÖ OpenAI SDK works with llama-server!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42f780",
   "metadata": {},
   "source": [
    "## Step 11: OpenAI Client Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üåä OPENAI CLIENT STREAMING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüí¨ Streaming via OpenAI client:\\n\")\n",
    "\n",
    "stream = openai_client.chat.completions.create(\n",
    "    model=\"gemma-3-1b-it\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain tensor cores in 3 sentences.\"}],\n",
    "    max_tokens=100,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n‚úÖ OpenAI streaming works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadc8c14",
   "metadata": {},
   "source": [
    "## Step 12: Using requests (Raw HTTP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì° RAW HTTP REQUESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:8080\"\n",
    "\n",
    "# Health check\n",
    "health = requests.get(f\"{BASE_URL}/health\").json()\n",
    "print(f\"\\nüìä Health: {health}\")\n",
    "\n",
    "# Chat completion via HTTP\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/v1/chat/completions\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json={\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "data = response.json()\n",
    "print(f\"\\nüìù Response via HTTP:\")\n",
    "print(data['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95152bbd",
   "metadata": {},
   "source": [
    "## Step 13: Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ BATCH PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"What is JavaScript?\",\n",
    "    \"What is Rust?\",\n",
    "    \"What is Go?\",\n",
    "]\n",
    "\n",
    "results = []\n",
    "start = time.time()\n",
    "\n",
    "print(f\"\\nüèÉ Processing {len(prompts)} prompts...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=30,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.split('.')[0] + '.'  # First sentence\n",
    "    results.append((prompt, answer))\n",
    "    print(f\"   {i}. {prompt}\")\n",
    "    print(f\"      ‚Üí {answer[:60]}...\")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nüìä Batch complete: {len(prompts)} prompts in {elapsed:.2f}s\")\n",
    "print(f\"   Average: {elapsed/len(prompts):.2f}s per prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc42af",
   "metadata": {},
   "source": [
    "## Step 14: Multi-turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2379fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üí¨ MULTI-TURN CONVERSATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python tutor.\"},\n",
    "]\n",
    "\n",
    "turns = [\n",
    "    \"What is a list in Python?\",\n",
    "    \"How do I add an item to it?\",\n",
    "    \"Show me an example.\"\n",
    "]\n",
    "\n",
    "for turn in turns:\n",
    "    print(f\"\\nüë§ User: {turn}\")\n",
    "    \n",
    "    # Add user message\n",
    "    conversation.append({\"role\": \"user\", \"content\": turn})\n",
    "    \n",
    "    # Get response\n",
    "    response = client.chat_completion(\n",
    "        messages=conversation,\n",
    "        max_tokens=100,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    assistant_msg = response.choices[0].message.content\n",
    "    print(f\"ü§ñ Assistant: {assistant_msg}\")\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
    "\n",
    "print(f\"\\nüìä Conversation length: {len(conversation)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddc104",
   "metadata": {},
   "source": [
    "## Step 15: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõë Stopping server...\")\n",
    "server.stop()\n",
    "print(\"‚úÖ Server stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa597481",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "### Client Options:\n",
    "| Method | Best For |\n",
    "|--------|----------|\n",
    "| `llcuda.api.client` | Native llcuda integration |\n",
    "| `openai` SDK | Drop-in OpenAI replacement |\n",
    "| `requests` | Raw HTTP, custom integrations |\n",
    "\n",
    "### Key Endpoints:\n",
    "- `/v1/chat/completions` - Chat API (recommended)\n",
    "- `/v1/completions` - Text completion\n",
    "- `/v1/embeddings` - Text embeddings\n",
    "- `/tokenize` - Tokenization\n",
    "- `/health` - Server status\n",
    "\n",
    "### Code Pattern:\n",
    "```python\n",
    "# Native llcuda\n",
    "from llcuda.api.client import LlamaCppClient\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# OpenAI SDK\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:8080/v1\", api_key=\"na\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [08-nccl-pytorch](08-nccl-pytorch-llcuda-v2.2.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
