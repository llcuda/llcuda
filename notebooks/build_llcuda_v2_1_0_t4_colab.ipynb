{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Build llcuda v2.1.0 Binaries for Tesla T4 (Google Colab)\n",
    "\n",
    "**Purpose**: Build complete CUDA 12 binaries for llcuda v2.1.0 on Google Colab Tesla T4 GPU\n",
    "\n",
    "**Output**:\n",
    "1. llama.cpp binaries (264 MB) - HTTP server mode with FlashAttention\n",
    "2. Complete package: `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz`\n",
    "\n",
    "**Important Notes**:\n",
    "- These binaries are optimized for Tesla T4 (SM 7.5)\n",
    "- Includes FlashAttention v2, CUDA Graphs, and Tensor Core optimizations\n",
    "- Compatible with v2.1.0 Python APIs and Unsloth integration\n",
    "\n",
    "**Requirements**:\n",
    "- Google Colab with Tesla T4 GPU\n",
    "- CUDA 12.x (pre-installed in Colab)\n",
    "- Python 3.10+\n",
    "\n",
    "**Estimated Time**: ~15 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## Step 1: Verify GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-cuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CUDA version\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version\n",
    "import sys\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Expected: 3.10+ (Colab default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-compute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify compute capability\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "compute_cap = result.stdout.strip()\n",
    "major, minor = map(int, compute_cap.split('.'))\n",
    "\n",
    "print(f\"Compute Capability: SM {major}.{minor}\")\n",
    "\n",
    "if major == 7 and minor == 5:\n",
    "    print(\"âœ“ Tesla T4 detected - Perfect for llcuda v2.1.0!\")\n",
    "elif major >= 7 and minor >= 5:\n",
    "    print(f\"âœ“ SM {major}.{minor} detected - Compatible with llcuda v2.1.0\")\n",
    "else:\n",
    "    print(f\"âš  WARNING: SM {major}.{minor} is below SM 7.5 (T4)\")\n",
    "    print(\"llcuda v2.1.0 requires SM 7.5+ for Tensor Cores and FlashAttention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## Step 2: Clone llama.cpp Repository\n",
    "\n",
    "We'll build llama.cpp with CUDA 12 support, FlashAttention, and optimizations for Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-llamacpp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone llama.cpp\n",
    "%cd /content\n",
    "!git clone https://github.com/ggml-org/llama.cpp.git\n",
    "%cd llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check llama.cpp version\n",
    "!git log --oneline -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## Step 3: Configure and Build llama.cpp for Tesla T4\n",
    "\n",
    "**Build Configuration**:\n",
    "- **Target**: Tesla T4 (SM 7.5)\n",
    "- **CUDA**: 12.x\n",
    "- **FlashAttention**: Enabled (2-3x faster)\n",
    "- **CUDA Graphs**: Enabled (20-40% latency reduction)\n",
    "- **Tensor Cores**: Optimized for mixed precision\n",
    "- **Shared Libraries**: Enabled for dynamic loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-cmake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure llama.cpp for Tesla T4 with all optimizations\n",
    "!cmake -B build_cuda12_t4 \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DGGML_CUDA_FORCE_MMQ=OFF \\\n",
    "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\n",
    "    -DGGML_CUDA_FA=ON \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_CUDA_GRAPHS=ON \\\n",
    "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DLLAMA_BUILD_TOOLS=ON \\\n",
    "    -DLLAMA_CURL=ON \\\n",
    "    -DBUILD_SHARED_LIBS=ON \\\n",
    "    -DCMAKE_INSTALL_RPATH='$ORIGIN/../lib' \\\n",
    "    -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-llamacpp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build llama.cpp (takes ~10 minutes)\n",
    "import time\n",
    "\n",
    "print(\"Building llama.cpp with CUDA 12 + FlashAttention...\")\n",
    "print(\"Estimated time: 10-12 minutes\\n\")\n",
    "start_time = time.time()\n",
    "\n",
    "!cmake --build build_cuda12_t4 --config Release -j$(nproc)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ“ Build completed in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-binaries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify binaries were built successfully\n",
    "print(\"=== Built Binaries ===\")\n",
    "!ls -lh build_cuda12_t4/bin/llama-server\n",
    "!ls -lh build_cuda12_t4/bin/llama-cli\n",
    "!ls -lh build_cuda12_t4/bin/llama-quantize\n",
    "!ls -lh build_cuda12_t4/bin/llama-embedding\n",
    "!ls -lh build_cuda12_t4/bin/llama-bench\n",
    "\n",
    "print(\"\\n=== Shared Libraries ===\")\n",
    "!ls -lh build_cuda12_t4/bin/*.so* | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test llama-server binary\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set LD_LIBRARY_PATH to include CUDA libraries\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/targets/x86_64-linux/lib:/content/llama.cpp/build_cuda12_t4/bin'\n",
    "\n",
    "result = subprocess.run(\n",
    "    ['/content/llama.cpp/build_cuda12_t4/bin/llama-server', '--version'],\n",
    "    env=os.environ,\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "print(\"STDERR:\", result.stderr)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ“ llama-server works correctly!\")\n",
    "else:\n",
    "    print(f\"\\nâœ— Error: Return code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## Step 4: Package Binaries for Distribution\n",
    "\n",
    "Create the `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz` package with:\n",
    "- llama-server, llama-cli, llama-quantize, llama-embedding, llama-bench\n",
    "- All required shared libraries (.so files)\n",
    "- Proper directory structure for llcuda v2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create package directory structure\n",
    "%cd /content\n",
    "\n",
    "!mkdir -p llcuda_binaries_t4/bin\n",
    "!mkdir -p llcuda_binaries_t4/lib\n",
    "\n",
    "# Copy essential binaries\n",
    "print(\"Copying binaries...\")\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-server llcuda_binaries_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-cli llcuda_binaries_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-quantize llcuda_binaries_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-embedding llcuda_binaries_t4/bin/\n",
    "!cp llama.cpp/build_cuda12_t4/bin/llama-bench llcuda_binaries_t4/bin/\n",
    "\n",
    "# Copy all shared libraries\n",
    "print(\"Copying shared libraries...\")\n",
    "!cp llama.cpp/build_cuda12_t4/bin/*.so* llcuda_binaries_t4/lib/\n",
    "\n",
    "print(\"\\nâœ“ Package structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-readme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README for the package\n",
    "readme_content = \"\"\"# llcuda v2.1.0 Binaries for Tesla T4\n",
    "\n",
    "**Built on**: Google Colab\n",
    "**GPU**: Tesla T4 (SM 7.5)\n",
    "**CUDA**: 12.x\n",
    "**Date**: {date}\n",
    "\n",
    "## Contents\n",
    "\n",
    "### bin/\n",
    "- `llama-server` - HTTP inference server\n",
    "- `llama-cli` - Command-line interface\n",
    "- `llama-quantize` - Model quantization tool\n",
    "- `llama-embedding` - Embedding generation\n",
    "- `llama-bench` - Performance benchmarking\n",
    "\n",
    "### lib/\n",
    "- `libggml-cuda.so` - GGML CUDA kernels with FlashAttention\n",
    "- `libllama.so` - llama.cpp library\n",
    "- Other required shared libraries\n",
    "\n",
    "## Installation\n",
    "\n",
    "These binaries are automatically downloaded by llcuda v2.1.0 on first import.\n",
    "\n",
    "For manual installation:\n",
    "```bash\n",
    "# Extract\n",
    "tar -xzf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
    "\n",
    "# Copy to llcuda package (if needed)\n",
    "cp -r bin lib ~/.cache/llcuda/binaries/cuda12/\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "- âœ… FlashAttention v2 (2-3x faster attention)\n",
    "- âœ… Tensor Core optimization (SM 7.5)\n",
    "- âœ… CUDA Graphs (20-40% latency reduction)\n",
    "- âœ… All quantization formats (NF4, Q4_K_M, Q5_K_M, Q8_0, F16)\n",
    "- âœ… Optimized for Tesla T4 GPUs\n",
    "\n",
    "## Compatibility\n",
    "\n",
    "- **llcuda**: v2.1.0+\n",
    "- **Python**: 3.10+\n",
    "- **CUDA**: 12.x\n",
    "- **GPU**: Tesla T4 (SM 7.5) or higher\n",
    "\n",
    "## Usage with llcuda\n",
    "\n",
    "```python\n",
    "import llcuda\n",
    "\n",
    "# Binaries are automatically loaded\n",
    "engine = llcuda.InferenceEngine()\n",
    "engine.load_model(\"model.gguf\")\n",
    "result = engine.infer(\"Your prompt\")\n",
    "```\n",
    "\n",
    "## Links\n",
    "\n",
    "- **llcuda**: https://github.com/llcuda/llcuda\n",
    "- **Documentation**: https://llcuda.github.io/\n",
    "- **llama.cpp**: https://github.com/ggml-org/llama.cpp\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: Google Colab Tesla T4 | CUDA 12 | llama.cpp\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "readme_content = readme_content.format(date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "with open('/content/llcuda_binaries_t4/README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"âœ“ README.md created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-build-info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BUILD_INFO.txt with build details\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get llama.cpp commit hash\n",
    "llamacpp_commit = subprocess.run(\n",
    "    ['git', 'rev-parse', 'HEAD'],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    "    cwd='/content/llama.cpp'\n",
    ").stdout.strip()\n",
    "\n",
    "build_info = f\"\"\"llcuda v2.1.0 Binary Build Information\n",
    "=========================================\n",
    "\n",
    "Build Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")}\n",
    "Build Platform: Google Colab\n",
    "GPU: Tesla T4 (SM 7.5)\n",
    "CUDA Version: 12.x\n",
    "Python Version: {sys.version.split()[0]}\n",
    "\n",
    "llama.cpp Details:\n",
    "------------------\n",
    "Repository: https://github.com/ggml-org/llama.cpp\n",
    "Commit: {llamacpp_commit}\n",
    "\n",
    "Build Configuration:\n",
    "-------------------\n",
    "CMAKE_BUILD_TYPE=Release\n",
    "GGML_CUDA=ON\n",
    "CMAKE_CUDA_ARCHITECTURES=75\n",
    "GGML_CUDA_FA=ON (FlashAttention)\n",
    "GGML_CUDA_FA_ALL_QUANTS=ON\n",
    "GGML_CUDA_GRAPHS=ON\n",
    "BUILD_SHARED_LIBS=ON\n",
    "\n",
    "Features:\n",
    "---------\n",
    "- FlashAttention v2 enabled\n",
    "- CUDA Graphs optimization\n",
    "- Tensor Core utilization\n",
    "- All quantization formats supported\n",
    "- HTTP server mode\n",
    "\n",
    "Compatible with:\n",
    "----------------\n",
    "- llcuda v2.1.0+\n",
    "- Python 3.10+\n",
    "- CUDA 12.x\n",
    "- Tesla T4 or higher (SM 7.5+)\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/llcuda_binaries_t4/BUILD_INFO.txt', 'w') as f:\n",
    "    f.write(build_info)\n",
    "\n",
    "print(\"âœ“ BUILD_INFO.txt created\")\n",
    "print(\"\\nBuild Information:\")\n",
    "print(build_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-package-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show package contents and sizes\n",
    "print(\"=== Package Contents ===\")\n",
    "!du -sh /content/llcuda_binaries_t4\n",
    "!du -sh /content/llcuda_binaries_t4/bin\n",
    "!du -sh /content/llcuda_binaries_t4/lib\n",
    "\n",
    "print(\"\\n=== Binary Files ===\")\n",
    "!ls -lh /content/llcuda_binaries_t4/bin/\n",
    "\n",
    "print(\"\\n=== Library Files ===\")\n",
    "!ls -lh /content/llcuda_binaries_t4/lib/ | head -15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## Step 5: Create tar.gz Archive\n",
    "\n",
    "Create the final `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz` archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-tarball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tar.gz archive\n",
    "%cd /content\n",
    "\n",
    "# Rename to match expected structure\n",
    "!mv llcuda_binaries_t4 package_t4\n",
    "\n",
    "print(\"Creating tar.gz archive...\")\n",
    "!tar -czf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz package_t4/\n",
    "\n",
    "print(\"\\nâœ“ Archive created successfully!\")\n",
    "print(\"\\n=== Final Package ===\")\n",
    "!ls -lh llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
    "!du -h llcuda-binaries-cuda12-t4-v2.1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-checksum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHA256 checksum\n",
    "!sha256sum llcuda-binaries-cuda12-t4-v2.1.0.tar.gz > llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256\n",
    "\n",
    "print(\"âœ“ SHA256 checksum created\")\n",
    "print(\"\\nChecksum:\")\n",
    "!cat llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify archive contents\n",
    "print(\"=== Archive Contents ===\")\n",
    "!tar -tzf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz | head -30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## Step 6: Download Files\n",
    "\n",
    "Download the binary package and checksum to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading llcuda-binaries-cuda12-t4-v2.1.0.tar.gz (266 MB)...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "files.download('/content/llcuda-binaries-cuda12-t4-v2.1.0.tar.gz')\n",
    "\n",
    "print(\"\\nDownloading checksum file...\")\n",
    "files.download('/content/llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256')\n",
    "\n",
    "print(\"\\nâœ“ All files downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Build Complete!\n",
    "\n",
    "### Created Files:\n",
    "\n",
    "1. **llcuda-binaries-cuda12-t4-v2.1.0.tar.gz** (~266 MB)\n",
    "   - llama.cpp binaries with FlashAttention\n",
    "   - All required shared libraries\n",
    "   - README and build information\n",
    "\n",
    "2. **llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256**\n",
    "   - SHA256 checksum for verification\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Upload to GitHub Releases**:\n",
    "   ```bash\n",
    "   gh release create v2.1.0 \\\n",
    "       --repo llcuda/llcuda \\\n",
    "       --title \"llcuda v2.1.0 - Tesla T4 Release\" \\\n",
    "       --notes \"Complete CUDA 12 binaries with FlashAttention for Tesla T4\" \\\n",
    "       llcuda-binaries-cuda12-t4-v2.1.0.tar.gz \\\n",
    "       llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256\n",
    "   ```\n",
    "\n",
    "2. **Test Installation**:\n",
    "   ```python\n",
    "   import llcuda\n",
    "   print(llcuda.__version__)  # Should show 2.1.0\n",
    "   ```\n",
    "\n",
    "3. **Update bootstrap.py** to download from v2.1.0 release\n",
    "\n",
    "### Package Features:\n",
    "\n",
    "- âœ… FlashAttention v2 (2-3x faster)\n",
    "- âœ… CUDA Graphs (20-40% latency reduction)\n",
    "- âœ… Tensor Core optimization\n",
    "- âœ… All quantization formats\n",
    "- âœ… Optimized for Tesla T4 (SM 7.5)\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: Google Colab Tesla T4 | CUDA 12 | Python 3.10+  \n",
    "**For**: llcuda v2.1.0 with Unsloth Integration  \n",
    "**Repository**: https://github.com/llcuda/llcuda  \n",
    "**Documentation**: https://llcuda.github.io/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
