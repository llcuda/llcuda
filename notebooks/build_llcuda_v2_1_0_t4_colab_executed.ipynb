{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Build llcuda v2.1.0 Binaries for Tesla T4 (Google Colab)\n",
        "\n",
        "**Purpose**: Build complete CUDA 12 binaries for llcuda v2.1.0 on Google Colab Tesla T4 GPU\n",
        "\n",
        "**Output**:\n",
        "1. llama.cpp binaries (264 MB) - HTTP server mode with FlashAttention\n",
        "2. Complete package: `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz`\n",
        "\n",
        "**Important Notes**:\n",
        "- These binaries are optimized for Tesla T4 (SM 7.5)\n",
        "- Includes FlashAttention v2, CUDA Graphs, and Tensor Core optimizations\n",
        "- Compatible with v2.1.0 Python APIs and Unsloth integration\n",
        "\n",
        "**Requirements**:\n",
        "- Google Colab with Tesla T4 GPU\n",
        "- CUDA 12.x (pre-installed in Colab)\n",
        "- Python 3.10+\n",
        "\n",
        "**Estimated Time**: ~15 minutes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step1",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Verify GPU and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "check-gpu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-gpu",
        "outputId": "7bf4279c-df41-41f2-cffe-5eba7430a96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, compute_cap, driver_version, memory.total [MiB]\n",
            "Tesla T4, 7.5, 550.54.15, 15360 MiB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "check-cuda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-cuda",
        "outputId": "8dd062ae-b131-42cf-8ab1-24de40157768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ],
      "source": [
        "# Verify CUDA version\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "check-python",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-python",
        "outputId": "22cd221d-95c9-441f-9e91-f5793f03993b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Expected: 3.10+ (Colab default)\n"
          ]
        }
      ],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Expected: 3.10+ (Colab default)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "verify-compute",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify-compute",
        "outputId": "6a94fb55-608c-409b-9171-2e6f9c962243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compute Capability: SM 7.5\n",
            "✓ Tesla T4 detected - Perfect for llcuda v2.1.0!\n"
          ]
        }
      ],
      "source": [
        "# Verify compute capability\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=compute_cap', '--format=csv,noheader'],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "compute_cap = result.stdout.strip()\n",
        "major, minor = map(int, compute_cap.split('.'))\n",
        "\n",
        "print(f\"Compute Capability: SM {major}.{minor}\")\n",
        "\n",
        "if major == 7 and minor == 5:\n",
        "    print(\"✓ Tesla T4 detected - Perfect for llcuda v2.1.0!\")\n",
        "elif major >= 7 and minor >= 5:\n",
        "    print(f\"✓ SM {major}.{minor} detected - Compatible with llcuda v2.1.0\")\n",
        "else:\n",
        "    print(f\"⚠ WARNING: SM {major}.{minor} is below SM 7.5 (T4)\")\n",
        "    print(\"llcuda v2.1.0 requires SM 7.5+ for Tensor Cores and FlashAttention\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step2",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Clone llama.cpp Repository\n",
        "\n",
        "We'll build llama.cpp with CUDA 12 support, FlashAttention, and optimizations for Tesla T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "clone-llamacpp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone-llamacpp",
        "outputId": "3f1d5417-2e64-4895-868f-8c7264568bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 75979, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 75979 (delta 29), reused 17 (delta 16), pack-reused 75924 (from 3)\u001b[K\n",
            "Receiving objects: 100% (75979/75979), 279.54 MiB | 31.18 MiB/s, done.\n",
            "Resolving deltas: 100% (55154/55154), done.\n",
            "/content/llama.cpp\n"
          ]
        }
      ],
      "source": [
        "# Clone llama.cpp\n",
        "%cd /content\n",
        "!git clone https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "check-version",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-version",
        "outputId": "6d68ef54-2f45-4ab5-e7a4-7872d929b3d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m516a4ca9b\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m refactor : remove libcurl, use OpenSSL when available (#18828)\n",
            "\u001b[33m3e4bb2966\u001b[m\u001b[33m (\u001b[m\u001b[1;33mtag: b7735\u001b[m\u001b[33m)\u001b[m vulkan: Check maxStorageBufferRange in supports_op (#18709)\n",
            "\u001b[33m47f961249\u001b[m llama-model: fix unfortunate typo (#18832)\n",
            "\u001b[33m01cbdfd7e\u001b[m CUDA : fix typo in clang pragma comment [no ci] (#18830)\n",
            "\u001b[33m635ef78ec\u001b[m vulkan: work around Intel fp16 bug in mmq (#18814)\n"
          ]
        }
      ],
      "source": [
        "# Check llama.cpp version\n",
        "!git log --oneline -5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step3",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Configure and Build llama.cpp for Tesla T4\n",
        "\n",
        "**Build Configuration**:\n",
        "- **Target**: Tesla T4 (SM 7.5)\n",
        "- **CUDA**: 12.x\n",
        "- **FlashAttention**: Enabled (2-3x faster)\n",
        "- **CUDA Graphs**: Enabled (20-40% latency reduction)\n",
        "- **Tensor Cores**: Optimized for mixed precision\n",
        "- **Shared Libraries**: Enabled for dynamic loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "configure-cmake",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "configure-cmake",
        "outputId": "dd97a160-c953-460c-f6c0-21e315dd15d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "\u001b[33mCMake Warning at CMakeLists.txt:121 (message):\n",
            "  LLAMA_CURL option is deprecated and will be ignored\n",
            "\n",
            "\u001b[0m\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "-- CUDA Toolkit found\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n",
            "-- CUDA host compiler is GNU 11.4.0\n",
            "-- Including CUDA backend\n",
            "-- ggml version: 0.9.5\n",
            "-- ggml commit:  516a4ca9b\n",
            "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
            "-- OpenSSL found: 3.0.2\n",
            "-- Downloading tinyllamas/stories15M-q4_0.gguf from ggml-org/models...\n",
            "-- Generating embedded license file for target: common\n",
            "-- Configuring done (5.8s)\n",
            "-- Generating done (0.3s)\n",
            "-- Build files have been written to: /content/llama.cpp/build_cuda12_t4\n"
          ]
        }
      ],
      "source": [
        "# Configure llama.cpp for Tesla T4 with all optimizations\n",
        "!cmake -B build_cuda12_t4 \\\n",
        "    -DCMAKE_BUILD_TYPE=Release \\\n",
        "    -DGGML_CUDA=ON \\\n",
        "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
        "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n",
        "    -DGGML_NATIVE=OFF \\\n",
        "    -DGGML_CUDA_FORCE_MMQ=OFF \\\n",
        "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\n",
        "    -DGGML_CUDA_FA=ON \\\n",
        "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
        "    -DGGML_CUDA_GRAPHS=ON \\\n",
        "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\n",
        "    -DLLAMA_BUILD_SERVER=ON \\\n",
        "    -DLLAMA_BUILD_TOOLS=ON \\\n",
        "    -DLLAMA_CURL=ON \\\n",
        "    -DBUILD_SHARED_LIBS=ON \\\n",
        "    -DCMAKE_INSTALL_RPATH='$ORIGIN/../lib' \\\n",
        "    -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "build-llamacpp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "build-llamacpp",
        "outputId": "c93be9b7-8361-4ea1-c27c-0ceb929e1717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building llama.cpp with CUDA 12 + FlashAttention...\n",
            "Estimated time: 10-12 minutes\n",
            "\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  0%] Built target build_info\n",
            "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  3%] Built target ggml-base\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  3%] Built target sha256\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  4%] Built target xxhash\n",
            "[  4%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  4%] Built target sha1\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  5%] Built target llama-llava-cli\n",
            "[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  5%] Built target llama-gemma3-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  6%] Built target llama-minicpmv-cli\n",
            "[  6%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  6%] Built target llama-qwen2vl-cli\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
            "[  7%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[  7%] Built target cpp-httplib\n",
            "[  7%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  9%] Built target ggml-cpu\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_1.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_1.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q8_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-f16.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_1.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_1.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q8_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-f16.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_0.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_1.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_0.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_1.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q8_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-f16.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_1.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_1.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q8_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-f16.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_1.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_1.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q8_0.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-f16.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_0.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_1.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_0.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_1.cu.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n",
            "[ 37%] Built target ggml-cuda\n",
            "[ 37%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 37%] Built target ggml\n",
            "[ 37%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 37%] Built target llama-gguf-hash\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 38%] Built target llama-gguf\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 61%] Built target llama\n",
            "[ 62%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 65%] Built target mtmd\n",
            "[ 66%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 66%] Built target test-c\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 67%] Built target llama-simple\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 68%] Built target llama-simple-chat\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 70%] Built target common\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 71%] Built target test-tokenizer-0\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 72%] Built target test-sampling\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 72%] Built target test-grammar-parser\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 73%] Built target test-llama-grammar\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 73%] Built target test-grammar-integration\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 74%] Built target test-json-schema-to-grammar\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 75%] Built target test-quantize-stats\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 75%] Built target test-gbnf-validator\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 75%] Built target test-tokenizer-1-bpe\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 75%] Built target test-tokenizer-1-spm\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 76%] Built target test-chat-parser\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 76%] Built target test-chat\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 77%] Built target test-chat-template\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 77%] Built target test-json-partial\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 77%] Built target test-log\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
            "[ 77%] Built target test-chat-peg-parser\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 77%] Built target test-regex-partial\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 78%] Built target test-thread-safety\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 78%] Built target test-arg-parser\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
            "[ 78%] Built target test-peg-parser\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 80%] Built target test-opt\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 80%] Built target test-gguf\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 81%] Built target test-model-load-cancel\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 82%] Built target test-autorelease\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n",
            "[ 83%] Built target test-backend-sampler\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
            "[ 84%] Built target test-state-restore-fragmented\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 84%] Built target test-barrier\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 85%] Built target test-quantize-fns\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 85%] Built target test-quantize-perf\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 86%] Built target test-rope\n",
            "[ 86%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 86%] Built target test-mtmd-c-api\n",
            "[ 86%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 87%] Built target test-alloc\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 87%] Built target llama-batched\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n",
            "[ 88%] Built target llama-debug\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 88%] Built target llama-embedding\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 89%] Built target llama-eval-callback\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 89%] Built target test-backend-ops\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
            "[ 89%] Built target llama-idle\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 89%] Built target llama-lookahead\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 89%] Built target llama-lookup\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 90%] Built target llama-lookup-create\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 90%] Built target llama-lookup-merge\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 90%] Built target llama-lookup-stats\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 91%] Built target llama-parallel\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 91%] Built target llama-passkey\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 92%] Built target llama-retrieval\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 92%] Built target llama-save-load-state\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 92%] Built target llama-speculative-simple\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 92%] Built target llama-speculative\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 92%] Built target llama-gen-docs\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 92%] Built target llama-finetune\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 92%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 92%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 92%] Built target llama-diffusion-cli\n",
            "[ 92%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 92%] Built target llama-vdot\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 92%] Built target llama-q8dot\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 94%] Built target llama-gguf-split\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 94%] Built target llama-batched-bench\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 94%] Built target llama-imatrix\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 94%] Built target llama-bench\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
            "[ 95%] Built target llama-completion\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 96%] Built target llama-quantize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 96%] Built target llama-perplexity\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 97%] Built target llama-tokenize\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 98%] Built target llama-tts\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 98%] Built target llama-cvector-generator\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 98%] Built target llama-export-lora\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
            "[ 99%] Built target llama-fit-params\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
            "[ 99%] Built target server-context\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 99%] Built target llama-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "\n",
            "✓ Build completed in 52.8 minutes\n"
          ]
        }
      ],
      "source": [
        "# Build llama.cpp (takes ~10 minutes)\n",
        "import time\n",
        "\n",
        "print(\"Building llama.cpp with CUDA 12 + FlashAttention...\")\n",
        "print(\"Estimated time: 10-12 minutes\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "!cmake --build build_cuda12_t4 --config Release -j$(nproc)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n✓ Build completed in {elapsed/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "verify-binaries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify-binaries",
        "outputId": "024a15ce-f84a-403e-b07b-0184097d3470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Built Binaries ===\n",
            "-rwxr-xr-x 1 root root 6.7M Jan 14 18:54 build_cuda12_t4/bin/llama-server\n",
            "-rwxr-xr-x 1 root root 5.1M Jan 14 18:54 build_cuda12_t4/bin/llama-cli\n",
            "-rwxr-xr-x 1 root root 434K Jan 14 18:53 build_cuda12_t4/bin/llama-quantize\n",
            "-rwxr-xr-x 1 root root 4.2M Jan 14 18:52 build_cuda12_t4/bin/llama-embedding\n",
            "-rwxr-xr-x 1 root root 581K Jan 14 18:52 build_cuda12_t4/bin/llama-bench\n",
            "\n",
            "=== Shared Libraries ===\n",
            "lrwxrwxrwx 1 root root   17 Jan 14 18:02 build_cuda12_t4/bin/libggml-base.so -> libggml-base.so.0\n",
            "lrwxrwxrwx 1 root root   21 Jan 14 18:02 build_cuda12_t4/bin/libggml-base.so.0 -> libggml-base.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 721K Jan 14 18:02 build_cuda12_t4/bin/libggml-base.so.0.9.5\n",
            "lrwxrwxrwx 1 root root   16 Jan 14 18:03 build_cuda12_t4/bin/libggml-cpu.so -> libggml-cpu.so.0\n",
            "lrwxrwxrwx 1 root root   20 Jan 14 18:03 build_cuda12_t4/bin/libggml-cpu.so.0 -> libggml-cpu.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 949K Jan 14 18:03 build_cuda12_t4/bin/libggml-cpu.so.0.9.5\n",
            "lrwxrwxrwx 1 root root   17 Jan 14 18:44 build_cuda12_t4/bin/libggml-cuda.so -> libggml-cuda.so.0\n",
            "lrwxrwxrwx 1 root root   21 Jan 14 18:44 build_cuda12_t4/bin/libggml-cuda.so.0 -> libggml-cuda.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 221M Jan 14 18:44 build_cuda12_t4/bin/libggml-cuda.so.0.9.5\n",
            "lrwxrwxrwx 1 root root   12 Jan 14 18:44 build_cuda12_t4/bin/libggml.so -> libggml.so.0\n"
          ]
        }
      ],
      "source": [
        "# Verify binaries were built successfully\n",
        "print(\"=== Built Binaries ===\")\n",
        "!ls -lh build_cuda12_t4/bin/llama-server\n",
        "!ls -lh build_cuda12_t4/bin/llama-cli\n",
        "!ls -lh build_cuda12_t4/bin/llama-quantize\n",
        "!ls -lh build_cuda12_t4/bin/llama-embedding\n",
        "!ls -lh build_cuda12_t4/bin/llama-bench\n",
        "\n",
        "print(\"\\n=== Shared Libraries ===\")\n",
        "!ls -lh build_cuda12_t4/bin/*.so* | head -10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "test-server",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test-server",
        "outputId": "5549ecb8-ade9-4556-ce2c-50f229901fa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libcuda not found, defaulting to standard path.\n",
            "Testing with LD_LIBRARY_PATH: /usr/local/cuda-12.5/compat:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/content/llama.cpp/build_cuda12_t4/bin:/usr/lib/x86_64-linux-gnu\n",
            "\n",
            "✓ llama-server works! \n",
            "Version: \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# 1. Find libcuda.so.1 location safely\n",
        "result = subprocess.run(\n",
        "    ['find', '/usr', '-name', 'libcuda.so.1', '-type', 'f'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.DEVNULL,\n",
        "    text=True\n",
        ")\n",
        "libcuda_path = result.stdout.strip().split('\\n')[0] if result.stdout else ''\n",
        "\n",
        "if libcuda_path:\n",
        "    libcuda_dir = os.path.dirname(libcuda_path)\n",
        "    print(f\"Found libcuda at: {libcuda_dir}\")\n",
        "else:\n",
        "    libcuda_dir = '/usr/lib/x86_64-linux-gnu'\n",
        "    print(\"libcuda not found, defaulting to standard path.\")\n",
        "\n",
        "# 2. Set comprehensive LD_LIBRARY_PATH\n",
        "# Added the compat folder which is often needed for T4/CUDA 12.x in Colab\n",
        "cuda_paths = [\n",
        "    '/usr/local/cuda-12.5/compat',\n",
        "    libcuda_dir,\n",
        "    '/usr/local/cuda/lib64',\n",
        "    '/content/llama.cpp/build_cuda12_t4/bin',\n",
        "    '/usr/lib/x86_64-linux-gnu'\n",
        "]\n",
        "os.environ['LD_LIBRARY_PATH'] = ':'.join(cuda_paths)\n",
        "\n",
        "print(f\"Testing with LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\\n\")\n",
        "\n",
        "# 3. Run the version check\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        ['/content/llama.cpp/build_cuda12_t4/bin/llama-server', '--version'],\n",
        "        env=os.environ,\n",
        "        capture_output=True, # This is fine here because we aren't setting stderr manually\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"✓ llama-server works! \\nVersion: {result.stdout}\")\n",
        "    else:\n",
        "        print(f\"✗ Error: Return code {result.returncode}\")\n",
        "        print(f\"STDERR: {result.stderr}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Get the existing system paths to avoid losing libnvidia-ml.so\n",
        "current_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
        "\n",
        "# 2. Define your new required paths\n",
        "build_paths = [\n",
        "    '/usr/local/cuda-12.5/compat',\n",
        "    '/usr/local/cuda/lib64',\n",
        "    '/content/llama.cpp/build_cuda12_t4/bin',\n",
        "    '/usr/lib64-nvidia',           # Critical: This is where libnvidia-ml.so usually lives\n",
        "    '/usr/local/nvidia/lib64',     # Common secondary location\n",
        "    '/usr/lib/x86_64-linux-gnu'\n",
        "]\n",
        "\n",
        "# 3. Combine them (filtering for paths that actually exist)\n",
        "valid_paths = [p for p in build_paths if os.path.exists(p)]\n",
        "if current_path:\n",
        "    valid_paths.append(current_path)\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] = ':'.join(valid_paths)\n",
        "\n",
        "print(\"✅ LD_LIBRARY_PATH restored and updated.\")\n",
        "print(f\"Final Path: {os.environ['LD_LIBRARY_PATH']}\")\n",
        "\n",
        "# 4. Verify that nvidia-smi is back online\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMycMvfE8qSw",
        "outputId": "fe42f6d3-2985-4272-8e0e-dda2641bbb89"
      },
      "id": "cMycMvfE8qSw",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LD_LIBRARY_PATH restored and updated.\n",
            "Final Path: /usr/local/cuda-12.5/compat:/usr/local/cuda/lib64:/content/llama.cpp/build_cuda12_t4/bin:/usr/lib64-nvidia:/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.5/compat:/usr/local/cuda/lib64:/content/llama.cpp/build_cuda12_t4/bin:/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.5/compat:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/content/llama.cpp/build_cuda12_t4/bin:/usr/lib/x86_64-linux-gnu\n",
            "name, memory.total [MiB]\n",
            "Tesla T4, 15360 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step4",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Package Binaries for Distribution\n",
        "\n",
        "Create the `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz` package with:\n",
        "- llama-server, llama-cli, llama-quantize, llama-embedding, llama-bench\n",
        "- All required shared libraries (.so files)\n",
        "- Proper directory structure for llcuda v2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "create-package",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-package",
        "outputId": "567537bd-7103-43e3-d9a8-7a1d2e02bd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Copying binaries...\n",
            "Copying shared libraries...\n",
            "\n",
            "✓ Package structure created\n"
          ]
        }
      ],
      "source": [
        "# Create package directory structure\n",
        "%cd /content\n",
        "\n",
        "!mkdir -p llcuda_binaries_t4/bin\n",
        "!mkdir -p llcuda_binaries_t4/lib\n",
        "\n",
        "# Copy essential binaries\n",
        "print(\"Copying binaries...\")\n",
        "!cp llama.cpp/build_cuda12_t4/bin/llama-server llcuda_binaries_t4/bin/\n",
        "!cp llama.cpp/build_cuda12_t4/bin/llama-cli llcuda_binaries_t4/bin/\n",
        "!cp llama.cpp/build_cuda12_t4/bin/llama-quantize llcuda_binaries_t4/bin/\n",
        "!cp llama.cpp/build_cuda12_t4/bin/llama-embedding llcuda_binaries_t4/bin/\n",
        "!cp llama.cpp/build_cuda12_t4/bin/llama-bench llcuda_binaries_t4/bin/\n",
        "\n",
        "# Copy all shared libraries\n",
        "print(\"Copying shared libraries...\")\n",
        "!cp llama.cpp/build_cuda12_t4/bin/*.so* llcuda_binaries_t4/lib/\n",
        "\n",
        "print(\"\\n✓ Package structure created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "create-readme",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-readme",
        "outputId": "9f57e349-76c3-4289-ccd4-a59a8b3ec32d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ README.md created\n"
          ]
        }
      ],
      "source": [
        "# Create README for the package\n",
        "readme_content = \"\"\"# llcuda v2.1.0 Binaries for Tesla T4\n",
        "\n",
        "**Built on**: Google Colab\n",
        "**GPU**: Tesla T4 (SM 7.5)\n",
        "**CUDA**: 12.x\n",
        "**Date**: {date}\n",
        "\n",
        "## Contents\n",
        "\n",
        "### bin/\n",
        "- `llama-server` - HTTP inference server\n",
        "- `llama-cli` - Command-line interface\n",
        "- `llama-quantize` - Model quantization tool\n",
        "- `llama-embedding` - Embedding generation\n",
        "- `llama-bench` - Performance benchmarking\n",
        "\n",
        "### lib/\n",
        "- `libggml-cuda.so` - GGML CUDA kernels with FlashAttention\n",
        "- `libllama.so` - llama.cpp library\n",
        "- Other required shared libraries\n",
        "\n",
        "## Installation\n",
        "\n",
        "These binaries are automatically downloaded by llcuda v2.1.0 on first import.\n",
        "\n",
        "For manual installation:\n",
        "```bash\n",
        "# Extract\n",
        "tar -xzf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
        "\n",
        "# Copy to llcuda package (if needed)\n",
        "cp -r bin lib ~/.cache/llcuda/binaries/cuda12/\n",
        "```\n",
        "\n",
        "## Features\n",
        "\n",
        "- ✅ FlashAttention v2 (2-3x faster attention)\n",
        "- ✅ Tensor Core optimization (SM 7.5)\n",
        "- ✅ CUDA Graphs (20-40% latency reduction)\n",
        "- ✅ All quantization formats (NF4, Q4_K_M, Q5_K_M, Q8_0, F16)\n",
        "- ✅ Optimized for Tesla T4 GPUs\n",
        "\n",
        "## Compatibility\n",
        "\n",
        "- **llcuda**: v2.1.0+\n",
        "- **Python**: 3.10+\n",
        "- **CUDA**: 12.x\n",
        "- **GPU**: Tesla T4 (SM 7.5) or higher\n",
        "\n",
        "## Usage with llcuda\n",
        "\n",
        "```python\n",
        "import llcuda\n",
        "\n",
        "# Binaries are automatically loaded\n",
        "engine = llcuda.InferenceEngine()\n",
        "engine.load_model(\"model.gguf\")\n",
        "result = engine.infer(\"Your prompt\")\n",
        "```\n",
        "\n",
        "## Links\n",
        "\n",
        "- **llcuda**: https://github.com/llcuda/llcuda\n",
        "- **Documentation**: https://llcuda.github.io/\n",
        "- **llama.cpp**: https://github.com/ggml-org/llama.cpp\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: Google Colab Tesla T4 | CUDA 12 | llama.cpp\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime\n",
        "readme_content = readme_content.format(date=datetime.now().strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "with open('/content/llcuda_binaries_t4/README.md', 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"✓ README.md created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "create-build-info",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-build-info",
        "outputId": "b4b42a75-e31f-4e29-b745-90b96a8d64b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ BUILD_INFO.txt created\n",
            "\n",
            "Build Information:\n",
            "llcuda v2.1.0 Binary Build Information\n",
            "=========================================\n",
            "\n",
            "Build Date: 2026-01-14 19:26:45 UTC\n",
            "Build Platform: Google Colab\n",
            "GPU: Tesla T4 (SM 7.5)\n",
            "CUDA Version: 12.x\n",
            "Python Version: 3.12.12\n",
            "\n",
            "llama.cpp Details:\n",
            "------------------\n",
            "Repository: https://github.com/ggml-org/llama.cpp\n",
            "Commit: 516a4ca9b5f2fa72c2a71f412929a67cf76a6213\n",
            "\n",
            "Build Configuration:\n",
            "-------------------\n",
            "CMAKE_BUILD_TYPE=Release\n",
            "GGML_CUDA=ON\n",
            "CMAKE_CUDA_ARCHITECTURES=75\n",
            "GGML_CUDA_FA=ON (FlashAttention)\n",
            "GGML_CUDA_FA_ALL_QUANTS=ON\n",
            "GGML_CUDA_GRAPHS=ON\n",
            "BUILD_SHARED_LIBS=ON\n",
            "\n",
            "Features:\n",
            "---------\n",
            "- FlashAttention v2 enabled\n",
            "- CUDA Graphs optimization\n",
            "- Tensor Core utilization\n",
            "- All quantization formats supported\n",
            "- HTTP server mode\n",
            "\n",
            "Compatible with:\n",
            "----------------\n",
            "- llcuda v2.1.0+\n",
            "- Python 3.10+\n",
            "- CUDA 12.x\n",
            "- Tesla T4 or higher (SM 7.5+)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create BUILD_INFO.txt with build details\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "# Get llama.cpp commit hash\n",
        "llamacpp_commit = subprocess.run(\n",
        "    ['git', 'rev-parse', 'HEAD'],\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        "    cwd='/content/llama.cpp'\n",
        ").stdout.strip()\n",
        "\n",
        "build_info = f\"\"\"llcuda v2.1.0 Binary Build Information\n",
        "=========================================\n",
        "\n",
        "Build Date: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S UTC\")}\n",
        "Build Platform: Google Colab\n",
        "GPU: Tesla T4 (SM 7.5)\n",
        "CUDA Version: 12.x\n",
        "Python Version: {sys.version.split()[0]}\n",
        "\n",
        "llama.cpp Details:\n",
        "------------------\n",
        "Repository: https://github.com/ggml-org/llama.cpp\n",
        "Commit: {llamacpp_commit}\n",
        "\n",
        "Build Configuration:\n",
        "-------------------\n",
        "CMAKE_BUILD_TYPE=Release\n",
        "GGML_CUDA=ON\n",
        "CMAKE_CUDA_ARCHITECTURES=75\n",
        "GGML_CUDA_FA=ON (FlashAttention)\n",
        "GGML_CUDA_FA_ALL_QUANTS=ON\n",
        "GGML_CUDA_GRAPHS=ON\n",
        "BUILD_SHARED_LIBS=ON\n",
        "\n",
        "Features:\n",
        "---------\n",
        "- FlashAttention v2 enabled\n",
        "- CUDA Graphs optimization\n",
        "- Tensor Core utilization\n",
        "- All quantization formats supported\n",
        "- HTTP server mode\n",
        "\n",
        "Compatible with:\n",
        "----------------\n",
        "- llcuda v2.1.0+\n",
        "- Python 3.10+\n",
        "- CUDA 12.x\n",
        "- Tesla T4 or higher (SM 7.5+)\n",
        "\"\"\"\n",
        "\n",
        "with open('/content/llcuda_binaries_t4/BUILD_INFO.txt', 'w') as f:\n",
        "    f.write(build_info)\n",
        "\n",
        "print(\"✓ BUILD_INFO.txt created\")\n",
        "print(\"\\nBuild Information:\")\n",
        "print(build_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "show-package-size",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show-package-size",
        "outputId": "1775653d-7d01-4f62-de5d-107468aa89d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Package Contents ===\n",
            "696M\t/content/llcuda_binaries_t4\n",
            "17M\t/content/llcuda_binaries_t4/bin\n",
            "679M\t/content/llcuda_binaries_t4/lib\n",
            "\n",
            "=== Binary Files ===\n",
            "total 17M\n",
            "-rwxr-xr-x 1 root root 581K Jan 14 19:26 llama-bench\n",
            "-rwxr-xr-x 1 root root 5.1M Jan 14 19:26 llama-cli\n",
            "-rwxr-xr-x 1 root root 4.2M Jan 14 19:26 llama-embedding\n",
            "-rwxr-xr-x 1 root root 434K Jan 14 19:26 llama-quantize\n",
            "-rwxr-xr-x 1 root root 6.7M Jan 14 19:26 llama-server\n",
            "\n",
            "=== Library Files ===\n",
            "total 679M\n",
            "-rwxr-xr-x 1 root root 721K Jan 14 19:26 libggml-base.so\n",
            "-rwxr-xr-x 1 root root 721K Jan 14 19:26 libggml-base.so.0\n",
            "-rwxr-xr-x 1 root root 721K Jan 14 19:26 libggml-base.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 949K Jan 14 19:26 libggml-cpu.so\n",
            "-rwxr-xr-x 1 root root 949K Jan 14 19:26 libggml-cpu.so.0\n",
            "-rwxr-xr-x 1 root root 949K Jan 14 19:26 libggml-cpu.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 221M Jan 14 19:26 libggml-cuda.so\n",
            "-rwxr-xr-x 1 root root 221M Jan 14 19:26 libggml-cuda.so.0\n",
            "-rwxr-xr-x 1 root root 221M Jan 14 19:26 libggml-cuda.so.0.9.5\n",
            "-rwxr-xr-x 1 root root  54K Jan 14 19:26 libggml.so\n",
            "-rwxr-xr-x 1 root root  54K Jan 14 19:26 libggml.so.0\n",
            "-rwxr-xr-x 1 root root  54K Jan 14 19:26 libggml.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 14 19:26 libllama.so\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 14 19:26 libllama.so.0\n"
          ]
        }
      ],
      "source": [
        "# Show package contents and sizes\n",
        "print(\"=== Package Contents ===\")\n",
        "!du -sh /content/llcuda_binaries_t4\n",
        "!du -sh /content/llcuda_binaries_t4/bin\n",
        "!du -sh /content/llcuda_binaries_t4/lib\n",
        "\n",
        "print(\"\\n=== Binary Files ===\")\n",
        "!ls -lh /content/llcuda_binaries_t4/bin/\n",
        "\n",
        "print(\"\\n=== Library Files ===\")\n",
        "!ls -lh /content/llcuda_binaries_t4/lib/ | head -15"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step5",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Create tar.gz Archive\n",
        "\n",
        "Create the final `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz` archive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "create-tarball",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-tarball",
        "outputId": "3f96dfd2-83ed-410a-d635-8dc0cc658ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Creating tar.gz archive...\n",
            "\n",
            "✓ Archive created successfully!\n",
            "\n",
            "=== Final Package ===\n",
            "-rw-r--r-- 1 root root 267M Jan 14 19:27 llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
            "267M\tllcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Create the tar.gz archive\n",
        "%cd /content\n",
        "\n",
        "# Rename to match expected structure\n",
        "!mv llcuda_binaries_t4 package_t4\n",
        "\n",
        "print(\"Creating tar.gz archive...\")\n",
        "!tar -czf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz package_t4/\n",
        "\n",
        "print(\"\\n✓ Archive created successfully!\")\n",
        "print(\"\\n=== Final Package ===\")\n",
        "!ls -lh llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
        "!du -h llcuda-binaries-cuda12-t4-v2.1.0.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "create-checksum",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-checksum",
        "outputId": "de43540d-4704-46b8-c763-e967b3e0ade4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SHA256 checksum created\n",
            "\n",
            "Checksum:\n",
            "1bc2dd6d837f3b2ffcc5aee90ad65829aeba63dd5f01505adb2437cd417bf5db  llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n"
          ]
        }
      ],
      "source": [
        "# Create SHA256 checksum\n",
        "!sha256sum llcuda-binaries-cuda12-t4-v2.1.0.tar.gz > llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256\n",
        "\n",
        "print(\"✓ SHA256 checksum created\")\n",
        "print(\"\\nChecksum:\")\n",
        "!cat llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "verify-archive",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "verify-archive",
        "outputId": "b7a62b59-3e06-4412-f01f-342546f5e426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Archive Contents ===\n",
            "package_t4/\n",
            "package_t4/README.md\n",
            "package_t4/lib/\n",
            "package_t4/lib/libggml-base.so.0\n",
            "package_t4/lib/libggml-cpu.so.0.9.5\n",
            "package_t4/lib/libllama.so.0\n",
            "package_t4/lib/libggml.so.0.9.5\n",
            "package_t4/lib/libggml-base.so\n",
            "package_t4/lib/libmtmd.so.0.0.7736\n",
            "package_t4/lib/libggml.so.0\n",
            "package_t4/lib/libggml-cpu.so.0\n",
            "package_t4/lib/libllama.so\n",
            "package_t4/lib/libmtmd.so\n",
            "package_t4/lib/libggml.so\n",
            "package_t4/lib/libllama.so.0.0.7736\n",
            "package_t4/lib/libggml-base.so.0.9.5\n",
            "package_t4/lib/libggml-cpu.so\n",
            "package_t4/lib/libggml-cuda.so.0\n",
            "package_t4/lib/libggml-cuda.so\n",
            "package_t4/lib/libmtmd.so.0\n",
            "package_t4/lib/libggml-cuda.so.0.9.5\n",
            "package_t4/bin/\n",
            "package_t4/bin/llama-bench\n",
            "package_t4/bin/llama-server\n",
            "package_t4/bin/llama-cli\n",
            "package_t4/bin/llama-embedding\n",
            "package_t4/bin/llama-quantize\n",
            "package_t4/BUILD_INFO.txt\n"
          ]
        }
      ],
      "source": [
        "# Verify archive contents\n",
        "print(\"=== Archive Contents ===\")\n",
        "!tar -tzf llcuda-binaries-cuda12-t4-v2.1.0.tar.gz | head -30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step6",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Download Files\n",
        "\n",
        "Download the binary package and checksum to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "download-files",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "download-files",
        "outputId": "b53c7bca-f10a-4d6f-8cbf-56e6f1519842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading llcuda-binaries-cuda12-t4-v2.1.0.tar.gz (266 MB)...\n",
            "This may take a few minutes...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8fd4c132-3ad3-4e21-b599-6ea80b49cd6f\", \"llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\", 279592599)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading checksum file...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_43067618-deb9-4161-be47-2ed3efe5c92c\", \"llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256\", 106)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ All files downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Download files\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading llcuda-binaries-cuda12-t4-v2.1.0.tar.gz (266 MB)...\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "files.download('/content/llcuda-binaries-cuda12-t4-v2.1.0.tar.gz')\n",
        "\n",
        "print(\"\\nDownloading checksum file...\")\n",
        "files.download('/content/llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256')\n",
        "\n",
        "print(\"\\n✓ All files downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## 🎉 Build Complete!\n",
        "\n",
        "### Created Files:\n",
        "\n",
        "1. **llcuda-binaries-cuda12-t4-v2.1.0.tar.gz** (~266 MB)\n",
        "   - llama.cpp binaries with FlashAttention\n",
        "   - All required shared libraries\n",
        "   - README and build information\n",
        "\n",
        "2. **llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256**\n",
        "   - SHA256 checksum for verification\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Upload to GitHub Releases**:\n",
        "   ```bash\n",
        "   gh release create v2.1.0 \\\n",
        "       --repo llcuda/llcuda \\\n",
        "       --title \"llcuda v2.1.0 - Tesla T4 Release\" \\\n",
        "       --notes \"Complete CUDA 12 binaries with FlashAttention for Tesla T4\" \\\n",
        "       llcuda-binaries-cuda12-t4-v2.1.0.tar.gz \\\n",
        "       llcuda-binaries-cuda12-t4-v2.1.0.tar.gz.sha256\n",
        "   ```\n",
        "\n",
        "2. **Test Installation**:\n",
        "   ```python\n",
        "   import llcuda\n",
        "   print(llcuda.__version__)  # Should show 2.1.0\n",
        "   ```\n",
        "\n",
        "3. **Update bootstrap.py** to download from v2.1.0 release\n",
        "\n",
        "### Package Features:\n",
        "\n",
        "- ✅ FlashAttention v2 (2-3x faster)\n",
        "- ✅ CUDA Graphs (20-40% latency reduction)\n",
        "- ✅ Tensor Core optimization\n",
        "- ✅ All quantization formats\n",
        "- ✅ Optimized for Tesla T4 (SM 7.5)\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: Google Colab Tesla T4 | CUDA 12 | Python 3.10+  \n",
        "**For**: llcuda v2.1.0 with Unsloth Integration  \n",
        "**Repository**: https://github.com/llcuda/llcuda  \n",
        "**Documentation**: https://llcuda.github.io/"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ugoZ43H_MTJ"
      },
      "id": "1ugoZ43H_MTJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}