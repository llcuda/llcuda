{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b072b47",
   "metadata": {},
   "source": [
    "## Step 1: Verify Dual GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç SPLIT-GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPUs\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpus = result.stdout.strip().split('\\n')\n",
    "print(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   {gpu}\")\n",
    "\n",
    "if len(gpus) >= 2:\n",
    "    print(\"\\n‚úÖ Dual T4 ready for split-GPU operation!\")\n",
    "    print(\"   GPU 0 ‚Üí llama-server (LLM)\")\n",
    "    print(\"   GPU 1 ‚Üí RAPIDS/Graphistry\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Need 2 GPUs for split operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cdff2",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8354f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
    "\n",
    "# Install cuGraph (matching Kaggle RAPIDS 25.6.0)\n",
    "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n",
    "\n",
    "# Install Graphistry\n",
    "!pip install -q graphistry\n",
    "\n",
    "# Verify installations\n",
    "import llcuda\n",
    "print(f\"\\n‚úÖ llcuda {llcuda.__version__} installed\")\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "    import cugraph\n",
    "    print(f\"‚úÖ cuDF {cudf.__version__}\")\n",
    "    print(f\"‚úÖ cuGraph {cugraph.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è RAPIDS: {e}\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"‚úÖ Graphistry {graphistry.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Graphistry: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98692cf5",
   "metadata": {},
   "source": [
    "## Step 3: Download GGUF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eabed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Download a model that fits on single GPU (leaving GPU 1 free)\n",
    "MODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\n",
    "MODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"üì• Downloading {MODEL_FILE}...\")\n",
    "print(f\"   This will run on GPU 0 only.\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    filename=MODEL_FILE,\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "\n",
    "size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "print(f\"\\n‚úÖ Model downloaded: {model_path}\")\n",
    "print(f\"   Size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12de7fb",
   "metadata": {},
   "source": [
    "## Step 4: Start llama-server on GPU 0 Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e958374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda.server import ServerManager\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration for GPU 0 ONLY (leave GPU 1 for RAPIDS)\n",
    "print(\"\\nüìã Configuration:\")\n",
    "print(\"   GPU 0: 100% (llama-server)\")\n",
    "print(\"   GPU 1: 0% (reserved for RAPIDS)\")\n",
    "\n",
    "server = ServerManager()\n",
    "server.start_server(\n",
    "    model_path=model_path,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8080,\n",
    "    \n",
    "    # GPU 0 only configuration\n",
    "    gpu_layers=99,\n",
    "    tensor_split=\"1.0,0.0\",  # 100% on GPU 0, 0% on GPU 1\n",
    "    \n",
    "    # Optimize for single GPU\n",
    "    ctx_size=4096,\n",
    "    flash_attention=True,\n",
    ")\n",
    "\n",
    "if server.check_server_health():\n",
    "    print(\"\\n‚úÖ llama-server running on GPU 0!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280be76",
   "metadata": {},
   "source": [
    "## Step 5: Verify GPU Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä GPU MEMORY SPLIT VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.free --format=csv\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,memory.free\", \"--format=csv,noheader,nounits\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "lines = result.stdout.strip().split('\\n')\n",
    "if len(lines) >= 2:\n",
    "    gpu1_free = int(lines[1].split(',')[1].strip())\n",
    "    print(f\"\\n‚úÖ GPU 1 has {gpu1_free} MiB free for RAPIDS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed80c1",
   "metadata": {},
   "source": [
    "## Step 6: Initialize RAPIDS on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccee815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Force RAPIDS to use GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî• INITIALIZING RAPIDS ON GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# Verify we're on the right GPU\n",
    "print(f\"\\nüìä RAPIDS GPU Info:\")\n",
    "device = cp.cuda.Device(0)  # Device 0 in filtered view = actual GPU 1\n",
    "print(f\"   Device: {device.id} (filtered view)\")\n",
    "print(f\"   Actual GPU: 1 (Tesla T4)\")\n",
    "\n",
    "# Test cuDF on GPU 1\n",
    "test_df = cudf.DataFrame({\n",
    "    'source': [0, 1, 2, 3, 4],\n",
    "    'target': [1, 2, 3, 4, 0],\n",
    "    'weight': [1.0, 2.0, 1.5, 0.5, 3.0]\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ cuDF working on GPU 1\")\n",
    "print(f\"   Test DataFrame: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d2aee",
   "metadata": {},
   "source": [
    "## Step 7: Create Sample Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069395a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä CREATING SAMPLE GRAPH ON GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a sample social network graph\n",
    "edges = cudf.DataFrame({\n",
    "    'source': [0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9],\n",
    "    'target': [1, 2, 3, 2, 4, 3, 5, 4, 6, 5, 6, 7, 7, 8, 9, 0],\n",
    "})\n",
    "\n",
    "# Node labels\n",
    "node_names = {\n",
    "    0: \"Alice\", 1: \"Bob\", 2: \"Charlie\", 3: \"Diana\",\n",
    "    4: \"Eve\", 5: \"Frank\", 6: \"Grace\", 7: \"Henry\",\n",
    "    8: \"Ivy\", 9: \"Jack\"\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Graph created:\")\n",
    "print(f\"   Nodes: {len(node_names)}\")\n",
    "print(f\"   Edges: {len(edges)}\")\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source='source', destination='target')\n",
    "\n",
    "print(f\"\\n‚úÖ cuGraph graph created on GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562dcfd",
   "metadata": {},
   "source": [
    "## Step 8: Run Graph Analytics on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df643fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî¨ GPU-ACCELERATED GRAPH ANALYTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# PageRank\n",
    "print(\"\\nüìä PageRank Analysis:\")\n",
    "pagerank = cugraph.pagerank(G)\n",
    "pagerank = pagerank.sort_values('pagerank', ascending=False)\n",
    "\n",
    "for _, row in pagerank.to_pandas().head(5).iterrows():\n",
    "    node_id = int(row['vertex'])\n",
    "    score = row['pagerank']\n",
    "    name = node_names.get(node_id, f\"Node {node_id}\")\n",
    "    print(f\"   {name}: {score:.4f}\")\n",
    "\n",
    "# Betweenness Centrality\n",
    "print(\"\\nüìä Betweenness Centrality:\")\n",
    "bc = cugraph.betweenness_centrality(G)\n",
    "bc = bc.sort_values('betweenness_centrality', ascending=False)\n",
    "\n",
    "for _, row in bc.to_pandas().head(5).iterrows():\n",
    "    node_id = int(row['vertex'])\n",
    "    score = row['betweenness_centrality']\n",
    "    name = node_names.get(node_id, f\"Node {node_id}\")\n",
    "    print(f\"   {name}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Graph analytics computed on GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddaf3d2",
   "metadata": {},
   "source": [
    "## Step 9: Use LLM to Analyze Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e664414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset CUDA_VISIBLE_DEVICES to access llama-server\n",
    "del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "\n",
    "from llcuda.api.client import LlamaCppClient\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ü§ñ LLM ANALYSIS OF GRAPH RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get top PageRank nodes\n",
    "top_nodes = pagerank.to_pandas().head(3)\n",
    "top_names = [node_names[int(row['vertex'])] for _, row in top_nodes.iterrows()]\n",
    "\n",
    "# Create prompt for LLM\n",
    "prompt = f\"\"\"I have a social network graph with 10 people. \n",
    "The PageRank analysis shows the most influential people are: {', '.join(top_names)}.\n",
    "The betweenness centrality shows who are the key connectors in the network.\n",
    "\n",
    "Based on this, what insights can you provide about the network structure? \n",
    "Keep your response to 3-4 sentences.\"\"\"\n",
    "\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nüìù LLM Analysis (GPU 0):\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n‚úÖ Simultaneous GPU operation:\")\n",
    "print(\"   GPU 0: LLM inference\")\n",
    "print(\"   GPU 1: Graph analytics (previously computed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe99d8b",
   "metadata": {},
   "source": [
    "## Step 10: Graphistry Visualization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb89d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphistry\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä GRAPHISTRY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to pandas for Graphistry (works in-notebook)\n",
    "edges_pd = edges.to_pandas()\n",
    "edges_pd['source_name'] = edges_pd['source'].map(node_names)\n",
    "edges_pd['target_name'] = edges_pd['target'].map(node_names)\n",
    "\n",
    "# Create nodes DataFrame with metrics\n",
    "pagerank_pd = pagerank.to_pandas()\n",
    "bc_pd = bc.to_pandas()\n",
    "\n",
    "nodes_pd = pd.DataFrame({\n",
    "    'node_id': list(node_names.keys()),\n",
    "    'name': list(node_names.values())\n",
    "})\n",
    "nodes_pd = nodes_pd.merge(\n",
    "    pagerank_pd.rename(columns={'vertex': 'node_id'}),\n",
    "    on='node_id'\n",
    ")\n",
    "nodes_pd = nodes_pd.merge(\n",
    "    bc_pd.rename(columns={'vertex': 'node_id'}),\n",
    "    on='node_id'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Prepared for visualization:\")\n",
    "print(f\"   Nodes: {len(nodes_pd)}\")\n",
    "print(f\"   Edges: {len(edges_pd)}\")\n",
    "\n",
    "# Note: Graphistry requires registration for full visualization\n",
    "# For demo purposes, we'll show the prepared data\n",
    "print(f\"\\nüìã Node Metrics:\")\n",
    "print(nodes_pd[['name', 'pagerank', 'betweenness_centrality']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b480f",
   "metadata": {},
   "source": [
    "## Step 11: Interactive LLM + Graph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f501060",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîÑ INTERACTIVE LLM + GRAPH WORKFLOW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_node(node_name):\n",
    "    \"\"\"Use LLM to analyze a specific node's network position.\"\"\"\n",
    "    node_data = nodes_pd[nodes_pd['name'] == node_name].iloc[0]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the network position of {node_name}:\n",
    "    - PageRank score: {node_data['pagerank']:.4f} (higher = more influential)\n",
    "    - Betweenness centrality: {node_data['betweenness_centrality']:.4f} (higher = more connections)\n",
    "    \n",
    "    What does this tell us about {node_name}'s role in the network? Answer in 2 sentences.\"\"\"\n",
    "    \n",
    "    response = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Analyze top 3 nodes\n",
    "print(\"\\nüîç Node Analysis:\")\n",
    "for name in ['Alice', 'Charlie', 'Frank']:\n",
    "    print(f\"\\nüìå {name}:\")\n",
    "    analysis = analyze_node(name)\n",
    "    print(f\"   {analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abed4d5",
   "metadata": {},
   "source": [
    "## Step 12: Monitor Both GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912281d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä DUAL GPU MONITORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nüí° Split-GPU Operation:\")\n",
    "print(\"   GPU 0: llama-server (GGUF model loaded)\")\n",
    "print(\"   GPU 1: RAPIDS memory (cuDF/cuGraph data structures)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10008ddf",
   "metadata": {},
   "source": [
    "## Step 13: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e681ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõë Stopping llama-server...\")\n",
    "server.stop_server()\n",
    "\n",
    "# Clear RAPIDS memory\n",
    "import gc\n",
    "del G, edges, pagerank, bc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ Resources cleaned up\")\n",
    "print(\"\\nüìä Final GPU Status:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533e7be",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "### Split-GPU Architecture:\n",
    "- **GPU 0**: llama-server with `tensor_split=[1.0, 0.0]`\n",
    "- **GPU 1**: RAPIDS/cuGraph via `CUDA_VISIBLE_DEVICES=\"1\"`\n",
    "\n",
    "### Key Integration Points:\n",
    "1. ‚úÖ LLM for natural language analysis\n",
    "2. ‚úÖ cuGraph for GPU-accelerated graph algorithms\n",
    "3. ‚úÖ Graphistry for visualization\n",
    "4. ‚úÖ Combined insights from both\n",
    "\n",
    "### Code Pattern:\n",
    "```python\n",
    "# GPU 0: llama-server\n",
    "config = ServerConfig(tensor_split=[1.0, 0.0], main_gpu=0)\n",
    "\n",
    "# GPU 1: RAPIDS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import cudf, cugraph  # Uses GPU 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [07-openai-api-client](07-openai-api-client-llcuda-v2.2.0.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
