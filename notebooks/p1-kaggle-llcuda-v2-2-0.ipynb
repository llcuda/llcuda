{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a36ca5",
   "metadata": {},
   "source": [
    "# llcuda v2.2.0 - Kaggle 2√ó T4 Build Notebook\n",
    "\n",
    "## Architecture: Split-GPU Workload\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         GPU 0             ‚îÇ            GPU 1              ‚îÇ\n",
    "‚îÇ  llama-server (GGUF)      ‚îÇ  RAPIDS + Graphistry          ‚îÇ\n",
    "‚îÇ  LLM Inference            ‚îÇ  Graph Visualization (cuGraph)‚îÇ\n",
    "‚îÇ  15GB VRAM                ‚îÇ  15GB VRAM                    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "This notebook builds llcuda binaries for **split-GPU** operation:\n",
    "- **GPU 0**: llama-server with GGUF model (LLM inference)\n",
    "- **GPU 1**: RAPIDS/Graphistry with cuDF/cuGraph (graph simulation)\n",
    "\n",
    "## Step 1: Verify Kaggle GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b107f3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:38:25.404651Z",
     "iopub.status.busy": "2026-01-16T18:38:25.404339Z",
     "iopub.status.idle": "2026-01-16T18:38:25.812304Z",
     "shell.execute_reply": "2026-01-16T18:38:25.811466Z",
     "shell.execute_reply.started": "2026-01-16T18:38:25.404624Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KAGGLE GPU ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "\n",
      "üìä Detected GPUs: 2\n",
      "   GPU 0: Tesla T4 (UUID: GPU-4e77617b-de03-107e-97b0-19bc9314fdbe)\n",
      "   GPU 1: Tesla T4 (UUID: GPU-6947ea1a-269e-4bbc-2789-42017eb1cc64)\n",
      "\n",
      "üìä CUDA Version:\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "\n",
      "üìä VRAM Summary:\n",
      "index, name, memory.total [MiB]\n",
      "0, Tesla T4, 15360 MiB\n",
      "1, Tesla T4, 15360 MiB\n",
      "\n",
      "‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\n"
     ]
    }
   ],
   "source": [
    "# Verify we have 2√ó T4 GPUs\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check nvidia-smi\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "gpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\n",
    "print(f\"\\nüìä Detected GPUs: {len(gpu_lines)}\")\n",
    "for line in gpu_lines:\n",
    "    print(f\"   {line}\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(\"\\nüìä CUDA Version:\")\n",
    "!nvcc --version | grep release\n",
    "\n",
    "# Check total VRAM\n",
    "print(\"\\nüìä VRAM Summary:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# Verify we have 2 GPUs\n",
    "if len(gpu_lines) >= 2:\n",
    "    print(\"\\n‚úÖ Multi-GPU environment confirmed! Ready for dual-T4 build.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Less than 2 GPUs detected!\")\n",
    "    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334985f0",
   "metadata": {},
   "source": [
    "## Step 2: Verify/Install Build Dependencies\n",
    "\n",
    "**Note:** Kaggle 2√ó T4 comes with cmake 3.31.6 and ninja 1.13.0 pre-installed.\n",
    "We only install what's missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3ce1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:38:28.383792Z",
     "iopub.status.busy": "2026-01-16T18:38:28.383000Z",
     "iopub.status.idle": "2026-01-16T18:38:54.450942Z",
     "shell.execute_reply": "2026-01-16T18:38:54.450046Z",
     "shell.execute_reply.started": "2026-01-16T18:38:28.383757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing build dependencies...\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Selecting previously unselected package libhiredis0.14:amd64.\n",
      "(Reading database ... 129073 files and directories currently installed.)\n",
      "Preparing to unpack .../libhiredis0.14_0.14.1-2_amd64.deb ...\n",
      "Unpacking libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Selecting previously unselected package ccache.\n",
      "Preparing to unpack .../ccache_4.5.1-1_amd64.deb ...\n",
      "Unpacking ccache (4.5.1-1) ...\n",
      "Selecting previously unselected package ninja-build.\n",
      "Preparing to unpack .../ninja-build_1.10.1-1_amd64.deb ...\n",
      "Unpacking ninja-build (1.10.1-1) ...\n",
      "Setting up ninja-build (1.10.1-1) ...\n",
      "Setting up libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Setting up ccache (4.5.1-1) ...\n",
      "Updating symlinks in /usr/lib/ccache ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\n",
      "‚úÖ Build dependencies installed\n",
      "cmake version 3.31.6\n",
      "1.13.0.git.kitware.jobserver-pipe-1\n",
      "CPU times: user 333 ms, sys: 91.5 ms, total: 424 ms\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check pre-installed build tools (Kaggle 2√ó T4 has cmake/ninja)\n",
    "import subprocess\n",
    "\n",
    "print(\"Checking build dependencies...\")\n",
    "\n",
    "# Check CMake\n",
    "cmake_result = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\n",
    "if cmake_result.returncode == 0:\n",
    "    cmake_ver = cmake_result.stdout.split(\"\\n\")[0]\n",
    "    print(f\"‚úÖ {cmake_ver}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CMake not found, installing...\")\n",
    "    !apt-get update -qq && apt-get install -y -qq cmake\n",
    "\n",
    "# Check Ninja  \n",
    "ninja_result = subprocess.run([\"ninja\", \"--version\"], capture_output=True, text=True)\n",
    "if ninja_result.returncode == 0:\n",
    "    print(f\"‚úÖ Ninja {ninja_result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Ninja not found, installing...\")\n",
    "    !apt-get install -y -qq ninja-build\n",
    "\n",
    "# Check ccache (optional but speeds up rebuilds)\n",
    "ccache_result = subprocess.run([\"which\", \"ccache\"], capture_output=True, text=True)\n",
    "if ccache_result.returncode != 0:\n",
    "    print(\"üì¶ Installing ccache...\")\n",
    "    !apt-get install -y -qq ccache\n",
    "\n",
    "# Install Python dependencies (minimal - most are pre-installed on Kaggle)\n",
    "print(\"\\nüì¶ Checking Python packages...\")\n",
    "required_py = [\"huggingface_hub\", \"sseclient-py\"]\n",
    "for pkg in required_py:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"   ‚úÖ {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"   üì¶ Installing {pkg}...\")\n",
    "        !pip install -q {pkg}\n",
    "\n",
    "print(\"\\n‚úÖ Build dependencies ready\")\n",
    "!cmake --version | head -1\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eee12f",
   "metadata": {},
   "source": [
    "## Step 2b: Verify RAPIDS + Graphistry (GPU 1 Workload)\n",
    "\n",
    "**Note:** Kaggle 2√ó T4 environments come with RAPIDS pre-installed:\n",
    "- cudf-cu12 25.6.0\n",
    "- cuml-cu12 25.6.0  \n",
    "- cugraph (via libcugraph-cu12)\n",
    "- pylibraft-cu12 25.6.0\n",
    "- raft-dask-cu12 25.6.0\n",
    "\n",
    "We only install what's missing (graphistry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097adecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:39:02.857182Z",
     "iopub.status.busy": "2026-01-16T18:39:02.856398Z",
     "iopub.status.idle": "2026-01-16T18:39:35.747452Z",
     "shell.execute_reply": "2026-01-16T18:39:35.746728Z",
     "shell.execute_reply.started": "2026-01-16T18:39:02.857145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "INSTALLING RAPIDS + GRAPHISTRY FOR GPU 1\n",
      "======================================================================\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m202.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m169.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "üì¶ Installed packages:\n",
      "   cuDF version: 25.06.00\n",
      "   cuGraph version: 25.06.00\n",
      "   Graphistry version: 0.50.4\n",
      "\n",
      "‚úÖ RAPIDS + Graphistry installed for GPU 1\n",
      "CPU times: user 6.29 s, sys: 1.03 s, total: 7.33 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Verify RAPIDS is pre-installed on Kaggle 2√ó T4\n",
    "# Only install missing packages (graphistry)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VERIFYING RAPIDS + INSTALLING GRAPHISTRY FOR GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check pre-installed RAPIDS packages\n",
    "import subprocess\n",
    "rapids_packages = [\"cudf-cu12\", \"cuml-cu12\", \"pylibraft-cu12\", \"raft-dask-cu12\"]\n",
    "print(\"\\nüì¶ Checking pre-installed RAPIDS packages:\")\n",
    "for pkg in rapids_packages:\n",
    "    result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n",
    "    if \"Version:\" in result.stdout:\n",
    "        version = [l for l in result.stdout.split(\"\\n\") if l.startswith(\"Version:\")][0]\n",
    "        print(f\"   ‚úÖ {pkg}: {version.split(': ')[1]}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {pkg}: NOT FOUND - installing...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"-q\", \"--extra-index-url=https://pypi.nvidia.com\", pkg])\n",
    "\n",
    "# Check for cugraph (may be named differently)\n",
    "print(\"\\nüì¶ Checking cuGraph:\")\n",
    "try:\n",
    "    import cugraph\n",
    "    print(f\"   ‚úÖ cugraph: {cugraph.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"   ‚ö†Ô∏è  cugraph not directly available, checking pylibcugraph...\")\n",
    "    try:\n",
    "        import pylibcugraph\n",
    "        print(f\"   ‚úÖ pylibcugraph available\")\n",
    "        # Install cugraph Python bindings if needed\n",
    "        !pip install -q --extra-index-url=https://pypi.nvidia.com cugraph-cu12\n",
    "        import cugraph\n",
    "        print(f\"   ‚úÖ cugraph installed: {cugraph.__version__}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "\n",
    "# Install Graphistry (not pre-installed) - minimal version to avoid conflicts\n",
    "print(\"\\nüì¶ Installing Graphistry (minimal):\")\n",
    "!pip install -q graphistry\n",
    "\n",
    "# Verify all imports work\n",
    "print(\"\\nüì¶ Final verification:\")\n",
    "import cudf\n",
    "print(f\"   cuDF version: {cudf.__version__}\")\n",
    "\n",
    "import cugraph\n",
    "print(f\"   cuGraph version: {cugraph.__version__}\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"   Graphistry version: {graphistry.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Graphistry: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ RAPIDS + Graphistry ready for GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69ed34",
   "metadata": {},
   "source": [
    "## Step 3: Clone llama.cpp (Latest Stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fa6016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:39:35.749269Z",
     "iopub.status.busy": "2026-01-16T18:39:35.748635Z",
     "iopub.status.idle": "2026-01-16T18:39:40.316119Z",
     "shell.execute_reply": "2026-01-16T18:39:40.315251Z",
     "shell.execute_reply.started": "2026-01-16T18:39:35.749244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning llama.cpp...\n",
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 2395, done.\u001b[K\n",
      "remote: Counting objects: 100% (2395/2395), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1873/1873), done.\u001b[K\n",
      "remote: Total 2395 (delta 519), reused 1570 (delta 450), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (2395/2395), 27.25 MiB | 17.91 MiB/s, done.\n",
      "Resolving deltas: 100% (519/519), done.\n",
      "\n",
      "üì¶ llama.cpp Version:\n",
      "\u001b[33m388ce82\u001b[m\u001b[33m (\u001b[m\u001b[1;34mgrafted\u001b[m\u001b[33m, \u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m ggml : extend ggml_pool_1d + metal (#16429)\n",
      "388ce82\n",
      "CPU times: user 80.7 ms, sys: 66.8 ms, total: 148 ms\n",
      "Wall time: 4.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Clean any previous build\n",
    "!rm -rf llama.cpp\n",
    "\n",
    "# Clone llama.cpp\n",
    "print(\"Cloning llama.cpp...\")\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
    "\n",
    "os.chdir(\"llama.cpp\")\n",
    "\n",
    "# Get commit info\n",
    "print(\"\\nüì¶ llama.cpp Version:\")\n",
    "!git log -1 --oneline\n",
    "!git describe --tags --always 2>/dev/null || echo \"(no tag)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa16d20",
   "metadata": {},
   "source": [
    "## Step 4: Configure CMake for Dual T4 (SM 7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf67503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:39:53.005197Z",
     "iopub.status.busy": "2026-01-16T18:39:53.004764Z",
     "iopub.status.idle": "2026-01-16T18:39:59.964145Z",
     "shell.execute_reply": "2026-01-16T18:39:59.963309Z",
     "shell.execute_reply.started": "2026-01-16T18:39:53.005161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CMake for Kaggle 2√ó Tesla T4...\n",
      "\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/gcc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n",
      "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
      "-- CUDA Toolkit found\n",
      "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n",
      "-- CUDA host compiler is GNU 11.4.0\n",
      "-- Including CUDA backend\n",
      "-- ggml version: 0.9.5\n",
      "-- ggml commit:  388ce82\n",
      "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
      "-- OpenSSL found: 3.0.2\n",
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done (6.3s)\n",
      "\u001b[31mCMake Error at ggml/src/ggml-cuda/CMakeLists.txt:182 (target_link_libraries):\n",
      "  Target \"ggml-cuda\" links to:\n",
      "\n",
      "    CUDA::cuda_driver\n",
      "\n",
      "  but the target was not found.  Possible reasons include:\n",
      "\n",
      "    * There is a typo in the target name.\n",
      "    * A find_package call is missing for an IMPORTED target.\n",
      "    * An ALIAS target is missing.\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "-- Generating done (0.2s)\n",
      "\u001b[0mCMake Generate step failed.  Build files cannot be regenerated correctly.\u001b[0m\n",
      "\n",
      "‚úÖ CMake configuration complete!\n",
      "   Target: SM 7.5 (Tesla T4)\n",
      "   FlashAttention: All quantization types\n",
      "   Static linking: Enabled\n",
      "CPU times: user 103 ms, sys: 47.3 ms, total: 150 ms\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Clean previous build\n",
    "!rm -rf build\n",
    "\n",
    "print(\"Configuring CMake for Kaggle 2√ó Tesla T4...\")\n",
    "print(\"\")\n",
    "\n",
    "# CMake configuration for dual T4\n",
    "# Key flags:\n",
    "# - GGML_CUDA=ON: Enable CUDA backend\n",
    "# - CMAKE_CUDA_ARCHITECTURES=75: Tesla T4 (Turing, SM 7.5)\n",
    "# - GGML_CUDA_FA_ALL_QUANTS=ON: FlashAttention for ALL quantization types\n",
    "# - BUILD_SHARED_LIBS=OFF: Static linking for portability\n",
    "# - GGML_NATIVE=OFF: Don't optimize for build machine (for portability)\n",
    "\n",
    "cmake_cmd = \"\"\"\n",
    "cmake -B build -G Ninja \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DBUILD_SHARED_LIBS=OFF \\\n",
    "    -DLLAMA_BUILD_EXAMPLES=ON \\\n",
    "    -DLLAMA_BUILD_TESTS=OFF \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DCMAKE_C_COMPILER=gcc \\\n",
    "    -DCMAKE_CXX_COMPILER=g++\n",
    "\"\"\"\n",
    "\n",
    "!{cmake_cmd}\n",
    "\n",
    "print(\"\\n‚úÖ CMake configuration complete!\")\n",
    "print(\"   Target: SM 7.5 (Tesla T4)\")\n",
    "print(\"   FlashAttention: All quantization types\")\n",
    "print(\"   Static linking: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267abd9c",
   "metadata": {},
   "source": [
    "## Step 5: Build llama.cpp (This takes ~8-12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Get CPU count for parallel build\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Building with {cpu_count} parallel jobs...\")\n",
    "print(\"This will take approximately 8-12 minutes.\\n\")\n",
    "\n",
    "# Build\n",
    "build_result = os.system(f\"cmake --build build --config Release -j{cpu_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify build succeeded\n",
    "if build_result == 0 and os.path.exists(\"build/bin/llama-server\"):\n",
    "    print(\"‚úÖ BUILD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    !ls -lh build/bin/llama-server\n",
    "else:\n",
    "    print(\"‚ùå BUILD FAILED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Check the build output above for errors.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dcf23",
   "metadata": {},
   "source": [
    "## Step 6: Verify Built Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d961d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Built binaries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh llama-* 2>/dev/null | head -20\n",
    "\n",
    "print(\"\\nKey binary sizes:\")\n",
    "!du -h llama-server llama-cli llama-quantize 2>/dev/null\n",
    "\n",
    "print(\"\\nChecking CUDA support in llama-server:\")\n",
    "!./llama-server --help 2>&1 | grep -i \"cuda\\|gpu\\|ngl\" | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6abd2f",
   "metadata": {},
   "source": [
    "## Step 7: Test Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Testing multi-GPU CLI flags:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for multi-GPU flags\n",
    "print(\"\\nüìå --tensor-split (VRAM distribution):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"tensor-split\"\n",
    "\n",
    "print(\"\\nüìå --split-mode (layer/row splitting):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"split-mode\"\n",
    "\n",
    "print(\"\\nüìå --main-gpu (primary GPU selection):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"main-gpu\"\n",
    "\n",
    "print(\"\\n‚úÖ Multi-GPU support confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0b99",
   "metadata": {},
   "source": [
    "## Step 8: Create llcuda v2.2.0 Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb728ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Package info\n",
    "VERSION = \"2.2.0\"\n",
    "BUILD_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "PACKAGE_NAME = f\"llcuda-v{VERSION}-cuda12-kaggle-t4x2\"\n",
    "PACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n",
    "\n",
    "print(f\"Creating package: {PACKAGE_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/include\", exist_ok=True)\n",
    "\n",
    "# Binaries to include\n",
    "BUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\n",
    "binaries = [\n",
    "    # Core server\n",
    "    \"llama-server\",\n",
    "    \"llama-cli\",\n",
    "    # Quantization & conversion\n",
    "    \"llama-quantize\",\n",
    "    \"llama-gguf\",\n",
    "    \"llama-gguf-hash\",\n",
    "    \"llama-gguf-split\",\n",
    "    \"llama-imatrix\",\n",
    "    # LoRA & embedding\n",
    "    \"llama-export-lora\",\n",
    "    \"llama-embedding\",\n",
    "    # Utilities\n",
    "    \"llama-tokenize\",\n",
    "    \"llama-infill\",\n",
    "    \"llama-perplexity\",\n",
    "    \"llama-bench\",\n",
    "    \"llama-cvector-generator\",\n",
    "]\n",
    "\n",
    "# Copy binaries\n",
    "copied = []\n",
    "for binary in binaries:\n",
    "    src = f\"{BUILD_BIN}/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n",
    "        os.chmod(f\"{PACKAGE_DIR}/bin/{binary}\", 0o755)\n",
    "        copied.append(binary)\n",
    "        print(f\"  ‚úÖ {binary}\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  {binary} (not found)\")\n",
    "\n",
    "print(f\"\\nüì¶ Copied {len(copied)}/{len(binaries)} binaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c7058",
   "metadata": {},
   "source": [
    "## Step 9: Create Package Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get llama.cpp info\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "commit_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n",
    "commit_date = subprocess.getoutput(\"git log -1 --format=%ci\")\n",
    "commit_msg = subprocess.getoutput(\"git log -1 --format=%s\")\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = subprocess.getoutput(\"nvcc --version | grep release | sed 's/.*release //' | cut -d, -f1\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"package\": \"llcuda\",\n",
    "    \"version\": VERSION,\n",
    "    \"build_date\": datetime.now().isoformat(),\n",
    "    \"platform\": {\n",
    "        \"name\": \"kaggle\",\n",
    "        \"gpu_count\": 2,\n",
    "        \"gpu_model\": \"Tesla T4\",\n",
    "        \"vram_per_gpu_gb\": 15,\n",
    "        \"total_vram_gb\": 30,\n",
    "        \"compute_capability\": \"7.5\",\n",
    "        \"architecture\": \"Turing\"\n",
    "    },\n",
    "    \"cuda\": {\n",
    "        \"version\": cuda_version,\n",
    "        \"architectures\": [\"sm_75\"],\n",
    "        \"flash_attention\": True,\n",
    "        \"flash_attention_all_quants\": True\n",
    "    },\n",
    "    \"llama_cpp\": {\n",
    "        \"commit\": commit_hash,\n",
    "        \"commit_date\": commit_date,\n",
    "        \"commit_message\": commit_msg,\n",
    "        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
    "    },\n",
    "    \"multi_gpu\": {\n",
    "        \"supported\": True,\n",
    "        \"method\": \"native_cuda\",\n",
    "        \"modes\": {\n",
    "            \"tensor_split\": {\n",
    "                \"description\": \"Split model across both GPUs for larger models\",\n",
    "                \"flags\": [\"--tensor-split 0.5,0.5\", \"--split-mode layer\"],\n",
    "                \"use_case\": \"Large GGUF models (>15GB)\"\n",
    "            },\n",
    "            \"split_workload\": {\n",
    "                \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n",
    "                \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n",
    "                \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n",
    "            }\n",
    "        },\n",
    "        \"recommended_config\": {\n",
    "            \"tensor_split\": \"0.5,0.5\",\n",
    "            \"split_mode\": \"layer\",\n",
    "            \"n_gpu_layers\": -1\n",
    "        }\n",
    "    },\n",
    "    \"split_workload\": {\n",
    "        \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n",
    "        \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n",
    "        \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n",
    "        \"rapids_packages\": [\"cudf-cu12\", \"cuml-cu12\", \"cugraph-cu12\"],\n",
    "        \"graphistry_packages\": [\"graphistry[ai]\"],\n",
    "        \"usage\": {\n",
    "            \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n",
    "            \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n",
    "        }\n",
    "    },\n",
    "    \"binaries\": copied,\n",
    "    \"features\": [\n",
    "        \"multi-gpu-tensor-split\",\n",
    "        \"split-workload-architecture\",\n",
    "        \"flash-attention-all-quants\",\n",
    "        \"openai-compatible-api\",\n",
    "        \"anthropic-compatible-api\",\n",
    "        \"29-quantization-formats\",\n",
    "        \"lora-adapters\",\n",
    "        \"grammar-constraints\",\n",
    "        \"json-schema-output\",\n",
    "        \"embeddings-reranking\",\n",
    "        \"streaming-sse\",\n",
    "        \"kv-cache-slots\",\n",
    "        \"speculative-decoding\"\n",
    "    ],\n",
    "    \"unsloth_integration\": {\n",
    "        \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n",
    "        \"workflow\": \"Unsloth (training) ‚Üí GGUF (conversion) ‚Üí llcuda (inference)\",\n",
    "        \"supported_exports\": [\"f16\", \"q8_0\", \"q4_k_m\", \"q5_k_m\", \"iq4_xs\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write metadata\n",
    "os.chdir(\"/kaggle/working\")\n",
    "with open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"üìã Package Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b1740",
   "metadata": {},
   "source": [
    "## Step 10: Create README and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_content = f'''# llcuda v{VERSION} - Kaggle 2√ó Tesla T4 Build\n",
    "\n",
    "Pre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n",
    "\n",
    "## üéØ Unsloth Integration\n",
    "\n",
    "llcuda is the **CUDA 12 inference backend for Unsloth**:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   UNSLOTH   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   LLCUDA    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  llama-server   ‚îÇ\n",
    "‚îÇ  Training   ‚îÇ    ‚îÇ  GGUF Conv  ‚îÇ    ‚îÇ  Multi-GPU Inf  ‚îÇ\n",
    "‚îÇ  Fine-tune  ‚îÇ    ‚îÇ  Quantize   ‚îÇ    ‚îÇ  2√ó T4 (30GB)   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "### 1. Extract Package\n",
    "```bash\n",
    "tar -xzf llcuda-v{VERSION}-cuda12-kaggle-t4x2.tar.gz\n",
    "cd llcuda-v{VERSION}-cuda12-kaggle-t4x2\n",
    "chmod +x bin/*\n",
    "```\n",
    "\n",
    "### 2. Start Multi-GPU Server\n",
    "```bash\n",
    "./bin/llama-server \\\\\n",
    "    -m /path/to/model.gguf \\\\\n",
    "    -ngl 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    -fa \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port 8080 \\\\\n",
    "    -c 8192\n",
    "```\n",
    "\n",
    "### 3. Use with Python\n",
    "```python\n",
    "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
    "\n",
    "# Get optimal config for Kaggle\n",
    "config = kaggle_t4_dual_config()\n",
    "print(config.to_cli_args())\n",
    "\n",
    "# Connect to server\n",
    "client = LlamaCppClient(\"http://localhost:8080\")\n",
    "\n",
    "# OpenAI-compatible chat\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "## üìä Multi-GPU Flags\n",
    "\n",
    "| Flag | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| `-ngl 99` | Offload all layers to GPU | Required |\n",
    "| `--tensor-split` | VRAM ratio per GPU | `0.5,0.5` |\n",
    "| `--split-mode` | Split strategy | `layer` or `row` |\n",
    "| `--main-gpu` | Primary GPU ID | `0` |\n",
    "| `-fa` | FlashAttention | Recommended |\n",
    "\n",
    "## üì¶ Recommended Models for 30GB VRAM\n",
    "\n",
    "| Model | Quant | Size | Context | Fits? |\n",
    "|-------|-------|------|---------|-------|\n",
    "| Llama 3.1 70B | IQ3_XS | ~25GB | 4K | ‚úÖ |\n",
    "| Qwen2.5 32B | Q4_K_M | ~19GB | 8K | ‚úÖ |\n",
    "| Gemma 2 27B | Q4_K_M | ~16GB | 8K | ‚úÖ |\n",
    "| Llama 3.1 8B | Q8_0 | ~9GB | 16K | ‚úÖ |\n",
    "| Mistral 7B | Q8_0 | ~8GB | 32K | ‚úÖ |\n",
    "\n",
    "## üîß Unsloth ‚Üí llcuda Workflow\n",
    "\n",
    "```python\n",
    "# 1. Fine-tune with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "# ... training ...\n",
    "\n",
    "# 2. Export to GGUF (Unsloth built-in)\n",
    "model.save_pretrained_gguf(\"my_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# 3. Run with llcuda\n",
    "# ./bin/llama-server -m my_model-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5\n",
    "```\n",
    "\n",
    "## üìã Build Info\n",
    "\n",
    "- **llcuda Version:** {VERSION}\n",
    "- **CUDA Version:** 12.4\n",
    "- **Target GPU:** Tesla T4 √ó 2\n",
    "- **Compute Capability:** SM 7.5 (Turing)\n",
    "- **FlashAttention:** All quantization types\n",
    "- **Build Date:** {BUILD_DATE}\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- [llcuda GitHub](https://github.com/llcuda/llcuda)\n",
    "- [Unsloth](https://github.com/unslothai/unsloth)\n",
    "- [llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úÖ README.md created\")\n",
    "print(f\"\\n{readme_content[:1500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e557cd",
   "metadata": {},
   "source": [
    "## Step 11: Create Helper Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create start-server.sh helper script\n",
    "start_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Start Multi-GPU Server\n",
    "# Usage: ./start-server.sh <model.gguf> [port]\n",
    "\n",
    "MODEL=\"$1\"\n",
    "PORT=\"${2:-8080}\"\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"Usage: $0 <model.gguf> [port]\"\n",
    "    echo \"Example: $0 qwen2.5-7b-Q4_K_M.gguf 8080\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Starting llama-server with dual T4 config...\"\n",
    "echo \"Model: $MODEL\"\n",
    "echo \"Port: $PORT\"\n",
    "echo \"\"\n",
    "\n",
    "\"$SCRIPT_DIR/bin/llama-server\" \\\\\n",
    "    --model \"$MODEL\" \\\\\n",
    "    --n-gpu-layers 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    --flash-attn \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port \"$PORT\" \\\\\n",
    "    --ctx-size 8192 \\\\\n",
    "    --batch-size 2048 \\\\\n",
    "    --ubatch-size 512 \\\\\n",
    "    --parallel 4\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/start-server.sh\", \"w\") as f:\n",
    "    f.write(start_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/start-server.sh\", 0o755)\n",
    "\n",
    "# Create quantize.sh helper script\n",
    "quantize_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Quantize Model\n",
    "# Usage: ./quantize.sh <input.gguf> <output.gguf> [quant_type]\n",
    "\n",
    "INPUT=\"$1\"\n",
    "OUTPUT=\"$2\"\n",
    "QUANT=\"${3:-Q4_K_M}\"\n",
    "\n",
    "if [ -z \"$INPUT\" ] || [ -z \"$OUTPUT\" ]; then\n",
    "    echo \"Usage: $0 <input.gguf> <output.gguf> [quant_type]\"\n",
    "    echo \"Quant types: Q4_K_M (default), Q8_0, Q5_K_M, IQ4_XS, etc.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Quantizing: $INPUT ‚Üí $OUTPUT ($QUANT)\"\n",
    "\"$SCRIPT_DIR/bin/llama-quantize\" \"$INPUT\" \"$OUTPUT\" \"$QUANT\"\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/quantize.sh\", \"w\") as f:\n",
    "    f.write(quantize_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/quantize.sh\", 0o755)\n",
    "\n",
    "print(\"‚úÖ Helper scripts created:\")\n",
    "print(\"   - start-server.sh\")\n",
    "print(\"   - quantize.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e467ab",
   "metadata": {},
   "source": [
    "## Step 12: Create Distribution Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "TARBALL = f\"{PACKAGE_NAME}.tar.gz\"\n",
    "\n",
    "print(f\"Creating distribution archive: {TARBALL}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tarball\n",
    "!tar -czvf {TARBALL} {PACKAGE_NAME}\n",
    "\n",
    "# Calculate SHA256\n",
    "with open(TARBALL, \"rb\") as f:\n",
    "    sha256 = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Write checksum file\n",
    "with open(f\"{TARBALL}.sha256\", \"w\") as f:\n",
    "    f.write(f\"{sha256}  {TARBALL}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ DISTRIBUTION PACKAGE READY\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh {TARBALL}*\n",
    "print(f\"\\nSHA256: {sha256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214c3f",
   "metadata": {},
   "source": [
    "## Step 13: Test Multi-GPU Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small test model and verify multi-GPU works\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"Downloading small test model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"‚úÖ Model: {model_path}\")\n",
    "\n",
    "# Start server with multi-GPU\n",
    "print(\"\\nStarting llama-server with dual T4 config...\")\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"--tensor-split\", \"0.5,0.5\",\n",
    "    \"--split-mode\", \"layer\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "print(f\"Command: {' '.join(server_cmd)}\")\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "print(\"\\nWaiting for server to start...\")\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"‚úÖ Server ready in {i+1}s!\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Server startup timeout\")\n",
    "\n",
    "# Check GPU usage\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ec572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"Testing multi-GPU inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start = time.time()\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    timeout=60\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    usage = result.get(\"usage\", {})\n",
    "    \n",
    "    print(f\"‚úÖ Response ({elapsed:.2f}s):\")\n",
    "    print(f\"   {content}\")\n",
    "    print(f\"\\nüìä Tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "    if usage.get('completion_tokens'):\n",
    "        tps = usage['completion_tokens'] / elapsed\n",
    "        print(f\"üìä Speed: {tps:.1f} tokens/sec\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - stop server\n",
    "print(\"Stopping server...\")\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"‚úÖ Server stopped\")\n",
    "\n",
    "# Show final GPU state\n",
    "print(\"\\nüìä Final GPU State:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f0e65",
   "metadata": {},
   "source": [
    "## Step 13b: Test Split-GPU Architecture (LLM + Graphistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split-GPU Architecture Demo:\n",
    "- GPU 0: llama-server (LLM inference)\n",
    "- GPU 1: RAPIDS/Graphistry (graph simulation)\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLIT-GPU ARCHITECTURE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 0: Start llama-server (LLM)\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 0: Starting llama-server...\")\n",
    "\n",
    "# Force llama-server to use GPU 0 only\n",
    "llama_env = os.environ.copy()\n",
    "llama_env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env=llama_env\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"   ‚úÖ llama-server ready on GPU 0 ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Server timeout\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 1: RAPIDS/Graphistry graph operations\n",
    "# ============================================================================\n",
    "print(\"\\nüîß GPU 1: Running RAPIDS graph simulation...\")\n",
    "\n",
    "# Force RAPIDS to use GPU 1 only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "# Create sample graph data (simulating knowledge graph from LLM)\n",
    "edges = cudf.DataFrame({\n",
    "    \"src\": [0, 1, 2, 3, 4, 0, 1, 2],\n",
    "    \"dst\": [1, 2, 3, 4, 0, 2, 3, 4],\n",
    "    \"weight\": [1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 1.0, 0.8]\n",
    "})\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "\n",
    "print(f\"   Graph: {G.number_of_vertices()} vertices, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Run PageRank on GPU 1\n",
    "pagerank = cugraph.pagerank(G)\n",
    "print(f\"   PageRank computed: {len(pagerank)} nodes\")\n",
    "print(f\"   Top node: {pagerank.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Combined workflow: LLM query ‚Üí Graph update\n",
    "# ============================================================================\n",
    "print(\"\\nüîó Combined LLM + Graph workflow...\")\n",
    "\n",
    "# Reset CUDA_VISIBLE_DEVICES for requests\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Query LLM on GPU 0\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"List 3 related concepts to 'machine learning'\"}],\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"   LLM (GPU 0): {llm_output[:100]}...\")\n",
    "    \n",
    "    # Simulate adding LLM-derived edges to graph\n",
    "    new_edges = cudf.DataFrame({\n",
    "        \"src\": [5, 5, 5],\n",
    "        \"dst\": [0, 1, 2],\n",
    "        \"weight\": [1.0, 1.0, 1.0]\n",
    "    })\n",
    "    all_edges = cudf.concat([edges, new_edges])\n",
    "    G2 = cugraph.Graph()\n",
    "    G2.from_cudf_edgelist(all_edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "    print(f\"   Graph (GPU 1): Updated to {G2.number_of_vertices()} vertices\")\n",
    "\n",
    "print(\"\\nüìä GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"\\n‚úÖ Split-GPU test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bb500",
   "metadata": {},
   "source": [
    "## Step 13b: llcuda v2.2.0 Module Integration Demo\n",
    "\n",
    "Demonstrate the new Graphistry and Louie.AI modules from llcuda v2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# llcuda v2.2.0 Module Integration Demo\n",
    "# ============================================================================\n",
    "# This demonstrates the new Graphistry and Louie.AI modules\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"llcuda v2.2.0 MODULE INTEGRATION DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install llcuda from GitHub (use main branch or specific version)\n",
    "!pip install -q git+https://github.com/llcuda/llcuda.git\n",
    "\n",
    "import llcuda\n",
    "\n",
    "print(f\"\\nüì¶ llcuda version: {llcuda.__version__}\")\n",
    "print(f\"\\nüìã Available exports:\")\n",
    "print(f\"   {llcuda.__all__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SplitGPUConfig - Configure Split-GPU Workloads\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. SplitGPUConfig Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config = llcuda.SplitGPUConfig(llm_gpu=0, graph_gpu=1)\n",
    "print(f\"   LLM GPU: {config.llm_gpu}\")\n",
    "print(f\"   Graph GPU: {config.graph_gpu}\")\n",
    "\n",
    "# Get environment variables for each GPU\n",
    "print(f\"\\n   LLM env: {config.llm_env()}\")\n",
    "print(f\"   Graph env: {config.graph_env()}\")\n",
    "\n",
    "# Generate llama-server command\n",
    "model_path = f\"/kaggle/working/{PACKAGE_NAME}/models/gemma-3-1b-Q4_K_M.gguf\"\n",
    "cmd = config.llama_server_cmd(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=99,\n",
    "    flash_attention=True,\n",
    "    port=8080\n",
    ")\n",
    "print(f\"\\n   Server command:\\n   {' '.join(cmd)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Graphistry Module - Graph Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. Graphistry Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.graphistry import GraphWorkload, RAPIDSBackend, check_rapids_available\n",
    "\n",
    "# Check RAPIDS availability\n",
    "rapids_status = check_rapids_available()\n",
    "print(f\"   RAPIDS status: {rapids_status}\")\n",
    "\n",
    "# Create GraphWorkload on GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "workload = GraphWorkload(gpu_id=1)\n",
    "\n",
    "# Sample entities and relationships (simulating LLM-extracted knowledge)\n",
    "entities = [\n",
    "    {\"id\": \"Machine Learning\", \"type\": \"field\", \"properties\": {\"year\": 1959}},\n",
    "    {\"id\": \"Deep Learning\", \"type\": \"field\", \"properties\": {\"year\": 2006}},\n",
    "    {\"id\": \"Neural Networks\", \"type\": \"concept\"},\n",
    "    {\"id\": \"Transformers\", \"type\": \"architecture\", \"properties\": {\"year\": 2017}},\n",
    "    {\"id\": \"GPT\", \"type\": \"model\"},\n",
    "    {\"id\": \"BERT\", \"type\": \"model\"},\n",
    "    {\"id\": \"CNN\", \"type\": \"architecture\"},\n",
    "]\n",
    "\n",
    "relationships = [\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Deep Learning\", \"type\": \"contains\", \"weight\": 0.9},\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Neural Networks\", \"type\": \"uses\", \"weight\": 0.85},\n",
    "    {\"source\": \"Deep Learning\", \"target\": \"Transformers\", \"type\": \"includes\", \"weight\": 0.95},\n",
    "    {\"source\": \"Transformers\", \"target\": \"GPT\", \"type\": \"basis_for\", \"weight\": 0.9},\n",
    "    {\"source\": \"Transformers\", \"target\": \"BERT\", \"type\": \"basis_for\", \"weight\": 0.88},\n",
    "    {\"source\": \"Neural Networks\", \"target\": \"CNN\", \"type\": \"type_of\", \"weight\": 0.8},\n",
    "]\n",
    "\n",
    "# Create knowledge graph using the correct API\n",
    "g = workload.create_knowledge_graph(entities, relationships)\n",
    "print(f\"   Knowledge graph created with Graphistry\")\n",
    "\n",
    "# Run PageRank using edges DataFrame (correct API)\n",
    "import pandas as pd\n",
    "edges_df = pd.DataFrame([\n",
    "    {\"src\": r[\"source\"], \"dst\": r[\"target\"], \"weight\": r.get(\"weight\", 1.0)}\n",
    "    for r in relationships\n",
    "])\n",
    "pagerank_result = workload.run_pagerank(edges_df)\n",
    "print(f\"   PageRank: top node = {pagerank_result.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Louie Module - Natural Language Graph Queries\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. Louie Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.louie import LouieClient, KnowledgeExtractor\n",
    "\n",
    "# Initialize Louie client (connected to llama-server on GPU 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "louie = LouieClient(llm_endpoint=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Knowledge extraction example\n",
    "text = \"\"\"\n",
    "NVIDIA develops GPUs for deep learning. The Tesla T4 is optimized for inference.\n",
    "llcuda v2.2.0 runs on Tesla T4 with FlashAttention enabled.\n",
    "cuGraph provides GPU-accelerated graph analytics.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"   Input text: {text[:60]}...\")\n",
    "\n",
    "# Extract entities (requires running LLM server)\n",
    "try:\n",
    "    entities = louie.extract_entities(text)\n",
    "    print(f\"   Extracted entities: {entities[:3]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   (LLM server required for entity extraction)\")\n",
    "    # Simulated output\n",
    "    entities = [\n",
    "        {\"name\": \"NVIDIA\", \"type\": \"ORG\"},\n",
    "        {\"name\": \"Tesla T4\", \"type\": \"PRODUCT\"},\n",
    "        {\"name\": \"llcuda\", \"type\": \"SOFTWARE\"},\n",
    "        {\"name\": \"cuGraph\", \"type\": \"SOFTWARE\"}\n",
    "    ]\n",
    "    print(f\"   Demo entities: {entities}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RAPIDS Backend Direct Access\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. RAPIDS Backend Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "backend = RAPIDSBackend()\n",
    "\n",
    "# Create a cuDF DataFrame\n",
    "import cudf\n",
    "gpu_edges = cudf.DataFrame({\n",
    "    \"source\": [0, 1, 2, 3, 4, 0, 1],\n",
    "    \"target\": [1, 2, 3, 4, 0, 2, 3],\n",
    "    \"weight\": [1.0, 0.8, 0.9, 0.7, 1.0, 0.6, 0.85]\n",
    "})\n",
    "\n",
    "# Run graph algorithms\n",
    "import cugraph\n",
    "G_rapids = cugraph.Graph()\n",
    "G_rapids.from_cudf_edgelist(gpu_edges, source=\"source\", destination=\"target\")\n",
    "\n",
    "# Louvain community detection\n",
    "louvain = cugraph.louvain(G_rapids)\n",
    "print(f\"   Louvain communities: {louvain['partition'].nunique()} detected\")\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness = cugraph.betweenness_centrality(G_rapids)\n",
    "top_node = betweenness.nlargest(1, 'betweenness_centrality')['vertex'].values[0]\n",
    "print(f\"   Highest betweenness: node {top_node}\")\n",
    "\n",
    "print(\"\\n‚úÖ llcuda v2.2.0 module integration complete!\")\n",
    "print(\"   All new APIs functional on Kaggle 2√ó T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aebf7",
   "metadata": {},
   "source": [
    "## Step 14: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c32db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéâ llcuda v2.2.0 BUILD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüì¶ Distribution Package:\")\n",
    "!ls -lh {PACKAGE_NAME}.tar.gz\n",
    "\n",
    "print(f\"\\nüìÅ Package Contents:\")\n",
    "!ls -la {PACKAGE_NAME}/\n",
    "\n",
    "print(f\"\\nüîß Binaries:\")\n",
    "!ls -lh {PACKAGE_NAME}/bin/ | head -10\n",
    "\n",
    "print(f\"\\nüìã Metadata Summary:\")\n",
    "print(f\"   Version: {VERSION}\")\n",
    "print(f\"   Platform: Kaggle 2√ó Tesla T4\")\n",
    "print(f\"   CUDA: {cuda_version}\")\n",
    "print(f\"   Compute: SM 7.5 (Turing)\")\n",
    "print(f\"   FlashAttention: ‚úÖ All quants\")\n",
    "print(f\"   Multi-GPU: ‚úÖ Native CUDA\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Download: {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   2. Extract: tar -xzf {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   3. Run: ./start-server.sh model.gguf 8080\")\n",
    "\n",
    "print(f\"\\nüì• Download from Kaggle Output tab\")\n",
    "print(f\"   or copy to output: !cp {PACKAGE_NAME}.tar.gz /kaggle/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Kaggle output for download\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"/kaggle/output\", exist_ok=True)\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\", \"/kaggle/output/\")\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz.sha256\", \"/kaggle/output/\")\n",
    "\n",
    "print(\"‚úÖ Package copied to /kaggle/output/ for download\")\n",
    "!ls -lh /kaggle/output/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
