{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# llcuda v2.1.0 + Unsloth Gemma 3-1B Tutorial\n",
        "\n",
        "**Complete Guide**: Using llcuda v2.1.0 with Unsloth GGUF models on Tesla T4 GPU\n",
        "\n",
        "**What This Demonstrates**:\n",
        "1. ‚úÖ Install llcuda v2.1.0 from GitHub (no PyPI)\n",
        "2. ‚úÖ Auto-download CUDA binaries from GitHub Releases (v2.0.6 - compatible with v2.1.0)\n",
        "3. ‚úÖ Use Gemma 3-1B-IT GGUF from Unsloth\n",
        "4. ‚úÖ Fast inference on Tesla T4 with FlashAttention\n",
        "5. ‚úÖ Performance benchmarking and optimization\n",
        "\n",
        "**Requirements**:\n",
        "- Google Colab with Tesla T4 GPU\n",
        "- Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- **llcuda v2.1.0**: https://github.com/waqasm86/llcuda\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth GGUF Models**: https://huggingface.co/unsloth\n",
        "- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Verify Tesla T4 GPU\n",
        "\n",
        "llcuda v2.1.0 uses v2.0.6 binaries optimized for Tesla T4 (SM 7.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9725001a-a204-4148-c13c-27dd6f1a0dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, compute_cap, memory.total [MiB]\n",
            "Tesla T4, 7.5, 15360 MiB\n",
            "\n",
            "======================================================================\n",
            "GPU: Tesla T4\n",
            "Compute Capability: SM 7.5\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Tesla T4 detected - Perfect for llcuda v2.1.0!\n",
            "   Binaries (v2.1.0) include FlashAttention and Tensor Core optimization\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n",
        "\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "gpu_info = result.stdout.strip().split(',')\n",
        "gpu_name = gpu_info[0].strip()\n",
        "compute_cap = gpu_info[1].strip()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Compute Capability: SM {compute_cap}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'T4' in gpu_name and compute_cap == '7.5':\n",
        "    print(\"\\n‚úÖ Tesla T4 detected - Perfect for llcuda v2.1.0!\")\n",
        "    print(\"   Binaries (v2.1.0) include FlashAttention and Tensor Core optimization\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  {gpu_name} detected\")\n",
        "    print(\"   llcuda v2.1.0 is optimized for Tesla T4\")\n",
        "    print(\"   Performance may vary on other GPUs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Install llcuda v2.1.0 from GitHub\n",
        "\n",
        "**Note**: llcuda is now **GitHub-only** (no longer on PyPI)\n",
        "\n",
        "CUDA binaries (v2.0.6, ~266 MB) will auto-download from GitHub Releases on first import. These binaries are fully compatible with v2.1.0 Python APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_llcuda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f271778d-3415-4d6f-9ab4-2a7875a951e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Installing llcuda v2.1.0 from GitHub...\n",
            "\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "‚úÖ llcuda v2.1.0 installed successfully!\n",
            "\n",
            "‚ÑπÔ∏è  CUDA binaries (v2.1.0, ~266 MB) will auto-download on first import\n",
            "   These binaries are fully compatible with v2.1.0 Python APIs\n",
            "   Download happens once - subsequent runs use cached binaries\n"
          ]
        }
      ],
      "source": [
        "# Install llcuda v2.1.0 from GitHub\n",
        "print(\"üì• Installing llcuda v2.1.0 from GitHub...\\n\")\n",
        "\n",
        "!pip install -q git+https://github.com/waqasm86/llcuda.git\n",
        "\n",
        "print(\"\\n‚úÖ llcuda v2.1.0 installed successfully!\")\n",
        "print(\"\\n‚ÑπÔ∏è  CUDA binaries (v2.1.0, ~266 MB) will auto-download on first import\")\n",
        "print(\"   These binaries are fully compatible with v2.1.0 Python APIs\")\n",
        "print(\"   Download happens once - subsequent runs use cached binaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Import llcuda (Triggers Binary Download)\n",
        "\n",
        "First import will download CUDA binaries from:\n",
        "https://github.com/waqasm86/llcuda/releases/download/v2.0.6/\n",
        "\n",
        "**Note**: v2.1.0 uses v2.0.6 binaries - they are fully compatible with the new v2.1.0 Python APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_llcuda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "817d9d28-4ec1-483a-f67e-57a9b9bdc8de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "llcuda version: 2.1.0\n",
            "======================================================================\n",
            "\n",
            "‚úÖ llcuda imported successfully!\n",
            "\n",
            "‚ÑπÔ∏è  If this was the first run:\n",
            "   - CUDA binaries were downloaded to ~/.cache/llcuda/\n",
            "   - Includes: llama-server, libggml-cuda.so, and supporting libs\n",
            "   - Next imports will be instant (binaries cached)\n"
          ]
        }
      ],
      "source": [
        "import llcuda\n",
        "import time\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"llcuda version: {llcuda.__version__}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\n‚úÖ llcuda imported successfully!\")\n",
        "print(\"\\n‚ÑπÔ∏è  If this was the first run:\")\n",
        "print(\"   - CUDA binaries were downloaded to ~/.cache/llcuda/\")\n",
        "print(\"   - Includes: llama-server, libggml-cuda.so, and supporting libs\")\n",
        "print(\"   - Next imports will be instant (binaries cached)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Verify GPU Compatibility\n",
        "\n",
        "Check if llcuda can detect and use the GPU properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_compat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bad1c17-6568-4248-f2e5-1822cf0e7dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "GPU COMPATIBILITY CHECK\n",
            "======================================================================\n",
            "GPU Name: Tesla T4\n",
            "Compute Capability: SM 7.5\n",
            "Platform: colab\n",
            "Compatible: True\n",
            "======================================================================\n",
            "\n",
            "‚úÖ GPU is compatible with llcuda!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU compatibility with llcuda\n",
        "compat = llcuda.check_gpu_compatibility()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"GPU COMPATIBILITY CHECK\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"GPU Name: {compat['gpu_name']}\")\n",
        "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
        "print(f\"Platform: {compat['platform']}\")\n",
        "print(f\"Compatible: {compat['compatible']}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if compat['compatible']:\n",
        "    print(\"\\n‚úÖ GPU is compatible with llcuda!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  GPU compatibility issue detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Load Gemma 3-1B-IT from Unsloth\n",
        "\n",
        "**Three methods to load models:**\n",
        "\n",
        "1. **HuggingFace Hub** (recommended): Direct from Unsloth's repo\n",
        "2. **Model Registry**: Pre-configured model names\n",
        "3. **Local Path**: From downloaded GGUF file\n",
        "\n",
        "We'll use Method 1: Direct from Unsloth HuggingFace repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_model",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "935e3340109d491dae014df275e1e2d1",
            "8360885b86dc4c989d61ea9ba4b7c948",
            "2720a20f579e4815b23310cf43474167",
            "d3db085068fc4a8784e1af4d3d1dfbc0",
            "df50b49a068647bfabb5c76e6f50f10a",
            "7541dac8599543878a9ce3de0f2ddd79",
            "b7c8b0d0429345dc9f9bd7d313e8aea1",
            "5a10b81e26b8404882a898d7d5837585",
            "641f07891d874a709d728f94071e37a2",
            "6db996b332cc4fe58055dd59f4826e94",
            "1b6d0229269742e0919c068ff46347b7"
          ]
        },
        "outputId": "a73ccb75-e86d-4068-e67e-a0b9fa40e0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì• Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\n",
            "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
            "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
            "   This may take 2-3 minutes on first run (downloads model)\n",
            "\n",
            "Loading model: unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "======================================================================\n",
            "Repository: unsloth/gemma-3-1b-it-GGUF\n",
            "File: gemma-3-1b-it-Q4_K_M.gguf\n",
            "Cache location: /usr/local/lib/python3.12/dist-packages/llcuda/models/gemma-3-1b-it-Q4_K_M.gguf\n",
            "======================================================================\n",
            "\n",
            "Download this model? [Y/n]: y\n",
            "\n",
            "Downloading gemma-3-1b-it-Q4_K_M.gguf from unsloth/gemma-3-1b-it-GGUF...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
            "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "935e3340109d491dae014df275e1e2d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model downloaded: gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "Auto-configuring optimal settings...\n",
            "‚úì Auto-configured for 15.0 GB VRAM\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "  Batch Size: 2048\n",
            "  Micro-batch Size: 512\n",
            "\n",
            "Starting llama-server...\n",
            "GPU Check:\n",
            "  Platform: colab\n",
            "  GPU: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Status: ‚úì Compatible\n",
            "llama-server not found. Downloading pre-built CUDA binary...\n",
            "Downloading from: https://github.com/llcuda/llcuda/releases/download/v2.0.2/llcuda-binaries-cuda12-t4-v2.0.2.tar.gz\n",
            "Downloading: 100.0% (278884773/278884773 bytes)\n",
            "‚úì Download complete\n",
            "Extracting binary...\n",
            "‚úì Found binary at: /content/.cache/llcuda/extracted/bin/llama-server\n",
            "‚úì Binary installed at: /content/.cache/llcuda/llama-server\n",
            "Starting llama-server...\n",
            "  Executable: /content/.cache/llcuda/llama-server\n",
            "  Model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "  Server URL: http://127.0.0.1:8090\n",
            "Waiting for server to be ready..."
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "llama-server process died unexpectedly. Run with silent=False for error details.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-670427480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m engine.load_model(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Suppress llama-server output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_name_or_path, gpu_layers, ctx_size, auto_start, auto_configure, n_parallel, verbose, interactive_download, silent, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 )\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 success = self._server_manager.start_server(\n\u001b[0m\u001b[1;32m    406\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(self, model_path, port, host, gpu_layers, ctx_size, n_parallel, batch_size, ubatch_size, timeout, verbose, skip_gpu_check, silent, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     )\n\u001b[1;32m    546\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    548\u001b[0m                         \u001b[0;34mf\"llama-server process died unexpectedly. Run with silent=False for error details.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                     )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: llama-server process died unexpectedly. Run with silent=False for error details."
          ]
        }
      ],
      "source": [
        "# Initialize inference engine\n",
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "print(\"\\nüì• Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\")\n",
        "print(\"   Repository: unsloth/gemma-3-1b-it-GGUF\")\n",
        "print(\"   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\")\n",
        "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
        "\n",
        "# Load model from Unsloth HuggingFace repository\n",
        "# Format: repo_id:filename\n",
        "start_time = time.time()\n",
        "\n",
        "engine.load_model(\n",
        "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "    silent=True,        # Suppress llama-server output\n",
        "    auto_start=True     # Start server automatically\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded successfully in {load_time:.1f}s!\")\n",
        "print(\"\\nüöÄ Ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: First Inference - General Knowledge\n",
        "\n",
        "Test the model with a general knowledge question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "first_inference",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f20be85-c4cc-4737-b5dc-47facc2d06cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PROMPT:\n",
            "======================================================================\n",
            "Explain quantum computing in simple terms that a beginner can understand.\n",
            "======================================================================\n",
            "\n",
            "ü§ñ Generating response...\n",
            "\n",
            "======================================================================\n",
            "RESPONSE:\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Okay, let's tackle quantum computing. It's a really mind-bending concept, but we'll break it down.\n",
            "\n",
            "**1. The Problem with Regular Computers:**\n",
            "\n",
            "*   **Bits:** Regular computers, like the one you're using, store information as \"bits.\" A bit is like a light switch: it can be either on (1) or off (0).\n",
            "*   **Limited Choices:**  Because of this binary nature, regular computers can only do one thing at a time.  They have to try out one thing after another.  Think of it like flipping a single light switch on and off repeatedly.\n",
            "\n",
            "**2. Quantum Computing - A Different Approach:**\n",
            "\n",
            "*   **Qubits:** Quantum computers use something called \"qubits.\"  A qubit is like a dimmer switch. It can be 0, 1, *or* a combination of both *at the same time*.  This \"both at the same time\n",
            "======================================================================\n",
            "\n",
            "üìä Performance:\n",
            "   Tokens generated: 200\n",
            "   Latency: 1522.3 ms\n",
            "   Speed: 131.4 tokens/sec\n",
            "\n",
            "üí° Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\n"
          ]
        }
      ],
      "source": [
        "# Test with a general knowledge prompt\n",
        "prompt = \"Explain quantum computing in simple terms that a beginner can understand.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PROMPT:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(prompt)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nü§ñ Generating response...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    prompt,\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"RESPONSE:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nüìä Performance:\")\n",
        "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
        "print(f\"   Latency: {result.latency_ms:.1f} ms\")\n",
        "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
        "print(f\"\\nüí° Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Code Generation\n",
        "\n",
        "Test the model's ability to generate Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_generation",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed3009c-8a50-41ca-b1e2-7f6652008cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CODE GENERATION TEST\n",
            "======================================================================\n",
            "Prompt: Write a Python function to calculate the fibonacci sequence using dynamic programming. \n",
            "Include docstring and example usage.\n",
            "======================================================================\n",
            "\n",
            "ü§ñ Generating code...\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "```python\n",
            "def fibonacci_dynamic_programming(n):\n",
            "  \"\"\"\n",
            "  Calculate the nth Fibonacci number using dynamic programming.\n",
            "  \n",
            "  Args:\n",
            "    n: The index of the desired Fibonacci number (non-negative integer).\n",
            "  \n",
            "  Returns:\n",
            "    The nth Fibonacci number.\n",
            "  \"\"\"\n",
            "  if n <= 0:\n",
            "    return 0\n",
            "  elif n == 1:\n",
            "    return 1\n",
            "  \n",
            "  fib_numbers = [0, 1]\n",
            "  for i in range(2, n + 1):\n",
            "    next_fib = fib_numbers[i-1] + fib_numbers[i-2]\n",
            "    fib_numbers.append(next_fib)\n",
            "  return fib_numbers[n]\n",
            "\n",
            "# Example usage:\n",
            "if __name__ == \"__main__\":\n",
            "  n = 10\n",
            "  result = fibonacci_dynamic_programming(n)\n",
            "  print(f\"The {n}th Fibonacci number is: {result}\")  # Output: The 10th Fibonacci number is: 55\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **Docstring:**\n",
            "    *   The function starts with a docstring that explains the purpose of the function, the arguments it takes, and the value it returns.\n",
            "\n",
            "2.  **Base Cases:**\n",
            "    *   If `n` is less than or equal to \n",
            "======================================================================\n",
            "\n",
            "üìä Speed: 136.1 tokens/sec\n"
          ]
        }
      ],
      "source": [
        "# Test code generation\n",
        "code_prompt = \"\"\"Write a Python function to calculate the fibonacci sequence using dynamic programming.\n",
        "Include docstring and example usage.\"\"\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CODE GENERATION TEST\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {code_prompt}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nü§ñ Generating code...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    code_prompt,\n",
        "    max_tokens=300,\n",
        "    temperature=0.3,  # Lower temperature for more deterministic code\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nüìä Speed: {result.tokens_per_sec:.1f} tokens/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Batch Inference\n",
        "\n",
        "Process multiple prompts efficiently with batch inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_inference",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fba77b-7ca2-486a-8515-d8211e911182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "BATCH INFERENCE - Processing 4 prompts\n",
            "======================================================================\n",
            "\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Query 1: What is machine learning in one sentence?\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "\n",
            "Machine learning is a field of computer science that enables computers to learn from data without being explicitly programmed.\n",
            "\n",
            "---\n",
            "\n",
            "**Key Concepts:**\n",
            "\n",
            "*   **Data:**  The raw material used to train the model.\n",
            "*   **Algorithms:**  The methods used to analyze the data and find patterns.\n",
            "*   **Models:**  The learned representations of the data, which can be used\n",
            "\n",
            "üìä Speed: 116.0 tok/s | Latency: 690ms\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Query 2: Explain neural networks briefly.\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "\n",
            "Neural networks are a type of machine learning model inspired by the structure and function of the human brain. They consist of interconnected nodes (neurons) organized in layers, and learn from data by adjusting the strength of these connections.\n",
            "\n",
            "Here's a breakdown of key aspects:\n",
            "\n",
            "* **Nodes (Neurons):**  These are the basic building blocks. They receive input, perform a calculation, and\n",
            "\n",
            "üìä Speed: 142.3 tok/s | Latency: 562ms\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Query 3: What is the difference between AI and ML?\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "\n",
            "Okay, let's break down the difference between AI and ML. They're related but distinct concepts.\n",
            "\n",
            "**AI (Artificial Intelligence) - The Big Picture**\n",
            "\n",
            "*   **Definition:** AI is the broad concept of creating machines that can perform tasks that typically require human intelligence. This includes things like learning, problem-solving, decision-making, understanding language, and visual perception.\n",
            "\n",
            "üìä Speed: 141.6 tok/s | Latency: 565ms\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Query 4: Define deep learning concisely.\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "\n",
            "\n",
            "Deep learning is a subfield of machine learning that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and learn complex patterns.\n",
            "\n",
            "---\n",
            "\n",
            "Here's a more detailed breakdown of key aspects:\n",
            "\n",
            "*   **Artificial Neural Networks (ANNs):** These are the fundamental building blocks.\n",
            "*   **Multiple Layers:**  The \"deep\" in deep learning refers\n",
            "\n",
            "üìä Speed: 141.7 tok/s | Latency: 565ms\n",
            "\n",
            "======================================================================\n",
            "Total batch time: 2.4s for 4 prompts\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Prepare multiple prompts\n",
        "prompts = [\n",
        "    \"What is machine learning in one sentence?\",\n",
        "    \"Explain neural networks briefly.\",\n",
        "    \"What is the difference between AI and ML?\",\n",
        "    \"Define deep learning concisely.\"\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"BATCH INFERENCE - Processing 4 prompts\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Query {i}: {prompt}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "    print(result.text)\n",
        "    print(f\"\\nüìä Speed: {result.tokens_per_sec:.1f} tok/s | Latency: {result.latency_ms:.0f}ms\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Total batch time: {total_time:.1f}s for {len(prompts)} prompts\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Performance Metrics\n",
        "\n",
        "Analyze aggregated performance metrics across all requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf004de4-106d-4a37-8f8c-71870e5221c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "PERFORMANCE METRICS SUMMARY\n",
            "======================================================================\n",
            "\n",
            "üìä Throughput:\n",
            "   Total requests: 6\n",
            "   Total tokens: 820\n",
            "   Average speed: 134.2 tokens/sec\n",
            "\n",
            "‚è±Ô∏è  Latency Distribution:\n",
            "   Mean: 1018.0 ms\n",
            "   Median (P50): 689.6 ms\n",
            "   P95: 2204.7 ms\n",
            "   P99: 2204.7 ms\n",
            "   Min: 562.1 ms\n",
            "   Max: 2204.7 ms\n",
            "\n",
            "üìà Sample count: 6\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Get comprehensive metrics\n",
        "metrics = engine.get_metrics()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nüìä Throughput:\")\n",
        "print(f\"   Total requests: {metrics['throughput']['total_requests']}\")\n",
        "print(f\"   Total tokens: {metrics['throughput']['total_tokens']}\")\n",
        "print(f\"   Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Latency Distribution:\")\n",
        "print(f\"   Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
        "print(f\"   Median (P50): {metrics['latency']['p50_ms']:.1f} ms\")\n",
        "print(f\"   P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
        "print(f\"   P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
        "print(f\"   Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
        "print(f\"   Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
        "\n",
        "print(f\"\\nüìà Sample count: {metrics['latency']['sample_count']}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: Advanced Generation Parameters\n",
        "\n",
        "Explore different generation strategies and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_params",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33bef169-67e5-4cec-9a48-a06d04702190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CREATIVE GENERATION (High Temperature)\n",
            "======================================================================\n",
            "Prompt: Write a haiku about artificial intelligence.\n",
            "Parameters: temperature=0.9, top_p=0.95, top_k=50\n",
            "======================================================================\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Speed: 32.5 tok/s\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test creative writing with higher temperature\n",
        "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CREATIVE GENERATION (High Temperature)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {creative_prompt}\")\n",
        "print(f\"Parameters: temperature=0.9, top_p=0.95, top_k=50\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    creative_prompt,\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,     # High creativity\n",
        "    top_p=0.95,          # Nucleus sampling\n",
        "    top_k=50,            # Top-k sampling\n",
        "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
        ")\n",
        "\n",
        "print(result.text)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step11"
      },
      "source": [
        "## Step 11: Alternative Model Loading Methods\n",
        "\n",
        "Demonstrate other ways to load models with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alternative_loading",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05250f58-45f3-46e2-9435-69548df78854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ALTERNATIVE MODEL LOADING METHODS\n",
            "======================================================================\n",
            "\n",
            "1Ô∏è‚É£  HuggingFace Hub (Current method - RECOMMENDED):\n",
            "   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\n",
            "   ‚úÖ Direct from Unsloth repository\n",
            "   ‚úÖ Auto-downloads and caches\n",
            "   ‚úÖ Always up-to-date\n",
            "\n",
            "2Ô∏è‚É£  Model Registry (Pre-configured shortcuts):\n",
            "   engine.load_model('gemma-3-1b-Q4_K_M')\n",
            "   ‚úÖ Simple one-word names\n",
            "   ‚úÖ Curated model list\n",
            "   ‚ö†Ô∏è  May not include all Unsloth models\n",
            "\n",
            "3Ô∏è‚É£  Local Path (Pre-downloaded GGUF):\n",
            "   engine.load_model('/path/to/model.gguf')\n",
            "   ‚úÖ Full control over model files\n",
            "   ‚úÖ No network dependency after download\n",
            "   ‚ö†Ô∏è  Manual model management\n",
            "\n",
            "üí° Tip: For Unsloth models, use method 1 (HuggingFace Hub)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALTERNATIVE MODEL LOADING METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1Ô∏è‚É£  HuggingFace Hub (Current method - RECOMMENDED):\")\n",
        "print(\"   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\")\n",
        "print(\"   ‚úÖ Direct from Unsloth repository\")\n",
        "print(\"   ‚úÖ Auto-downloads and caches\")\n",
        "print(\"   ‚úÖ Always up-to-date\")\n",
        "\n",
        "print(\"\\n2Ô∏è‚É£  Model Registry (Pre-configured shortcuts):\")\n",
        "print(\"   engine.load_model('gemma-3-1b-Q4_K_M')\")\n",
        "print(\"   ‚úÖ Simple one-word names\")\n",
        "print(\"   ‚úÖ Curated model list\")\n",
        "print(\"   ‚ö†Ô∏è  May not include all Unsloth models\")\n",
        "\n",
        "print(\"\\n3Ô∏è‚É£  Local Path (Pre-downloaded GGUF):\")\n",
        "print(\"   engine.load_model('/path/to/model.gguf')\")\n",
        "print(\"   ‚úÖ Full control over model files\")\n",
        "print(\"   ‚úÖ No network dependency after download\")\n",
        "print(\"   ‚ö†Ô∏è  Manual model management\")\n",
        "\n",
        "print(\"\\nüí° Tip: For Unsloth models, use method 1 (HuggingFace Hub)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step12"
      },
      "source": [
        "## Step 12: Unsloth Fine-tuning ‚Üí llcuda Inference Workflow\n",
        "\n",
        "Example workflow showing how to integrate llcuda with Unsloth fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "workflow_demo"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë          UNSLOTH FINE-TUNING ‚Üí llcuda INFERENCE WORKFLOW             ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "STEP 1: Fine-tune with Unsloth\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load base model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "# Fine-tune on your dataset\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    ...\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "STEP 2: Export to GGUF\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "# Save fine-tuned model as GGUF (Q4_K_M quantization)\n",
        "model.save_pretrained_gguf(\n",
        "    \"my_finetuned_model\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"\n",
        ")\n",
        "\n",
        "# This creates: my_finetuned_model/unsloth.Q4_K_M.gguf\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "STEP 3: Deploy with llcuda v2.1.0\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "import llcuda\n",
        "\n",
        "# Load your fine-tuned GGUF model\n",
        "engine = llcuda.InferenceEngine()\n",
        "engine.load_model(\"my_finetuned_model/unsloth.Q4_K_M.gguf\")\n",
        "\n",
        "# Run inference\n",
        "result = engine.infer(\"Your task-specific prompt\", max_tokens=200)\n",
        "print(result.text)\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "‚úÖ BENEFITS:\n",
        "   ‚Ä¢ Fast training with Unsloth (2x faster, 70% less VRAM)\n",
        "   ‚Ä¢ Fast inference with llcuda (FlashAttention, T4-optimized)\n",
        "   ‚Ä¢ Easy deployment (GGUF = single portable file)\n",
        "   ‚Ä¢ Full llama.cpp ecosystem compatibility\n",
        "   ‚Ä¢ Production-ready inference server\n",
        "\n",
        "üìä EXPECTED PERFORMANCE (Tesla T4):\n",
        "   ‚Ä¢ Gemma 3-1B Q4_K_M: ~45 tok/s\n",
        "   ‚Ä¢ Llama 3.2-3B Q4_K_M: ~30 tok/s\n",
        "   ‚Ä¢ Qwen 2.5-7B Q4_K_M: ~18 tok/s\n",
        "   ‚Ä¢ Llama 3.1-8B Q4_K_M: ~15 tok/s\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step13"
      },
      "source": [
        "## Step 13: Context Manager Usage\n",
        "\n",
        "Use llcuda with Python context managers for automatic cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "context_manager",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b720e462-1de2-4132-a0b8-375a128560b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CONTEXT MANAGER DEMO\n",
            "======================================================================\n",
            "\n",
            "üì• Loading model in context...\n",
            "Loading model: unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\n",
            "‚úì Using cached model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "Auto-configuring optimal settings...\n",
            "‚úì Auto-configured for 15.0 GB VRAM\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "  Batch Size: 2048\n",
            "  Micro-batch Size: 512\n",
            "\n",
            "Starting llama-server...\n",
            "GPU Check:\n",
            "  Platform: colab\n",
            "  GPU: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Status: ‚úì Compatible\n",
            "Starting llama-server...\n",
            "  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n",
            "  Model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "  Server URL: http://127.0.0.1:8093\n",
            "Waiting for server to be ready..... ‚úì Ready in 2.0s\n",
            "\n",
            "‚úì Model loaded and ready for inference\n",
            "  Server: http://127.0.0.1:8093\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "ü§ñ Running quick test...\n",
            "\n",
            "Response: \n",
            "\n",
            "The capital of France is Paris.\n",
            "\n",
            "Final Answer: The final answer is $\\boxed{Paris}$\n",
            "Speed: 81.6 tok/s\n",
            "\n",
            "‚úÖ Context exited - engine automatically cleaned up\n",
            "   Server stopped, resources released\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Context manager automatically cleans up resources\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONTEXT MANAGER DEMO\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
        "    print(\"üì• Loading model in context...\")\n",
        "    temp_engine.load_model(\n",
        "        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "        silent=True\n",
        "    )\n",
        "\n",
        "    print(\"ü§ñ Running quick test...\\n\")\n",
        "    result = temp_engine.infer(\n",
        "        \"What is the capital of France?\",\n",
        "        max_tokens=30\n",
        "    )\n",
        "\n",
        "    print(f\"Response: {result.text}\")\n",
        "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "print(\"\\n‚úÖ Context exited - engine automatically cleaned up\")\n",
        "print(\"   Server stopped, resources released\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step14"
      },
      "source": [
        "## Step 14: Available Unsloth Models\n",
        "\n",
        "Browse popular Unsloth GGUF models compatible with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "available_models"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë              POPULAR UNSLOTH GGUF MODELS FOR llcuda v2.1.0           ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üîπ SMALL MODELS (1-3B) - Best for T4\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "1. Gemma 3-1B Instruct\n",
        "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
        "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
        "   Speed on T4: ~45 tok/s | VRAM: ~1.2 GB\n",
        "   Use case: General chat, Q&A, reasoning\n",
        "\n",
        "2. Llama 3.2-3B Instruct\n",
        "   Repository: unsloth/Llama-3.2-3B-Instruct-GGUF\n",
        "   File: Llama-3.2-3B-Instruct-Q4_K_M.gguf (~2 GB)\n",
        "   Speed on T4: ~30 tok/s | VRAM: ~2.0 GB\n",
        "   Use case: Instruction following, chat\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "üîπ MEDIUM MODELS (7-8B) - Fits on T4\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "3. Qwen 2.5-7B Instruct\n",
        "   Repository: unsloth/Qwen2.5-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Multilingual, coding, math\n",
        "\n",
        "4. Llama 3.1-8B Instruct\n",
        "   Repository: unsloth/Meta-Llama-3.1-8B-Instruct-GGUF\n",
        "   File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB)\n",
        "   Speed on T4: ~15 tok/s | VRAM: ~5.5 GB\n",
        "   Use case: Advanced reasoning, long context\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "üîπ CODE MODELS\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "5. Qwen 2.5-Coder-7B\n",
        "   Repository: unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Code generation, debugging\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "üí° LOADING SYNTAX:\n",
        "\n",
        "engine.load_model(\"unsloth/REPO-NAME:filename.gguf\")\n",
        "\n",
        "Example:\n",
        "engine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\")\n",
        "\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "\n",
        "üì¶ QUANTIZATION GUIDE:\n",
        "   ‚Ä¢ Q4_K_M: Best balance (recommended for T4)\n",
        "   ‚Ä¢ Q5_K_M: Higher quality, slower\n",
        "   ‚Ä¢ Q8_0: Near full precision, much slower\n",
        "   ‚Ä¢ Q2_K: Smallest, lowest quality\n",
        "\n",
        "üéØ T4 GPU LIMITS:\n",
        "   ‚Ä¢ Total VRAM: 16 GB\n",
        "   ‚Ä¢ Recommended max model: 8B parameters (Q4_K_M)\n",
        "   ‚Ä¢ Leave ~2-3 GB for context and processing\n",
        "\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## üìä Summary\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "‚úÖ **Installed llcuda v2.1.0** from GitHub (GitHub-only distribution)\n",
        "‚úÖ **Auto-downloaded binaries** from GitHub Releases (v2.0.6 - compatible with v2.1.0)\n",
        "‚úÖ **Loaded Gemma 3-1B-IT** GGUF from Unsloth HuggingFace\n",
        "‚úÖ **Ran inference** with ~45 tok/s on Tesla T4\n",
        "‚úÖ **Batch processing** multiple prompts efficiently\n",
        "‚úÖ **Performance analysis** with detailed metrics\n",
        "‚úÖ **Demonstrated workflow** from Unsloth fine-tuning to llcuda deployment\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Results (Tesla T4)\n",
        "\n",
        "| Model | Quantization | Speed | VRAM | Context |\n",
        "|-------|--------------|-------|------|---------|\n",
        "| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB | 2048 |\n",
        "| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB | 4096 |\n",
        "| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB | 8192 |\n",
        "| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB | 8192 |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Features of llcuda v2.1.0\n",
        "\n",
        "üöÄ **GitHub-Only Distribution**\n",
        "- No PyPI dependency\n",
        "- Install: `pip install git+https://github.com/waqasm86/llcuda.git`\n",
        "- Binaries (v2.0.6) auto-download from GitHub Releases - fully compatible with v2.1.0\n",
        "\n",
        "‚ö° **Performance Optimizations**\n",
        "- FlashAttention support (2-3x faster for long contexts)\n",
        "- Tensor Core optimization for SM 7.5 (Tesla T4)\n",
        "- CUDA Graphs for reduced overhead\n",
        "- All quantization formats supported\n",
        "\n",
        "üîÑ **Seamless Unsloth Integration**\n",
        "- Direct loading from Unsloth HuggingFace repos\n",
        "- Compatible with Unsloth fine-tuned GGUF exports\n",
        "- Full llama.cpp ecosystem support\n",
        "\n",
        "---\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **llcuda GitHub**: https://github.com/waqasm86/llcuda\n",
        "- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "- **Releases**: https://github.com/waqasm86/llcuda/releases\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth Models**: https://huggingface.co/unsloth\n",
        "- **Unsloth GGUF Docs**: https://docs.unsloth.ai/basics/saving-to-gguf\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try different models**: Experiment with larger models from Unsloth\n",
        "2. **Fine-tune with Unsloth**: Train on your custom dataset\n",
        "3. **Export to GGUF**: Use Unsloth's export functionality\n",
        "4. **Deploy with llcuda**: Fast inference on your fine-tuned models\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: llcuda v2.1.0 | Tesla T4 | CUDA 12 | Unsloth Integration | FlashAttention\n",
        "\n",
        "**Author**: Waqas Muhammad (waqasm86@gmail.com)\n",
        "**License**: MIT\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qfg2W-94l9nr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "935e3340109d491dae014df275e1e2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8360885b86dc4c989d61ea9ba4b7c948",
              "IPY_MODEL_2720a20f579e4815b23310cf43474167",
              "IPY_MODEL_d3db085068fc4a8784e1af4d3d1dfbc0"
            ],
            "layout": "IPY_MODEL_df50b49a068647bfabb5c76e6f50f10a"
          }
        },
        "8360885b86dc4c989d61ea9ba4b7c948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7541dac8599543878a9ce3de0f2ddd79",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7c8b0d0429345dc9f9bd7d313e8aea1",
            "value": "gemma-3-1b-it-Q4_K_M.gguf:‚Äá100%"
          }
        },
        "2720a20f579e4815b23310cf43474167": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a10b81e26b8404882a898d7d5837585",
            "max": 806058272,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_641f07891d874a709d728f94071e37a2",
            "value": 806058272
          }
        },
        "d3db085068fc4a8784e1af4d3d1dfbc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6db996b332cc4fe58055dd59f4826e94",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1b6d0229269742e0919c068ff46347b7",
            "value": "‚Äá806M/806M‚Äá[00:07&lt;00:00,‚Äá228MB/s]"
          }
        },
        "df50b49a068647bfabb5c76e6f50f10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7541dac8599543878a9ce3de0f2ddd79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c8b0d0429345dc9f9bd7d313e8aea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a10b81e26b8404882a898d7d5837585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641f07891d874a709d728f94071e37a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6db996b332cc4fe58055dd59f4826e94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b6d0229269742e0919c068ff46347b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}