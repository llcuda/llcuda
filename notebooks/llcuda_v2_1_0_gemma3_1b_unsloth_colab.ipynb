{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# llcuda v2.1.0 + Unsloth Gemma 3-1B Tutorial\n\n**Complete Guide**: Using llcuda v2.1.0 with Unsloth GGUF models on Tesla T4 GPU\n\n**What This Demonstrates**:\n1. âœ… Install llcuda v2.1.0 from GitHub (no PyPI)\n2. âœ… Auto-download CUDA binaries from GitHub Releases (v2.0.6 - compatible with v2.1.0)\n3. âœ… Use Gemma 3-1B-IT GGUF from Unsloth\n4. âœ… Fast inference on Tesla T4 with FlashAttention\n5. âœ… Performance benchmarking and optimization\n\n**Requirements**:\n- Google Colab with Tesla T4 GPU\n- Runtime â†’ Change runtime type â†’ T4 GPU\n\n---\n\n## References\n\n- **llcuda v2.1.0**: https://github.com/waqasm86/llcuda\n- **Unsloth**: https://github.com/unslothai/unsloth\n- **Unsloth GGUF Models**: https://huggingface.co/unsloth\n- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": "## Step 1: Verify Tesla T4 GPU\n\nllcuda v2.1.0 uses v2.0.6 binaries optimized for Tesla T4 (SM 7.5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": "# Check GPU configuration\n!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n\nimport subprocess\n\nresult = subprocess.run(\n    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n    capture_output=True, text=True\n)\n\ngpu_info = result.stdout.strip().split(',')\ngpu_name = gpu_info[0].strip()\ncompute_cap = gpu_info[1].strip()\n\nprint(f\"\\n{'='*70}\")\nprint(f\"GPU: {gpu_name}\")\nprint(f\"Compute Capability: SM {compute_cap}\")\nprint(f\"{'='*70}\")\n\nif 'T4' in gpu_name and compute_cap == '7.5':\n    print(\"\\nâœ… Tesla T4 detected - Perfect for llcuda v2.1.0!\")\n    print(\"   Binaries (v2.0.6) include FlashAttention and Tensor Core optimization\")\nelse:\n    print(f\"\\nâš ï¸  {gpu_name} detected\")\n    print(\"   llcuda v2.1.0 is optimized for Tesla T4\")\n    print(\"   Performance may vary on other GPUs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": "## Step 2: Install llcuda v2.1.0 from GitHub\n\n**Note**: llcuda is now **GitHub-only** (no longer on PyPI)\n\nCUDA binaries (v2.0.6, ~266 MB) will auto-download from GitHub Releases on first import. These binaries are fully compatible with v2.1.0 Python APIs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_llcuda"
   },
   "outputs": [],
   "source": "# Install llcuda v2.1.0 from GitHub\nprint(\"ğŸ“¥ Installing llcuda v2.1.0 from GitHub...\\n\")\n\n!pip install -q git+https://github.com/waqasm86/llcuda.git\n\nprint(\"\\nâœ… llcuda v2.1.0 installed successfully!\")\nprint(\"\\nâ„¹ï¸  CUDA binaries (v2.0.6, ~266 MB) will auto-download on first import\")\nprint(\"   These binaries are fully compatible with v2.1.0 Python APIs\")\nprint(\"   Download happens once - subsequent runs use cached binaries\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": "## Step 3: Import llcuda (Triggers Binary Download)\n\nFirst import will download CUDA binaries from:\nhttps://github.com/waqasm86/llcuda/releases/download/v2.0.6/\n\n**Note**: v2.1.0 uses v2.0.6 binaries - they are fully compatible with the new v2.1.0 Python APIs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_llcuda"
   },
   "outputs": [],
   "source": [
    "import llcuda\n",
    "import time\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"llcuda version: {llcuda.__version__}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nâœ… llcuda imported successfully!\")\n",
    "print(\"\\nâ„¹ï¸  If this was the first run:\")\n",
    "print(\"   - CUDA binaries were downloaded to ~/.cache/llcuda/\")\n",
    "print(\"   - Includes: llama-server, libggml-cuda.so, and supporting libs\")\n",
    "print(\"   - Next imports will be instant (binaries cached)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Verify GPU Compatibility\n",
    "\n",
    "Check if llcuda can detect and use the GPU properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_compat"
   },
   "outputs": [],
   "source": [
    "# Check GPU compatibility with llcuda\n",
    "compat = llcuda.check_gpu_compatibility()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"GPU COMPATIBILITY CHECK\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"GPU Name: {compat['gpu_name']}\")\n",
    "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
    "print(f\"Platform: {compat['platform']}\")\n",
    "print(f\"Compatible: {compat['compatible']}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if compat['compatible']:\n",
    "    print(\"\\nâœ… GPU is compatible with llcuda!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  GPU compatibility issue detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "## Step 5: Load Gemma 3-1B-IT from Unsloth\n",
    "\n",
    "**Three methods to load models:**\n",
    "\n",
    "1. **HuggingFace Hub** (recommended): Direct from Unsloth's repo\n",
    "2. **Model Registry**: Pre-configured model names\n",
    "3. **Local Path**: From downloaded GGUF file\n",
    "\n",
    "We'll use Method 1: Direct from Unsloth HuggingFace repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "print(\"\\nğŸ“¥ Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\")\n",
    "print(\"   Repository: unsloth/gemma-3-1b-it-GGUF\")\n",
    "print(\"   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\")\n",
    "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
    "\n",
    "# Load model from Unsloth HuggingFace repository\n",
    "# Format: repo_id:filename\n",
    "start_time = time.time()\n",
    "\n",
    "engine.load_model(\n",
    "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "    silent=True,        # Suppress llama-server output\n",
    "    auto_start=True     # Start server automatically\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ… Model loaded successfully in {load_time:.1f}s!\")\n",
    "print(\"\\nğŸš€ Ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "## Step 6: First Inference - General Knowledge\n",
    "\n",
    "Test the model with a general knowledge question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "first_inference"
   },
   "outputs": [],
   "source": [
    "# Test with a general knowledge prompt\n",
    "prompt = \"Explain quantum computing in simple terms that a beginner can understand.\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROMPT:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(prompt)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nğŸ¤– Generating response...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"RESPONSE:\")\n",
    "print(f\"{'='*70}\")\n",
    "print(result.text)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Performance:\")\n",
    "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
    "print(f\"   Latency: {result.latency_ms:.1f} ms\")\n",
    "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nğŸ’¡ Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7"
   },
   "source": [
    "## Step 7: Code Generation\n",
    "\n",
    "Test the model's ability to generate Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "code_generation"
   },
   "outputs": [],
   "source": [
    "# Test code generation\n",
    "code_prompt = \"\"\"Write a Python function to calculate the fibonacci sequence using dynamic programming.\n",
    "Include docstring and example usage.\"\"\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CODE GENERATION TEST\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prompt: {code_prompt}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(\"\\nğŸ¤– Generating code...\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    code_prompt,\n",
    "    max_tokens=300,\n",
    "    temperature=0.3,  # Lower temperature for more deterministic code\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(result.text)\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8"
   },
   "source": [
    "## Step 8: Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently with batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "batch_inference"
   },
   "outputs": [],
   "source": [
    "# Prepare multiple prompts\n",
    "prompts = [\n",
    "    \"What is machine learning in one sentence?\",\n",
    "    \"Explain neural networks briefly.\",\n",
    "    \"What is the difference between AI and ML?\",\n",
    "    \"Define deep learning concisely.\"\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BATCH INFERENCE - Processing 4 prompts\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
    "    print(f\"\\n{'â”€'*70}\")\n",
    "    print(f\"Query {i}: {prompt}\")\n",
    "    print(f\"{'â”€'*70}\")\n",
    "    print(result.text)\n",
    "    print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tok/s | Latency: {result.latency_ms:.0f}ms\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total batch time: {total_time:.1f}s for {len(prompts)} prompts\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9"
   },
   "source": [
    "## Step 9: Performance Metrics\n",
    "\n",
    "Analyze aggregated performance metrics across all requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "metrics"
   },
   "outputs": [],
   "source": [
    "# Get comprehensive metrics\n",
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERFORMANCE METRICS SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Throughput:\")\n",
    "print(f\"   Total requests: {metrics['throughput']['total_requests']}\")\n",
    "print(f\"   Total tokens: {metrics['throughput']['total_tokens']}\")\n",
    "print(f\"   Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  Latency Distribution:\")\n",
    "print(f\"   Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
    "print(f\"   Median (P50): {metrics['latency']['p50_ms']:.1f} ms\")\n",
    "print(f\"   P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
    "print(f\"   P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
    "print(f\"   Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
    "print(f\"   Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Sample count: {metrics['latency']['sample_count']}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step10"
   },
   "source": [
    "## Step 10: Advanced Generation Parameters\n",
    "\n",
    "Explore different generation strategies and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "advanced_params"
   },
   "outputs": [],
   "source": [
    "# Test creative writing with higher temperature\n",
    "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CREATIVE GENERATION (High Temperature)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prompt: {creative_prompt}\")\n",
    "print(f\"Parameters: temperature=0.9, top_p=0.95, top_k=50\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "result = engine.infer(\n",
    "    creative_prompt,\n",
    "    max_tokens=100,\n",
    "    temperature=0.9,     # High creativity\n",
    "    top_p=0.95,          # Nucleus sampling\n",
    "    top_k=50,            # Top-k sampling\n",
    "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
    ")\n",
    "\n",
    "print(result.text)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step11"
   },
   "source": [
    "## Step 11: Alternative Model Loading Methods\n",
    "\n",
    "Demonstrate other ways to load models with llcuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alternative_loading"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALTERNATIVE MODEL LOADING METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£  HuggingFace Hub (Current method - RECOMMENDED):\")\n",
    "print(\"   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\")\n",
    "print(\"   âœ… Direct from Unsloth repository\")\n",
    "print(\"   âœ… Auto-downloads and caches\")\n",
    "print(\"   âœ… Always up-to-date\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£  Model Registry (Pre-configured shortcuts):\")\n",
    "print(\"   engine.load_model('gemma-3-1b-Q4_K_M')\")\n",
    "print(\"   âœ… Simple one-word names\")\n",
    "print(\"   âœ… Curated model list\")\n",
    "print(\"   âš ï¸  May not include all Unsloth models\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£  Local Path (Pre-downloaded GGUF):\")\n",
    "print(\"   engine.load_model('/path/to/model.gguf')\")\n",
    "print(\"   âœ… Full control over model files\")\n",
    "print(\"   âœ… No network dependency after download\")\n",
    "print(\"   âš ï¸  Manual model management\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: For Unsloth models, use method 1 (HuggingFace Hub)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step12"
   },
   "source": [
    "## Step 12: Unsloth Fine-tuning â†’ llcuda Inference Workflow\n",
    "\n",
    "Example workflow showing how to integrate llcuda with Unsloth fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "workflow_demo"
   },
   "outputs": [],
   "source": "print(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘          UNSLOTH FINE-TUNING â†’ llcuda INFERENCE WORKFLOW             â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSTEP 1: Fine-tune with Unsloth\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfrom unsloth import FastLanguageModel\n\n# Load base model\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"unsloth/gemma-3-1b-it\",\n    max_seq_length=2048,\n    load_in_4bit=True\n)\n\n# Add LoRA adapters\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n)\n\n# Fine-tune on your dataset\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    ...\n)\ntrainer.train()\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nSTEP 2: Export to GGUF\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Save fine-tuned model as GGUF (Q4_K_M quantization)\nmodel.save_pretrained_gguf(\n    \"my_finetuned_model\",\n    tokenizer,\n    quantization_method=\"q4_k_m\"\n)\n\n# This creates: my_finetuned_model/unsloth.Q4_K_M.gguf\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nSTEP 3: Deploy with llcuda v2.1.0\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nimport llcuda\n\n# Load your fine-tuned GGUF model\nengine = llcuda.InferenceEngine()\nengine.load_model(\"my_finetuned_model/unsloth.Q4_K_M.gguf\")\n\n# Run inference\nresult = engine.infer(\"Your task-specific prompt\", max_tokens=200)\nprint(result.text)\nprint(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nâœ… BENEFITS:\n   â€¢ Fast training with Unsloth (2x faster, 70% less VRAM)\n   â€¢ Fast inference with llcuda (FlashAttention, T4-optimized)\n   â€¢ Easy deployment (GGUF = single portable file)\n   â€¢ Full llama.cpp ecosystem compatibility\n   â€¢ Production-ready inference server\n\nğŸ“Š EXPECTED PERFORMANCE (Tesla T4):\n   â€¢ Gemma 3-1B Q4_K_M: ~45 tok/s\n   â€¢ Llama 3.2-3B Q4_K_M: ~30 tok/s\n   â€¢ Qwen 2.5-7B Q4_K_M: ~18 tok/s\n   â€¢ Llama 3.1-8B Q4_K_M: ~15 tok/s\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step13"
   },
   "source": [
    "## Step 13: Context Manager Usage\n",
    "\n",
    "Use llcuda with Python context managers for automatic cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "context_manager"
   },
   "outputs": [],
   "source": [
    "# Context manager automatically cleans up resources\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONTEXT MANAGER DEMO\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
    "    print(\"ğŸ“¥ Loading model in context...\")\n",
    "    temp_engine.load_model(\n",
    "        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
    "        silent=True\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ¤– Running quick test...\\n\")\n",
    "    result = temp_engine.infer(\n",
    "        \"What is the capital of France?\",\n",
    "        max_tokens=30\n",
    "    )\n",
    "\n",
    "    print(f\"Response: {result.text}\")\n",
    "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(\"\\nâœ… Context exited - engine automatically cleaned up\")\n",
    "print(\"   Server stopped, resources released\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step14"
   },
   "source": [
    "## Step 14: Available Unsloth Models\n",
    "\n",
    "Browse popular Unsloth GGUF models compatible with llcuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "available_models"
   },
   "outputs": [],
   "source": "print(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘              POPULAR UNSLOTH GGUF MODELS FOR llcuda v2.1.0           â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ”¹ SMALL MODELS (1-3B) - Best for T4\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n1. Gemma 3-1B Instruct\n   Repository: unsloth/gemma-3-1b-it-GGUF\n   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n   Speed on T4: ~45 tok/s | VRAM: ~1.2 GB\n   Use case: General chat, Q&A, reasoning\n\n2. Llama 3.2-3B Instruct\n   Repository: unsloth/Llama-3.2-3B-Instruct-GGUF\n   File: Llama-3.2-3B-Instruct-Q4_K_M.gguf (~2 GB)\n   Speed on T4: ~30 tok/s | VRAM: ~2.0 GB\n   Use case: Instruction following, chat\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ”¹ MEDIUM MODELS (7-8B) - Fits on T4\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n3. Qwen 2.5-7B Instruct\n   Repository: unsloth/Qwen2.5-7B-Instruct-GGUF\n   File: Qwen2.5-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n   Use case: Multilingual, coding, math\n\n4. Llama 3.1-8B Instruct\n   Repository: unsloth/Meta-Llama-3.1-8B-Instruct-GGUF\n   File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB)\n   Speed on T4: ~15 tok/s | VRAM: ~5.5 GB\n   Use case: Advanced reasoning, long context\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ”¹ CODE MODELS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n5. Qwen 2.5-Coder-7B\n   Repository: unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n   File: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n   Use case: Code generation, debugging\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ’¡ LOADING SYNTAX:\n\nengine.load_model(\"unsloth/REPO-NAME:filename.gguf\")\n\nExample:\nengine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\")\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“¦ QUANTIZATION GUIDE:\n   â€¢ Q4_K_M: Best balance (recommended for T4)\n   â€¢ Q5_K_M: Higher quality, slower\n   â€¢ Q8_0: Near full precision, much slower\n   â€¢ Q2_K: Smallest, lowest quality\n\nğŸ¯ T4 GPU LIMITS:\n   â€¢ Total VRAM: 16 GB\n   â€¢ Recommended max model: 8B parameters (Q4_K_M)\n   â€¢ Leave ~2-3 GB for context and processing\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "## ğŸ“Š Summary\n\n### What We Accomplished\n\nâœ… **Installed llcuda v2.1.0** from GitHub (GitHub-only distribution)\nâœ… **Auto-downloaded binaries** from GitHub Releases (v2.0.6 - compatible with v2.1.0)\nâœ… **Loaded Gemma 3-1B-IT** GGUF from Unsloth HuggingFace\nâœ… **Ran inference** with ~45 tok/s on Tesla T4\nâœ… **Batch processing** multiple prompts efficiently\nâœ… **Performance analysis** with detailed metrics\nâœ… **Demonstrated workflow** from Unsloth fine-tuning to llcuda deployment\n\n---\n\n### Performance Results (Tesla T4)\n\n| Model | Quantization | Speed | VRAM | Context |\n|-------|--------------|-------|------|---------|\n| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB | 2048 |\n| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB | 4096 |\n| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB | 8192 |\n| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB | 8192 |\n\n---\n\n### Key Features of llcuda v2.1.0\n\nğŸš€ **GitHub-Only Distribution**\n- No PyPI dependency\n- Install: `pip install git+https://github.com/waqasm86/llcuda.git`\n- Binaries (v2.0.6) auto-download from GitHub Releases - fully compatible with v2.1.0\n\nâš¡ **Performance Optimizations**\n- FlashAttention support (2-3x faster for long contexts)\n- Tensor Core optimization for SM 7.5 (Tesla T4)\n- CUDA Graphs for reduced overhead\n- All quantization formats supported\n\nğŸ”„ **Seamless Unsloth Integration**\n- Direct loading from Unsloth HuggingFace repos\n- Compatible with Unsloth fine-tuned GGUF exports\n- Full llama.cpp ecosystem support\n\n---\n\n### Resources\n\n- **llcuda GitHub**: https://github.com/waqasm86/llcuda\n- **Installation Guide**: https://github.com/waqasm86/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n- **Releases**: https://github.com/waqasm86/llcuda/releases\n- **Unsloth**: https://github.com/unslothai/unsloth\n- **Unsloth Models**: https://huggingface.co/unsloth\n- **Unsloth GGUF Docs**: https://docs.unsloth.ai/basics/saving-to-gguf\n\n---\n\n### Next Steps\n\n1. **Try different models**: Experiment with larger models from Unsloth\n2. **Fine-tune with Unsloth**: Train on your custom dataset\n3. **Export to GGUF**: Use Unsloth's export functionality\n4. **Deploy with llcuda**: Fast inference on your fine-tuned models\n\n---\n\n**Built with**: llcuda v2.1.0 | Tesla T4 | CUDA 12 | Unsloth Integration | FlashAttention\n\n**Author**: Waqas Muhammad (waqasm86@gmail.com)\n**License**: MIT\n"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}