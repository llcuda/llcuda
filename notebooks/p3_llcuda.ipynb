{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQz6gLY2Izek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VIcn8ciu2omx"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TsyGlz6dxJOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llcuda  > /dev/null"
      ],
      "metadata": {
        "id": "H3KsT6AN4rao"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "7-MwfFDL4rT9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "c1nBP5sUE4x3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L9CW57S9p6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vfIk7Sf3p6nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "82BSWS8gp6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3ufU1wZp6gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llcuda\n",
        "print(\"llcuda imported successfully ‚Äì setup complete if first run!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKkBaL3LE4nG",
        "outputId": "22b9a126-181c-44aa-bcc3-e728f0ccd474"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üéØ llcuda First-Time Setup\n",
            "============================================================\n",
            "\n",
            "üéÆ GPU Detected: Tesla T4 (Compute 7.5)\n",
            "üåê Platform: Colab\n",
            "\n",
            "üì• Downloading optimized binaries from GitHub...\n",
            "   URL: https://github.com/llcuda/llcuda/releases/download/v1.1.7/llcuda-binaries-cuda12.tar.gz\n",
            "   This is a one-time download (~120 MB)\n",
            "\n",
            "Downloading binaries: 100% (161.0/160.9 MB)\n",
            "\n",
            "üì¶ Extracting llcuda-binaries-cuda12.tar.gz...\n",
            "Found 128 files in archive\n",
            "Extracted 128 files to /root/.cache/llcuda/extract\n",
            "‚úÖ Extraction complete!\n",
            "üìÇ Installing binaries and libraries...\n",
            "  Found bin/ and lib/ directories\n",
            "  Copied 99 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n",
            "  Copied 20 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n",
            "‚úÖ Binaries installed successfully!\n",
            "\n",
            "============================================================\n",
            "‚úÖ llcuda Setup Complete!\n",
            "============================================================\n",
            "\n",
            "You can now use llcuda:\n",
            "\n",
            "  import llcuda\n",
            "  engine = llcuda.InferenceEngine()\n",
            "  engine.load_model('gemma-3-1b-Q4_K_M')  # Downloads model on first use\n",
            "  result = engine.infer('What is AI?')\n",
            "\n",
            "llcuda imported successfully ‚Äì setup complete if first run!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuOP4SG4xIEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lUpJrkNBxIBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z2CdZTsdxH7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJtUOlF8xH4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vsjEwctxHyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Directory containing the .so libraries (libmtmd.so.0, libggml-cuda.so, etc.)\n",
        "lib_dir = Path('/usr/local/lib/python3.12/dist-packages/llcuda/lib')\n",
        "\n",
        "if lib_dir.exists():\n",
        "    os.environ['LD_LIBRARY_PATH'] = str(lib_dir) + ':' + os.environ.get('LD_LIBRARY_PATH', '')\n",
        "    print(\"‚úì LD_LIBRARY_PATH set to include llcuda libraries\")\n",
        "    print(f\"LD_LIBRARY_PATH now includes: {lib_dir}\")\n",
        "else:\n",
        "    print(\"Warning: Library directory not found!\")"
      ],
      "metadata": {
        "id": "a3473jM9E7Wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compat = llcuda.check_gpu_compatibility()\n",
        "print(f\"Platform: {compat['platform']}\")\n",
        "print(f\"GPU: {compat['gpu_name']}\")\n",
        "print(f\"Compute Capability: {compat['compute_capability']}\")\n",
        "print(f\"Compatible: {compat['compatible']}\")\n",
        "\n",
        "assert compat['compatible'], \"GPU not compatible!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaePrsxAFrHL",
        "outputId": "3672f4d7-379a-4b81-de5b-84bfb14a93ae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Platform: colab\n",
            "GPU: Tesla T4\n",
            "Compute Capability: 7.5\n",
            "Compatible: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "engine.load_model(\n",
        "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "     silent=True,\n",
        "    gpu_layers=26,\n",
        "    ctx_size=2048\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully! Server is running.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "BBZx4WJBJrYl",
        "outputId": "4a4d357f-8c1e-4196-af9a-8c94a7a0616c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\n",
            "‚úì Using cached model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "Starting llama-server...\n",
            "GPU Check:\n",
            "  Platform: colab\n",
            "  GPU: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Status: ‚úì Compatible\n",
            "Starting llama-server...\n",
            "  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n",
            "  Model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "  GPU Layers: 26\n",
            "  Context Size: 2048\n",
            "  Server URL: http://127.0.0.1:8090\n",
            "Waiting for server to be ready....."
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'read'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3626832317.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceEngine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m engine.load_model(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_name_or_path, gpu_layers, ctx_size, auto_start, auto_configure, n_parallel, verbose, interactive_download, silent, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 )\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 success = self._server_manager.start_server(\n\u001b[0m\u001b[1;32m    396\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                     \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(self, model_path, port, host, gpu_layers, ctx_size, n_parallel, batch_size, ubatch_size, timeout, verbose, skip_gpu_check, silent, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;31m# Check if process died\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 stderr = self.server_process.stderr.read().decode(\n\u001b[0m\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'read'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "prompt = \"Explain quantum entanglement in simple terms.\"\n",
        "\n",
        "result = engine.infer(prompt, max_tokens=300, temperature=0.7)\n",
        "\n",
        "print(\"Response:\")\n",
        "print(result.text.strip())\n",
        "print(\"\\n--- Metrics ---\")\n",
        "print(f\"Tokens generated: {result.tokens_generated}\")\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tokens/second\")"
      ],
      "metadata": {
        "id": "9aXWrwEEJvQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell not executed yet.\n",
        "def chat():\n",
        "    print(\"Chat with Gemma 3 1B (type 'exit' to quit)\\n\")\n",
        "    history = []\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            break\n",
        "        full_prompt = \"\"\n",
        "        for msg in history:\n",
        "            full_prompt += f\"<start_of_turn>{msg['role']}\\n{msg['content']}<end_of_turn>\\n\"\n",
        "        full_prompt += f\"<start_of_turn>user\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "\n",
        "        result = engine.infer(full_prompt, max_tokens=400)\n",
        "        response = result.text.strip()\n",
        "        print(f\"Assistant: {response}\\n\")\n",
        "        history.append({\"role\": \"user\", \"content\": user_input})\n",
        "        history.append({\"role\": \"model\", \"content\": response})\n",
        "\n",
        "# chat()"
      ],
      "metadata": {
        "id": "5lzplxnXKSCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWbQMCwSKa2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}