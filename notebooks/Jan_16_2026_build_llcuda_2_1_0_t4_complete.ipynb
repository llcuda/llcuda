{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOxLSsMZIWxx"
      },
      "source": [
        "# llcuda v2.1.0 - Complete CUDA 12 Build for Tesla T4\n",
        "\n",
        "**Purpose**: Build unified CUDA 12 binaries package for llcuda as Unsloth inference backend\n",
        "\n",
        "**What This Notebook Does**:\n",
        "1. Clone llama.cpp, llcuda repositories\n",
        "2. Build llama.cpp with CUDA 12 + FlashAttention for Tesla T4\n",
        "3. Build llcuda Python package\n",
        "4. Create **ONE unified tar file** with everything\n",
        "5. Download the tar file\n",
        "\n",
        "**Output**: `llcuda-complete-cuda12-t4.tar.gz` (~350-400 MB)\n",
        "\n",
        "**Target**:\n",
        "- GPU: Tesla T4 (SM 7.5)\n",
        "- CUDA: 12.x\n",
        "- Platform: Google Colab\n",
        "- Integration: Unsloth + llcuda\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtYbtrQtIWx5"
      },
      "source": [
        "## Step 1: Verify Environment (Tesla T4 + CUDA 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaTrllhAIWx6",
        "outputId": "1c8ac81a-1a67-4912-8f2d-0d02488d2091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, compute_cap, driver_version, memory.total [MiB]\n",
            "Tesla T4, 7.5, 550.54.15, 15360 MiB\n",
            "\n",
            "============================================================\n",
            "GPU: Tesla T4\n",
            "Compute Capability: SM 7.5\n",
            "============================================================\n",
            "\n",
            "âœ… Perfect! Tesla T4 (SM 7.5) detected\n",
            "   This build will be optimized for your GPU\n"
          ]
        }
      ],
      "source": [
        "# Check GPU - Must be Tesla T4\n",
        "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv\n",
        "\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "gpu_info = result.stdout.strip().split(',')\n",
        "gpu_name = gpu_info[0].strip()\n",
        "compute_cap = gpu_info[1].strip()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Compute Capability: SM {compute_cap}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "if 'T4' in gpu_name and compute_cap == '7.5':\n",
        "    print(\"\\nâœ… Perfect! Tesla T4 (SM 7.5) detected\")\n",
        "    print(\"   This build will be optimized for your GPU\")\n",
        "elif compute_cap == '7.5':\n",
        "    print(f\"\\nâš ï¸  {gpu_name} (SM 7.5) - Should work but not T4\")\n",
        "else:\n",
        "    print(f\"\\nâŒ WARNING: SM {compute_cap} detected\")\n",
        "    print(\"   This notebook is optimized for SM 7.5 (Tesla T4)\")\n",
        "    print(\"   Build may not work optimally on your GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OtaYVeOIWyD",
        "outputId": "434d8d8b-bd25-45bb-dbee-6c269409683b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Expected: 3.10+ (Colab default)\n"
          ]
        }
      ],
      "source": [
        "# Check CUDA version\n",
        "!nvcc --version\n",
        "\n",
        "import sys\n",
        "print(f\"\\nPython: {sys.version}\")\n",
        "print(f\"Expected: 3.10+ (Colab default)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOh8epquIWyF"
      },
      "source": [
        "## Step 2: Install Build Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmqGzscwIWyH",
        "outputId": "d08ff9aa-ceb8-44f5-82d6-879e01842583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "âœ… Build dependencies installed\n"
          ]
        }
      ],
      "source": [
        "# Install build tools\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq build-essential cmake ninja-build git curl\n",
        "\n",
        "# Install Python build dependencies\n",
        "!pip install -q pybind11 setuptools>=77.0.0 wheel build\n",
        "\n",
        "print(\"\\nâœ… Build dependencies installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1ZLAd29IWyK"
      },
      "source": [
        "## Step 3: Clone Repositories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy541eG_IWyM",
        "outputId": "e8e408c5-4a3a-4e05-ec4a-8fed7951236b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¥ Cloning llama.cpp...\n",
            "âœ… llama.cpp cloned\n",
            "\n",
            "ðŸ“¥ Cloning llcuda v2.0.1...\n",
            "âœ… llcuda cloned\n",
            "total 24\n",
            "drwxr-xr-x  1 root root 4096 Jan 15 05:06 .\n",
            "drwxr-xr-x  1 root root 4096 Jan 15 04:57 ..\n",
            "drwxr-xr-x  4 root root 4096 Dec  9 14:41 .config\n",
            "drwxr-xr-x 26 root root 4096 Jan 15 05:06 llama.cpp\n",
            "drwxr-xr-x 11 root root 4096 Jan 15 05:06 llcuda\n",
            "drwxr-xr-x  1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "\n",
        "# Clean up previous builds\n",
        "!rm -rf llama.cpp llcuda\n",
        "\n",
        "# Clone llama.cpp (for inference server)\n",
        "print(\"\\nðŸ“¥ Cloning llama.cpp...\")\n",
        "!git clone -q https://github.com/ggerganov/llama.cpp.git\n",
        "print(\"âœ… llama.cpp cloned\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUe5AIDGV2qA",
        "outputId": "2f63da9b-b37a-4878-8680-99f577c7d205"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTgTLL8xV6cR",
        "outputId": "0b8abf82-c815-401b-a66c-8899d3acf56e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mllama.cpp\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf llcuda"
      ],
      "metadata": {
        "id": "2rw3emfbV6W9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Sem6FFfV6UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone llcuda (CUDA inference backend for Unsloth)\n",
        "print(\"\\nðŸ“¥ Cloning llcuda v2.0.1...\")\n",
        "!git clone -q https://github.com/waqasm86/llcuda.git\n",
        "print(\"âœ… llcuda cloned\")\n",
        "\n",
        "!ls -la"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b08H2KqKVxtB",
        "outputId": "913c8490-a093-4e58-b4ab-e3fc07316bc8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¥ Cloning llcuda v2.0.1...\n",
            "âœ… llcuda cloned\n",
            "total 24\n",
            "drwxr-xr-x  1 root root 4096 Jan 15 06:29 .\n",
            "drwxr-xr-x  1 root root 4096 Jan 15 04:57 ..\n",
            "drwxr-xr-x  4 root root 4096 Dec  9 14:41 .config\n",
            "drwxr-xr-x 27 root root 4096 Jan 15 06:01 llama.cpp\n",
            "drwxr-xr-x 11 root root 4096 Jan 15 06:29 llcuda\n",
            "drwxr-xr-x  1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAA-VQrfIWyO"
      },
      "source": [
        "## Step 4: Build llama.cpp with CUDA 12 + FlashAttention\n",
        "\n",
        "This builds the HTTP inference server with:\n",
        "- CUDA 12 support\n",
        "- FlashAttention enabled\n",
        "- Tensor Core optimization for Tesla T4\n",
        "- All quantization formats (Q4_K_M, Q8_0, NF4, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVpHSwwqIWyP",
        "outputId": "86edb17e-572c-44a7-8103-e84ab71ba216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¨ Configuring llama.cpp for Tesla T4...\n",
            "   - CUDA 12.x\n",
            "   - SM 7.5 (Tesla T4)\n",
            "   - FlashAttention: ON\n",
            "   - CUDA Graphs: ON\n",
            "   - Tensor Cores: Optimized\n",
            "\n",
            "This may take 1-2 minutes...\n",
            "\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "\u001b[33mCMake Warning at CMakeLists.txt:121 (message):\n",
            "  LLAMA_CURL option is deprecated and will be ignored\n",
            "\n",
            "\u001b[0m\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
            "-- CUDA Toolkit found\n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n",
            "-- CUDA host compiler is GNU 11.4.0\n",
            "-- Including CUDA backend\n",
            "-- ggml version: 0.9.5\n",
            "-- ggml commit:  36f013246\n",
            "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
            "-- OpenSSL found: 3.0.2\n",
            "-- Downloading tinyllamas/stories15M-q4_0.gguf from ggml-org/models...\n",
            "-- Generating embedded license file for target: common\n",
            "-- Configuring done (6.7s)\n",
            "-- Generating done (0.4s)\n",
            "-- Build files have been written to: /content/llama.cpp/build_t4\n",
            "\n",
            "âœ… CMake configuration complete\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/llama.cpp')\n",
        "\n",
        "print(\"\\nðŸ”¨ Configuring llama.cpp for Tesla T4...\")\n",
        "print(\"   - CUDA 12.x\")\n",
        "print(\"   - SM 7.5 (Tesla T4)\")\n",
        "print(\"   - FlashAttention: ON\")\n",
        "print(\"   - CUDA Graphs: ON\")\n",
        "print(\"   - Tensor Cores: Optimized\")\n",
        "print(\"\\nThis may take 1-2 minutes...\\n\")\n",
        "\n",
        "!cmake -B build_t4 \\\n",
        "    -DCMAKE_BUILD_TYPE=Release \\\n",
        "    -DGGML_CUDA=ON \\\n",
        "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
        "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n",
        "    -DGGML_NATIVE=OFF \\\n",
        "    -DGGML_CUDA_FORCE_MMQ=OFF \\\n",
        "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\n",
        "    -DGGML_CUDA_FA=ON \\\n",
        "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
        "    -DGGML_CUDA_GRAPHS=ON \\\n",
        "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\n",
        "    -DLLAMA_BUILD_SERVER=ON \\\n",
        "    -DLLAMA_BUILD_TOOLS=ON \\\n",
        "    -DLLAMA_CURL=ON \\\n",
        "    -DBUILD_SHARED_LIBS=ON \\\n",
        "    -DCMAKE_INSTALL_RPATH='$ORIGIN/../lib' \\\n",
        "    -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON\n",
        "\n",
        "print(\"\\nâœ… CMake configuration complete\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw5FlkISIWyQ",
        "outputId": "dcbd3d61-437a-42f1-ac1c-d99ba317fc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¨ Building llama.cpp...\n",
            "   â˜• Good time for a coffee break!\n",
            "\n",
            "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] Built target build_info\n",
            "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  2%] Built target ggml-base\n",
            "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  2%] Built target sha256\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  3%] Built target xxhash\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  3%] Built target sha1\n",
            "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  4%] Built target llama-llava-cli\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  4%] Built target llama-gemma3-cli\n",
            "[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  5%] Built target llama-minicpmv-cli\n",
            "[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  5%] Built target llama-qwen2vl-cli\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[  5%] Built target cpp-httplib\n",
            "[  5%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  9%] Built target ggml-cpu\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q8_0.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_1.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_10.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_11.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_12.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_13.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_14.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_15.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_16.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_2.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_3.cu.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_4.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_5.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_6.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_7.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_8.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmf-instance-ncols_9.cu.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-f16.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_0.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q4_1.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_0.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q5_1.cu.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-f16-q8_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-f16.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q4_1.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_0.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q5_1.cu.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_0-q8_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-f16.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q4_1.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_0.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q5_1.cu.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q4_1-q8_0.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-f16.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_0.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q4_1.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_0.cu.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q5_1.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_0-q8_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-f16.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q4_1.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_0.cu.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q5_1.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q5_1-q8_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-f16.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q4_1.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_0.cu.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q5_1.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-instance-q8_0-q8_0.cu.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CUDA shared library ../../../bin/libggml-cuda.so\u001b[0m\n",
            "[ 36%] Built target ggml-cuda\n",
            "[ 36%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml.so\u001b[0m\n",
            "[ 36%] Built target ggml\n",
            "[ 36%] \u001b[32mBuilding CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-hash\u001b[0m\n",
            "[ 36%] Built target llama-gguf-hash\n",
            "[ 36%] \u001b[32mBuilding CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf\u001b[0m\n",
            "[ 36%] Built target llama-gguf\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-cparams.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-graph.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-io.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-kv-cache-iswa.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model-saver.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/afmoe.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/apertus.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arcee.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arctic.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/arwkv7.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/baichuan.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bailingmoe2.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bert.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bitnet.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/bloom.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chameleon.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/chatglm.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/codeshell.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/cohere2-iswa.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/command-r.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dbrx.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deci.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/deepseek2.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dots1.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/dream.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5-moe.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/ernie4-5.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone4.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/exaone-moe.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon-h1.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/falcon.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma-embedding.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma2-iswa.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gemma3n-iswa.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4-moe.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/glm4.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gpt2.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/gptneox.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite-hybrid.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/granite.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grok.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/grovemoe.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-dense.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/hunyuan-moe.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/internlm2.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jais.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/jamba.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/lfm2.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada-moe.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llada.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama-iswa.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/llama.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/maincoder.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mamba.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mimo2-iswa.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minicpm3.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/minimax-m2.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/modern-bert.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mpt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/nemotron-h.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/neo-bert.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmo2.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/olmoe.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openai-moe-iswa.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/openelm.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/orion.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/pangu-embedded.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi2.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/phi3.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo2.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plamo3.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/plm.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2moe.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3vl-moe.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3moe.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/qwen3next.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/refact.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rnd1.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6-base.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv6qwen2.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7-base.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/rwkv7.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/seed-oss.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smallthinker.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/smollm3.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/stablelm.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/starcoder2.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-dec.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/t5-enc.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/wavtokenizer-dec.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/xverse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/mistral3.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object src/CMakeFiles/llama.dir/models/graph-context-mamba.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX shared library ../bin/libllama.so\u001b[0m\n",
            "[ 59%] Built target llama\n",
            "[ 59%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/arg.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-audio.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/mtmd-helper.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 63%] Built target mtmd\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 65%] Built target test-c\n",
            "[ 65%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 65%] Built target llama-simple\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/debug.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 66%] Built target llama-simple-chat\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 69%] Built target common\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 70%] Built target test-tokenizer-0\n",
            "[ 70%] Built target test-sampling\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 71%] Built target test-grammar-parser\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 72%] Built target test-llama-grammar\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 72%] Built target test-grammar-integration\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 73%] Built target test-json-schema-to-grammar\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 74%] Built target test-quantize-stats\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 74%] Built target test-gbnf-validator\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 74%] Built target test-tokenizer-1-bpe\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 74%] Built target test-tokenizer-1-spm\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 75%] Built target test-chat-parser\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 75%] Built target test-chat\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 76%] Built target test-chat-template\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 76%] Built target test-json-partial\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 76%] Built target test-log\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
            "[ 76%] Built target test-chat-peg-parser\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 76%] Built target test-regex-partial\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 76%] Built target test-thread-safety\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 77%] Built target test-arg-parser\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 78%] Built target test-opt\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 79%] Built target test-gguf\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
            "[ 79%] Built target test-peg-parser\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 80%] Built target test-model-load-cancel\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 81%] Built target test-autorelease\n",
            "[ 81%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n",
            "[ 82%] Built target test-backend-sampler\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
            "[ 83%] Built target test-state-restore-fragmented\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 83%] Built target test-barrier\n",
            "[ 83%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 84%] Built target test-quantize-fns\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 84%] Built target test-quantize-perf\n",
            "[ 84%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 85%] Built target test-rope\n",
            "[ 85%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 85%] Built target test-mtmd-c-api\n",
            "[ 85%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 86%] Built target test-alloc\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 86%] Built target llama-batched\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n",
            "[ 87%] Built target llama-debug\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 87%] Built target llama-embedding\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 88%] Built target llama-eval-callback\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
            "[ 88%] Built target llama-idle\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 88%] Built target test-backend-ops\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 88%] Built target llama-lookahead\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 89%] Built target llama-lookup\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 89%] Built target llama-lookup-merge\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 89%] Built target llama-lookup-create\n",
            "[ 89%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 89%] Built target llama-lookup-stats\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 90%] Built target llama-parallel\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 90%] Built target llama-passkey\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 91%] Built target llama-retrieval\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 91%] Built target llama-save-load-state\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 91%] Built target llama-speculative-simple\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 91%] Built target llama-speculative\n",
            "[ 91%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 92%] Built target llama-gen-docs\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 92%] Built target llama-finetune\n",
            "[ 92%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 92%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 92%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 92%] Built target llama-diffusion-cli\n",
            "[ 92%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 92%] Built target llama-vdot\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 93%] Built target llama-q8dot\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 95%] Built target llama-gguf-split\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 95%] Built target llama-batched-bench\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 95%] Built target llama-imatrix\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 95%] Built target llama-bench\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
            "[ 96%] Built target llama-completion\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 96%] Built target llama-quantize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 96%] Built target llama-perplexity\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 97%] Built target llama-tokenize\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 98%] Built target llama-tts\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 98%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
            "[ 98%] Built target server-context\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 98%] Built target llama-cvector-generator\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 99%] Built target llama-fit-params\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
            "[ 99%] Built target llama-export-lora\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 99%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[ 99%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[ 99%] Built target llama-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "\n",
            "âœ… llama.cpp build complete in 54.2 minutes\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\nðŸ”¨ Building llama.cpp...\")\n",
        "print(\"   â˜• Good time for a coffee break!\\n\")\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "!cmake --build build_t4 --config Release -j$(nproc)\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f\"\\nâœ… llama.cpp build complete in {elapsed/60:.1f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "B7HDfnkAVCpF",
        "outputId": "8f2a6398-90ec-4823-fd3e-fd26dfab8f10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/llama.cpp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwEtT0VPVLDs",
        "outputId": "70269ca9-6ed1-4ef9-c46e-5f6261729321"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AGENTS.md                       \u001b[0m\u001b[01;34mgguf-py\u001b[0m/\n",
            "AUTHORS                         \u001b[01;34mgrammars\u001b[0m/\n",
            "\u001b[01;34mbenches\u001b[0m/                        \u001b[01;34minclude\u001b[0m/\n",
            "\u001b[01;34mbuild_t4\u001b[0m/                       LICENSE\n",
            "\u001b[01;32mbuild-xcframework.sh\u001b[0m*           \u001b[01;34mlicenses\u001b[0m/\n",
            "\u001b[01;34mci\u001b[0m/                             Makefile\n",
            "CLAUDE.md                       \u001b[01;34mmedia\u001b[0m/\n",
            "\u001b[01;34mcmake\u001b[0m/                          \u001b[01;34mmodels\u001b[0m/\n",
            "CMakeLists.txt                  mypy.ini\n",
            "CMakePresets.json               \u001b[01;34mpocs\u001b[0m/\n",
            "CODEOWNERS                      poetry.lock\n",
            "\u001b[01;34mcommon\u001b[0m/                         pyproject.toml\n",
            "CONTRIBUTING.md                 pyrightconfig.json\n",
            "\u001b[01;32mconvert_hf_to_gguf.py\u001b[0m*          README.md\n",
            "\u001b[01;32mconvert_hf_to_gguf_update.py\u001b[0m*   \u001b[01;34mrequirements\u001b[0m/\n",
            "\u001b[01;32mconvert_llama_ggml_to_gguf.py\u001b[0m*  requirements.txt\n",
            "\u001b[01;32mconvert_lora_to_gguf.py\u001b[0m*        \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdocs\u001b[0m/                           SECURITY.md\n",
            "\u001b[01;34mexamples\u001b[0m/                       \u001b[01;34msrc\u001b[0m/\n",
            "flake.lock                      \u001b[01;34mtests\u001b[0m/\n",
            "flake.nix                       \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mggml\u001b[0m/                           \u001b[01;34mvendor\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "from google.colab import files\n",
        "\n",
        "# Define source folder and output archive name\n",
        "source_folder = 'build_t4'\n",
        "output_archive = 'build_t4_2_1_0.tar.gz'\n",
        "\n",
        "# Create the tar.gz archive\n",
        "with tarfile.open(output_archive, \"w:gz\") as tar:\n",
        "    tar.add(source_folder, arcname=os.path.basename(source_folder))\n",
        "\n",
        "print(f\"Archive {output_archive} created successfully!\")\n",
        "\n",
        "# Download the archive to your local machine\n",
        "files.download(output_archive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "w9V1nlulVe0d",
        "outputId": "be14a836-eef2-4a5e-8570-1190018486c8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive build_t4_2_1_0.tar.gz created successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eba9c282-da7e-43ba-b03e-8f4eacfb613d\", \"build_t4_2_1_0.tar.gz\", 263682020)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ja0X78dyVexV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AbLpLdwVesa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPx6KnBNIWyQ",
        "outputId": "9b9ef276-de45-4b8e-c383-3728ab123bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¦ Built binaries:\n",
            "============================================================\n",
            "-rwxr-xr-x 1 root root 6.7M Jan 15 06:00 build_t4/bin/llama-server\n",
            "-rwxr-xr-x 1 root root 5.1M Jan 15 06:00 build_t4/bin/llama-cli\n",
            "-rwxr-xr-x 1 root root 434K Jan 15 05:59 build_t4/bin/llama-quantize\n",
            "\n",
            "ðŸ“š CUDA libraries:\n",
            "============================================================\n",
            "lrwxrwxrwx 1 root root   17 Jan 15 05:50 build_t4/bin/libggml-cuda.so -> libggml-cuda.so.0\n",
            "lrwxrwxrwx 1 root root   21 Jan 15 05:50 build_t4/bin/libggml-cuda.so.0 -> libggml-cuda.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 221M Jan 15 05:50 build_t4/bin/libggml-cuda.so.0.9.5\n",
            "lrwxrwxrwx 1 root root   13 Jan 15 05:52 build_t4/bin/libllama.so -> libllama.so.0\n",
            "lrwxrwxrwx 1 root root   20 Jan 15 05:52 build_t4/bin/libllama.so.0 -> libllama.so.0.0.7739\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 15 05:52 build_t4/bin/libllama.so.0.0.7739\n",
            "\n",
            "ðŸ§ª Testing llama-server...\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "version: 7739 (36f013246)\n",
            "built with GNU 11.4.0 for Linux x86_64\n"
          ]
        }
      ],
      "source": [
        "# Verify binaries\n",
        "print(\"\\nðŸ“¦ Built binaries:\")\n",
        "print(\"=\"*60)\n",
        "!ls -lh build_t4/bin/llama-server\n",
        "!ls -lh build_t4/bin/llama-cli\n",
        "!ls -lh build_t4/bin/llama-quantize\n",
        "\n",
        "print(\"\\nðŸ“š CUDA libraries:\")\n",
        "print(\"=\"*60)\n",
        "!ls -lh build_t4/bin/libggml-cuda.so* | head -3\n",
        "!ls -lh build_t4/bin/libllama.so* | head -3\n",
        "\n",
        "# Test llama-server\n",
        "print(\"\\nðŸ§ª Testing llama-server...\")\n",
        "#!LD_LIBRARY_PATH=/content/llama.cpp/build_t4/bin:/usr/local/cuda/lib64 \\\n",
        "#    /content/llama.cpp/build_t4/bin/llama-server --version\n",
        "\n",
        "!LD_LIBRARY_PATH=/content/llama.cpp/build_t4/bin:/usr/local/cuda/lib64:/usr/lib64-nvidia \\\n",
        "    /content/llama.cpp/build_t4/bin/llama-server --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrfaTg0DYti8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ftShbAIWyR"
      },
      "source": [
        "## Step 5: Build llcuda Python Package\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This creates the llcuda Python package with CUDA backend for Unsloth integration."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "WKXtsJZBWXpw",
        "outputId": "b2e581af-67e9-4d60-973e-7a91257207c0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/llcuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade setuptools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euOiD-K1XHdb",
        "outputId": "6ec14255-8f57-4281-cdb4-b5d6fe470451"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lfZGPr-0XHY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LoC3M_LXHUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "loJfihG7XHOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvMIpeyWIWyR",
        "outputId": "c4e2a7ec-0bfa-41a2-d376-6fa518663912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version = \"2.1.0\"\n",
            "\n",
            "ðŸ”¨ Building llcuda Python package...\n",
            "   This takes ~2-3 minutes\n",
            "\n",
            "\u001b[1m* Getting build dependencies for wheel...\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
            "\n",
            "        By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  corresp(dist, value, root_dir)\n",
            "running egg_info\n",
            "writing llcuda.egg-info/PKG-INFO\n",
            "writing dependency_links to llcuda.egg-info/dependency_links.txt\n",
            "writing requirements to llcuda.egg-info/requires.txt\n",
            "writing top-level names to llcuda.egg-info/top_level.txt\n",
            "reading manifest file 'llcuda.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching '*.py' under directory 'llcuda/core'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/binaries'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/lib'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/models'\n",
            "warning: no previously-included files matching '*.gguf' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.tar.gz' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.so' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.so.*' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
            "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
            "warning: no previously-included files matching '.git*' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.swp' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.bak' found anywhere in distribution\n",
            "warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'llcuda.egg-info/SOURCES.txt'\n",
            "\u001b[1m* Building wheel...\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/setuptools/config/_apply_pyprojecttoml.py:82: SetuptoolsDeprecationWarning: `project.license` as a TOML table is deprecated\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please use a simple string containing a SPDX expression for `project.license`. You can also use `project.license-files`. (Both options available on setuptools>=77.0.0).\n",
            "\n",
            "        By 2026-Feb-18, you need to update your project and remove deprecated calls\n",
            "        or your builds will no longer be supported.\n",
            "\n",
            "        See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  corresp(dist, value, root_dir)\n",
            "running bdist_wheel\n",
            "running build\n",
            "running build_py\n",
            "creating build/lib/core\n",
            "copying core/__init__.py -> build/lib/core\n",
            "creating build/lib/llcuda\n",
            "copying llcuda/utils.py -> build/lib/llcuda\n",
            "copying llcuda/embeddings.py -> build/lib/llcuda\n",
            "copying llcuda/jupyter.py -> build/lib/llcuda\n",
            "copying llcuda/server.py -> build/lib/llcuda\n",
            "copying llcuda/chat.py -> build/lib/llcuda\n",
            "copying llcuda/models.py -> build/lib/llcuda\n",
            "copying llcuda/__init__.py -> build/lib/llcuda\n",
            "copying llcuda/gguf_parser.py -> build/lib/llcuda\n",
            "creating build/lib/llcuda/unsloth\n",
            "copying llcuda/unsloth/adapter.py -> build/lib/llcuda/unsloth\n",
            "copying llcuda/unsloth/exporter.py -> build/lib/llcuda/unsloth\n",
            "copying llcuda/unsloth/loader.py -> build/lib/llcuda/unsloth\n",
            "copying llcuda/unsloth/__init__.py -> build/lib/llcuda/unsloth\n",
            "creating build/lib/llcuda/quantization\n",
            "copying llcuda/quantization/gguf.py -> build/lib/llcuda/quantization\n",
            "copying llcuda/quantization/nf4.py -> build/lib/llcuda/quantization\n",
            "copying llcuda/quantization/dynamic.py -> build/lib/llcuda/quantization\n",
            "copying llcuda/quantization/__init__.py -> build/lib/llcuda/quantization\n",
            "creating build/lib/llcuda/_internal\n",
            "copying llcuda/_internal/bootstrap.py -> build/lib/llcuda/_internal\n",
            "copying llcuda/_internal/registry.py -> build/lib/llcuda/_internal\n",
            "copying llcuda/_internal/__init__.py -> build/lib/llcuda/_internal\n",
            "creating build/lib/llcuda/cuda\n",
            "copying llcuda/cuda/triton_kernels.py -> build/lib/llcuda/cuda\n",
            "copying llcuda/cuda/tensor_core.py -> build/lib/llcuda/cuda\n",
            "copying llcuda/cuda/__init__.py -> build/lib/llcuda/cuda\n",
            "copying llcuda/cuda/graphs.py -> build/lib/llcuda/cuda\n",
            "creating build/lib/llcuda/inference\n",
            "copying llcuda/inference/kv_cache.py -> build/lib/llcuda/inference\n",
            "copying llcuda/inference/flash_attn.py -> build/lib/llcuda/inference\n",
            "copying llcuda/inference/batch.py -> build/lib/llcuda/inference\n",
            "copying llcuda/inference/__init__.py -> build/lib/llcuda/inference\n",
            "running egg_info\n",
            "writing llcuda.egg-info/PKG-INFO\n",
            "writing dependency_links to llcuda.egg-info/dependency_links.txt\n",
            "writing requirements to llcuda.egg-info/requires.txt\n",
            "writing top-level names to llcuda.egg-info/top_level.txt\n",
            "reading manifest file 'llcuda.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "warning: no files found matching '*.py' under directory 'llcuda/core'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/binaries'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/lib'\n",
            "warning: no previously-included files matching '*' found under directory 'llcuda/models'\n",
            "warning: no previously-included files matching '*.gguf' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.tar.gz' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.so' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.so.*' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.pyo' found anywhere in distribution\n",
            "warning: no previously-included files matching '__pycache__' found anywhere in distribution\n",
            "warning: no previously-included files matching '.DS_Store' found anywhere in distribution\n",
            "warning: no previously-included files matching '.git*' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.swp' found anywhere in distribution\n",
            "warning: no previously-included files matching '*.bak' found anywhere in distribution\n",
            "warning: no previously-included files matching '*~' found anywhere in distribution\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'llcuda.egg-info/SOURCES.txt'\n",
            "installing to build/bdist.linux-x86_64/wheel\n",
            "running install\n",
            "running install_lib\n",
            "creating build/bdist.linux-x86_64/wheel\n",
            "creating build/bdist.linux-x86_64/wheel/core\n",
            "copying build/lib/core/__init__.py -> build/bdist.linux-x86_64/wheel/./core\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda/unsloth\n",
            "copying build/lib/llcuda/unsloth/adapter.py -> build/bdist.linux-x86_64/wheel/./llcuda/unsloth\n",
            "copying build/lib/llcuda/unsloth/exporter.py -> build/bdist.linux-x86_64/wheel/./llcuda/unsloth\n",
            "copying build/lib/llcuda/unsloth/loader.py -> build/bdist.linux-x86_64/wheel/./llcuda/unsloth\n",
            "copying build/lib/llcuda/unsloth/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda/unsloth\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda/quantization\n",
            "copying build/lib/llcuda/quantization/gguf.py -> build/bdist.linux-x86_64/wheel/./llcuda/quantization\n",
            "copying build/lib/llcuda/quantization/nf4.py -> build/bdist.linux-x86_64/wheel/./llcuda/quantization\n",
            "copying build/lib/llcuda/quantization/dynamic.py -> build/bdist.linux-x86_64/wheel/./llcuda/quantization\n",
            "copying build/lib/llcuda/quantization/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda/quantization\n",
            "copying build/lib/llcuda/utils.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "copying build/lib/llcuda/embeddings.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda/_internal\n",
            "copying build/lib/llcuda/_internal/bootstrap.py -> build/bdist.linux-x86_64/wheel/./llcuda/_internal\n",
            "copying build/lib/llcuda/_internal/registry.py -> build/bdist.linux-x86_64/wheel/./llcuda/_internal\n",
            "copying build/lib/llcuda/_internal/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda/_internal\n",
            "copying build/lib/llcuda/jupyter.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "copying build/lib/llcuda/server.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "copying build/lib/llcuda/chat.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "copying build/lib/llcuda/models.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda/cuda\n",
            "copying build/lib/llcuda/cuda/triton_kernels.py -> build/bdist.linux-x86_64/wheel/./llcuda/cuda\n",
            "copying build/lib/llcuda/cuda/tensor_core.py -> build/bdist.linux-x86_64/wheel/./llcuda/cuda\n",
            "copying build/lib/llcuda/cuda/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda/cuda\n",
            "copying build/lib/llcuda/cuda/graphs.py -> build/bdist.linux-x86_64/wheel/./llcuda/cuda\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda/inference\n",
            "copying build/lib/llcuda/inference/kv_cache.py -> build/bdist.linux-x86_64/wheel/./llcuda/inference\n",
            "copying build/lib/llcuda/inference/flash_attn.py -> build/bdist.linux-x86_64/wheel/./llcuda/inference\n",
            "copying build/lib/llcuda/inference/batch.py -> build/bdist.linux-x86_64/wheel/./llcuda/inference\n",
            "copying build/lib/llcuda/inference/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda/inference\n",
            "copying build/lib/llcuda/__init__.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "copying build/lib/llcuda/gguf_parser.py -> build/bdist.linux-x86_64/wheel/./llcuda\n",
            "running install_egg_info\n",
            "Copying llcuda.egg-info to build/bdist.linux-x86_64/wheel/./llcuda-2.1.0-py3.12.egg-info\n",
            "running install_scripts\n",
            "creating build/bdist.linux-x86_64/wheel/llcuda-2.1.0.dist-info/WHEEL\n",
            "creating '/content/llcuda/dist/.tmp-cbc1qgsl/llcuda-2.1.0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "adding 'core/__init__.py'\n",
            "adding 'llcuda/__init__.py'\n",
            "adding 'llcuda/chat.py'\n",
            "adding 'llcuda/embeddings.py'\n",
            "adding 'llcuda/gguf_parser.py'\n",
            "adding 'llcuda/jupyter.py'\n",
            "adding 'llcuda/models.py'\n",
            "adding 'llcuda/server.py'\n",
            "adding 'llcuda/utils.py'\n",
            "adding 'llcuda/_internal/__init__.py'\n",
            "adding 'llcuda/_internal/bootstrap.py'\n",
            "adding 'llcuda/_internal/registry.py'\n",
            "adding 'llcuda/cuda/__init__.py'\n",
            "adding 'llcuda/cuda/graphs.py'\n",
            "adding 'llcuda/cuda/tensor_core.py'\n",
            "adding 'llcuda/cuda/triton_kernels.py'\n",
            "adding 'llcuda/inference/__init__.py'\n",
            "adding 'llcuda/inference/batch.py'\n",
            "adding 'llcuda/inference/flash_attn.py'\n",
            "adding 'llcuda/inference/kv_cache.py'\n",
            "adding 'llcuda/quantization/__init__.py'\n",
            "adding 'llcuda/quantization/dynamic.py'\n",
            "adding 'llcuda/quantization/gguf.py'\n",
            "adding 'llcuda/quantization/nf4.py'\n",
            "adding 'llcuda/unsloth/__init__.py'\n",
            "adding 'llcuda/unsloth/adapter.py'\n",
            "adding 'llcuda/unsloth/exporter.py'\n",
            "adding 'llcuda/unsloth/loader.py'\n",
            "adding 'llcuda-2.1.0.dist-info/licenses/LICENSE'\n",
            "adding 'llcuda-2.1.0.dist-info/METADATA'\n",
            "adding 'llcuda-2.1.0.dist-info/WHEEL'\n",
            "adding 'llcuda-2.1.0.dist-info/top_level.txt'\n",
            "adding 'llcuda-2.1.0.dist-info/RECORD'\n",
            "removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[1m\u001b[92mSuccessfully built \u001b[4mllcuda-2.1.0-py3-none-any.whl\u001b[0m\u001b[1m\u001b[92m\u001b[0m\n",
            "\n",
            "âœ… llcuda package built\n",
            "-rw-r--r-- 1 root root 87K Jan 15 06:36 dist/llcuda-2.1.0-py3-none-any.whl\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content/llcuda')\n",
        "\n",
        "# Check llcuda version\n",
        "!cat pyproject.toml | grep \"^version\"\n",
        "\n",
        "print(\"\\nðŸ”¨ Building llcuda Python package...\")\n",
        "print(\"   This takes ~2-3 minutes\\n\")\n",
        "\n",
        "!python3 -m build --wheel --no-isolation\n",
        "\n",
        "print(\"\\nâœ… llcuda package built\")\n",
        "!ls -lh dist/*.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr0eV-oDIWyS"
      },
      "source": [
        "## Step 6: Create Unified Binary Package\n",
        "\n",
        "Creates a single tar file containing:\n",
        "- llama.cpp binaries (llama-server, llama-cli, etc.)\n",
        "- CUDA libraries (libggml-cuda.so with FlashAttention)\n",
        "- llcuda Python wheel\n",
        "- Installation scripts\n",
        "- Documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CZAh5ELIWyS",
        "outputId": "fe30e002-5134-4b82-98d4-2b13ad78e8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¦ Creating unified package structure...\n",
            "\n",
            "ðŸ“¥ Copying llama.cpp binaries...\n",
            "ðŸ“¥ Copying CUDA libraries...\n",
            "ðŸ“¥ Copying llcuda Python package...\n",
            "\n",
            "âœ… Package structure created\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "# Create package structure\n",
        "print(\"\\nðŸ“¦ Creating unified package structure...\")\n",
        "\n",
        "!rm -rf llcuda-complete-t4\n",
        "!mkdir -p llcuda-complete-t4/bin\n",
        "!mkdir -p llcuda-complete-t4/lib\n",
        "!mkdir -p llcuda-complete-t4/python\n",
        "!mkdir -p llcuda-complete-t4/docs\n",
        "\n",
        "# Copy llama.cpp binaries\n",
        "print(\"\\nðŸ“¥ Copying llama.cpp binaries...\")\n",
        "!cp llama.cpp/build_t4/bin/llama-server llcuda-complete-t4/bin/\n",
        "!cp llama.cpp/build_t4/bin/llama-cli llcuda-complete-t4/bin/\n",
        "!cp llama.cpp/build_t4/bin/llama-quantize llcuda-complete-t4/bin/\n",
        "!cp llama.cpp/build_t4/bin/llama-embedding llcuda-complete-t4/bin/\n",
        "!cp llama.cpp/build_t4/bin/llama-bench llcuda-complete-t4/bin/\n",
        "\n",
        "# Copy CUDA libraries\n",
        "print(\"ðŸ“¥ Copying CUDA libraries...\")\n",
        "!cp llama.cpp/build_t4/bin/*.so* llcuda-complete-t4/lib/\n",
        "\n",
        "# Copy llcuda Python wheel\n",
        "print(\"ðŸ“¥ Copying llcuda Python package...\")\n",
        "!cp llcuda/dist/*.whl llcuda-complete-t4/python/\n",
        "\n",
        "# Copy documentation\n",
        "!cp llcuda/README.md llcuda-complete-t4/docs/LLCUDA_README.md 2>/dev/null || echo \"No README found\"\n",
        "\n",
        "print(\"\\nâœ… Package structure created\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create installation script\n",
        "os.chdir('/content/llcuda-complete-t4')\n",
        "\n",
        "# Use r\"\"\" to make this a raw string, preventing Python's SyntaxWarning\n",
        "install_script = r\"\"\"#!/bin/bash\n",
        "# llcuda Complete Installation Script\n",
        "# Tesla T4 (SM 7.5) - CUDA 12.x\n",
        "\n",
        "set -e\n",
        "\n",
        "echo \"====================================================================\"\n",
        "echo \"llcuda Complete Installation - Tesla T4 CUDA 12\"\n",
        "echo \"====================================================================\"\n",
        "echo \"\"\n",
        "\n",
        "# Detect installation directory\n",
        "INSTALL_DIR=\"${1:-$HOME/.local/llcuda}\"\n",
        "echo \"ðŸ“ Installation directory: $INSTALL_DIR\"\n",
        "echo \"\"\n",
        "\n",
        "# Create directories\n",
        "mkdir -p $INSTALL_DIR/bin\n",
        "mkdir -p $INSTALL_DIR/lib\n",
        "\n",
        "# Copy binaries\n",
        "echo \"ðŸ“¥ Installing binaries...\"\n",
        "cp bin/* $INSTALL_DIR/bin/\n",
        "chmod +x $INSTALL_DIR/bin/*\n",
        "\n",
        "# Copy libraries\n",
        "echo \"ðŸ“¥ Installing libraries...\"\n",
        "cp lib/*.so* $INSTALL_DIR/lib/\n",
        "\n",
        "# Install Python package\n",
        "echo \"ðŸ“¥ Installing Python package...\"\n",
        "pip install python/*.whl --force-reinstall\n",
        "\n",
        "# Setup environment\n",
        "echo \"\"\n",
        "echo \"====================================================================\"\n",
        "echo \"âœ… Installation Complete!\"\n",
        "echo \"====================================================================\"\n",
        "echo \"\"\n",
        "echo \"Add these to your ~/.bashrc or session:\"\n",
        "echo \"\"\n",
        "echo \"export PATH=\\\"$INSTALL_DIR/bin:\\$PATH\\\"\"\n",
        "echo \"export LD_LIBRARY_PATH=\\\"$INSTALL_DIR/lib:\\$LD_LIBRARY_PATH\\\"\"\n",
        "echo \"export LLAMA_SERVER_PATH=\\\"$INSTALL_DIR/bin/llama-server\\\"\"\n",
        "echo \"\"\n",
        "echo \"Quick test:\"\n",
        "echo \"  python -c 'import llcuda; print(llcuda.__version__)'\"\n",
        "echo \"\"\n",
        "\"\"\"\n",
        "\n",
        "with open('install.sh', 'w') as f:\n",
        "    f.write(install_script)\n",
        "\n",
        "!chmod +x install.sh\n",
        "print(\"âœ… Installation script created without warnings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLrltmACatir",
        "outputId": "c83792a3-4e1a-40d4-fb91-c158a289817c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Installation script created without warnings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8JLAFuLIWyT",
        "outputId": "2b9d2013-ee99-4fbe-d118-2bb974878536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… README created\n"
          ]
        }
      ],
      "source": [
        "# Create README\n",
        "readme_content = \"\"\"# llcuda Complete - CUDA 12 Binaries for Tesla T4\n",
        "\n",
        "**Version**: 2.0.1\n",
        "**Built**: Google Colab\n",
        "**Target**: Tesla T4 (SM 7.5)\n",
        "**CUDA**: 12.x\n",
        "\n",
        "## Contents\n",
        "\n",
        "```\n",
        "llcuda-complete-t4/\n",
        "â”œâ”€â”€ bin/                    # Binaries\n",
        "â”‚   â”œâ”€â”€ llama-server       # HTTP inference server (6.5 MB)\n",
        "â”‚   â”œâ”€â”€ llama-cli          # CLI tool (4.2 MB)\n",
        "â”‚   â”œâ”€â”€ llama-quantize     # Model quantization\n",
        "â”‚   â”œâ”€â”€ llama-embedding    # Embedding generation\n",
        "â”‚   â””â”€â”€ llama-bench        # Benchmarking\n",
        "â”œâ”€â”€ lib/                    # CUDA Libraries\n",
        "â”‚   â”œâ”€â”€ libggml-cuda.so*   # CUDA kernels with FlashAttention (174 MB)\n",
        "â”‚   â”œâ”€â”€ libggml-base.so*   # Base GGML (721 KB)\n",
        "â”‚   â”œâ”€â”€ libggml-cpu.so*    # CPU fallback (1.1 MB)\n",
        "â”‚   â”œâ”€â”€ libllama.so*       # Llama.cpp core (2.9 MB)\n",
        "â”‚   â””â”€â”€ libmtmd.so*        # Multi-threading (877 KB)\n",
        "â”œâ”€â”€ python/                 # Python Package\n",
        "â”‚   â””â”€â”€ llcuda-*.whl       # llcuda wheel (~70 KB)\n",
        "â”œâ”€â”€ docs/                   # Documentation\n",
        "â”‚   â””â”€â”€ LLCUDA_README.md\n",
        "â”œâ”€â”€ install.sh              # Installation script\n",
        "â””â”€â”€ README.md               # This file\n",
        "```\n",
        "\n",
        "## Features\n",
        "\n",
        "âœ… **FlashAttention 2** - 2-3x faster for long contexts\n",
        "âœ… **Tensor Core Optimization** - FP16/INT8 acceleration\n",
        "âœ… **CUDA Graphs** - Reduced kernel launch overhead\n",
        "âœ… **All Quantization Types** - Q2_K through Q8_0, NF4\n",
        "âœ… **Unsloth Integration** - Backend for Unsloth fine-tuned models\n",
        "\n",
        "## Quick Installation\n",
        "\n",
        "```bash\n",
        "# Extract\n",
        "tar -xzf llcuda-complete-cuda12-t4.tar.gz\n",
        "cd llcuda-complete-t4\n",
        "\n",
        "# Install (default: ~/.local/llcuda)\n",
        "bash install.sh\n",
        "\n",
        "# Or install to custom directory\n",
        "bash install.sh /your/custom/path\n",
        "```\n",
        "\n",
        "## Manual Installation\n",
        "\n",
        "```bash\n",
        "# Copy binaries\n",
        "cp bin/* ~/.local/bin/\n",
        "cp lib/*.so* ~/.local/lib/\n",
        "\n",
        "# Install Python package\n",
        "pip install python/*.whl\n",
        "\n",
        "# Add to environment\n",
        "export PATH=\"$HOME/.local/bin:$PATH\"\n",
        "export LD_LIBRARY_PATH=\"$HOME/.local/lib:$LD_LIBRARY_PATH\"\n",
        "export LLAMA_SERVER_PATH=\"$HOME/.local/bin/llama-server\"\n",
        "```\n",
        "\n",
        "## Usage Examples\n",
        "\n",
        "### 1. Using llcuda with Unsloth GGUF Models\n",
        "\n",
        "```python\n",
        "import llcuda\n",
        "\n",
        "# Initialize engine\n",
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "# Load Unsloth GGUF model\n",
        "engine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n",
        "\n",
        "# Run inference\n",
        "result = engine.infer(\"What is AI?\", max_tokens=100)\n",
        "print(result.text)\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")  # ~45 tok/s on T4\n",
        "```\n",
        "\n",
        "### 2. Using llama-server Directly\n",
        "\n",
        "```bash\n",
        "# Download model\n",
        "wget https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf\n",
        "\n",
        "# Start server\n",
        "llama-server \\\n",
        "    --model gemma-3-1b-it-Q4_K_M.gguf \\\n",
        "    --n-gpu-layers 99 \\\n",
        "    --ctx-size 2048 \\\n",
        "    --port 8090\n",
        "```\n",
        "\n",
        "### 3. Model Registry (Built-in Models)\n",
        "\n",
        "```python\n",
        "import llcuda\n",
        "\n",
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "# Use model registry name\n",
        "engine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads\n",
        "\n",
        "result = engine.infer(\"Hello!\", max_tokens=50)\n",
        "print(result.text)\n",
        "```\n",
        "\n",
        "## Performance (Tesla T4)\n",
        "\n",
        "| Model | Speed | VRAM | Context |\n",
        "|-------|-------|------|--------|\n",
        "| Gemma 3-1B Q4_K_M | 45 tok/s | 1.2 GB | 2048 |\n",
        "| Llama 3.2-3B Q4_K_M | 30 tok/s | 2.0 GB | 4096 |\n",
        "| Qwen 2.5-7B Q4_K_M | 18 tok/s | 5.0 GB | 8192 |\n",
        "| Llama 3.1-8B Q4_K_M | 15 tok/s | 5.5 GB | 8192 |\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- **GPU**: Tesla T4 (SM 7.5) or compatible\n",
        "- **CUDA**: 12.x runtime\n",
        "- **Python**: 3.11+\n",
        "- **OS**: Linux x86_64\n",
        "\n",
        "## Build Information\n",
        "\n",
        "- **llama.cpp**: Built with SM 7.5, FlashAttention, CUDA Graphs\n",
        "- **GGML**: Version 0.9.5\n",
        "- **llcuda**: Version 2.0.1\n",
        "- **Built on**: Google Colab\n",
        "- **Compiler**: GCC + nvcc (CUDA 12)\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Library not found\n",
        "```bash\n",
        "export LD_LIBRARY_PATH=\"$HOME/.local/lib:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\n",
        "```\n",
        "\n",
        "### llama-server: libcuda.so.1 not found\n",
        "```bash\n",
        "# Find libcuda.so.1\n",
        "find /usr -name \"libcuda.so.1\" 2>/dev/null\n",
        "\n",
        "# Add to LD_LIBRARY_PATH\n",
        "export LD_LIBRARY_PATH=\"/path/to/cuda/lib:$LD_LIBRARY_PATH\"\n",
        "```\n",
        "\n",
        "### Import error in Python\n",
        "```bash\n",
        "pip install --force-reinstall python/*.whl\n",
        "```\n",
        "\n",
        "## Links\n",
        "\n",
        "- **llcuda GitHub**: https://github.com/waqasm86/llcuda\n",
        "- **llama.cpp**: https://github.com/ggerganov/llama.cpp\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **PyPI**: https://pypi.org/project/llcuda/\n",
        "\n",
        "## License\n",
        "\n",
        "MIT License - Same as llama.cpp and llcuda\n",
        "\n",
        "---\n",
        "\n",
        "**Built with â¤ï¸ for Unsloth + llcuda integration on Tesla T4**\n",
        "\"\"\"\n",
        "\n",
        "with open('README.md', 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "print(\"âœ… README created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxiX60OqIWyV",
        "outputId": "776f570a-0e13-4070-ac6a-12139a36d8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Metadata file created\n"
          ]
        }
      ],
      "source": [
        "# Create metadata file\n",
        "import datetime\n",
        "\n",
        "metadata = f\"\"\"# Build Metadata\n",
        "\n",
        "Build Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
        "Platform: Google Colab\n",
        "GPU: Tesla T4 (SM 7.5)\n",
        "CUDA: 12.x\n",
        "Python: {sys.version.split()[0]}\n",
        "\n",
        "## Components\n",
        "\n",
        "llama.cpp:\n",
        "  - Branch: main\n",
        "  - Features: FlashAttention, CUDA Graphs, Tensor Cores\n",
        "  - Compute: SM 7.5\n",
        "\n",
        "llcuda:\n",
        "  - Version: 2.0.1\n",
        "  - Repository: https://github.com/waqasm86/llcuda\n",
        "\n",
        "## Build Options\n",
        "\n",
        "CMAKE_BUILD_TYPE: Release\n",
        "GGML_CUDA: ON\n",
        "GGML_CUDA_FA: ON (FlashAttention)\n",
        "GGML_CUDA_FA_ALL_QUANTS: ON\n",
        "GGML_CUDA_GRAPHS: ON\n",
        "CMAKE_CUDA_ARCHITECTURES: 75\n",
        "BUILD_SHARED_LIBS: ON\n",
        "\"\"\"\n",
        "\n",
        "with open('BUILD_INFO.txt', 'w') as f:\n",
        "    f.write(metadata)\n",
        "\n",
        "print(\"âœ… Metadata file created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtwl2Eu7IWyV",
        "outputId": "68b80f01-35b8-404f-b3c9-8143d190aa7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ“¦ Package Contents\n",
            "======================================================================\n",
            "\\nðŸ“ Directory structure:\n",
            "/bin/bash: line 1: tree: command not found\n",
            "-rw-r--r-- 1 root root 4.3K Jan 15 06:37 ./README.md\n",
            "-rwxr-xr-x 1 root root 1.4K Jan 15 06:37 ./install.sh\n",
            "-rw-r--r-- 1 root root 87K Jan 15 06:37 ./python/llcuda-2.1.0-py3-none-any.whl\n",
            "-rwxr-xr-x 1 root root 721K Jan 15 06:37 ./lib/libggml-base.so.0\n",
            "-rwxr-xr-x 1 root root 949K Jan 15 06:37 ./lib/libggml-cpu.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 15 06:37 ./lib/libllama.so.0\n",
            "-rwxr-xr-x 1 root root 54K Jan 15 06:37 ./lib/libggml.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 721K Jan 15 06:37 ./lib/libggml-base.so\n",
            "-rwxr-xr-x 1 root root 894K Jan 15 06:37 ./lib/libmtmd.so.0.0.7739\n",
            "-rwxr-xr-x 1 root root 54K Jan 15 06:37 ./lib/libggml.so.0\n",
            "-rwxr-xr-x 1 root root 949K Jan 15 06:37 ./lib/libggml-cpu.so.0\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 15 06:37 ./lib/libllama.so\n",
            "-rwxr-xr-x 1 root root 894K Jan 15 06:37 ./lib/libmtmd.so\n",
            "-rwxr-xr-x 1 root root 54K Jan 15 06:37 ./lib/libggml.so\n",
            "-rwxr-xr-x 1 root root 2.9M Jan 15 06:37 ./lib/libllama.so.0.0.7739\n",
            "-rwxr-xr-x 1 root root 721K Jan 15 06:37 ./lib/libggml-base.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 949K Jan 15 06:37 ./lib/libggml-cpu.so\n",
            "-rwxr-xr-x 1 root root 221M Jan 15 06:37 ./lib/libggml-cuda.so.0\n",
            "-rwxr-xr-x 1 root root 221M Jan 15 06:37 ./lib/libggml-cuda.so\n",
            "-rwxr-xr-x 1 root root 894K Jan 15 06:37 ./lib/libmtmd.so.0\n",
            "-rwxr-xr-x 1 root root 221M Jan 15 06:37 ./lib/libggml-cuda.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 581K Jan 15 06:37 ./bin/llama-bench\n",
            "-rwxr-xr-x 1 root root 6.7M Jan 15 06:37 ./bin/llama-server\n",
            "-rwxr-xr-x 1 root root 5.1M Jan 15 06:37 ./bin/llama-cli\n",
            "-rwxr-xr-x 1 root root 4.2M Jan 15 06:37 ./bin/llama-embedding\n",
            "-rwxr-xr-x 1 root root 434K Jan 15 06:37 ./bin/llama-quantize\n",
            "-rw-r--r-- 1 root root 13K Jan 15 06:37 ./docs/LLCUDA_README.md\n",
            "-rw-r--r-- 1 root root 519 Jan 15 06:37 ./BUILD_INFO.txt\n",
            "\\nðŸ’¾ Size breakdown:\n",
            "696M\t.\n",
            "17M\tbin/\n",
            "679M\tlib/\n",
            "92K\tpython/\n",
            "\\nðŸ“Š File counts:\n",
            "Binaries: 5\n",
            "Libraries: 18\n",
            "Python packages: 1\n"
          ]
        }
      ],
      "source": [
        "# Show package contents and sizes\n",
        "os.chdir('/content/llcuda-complete-t4')\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“¦ Package Contents\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "!echo \"\\nðŸ“ Directory structure:\"\n",
        "!tree -L 2 -h || find . -maxdepth 2 -type f -exec ls -lh {} \\;\n",
        "\n",
        "!echo \"\\nðŸ’¾ Size breakdown:\"\n",
        "!du -sh .\n",
        "!du -sh bin/\n",
        "!du -sh lib/\n",
        "!du -sh python/\n",
        "\n",
        "!echo \"\\nðŸ“Š File counts:\"\n",
        "!echo \"Binaries: $(ls bin/ | wc -l)\"\n",
        "!echo \"Libraries: $(ls lib/*.so* | wc -l)\"\n",
        "!echo \"Python packages: $(ls python/*.whl | wc -l)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4mNawZ7IWyW"
      },
      "source": [
        "## Step 7: Create Tar Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOD2nCM_IWyW",
        "outputId": "2fb592b4-2b5a-4690-9fbd-2e3d7aa42d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¦ Creating tar archive...\n",
            "   This may take 2-3 minutes...\n",
            "\n",
            "\n",
            "âœ… Tar archive created!\n",
            "\n",
            "======================================================================\n",
            "-rw-r--r-- 1 root root 267M Jan 15 06:38 llcuda-complete-cuda12-t4.tar.gz\n",
            "267M\tllcuda-complete-cuda12-t4.tar.gz\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "os.chdir('/content')\n",
        "\n",
        "print(\"\\nðŸ“¦ Creating tar archive...\")\n",
        "print(\"   This may take 2-3 minutes...\\n\")\n",
        "\n",
        "!tar -czf llcuda-complete-cuda12-t4.tar.gz llcuda-complete-t4/\n",
        "\n",
        "print(\"\\nâœ… Tar archive created!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "!ls -lh llcuda-complete-cuda12-t4.tar.gz\n",
        "!du -h llcuda-complete-cuda12-t4.tar.gz\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rByFC3jHIWyX",
        "outputId": "de2e730e-3415-4404-9c6c-18618186442a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "953b612edcd3b99b66ae169180259de19a6ef5da1df8cdcacbc4b09fd128a5dd  llcuda-complete-cuda12-t4.tar.gz\n",
            "\n",
            "âœ… Checksum generated\n"
          ]
        }
      ],
      "source": [
        "# Generate SHA256 checksum\n",
        "!sha256sum llcuda-complete-cuda12-t4.tar.gz > llcuda-complete-cuda12-t4.tar.gz.sha256\n",
        "!cat llcuda-complete-cuda12-t4.tar.gz.sha256\n",
        "\n",
        "print(\"\\nâœ… Checksum generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWjXKwI6IWyX"
      },
      "source": [
        "## Step 8: Download Package\n",
        "\n",
        "Download the unified tar file to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "UvfEXmI5IWyY",
        "outputId": "8d8528cf-7e8c-4de6-f92b-24737d5df5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "ðŸ“¥ DOWNLOAD READY\n",
            "======================================================================\n",
            "\n",
            "File: llcuda-complete-cuda12-t4.tar.gz\n",
            "Size: ~350-400 MB (compressed)\n",
            "\n",
            "Downloading...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e25dd184-88e5-41fc-b826-b6b59e96b29f\", \"llcuda-complete-cuda12-t4.tar.gz\", 279676584)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Download started!\n",
            "\n",
            "Optionally download checksum:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a30e909c-6bcc-4d9d-a803-58a7ad814704\", \"llcuda-complete-cuda12-t4.tar.gz.sha256\", 99)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ðŸ“¥ DOWNLOAD READY\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nFile: llcuda-complete-cuda12-t4.tar.gz\")\n",
        "print(\"Size: ~350-400 MB (compressed)\")\n",
        "print(\"\\nDownloading...\\n\")\n",
        "\n",
        "files.download('/content/llcuda-complete-cuda12-t4.tar.gz')\n",
        "\n",
        "print(\"\\nâœ… Download started!\")\n",
        "print(\"\\nOptionally download checksum:\")\n",
        "files.download('/content/llcuda-complete-cuda12-t4.tar.gz.sha256')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('/content/llcuda-complete-cuda12-t4.tar.gz'))\n",
        "print(os.path.exists('/content/llcuda-complete-cuda12-t4.tar.gz.sha256'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5EzKV59cf72",
        "outputId": "3f8304e1-b158-46cc-e2ac-2a1e8f46be6e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zRlQ4tzcf4w",
        "outputId": "421bcc83-ce6c-405c-e1aa-4fc02a63c9ed"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mllama.cpp\u001b[0m/  llcuda-complete-cuda12-t4.tar.gz         \u001b[01;34mllcuda-complete-t4\u001b[0m/\n",
            "\u001b[01;34mllcuda\u001b[0m/     llcuda-complete-cuda12-t4.tar.gz.sha256  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#files.download('/content/llcuda-complete-cuda12-t4.tar.gz')\n",
        "#files.download('/content/llcuda-complete-cuda12-t4.tar.gz.sha256')"
      ],
      "metadata": {
        "id": "RGNIy3XTcf1w"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7AOU0Sccfyk",
        "outputId": "5623161a-f800-44ee-83ef-024ec224134a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cuda_info = llcuda.detect_cuda()\n",
        "print(f\"\\nCUDA available: {cuda_info['available']}\")\n",
        "if cuda_info['available'] and cuda_info['gpus']:\n",
        "    print(f\"GPU: {cuda_info['gpus'][0]['name']}\")\n",
        "    print(f\"Compute: SM {cuda_info['gpus'][0]['compute_capability']}\")\n",
        "else:\n",
        "    print(\"No CUDA GPU detected or available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzGewhXXdur9",
        "outputId": "53b9a6ed-1472-4c6b-ac4d-5eb85f895849"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CUDA available: False\n",
            "No CUDA GPU detected or available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUlAAisWdul9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QSm-4ItGduik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKE0_cYoIWyY"
      },
      "source": [
        "## Step 9: Quick Test (Optional)\n",
        "**bold text**\n",
        "Test the build directly in Colab before downloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IBprJOvpIWyY",
        "outputId": "52b81cd0-be98-44af-a961-1aea590e0c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m534.5/534.5 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m134.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.2 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n",
            "/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py:112: RuntimeWarning: llcuda bootstrap failed: cannot import name '__version__' from partially initialized module 'llcuda' (most likely due to a circular import) (/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py)\n",
            "Some features may not work. Please check your installation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llcuda: New version available (2.0.6) - pip install --upgrade git+https://github.com/llcuda/llcuda.git\n",
            "\n",
            "âœ… llcuda version: 2.1.0\n",
            "\n",
            "CUDA available: False\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2286434253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcuda_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nCUDA available: {cuda_info['available']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"GPU: {cuda_info['gpus'][0]['name']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Compute: SM {cuda_info['gpus'][0]['compute_capability']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Install llcuda from the built wheel\n",
        "!pip install -q /content/llcuda-complete-t4/python/*.whl --force-reinstall\n",
        "\n",
        "import llcuda\n",
        "print(f\"\\nâœ… llcuda version: {llcuda.__version__}\")\n",
        "\n",
        "# Set environment\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = '/content/llcuda-complete-t4/lib:/usr/local/cuda/lib64'\n",
        "os.environ['LLAMA_SERVER_PATH'] = '/content/llcuda-complete-t4/bin/llama-server'\n",
        "\n",
        "# Test GPU detection\n",
        "cuda_info = llcuda.detect_cuda()\n",
        "print(f\"\\nCUDA available: {cuda_info['available']}\")\n",
        "print(f\"GPU: {cuda_info['gpus'][0]['name']}\")\n",
        "print(f\"Compute: SM {cuda_info['gpus'][0]['compute_capability']}\")\n",
        "\n",
        "print(\"\\nâœ… llcuda works!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MD3II0redZbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNYXAbYIWyZ"
      },
      "source": [
        "## ðŸŽ‰ Build Complete!\n",
        "\n",
        "### What You Have\n",
        "\n",
        "**File**: `llcuda-complete-cuda12-t4.tar.gz` (~350-400 MB)\n",
        "\n",
        "**Contents**:\n",
        "- âœ… llama.cpp binaries (llama-server with FlashAttention)\n",
        "- âœ… CUDA libraries (libggml-cuda.so, etc.)\n",
        "- âœ… llcuda Python package (wheel)\n",
        "- âœ… Installation script\n",
        "- âœ… Documentation\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Extract on target system**:\n",
        "   ```bash\n",
        "   tar -xzf llcuda-complete-cuda12-t4.tar.gz\n",
        "   cd llcuda-complete-t4\n",
        "   bash install.sh\n",
        "   ```\n",
        "\n",
        "2. **Use with Unsloth**:\n",
        "   ```python\n",
        "   import llcuda\n",
        "   engine = llcuda.InferenceEngine()\n",
        "   engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
        "   result = engine.infer(\"Hello!\", max_tokens=50)\n",
        "   print(result.text)\n",
        "   ```\n",
        "\n",
        "3. **Upload to GitHub Releases** (optional):\n",
        "   ```bash\n",
        "   gh release upload v2.0.1 llcuda-complete-cuda12-t4.tar.gz\n",
        "   ```\n",
        "\n",
        "### Features\n",
        "\n",
        "âœ… Tesla T4 optimized (SM 7.5)\n",
        "âœ… FlashAttention enabled (2-3x faster)\n",
        "âœ… Tensor Core optimization\n",
        "âœ… CUDA 12.x compatible\n",
        "âœ… All quantization formats\n",
        "âœ… Unsloth integration ready\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: Google Colab | Tesla T4 | CUDA 12 | Python 3.10+\n",
        "**For**: llcuda v2.0.1 - Unsloth CUDA Backend\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $LD_LIBRARY_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDJfXWCxX5Bt",
        "outputId": "6bd2c5b0-436c-4b63-e237-22a5323059da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib64-nvidia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPqDanJsX4-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pw1nbf-6X47W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpsXYXN3X44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WCMJZNfZX413"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xQE9_kR0X4y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGFBPPNwX4wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f1omrGXbX4tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ansQ4Uo3X4qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test llama-server with corrected library path\n",
        "print(\"\\nðŸ§ª Testing llama-server...\")\n",
        "!LD_LIBRARY_PATH=/content/llama.cpp/build_t4/bin:/usr/local/cuda/lib64:/usr/lib64-nvidia \\\n",
        "    /content/llama.cpp/build_t4/bin/llama-server --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8LQPauIX4nT",
        "outputId": "1db54800-44dd-4b95-e0db-185d36fe6eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§ª Testing llama-server...\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "version: 7662 (56d2fed2b)\n",
            "built with GNU 11.4.0 for Linux x86_64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify binaries\n",
        "print(\"\\nðŸ“¦ Built binaries:\")\n",
        "print(\"=\"*60)\n",
        "!ls -lh build_t4/bin/llama-server\n",
        "!ls -lh build_t4/bin/llama-cli\n",
        "!ls -lh build_t4/bin/llama-quantize\n",
        "\n",
        "print(\"\\nðŸ“š CUDA libraries:\")\n",
        "print(\"=\"*60)\n",
        "!ls -lh build_t4/bin/libggml-cuda.so* | head -3\n",
        "!ls -lh build_t4/bin/libllama.so* | head -3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RfetsdvX4kb",
        "outputId": "ce8a75a4-d6ad-403b-f6fa-2657d429954e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“¦ Built binaries:\n",
            "============================================================\n",
            "-rwxr-xr-x 1 root root 6.5M Jan  7 16:40 build_t4/bin/llama-server\n",
            "-rwxr-xr-x 1 root root 4.2M Jan  7 16:39 build_t4/bin/llama-cli\n",
            "-rwxr-xr-x 1 root root 434K Jan  7 16:39 build_t4/bin/llama-quantize\n",
            "\n",
            "ðŸ“š CUDA libraries:\n",
            "============================================================\n",
            "lrwxrwxrwx 1 root root   17 Jan  7 16:30 build_t4/bin/libggml-cuda.so -> libggml-cuda.so.0\n",
            "lrwxrwxrwx 1 root root   21 Jan  7 16:30 build_t4/bin/libggml-cuda.so.0 -> libggml-cuda.so.0.9.5\n",
            "-rwxr-xr-x 1 root root 221M Jan  7 16:30 build_t4/bin/libggml-cuda.so.0.9.5\n",
            "lrwxrwxrwx 1 root root   13 Jan  7 16:32 build_t4/bin/libllama.so -> libllama.so.0\n",
            "lrwxrwxrwx 1 root root   20 Jan  7 16:32 build_t4/bin/libllama.so.0 -> libllama.so.0.0.7662\n",
            "-rwxr-xr-x 1 root root 2.9M Jan  7 16:32 build_t4/bin/libllama.so.0.0.7662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB0tKMkTdgdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcq5pStKdgaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvkrgREhdgUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "98K0j3JYdgRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LD_LIBRARY_PATH to include Nvidia drivers\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = '/content/llcuda-complete-t4/lib:/usr/local/cuda/lib64:/usr/lib64-nvidia'\n",
        "os.environ['LLAMA_SERVER_PATH'] = '/content/llcuda-complete-t4/bin/llama-server'\n",
        "\n",
        "# 2. Force reinstall the local wheel again to ensure correct version\n",
        "!pip install -q /content/llcuda-complete-t4/python/*.whl --force-reinstall\n",
        "\n",
        "import llcuda\n",
        "import importlib\n",
        "importlib.reload(llcuda) # Ensure we are using the freshly installed version\n",
        "\n",
        "print(f\"\\nâœ… llcuda version: {llcuda.__version__}\")\n",
        "\n",
        "# 3. Test GPU detection\n",
        "cuda_info = llcuda.detect_cuda()\n",
        "print(f\"\\nCUDA available: {cuda_info['available']}\")\n",
        "\n",
        "if cuda_info['available'] and len(cuda_info['gpus']) > 0:\n",
        "    print(f\"GPU: {cuda_info['gpus'][0]['name']}\")\n",
        "    print(f\"Compute: SM {cuda_info['gpus'][0]['compute_capability']}\")\n",
        "    print(\"\\nâœ… llcuda is correctly linked to Tesla T4!\")\n",
        "else:\n",
        "    print(\"\\nâŒ Error: GPU still not detected. Check !ls -l /usr/lib64-nvidia\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-lFhn6VYgBd",
        "outputId": "6aff7c35-aa17-4926-fb28-d33992428107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.0 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\n",
            "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mllcuda: New version available (2.0.1) - pip install --upgrade llcuda\n",
            "\n",
            "âœ… llcuda version: 1.2.2\n",
            "\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Compute: SM 7.5\n",
            "\n",
            "âœ… llcuda is correctly linked to Tesla T4!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LD_LIBRARY_PATH (Crucial for T4 drivers)\n",
        "import os\n",
        "os.environ['LD_LIBRARY_PATH'] = '/content/llcuda-complete-t4/lib:/usr/local/cuda/lib64:/usr/lib64-nvidia'\n",
        "os.environ['LLAMA_SERVER_PATH'] = '/content/llcuda-complete-t4/bin/llama-server'\n",
        "\n",
        "# 2. Reinstall with a pinned NumPy to satisfy most dependencies\n",
        "# We use --no-warn-conflicts to keep the output clean\n",
        "!pip install -q /content/llcuda-complete-t4/python/*.whl \"numpy<2.2\" --force-reinstall --no-warn-conflicts\n",
        "\n",
        "# 3. Force Python to see the NEW version without restarting the runtime\n",
        "import sys\n",
        "import importlib\n",
        "if 'llcuda' in sys.modules:\n",
        "    importlib.reload(sys.modules['llcuda'])\n",
        "import llcuda\n",
        "\n",
        "print(f\"ðŸŽ¯ Targeted Version: v2.0.1\")\n",
        "print(f\"âœ… Currently Loaded: {llcuda.__version__}\")\n",
        "\n",
        "# 4. Final Verification\n",
        "cuda_info = llcuda.detect_cuda()\n",
        "if cuda_info['available']:\n",
        "    gpu = cuda_info['gpus'][0]\n",
        "    print(f\"\\nðŸš€ GPU Status: {gpu['name']} is READY\")\n",
        "    print(f\"ðŸ“Š Compute Capability: {gpu['compute_capability']}\")\n",
        "    print(f\"ðŸ”— Link Status: Success\")\n",
        "else:\n",
        "    print(\"\\nâŒ Error: GPU detection failed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7ERXy0_dhNQ",
        "outputId": "cb32119a-7a69-429e-90e0-bc49d319bfa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hllcuda: New version available (2.0.1) - pip install --upgrade llcuda\n",
            "ðŸŽ¯ Targeted Version: v2.0.1\n",
            "âœ… Currently Loaded: 1.2.2\n",
            "\n",
            "ðŸš€ GPU Status: Tesla T4 is READY\n",
            "ðŸ“Š Compute Capability: 7.5\n",
            "ðŸ”— Link Status: Success\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MIdNVDCevAO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}