{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llcuda v2.0.1 - Complete CUDA 12 Build for Tesla T4\n",
    "\n",
    "**Purpose**: Build unified CUDA 12 binaries package for llcuda as Unsloth inference backend\n",
    "\n",
    "**What This Notebook Does**:\n",
    "1. Clone llama.cpp, llcuda repositories\n",
    "2. Build llama.cpp with CUDA 12 + FlashAttention for Tesla T4\n",
    "3. Build llcuda Python package\n",
    "4. Create **ONE unified tar file** with everything\n",
    "5. Download the tar file\n",
    "\n",
    "**Output**: `llcuda-complete-cuda12-t4.tar.gz` (~350-400 MB)\n",
    "\n",
    "**Target**:\n",
    "- GPU: Tesla T4 (SM 7.5)\n",
    "- CUDA: 12.x\n",
    "- Platform: Google Colab\n",
    "- Integration: Unsloth + llcuda\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Environment (Tesla T4 + CUDA 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU - Must be Tesla T4\n",
    "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpu_info = result.stdout.strip().split(',')\n",
    "gpu_name = gpu_info[0].strip()\n",
    "compute_cap = gpu_info[1].strip()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Compute Capability: SM {compute_cap}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if 'T4' in gpu_name and compute_cap == '7.5':\n",
    "    print(\"\\nâœ… Perfect! Tesla T4 (SM 7.5) detected\")\n",
    "    print(\"   This build will be optimized for your GPU\")\n",
    "elif compute_cap == '7.5':\n",
    "    print(f\"\\nâš ï¸  {gpu_name} (SM 7.5) - Should work but not T4\")\n",
    "else:\n",
    "    print(f\"\\nâŒ WARNING: SM {compute_cap} detected\")\n",
    "    print(\"   This notebook is optimized for SM 7.5 (Tesla T4)\")\n",
    "    print(\"   Build may not work optimally on your GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA version\n",
    "!nvcc --version\n",
    "\n",
    "import sys\n",
    "print(f\"\\nPython: {sys.version}\")\n",
    "print(f\"Expected: 3.10+ (Colab default)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Build Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install build tools\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq build-essential cmake ninja-build git curl\n",
    "\n",
    "# Install Python build dependencies\n",
    "!pip install -q pybind11 setuptools wheel build\n",
    "\n",
    "print(\"\\nâœ… Build dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clone Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clean up previous builds\n",
    "!rm -rf llama.cpp llcuda\n",
    "\n",
    "# Clone llama.cpp (for inference server)\n",
    "print(\"\\nðŸ“¥ Cloning llama.cpp...\")\n",
    "!git clone -q https://github.com/ggerganov/llama.cpp.git\n",
    "print(\"âœ… llama.cpp cloned\")\n",
    "\n",
    "# Clone llcuda (CUDA inference backend for Unsloth)\n",
    "print(\"\\nðŸ“¥ Cloning llcuda v2.0.1...\")\n",
    "!git clone -q https://github.com/waqasm86/llcuda.git\n",
    "print(\"âœ… llcuda cloned\")\n",
    "\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build llama.cpp with CUDA 12 + FlashAttention\n",
    "\n",
    "This builds the HTTP inference server with:\n",
    "- CUDA 12 support\n",
    "- FlashAttention enabled\n",
    "- Tensor Core optimization for Tesla T4\n",
    "- All quantization formats (Q4_K_M, Q8_0, NF4, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content/llama.cpp')\n",
    "\n",
    "print(\"\\nðŸ”¨ Configuring llama.cpp for Tesla T4...\")\n",
    "print(\"   - CUDA 12.x\")\n",
    "print(\"   - SM 7.5 (Tesla T4)\")\n",
    "print(\"   - FlashAttention: ON\")\n",
    "print(\"   - CUDA Graphs: ON\")\n",
    "print(\"   - Tensor Cores: Optimized\")\n",
    "print(\"\\nThis may take 1-2 minutes...\\n\")\n",
    "\n",
    "!cmake -B build_t4 \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DGGML_CUDA_FORCE_MMQ=OFF \\\n",
    "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\n",
    "    -DGGML_CUDA_FA=ON \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_CUDA_GRAPHS=ON \\\n",
    "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DLLAMA_BUILD_TOOLS=ON \\\n",
    "    -DLLAMA_CURL=ON \\\n",
    "    -DBUILD_SHARED_LIBS=ON \\\n",
    "    -DCMAKE_INSTALL_RPATH='$ORIGIN/../lib' \\\n",
    "    -DCMAKE_BUILD_WITH_INSTALL_RPATH=ON\n",
    "\n",
    "print(\"\\nâœ… CMake configuration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\nðŸ”¨ Building llama.cpp...\")\n",
    "print(\"   This takes ~10-15 minutes on Colab\")\n",
    "print(\"   â˜• Good time for a coffee break!\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "!cmake --build build_t4 --config Release -j$(nproc)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"\\nâœ… llama.cpp build complete in {elapsed/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify binaries\n",
    "print(\"\\nðŸ“¦ Built binaries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh build_t4/bin/llama-server\n",
    "!ls -lh build_t4/bin/llama-cli\n",
    "!ls -lh build_t4/bin/llama-quantize\n",
    "\n",
    "print(\"\\nðŸ“š CUDA libraries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh build_t4/bin/libggml-cuda.so* | head -3\n",
    "!ls -lh build_t4/bin/libllama.so* | head -3\n",
    "\n",
    "# Test llama-server\n",
    "print(\"\\nðŸ§ª Testing llama-server...\")\n",
    "!LD_LIBRARY_PATH=/content/llama.cpp/build_t4/bin:/usr/local/cuda/lib64 \\\n",
    "    /content/llama.cpp/build_t4/bin/llama-server --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build llcuda Python Package\n",
    "\n",
    "This creates the llcuda Python package with CUDA backend for Unsloth integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content/llcuda')\n",
    "\n",
    "# Check llcuda version\n",
    "!cat pyproject.toml | grep \"^version\"\n",
    "\n",
    "print(\"\\nðŸ”¨ Building llcuda Python package...\")\n",
    "print(\"   This takes ~2-3 minutes\\n\")\n",
    "\n",
    "!python -m build --wheel --no-isolation\n",
    "\n",
    "print(\"\\nâœ… llcuda package built\")\n",
    "!ls -lh dist/*.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Unified Binary Package\n",
    "\n",
    "Creates a single tar file containing:\n",
    "- llama.cpp binaries (llama-server, llama-cli, etc.)\n",
    "- CUDA libraries (libggml-cuda.so with FlashAttention)\n",
    "- llcuda Python wheel\n",
    "- Installation scripts\n",
    "- Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "\n",
    "# Create package structure\n",
    "print(\"\\nðŸ“¦ Creating unified package structure...\")\n",
    "\n",
    "!rm -rf llcuda-complete-t4\n",
    "!mkdir -p llcuda-complete-t4/bin\n",
    "!mkdir -p llcuda-complete-t4/lib\n",
    "!mkdir -p llcuda-complete-t4/python\n",
    "!mkdir -p llcuda-complete-t4/docs\n",
    "\n",
    "# Copy llama.cpp binaries\n",
    "print(\"\\nðŸ“¥ Copying llama.cpp binaries...\")\n",
    "!cp llama.cpp/build_t4/bin/llama-server llcuda-complete-t4/bin/\n",
    "!cp llama.cpp/build_t4/bin/llama-cli llcuda-complete-t4/bin/\n",
    "!cp llama.cpp/build_t4/bin/llama-quantize llcuda-complete-t4/bin/\n",
    "!cp llama.cpp/build_t4/bin/llama-embedding llcuda-complete-t4/bin/\n",
    "!cp llama.cpp/build_t4/bin/llama-bench llcuda-complete-t4/bin/\n",
    "\n",
    "# Copy CUDA libraries\n",
    "print(\"ðŸ“¥ Copying CUDA libraries...\")\n",
    "!cp llama.cpp/build_t4/bin/*.so* llcuda-complete-t4/lib/\n",
    "\n",
    "# Copy llcuda Python wheel\n",
    "print(\"ðŸ“¥ Copying llcuda Python package...\")\n",
    "!cp llcuda/dist/*.whl llcuda-complete-t4/python/\n",
    "\n",
    "# Copy documentation\n",
    "!cp llcuda/README.md llcuda-complete-t4/docs/LLCUDA_README.md 2>/dev/null || echo \"No README found\"\n",
    "\n",
    "print(\"\\nâœ… Package structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create installation script\n",
    "os.chdir('/content/llcuda-complete-t4')\n",
    "\n",
    "install_script = \"\"\"#!/bin/bash\n",
    "# llcuda Complete Installation Script\n",
    "# Tesla T4 (SM 7.5) - CUDA 12.x\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"====================================================================\"\n",
    "echo \"llcuda Complete Installation - Tesla T4 CUDA 12\"\n",
    "echo \"====================================================================\"\n",
    "echo \"\"\n",
    "\n",
    "# Detect installation directory\n",
    "INSTALL_DIR=\"${1:-$HOME/.local/llcuda}\"\n",
    "echo \"ðŸ“ Installation directory: $INSTALL_DIR\"\n",
    "echo \"\"\n",
    "\n",
    "# Create directories\n",
    "mkdir -p $INSTALL_DIR/bin\n",
    "mkdir -p $INSTALL_DIR/lib\n",
    "\n",
    "# Copy binaries\n",
    "echo \"ðŸ“¥ Installing binaries...\"\n",
    "cp bin/* $INSTALL_DIR/bin/\n",
    "chmod +x $INSTALL_DIR/bin/*\n",
    "\n",
    "# Copy libraries\n",
    "echo \"ðŸ“¥ Installing libraries...\"\n",
    "cp lib/*.so* $INSTALL_DIR/lib/\n",
    "\n",
    "# Install Python package\n",
    "echo \"ðŸ“¥ Installing Python package...\"\n",
    "pip install python/*.whl --force-reinstall\n",
    "\n",
    "# Setup environment\n",
    "echo \"\"\n",
    "echo \"====================================================================\"\n",
    "echo \"âœ… Installation Complete!\"\n",
    "echo \"====================================================================\"\n",
    "echo \"\"\n",
    "echo \"Add these to your ~/.bashrc or session:\"\n",
    "echo \"\"\n",
    "echo \"export PATH=\\\"$INSTALL_DIR/bin:\\$PATH\\\"\"\n",
    "echo \"export LD_LIBRARY_PATH=\\\"$INSTALL_DIR/lib:\\$LD_LIBRARY_PATH\\\"\"\n",
    "echo \"export LLAMA_SERVER_PATH=\\\"$INSTALL_DIR/bin/llama-server\\\"\"\n",
    "echo \"\"\n",
    "echo \"Quick test:\"\n",
    "echo \"  python -c 'import llcuda; print(llcuda.__version__)'\"\n",
    "echo \"\"\n",
    "\"\"\"\n",
    "\n",
    "with open('install.sh', 'w') as f:\n",
    "    f.write(install_script)\n",
    "\n",
    "!chmod +x install.sh\n",
    "print(\"âœ… Installation script created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create README\n",
    "readme_content = \"\"\"# llcuda Complete - CUDA 12 Binaries for Tesla T4\n",
    "\n",
    "**Version**: 2.0.1\n",
    "**Built**: Google Colab\n",
    "**Target**: Tesla T4 (SM 7.5)\n",
    "**CUDA**: 12.x\n",
    "\n",
    "## Contents\n",
    "\n",
    "```\n",
    "llcuda-complete-t4/\n",
    "â”œâ”€â”€ bin/                    # Binaries\n",
    "â”‚   â”œâ”€â”€ llama-server       # HTTP inference server (6.5 MB)\n",
    "â”‚   â”œâ”€â”€ llama-cli          # CLI tool (4.2 MB)\n",
    "â”‚   â”œâ”€â”€ llama-quantize     # Model quantization\n",
    "â”‚   â”œâ”€â”€ llama-embedding    # Embedding generation\n",
    "â”‚   â””â”€â”€ llama-bench        # Benchmarking\n",
    "â”œâ”€â”€ lib/                    # CUDA Libraries\n",
    "â”‚   â”œâ”€â”€ libggml-cuda.so*   # CUDA kernels with FlashAttention (174 MB)\n",
    "â”‚   â”œâ”€â”€ libggml-base.so*   # Base GGML (721 KB)\n",
    "â”‚   â”œâ”€â”€ libggml-cpu.so*    # CPU fallback (1.1 MB)\n",
    "â”‚   â”œâ”€â”€ libllama.so*       # Llama.cpp core (2.9 MB)\n",
    "â”‚   â””â”€â”€ libmtmd.so*        # Multi-threading (877 KB)\n",
    "â”œâ”€â”€ python/                 # Python Package\n",
    "â”‚   â””â”€â”€ llcuda-*.whl       # llcuda wheel (~70 KB)\n",
    "â”œâ”€â”€ docs/                   # Documentation\n",
    "â”‚   â””â”€â”€ LLCUDA_README.md\n",
    "â”œâ”€â”€ install.sh              # Installation script\n",
    "â””â”€â”€ README.md               # This file\n",
    "```\n",
    "\n",
    "## Features\n",
    "\n",
    "âœ… **FlashAttention 2** - 2-3x faster for long contexts\n",
    "âœ… **Tensor Core Optimization** - FP16/INT8 acceleration\n",
    "âœ… **CUDA Graphs** - Reduced kernel launch overhead\n",
    "âœ… **All Quantization Types** - Q2_K through Q8_0, NF4\n",
    "âœ… **Unsloth Integration** - Backend for Unsloth fine-tuned models\n",
    "\n",
    "## Quick Installation\n",
    "\n",
    "```bash\n",
    "# Extract\n",
    "tar -xzf llcuda-complete-cuda12-t4.tar.gz\n",
    "cd llcuda-complete-t4\n",
    "\n",
    "# Install (default: ~/.local/llcuda)\n",
    "bash install.sh\n",
    "\n",
    "# Or install to custom directory\n",
    "bash install.sh /your/custom/path\n",
    "```\n",
    "\n",
    "## Manual Installation\n",
    "\n",
    "```bash\n",
    "# Copy binaries\n",
    "cp bin/* ~/.local/bin/\n",
    "cp lib/*.so* ~/.local/lib/\n",
    "\n",
    "# Install Python package\n",
    "pip install python/*.whl\n",
    "\n",
    "# Add to environment\n",
    "export PATH=\"$HOME/.local/bin:$PATH\"\n",
    "export LD_LIBRARY_PATH=\"$HOME/.local/lib:$LD_LIBRARY_PATH\"\n",
    "export LLAMA_SERVER_PATH=\"$HOME/.local/bin/llama-server\"\n",
    "```\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### 1. Using llcuda with Unsloth GGUF Models\n",
    "\n",
    "```python\n",
    "import llcuda\n",
    "\n",
    "# Initialize engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load Unsloth GGUF model\n",
    "engine.load_model(\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\")\n",
    "\n",
    "# Run inference\n",
    "result = engine.infer(\"What is AI?\", max_tokens=100)\n",
    "print(result.text)\n",
    "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")  # ~45 tok/s on T4\n",
    "```\n",
    "\n",
    "### 2. Using llama-server Directly\n",
    "\n",
    "```bash\n",
    "# Download model\n",
    "wget https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf\n",
    "\n",
    "# Start server\n",
    "llama-server \\\n",
    "    --model gemma-3-1b-it-Q4_K_M.gguf \\\n",
    "    --n-gpu-layers 99 \\\n",
    "    --ctx-size 2048 \\\n",
    "    --port 8090\n",
    "```\n",
    "\n",
    "### 3. Model Registry (Built-in Models)\n",
    "\n",
    "```python\n",
    "import llcuda\n",
    "\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Use model registry name\n",
    "engine.load_model(\"gemma-3-1b-Q4_K_M\")  # Auto-downloads\n",
    "\n",
    "result = engine.infer(\"Hello!\", max_tokens=50)\n",
    "print(result.text)\n",
    "```\n",
    "\n",
    "## Performance (Tesla T4)\n",
    "\n",
    "| Model | Speed | VRAM | Context |\n",
    "|-------|-------|------|--------|\n",
    "| Gemma 3-1B Q4_K_M | 45 tok/s | 1.2 GB | 2048 |\n",
    "| Llama 3.2-3B Q4_K_M | 30 tok/s | 2.0 GB | 4096 |\n",
    "| Qwen 2.5-7B Q4_K_M | 18 tok/s | 5.0 GB | 8192 |\n",
    "| Llama 3.1-8B Q4_K_M | 15 tok/s | 5.5 GB | 8192 |\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- **GPU**: Tesla T4 (SM 7.5) or compatible\n",
    "- **CUDA**: 12.x runtime\n",
    "- **Python**: 3.11+\n",
    "- **OS**: Linux x86_64\n",
    "\n",
    "## Build Information\n",
    "\n",
    "- **llama.cpp**: Built with SM 7.5, FlashAttention, CUDA Graphs\n",
    "- **GGML**: Version 0.9.5\n",
    "- **llcuda**: Version 2.0.1\n",
    "- **Built on**: Google Colab\n",
    "- **Compiler**: GCC + nvcc (CUDA 12)\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Library not found\n",
    "```bash\n",
    "export LD_LIBRARY_PATH=\"$HOME/.local/lib:/usr/local/cuda/lib64:$LD_LIBRARY_PATH\"\n",
    "```\n",
    "\n",
    "### llama-server: libcuda.so.1 not found\n",
    "```bash\n",
    "# Find libcuda.so.1\n",
    "find /usr -name \"libcuda.so.1\" 2>/dev/null\n",
    "\n",
    "# Add to LD_LIBRARY_PATH\n",
    "export LD_LIBRARY_PATH=\"/path/to/cuda/lib:$LD_LIBRARY_PATH\"\n",
    "```\n",
    "\n",
    "### Import error in Python\n",
    "```bash\n",
    "pip install --force-reinstall python/*.whl\n",
    "```\n",
    "\n",
    "## Links\n",
    "\n",
    "- **llcuda GitHub**: https://github.com/waqasm86/llcuda\n",
    "- **llama.cpp**: https://github.com/ggerganov/llama.cpp\n",
    "- **Unsloth**: https://github.com/unslothai/unsloth\n",
    "- **PyPI**: https://pypi.org/project/llcuda/\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License - Same as llama.cpp and llcuda\n",
    "\n",
    "---\n",
    "\n",
    "**Built with â¤ï¸ for Unsloth + llcuda integration on Tesla T4**\n",
    "\"\"\"\n",
    "\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"âœ… README created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata file\n",
    "import datetime\n",
    "\n",
    "metadata = f\"\"\"# Build Metadata\n",
    "\n",
    "Build Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n",
    "Platform: Google Colab\n",
    "GPU: Tesla T4 (SM 7.5)\n",
    "CUDA: 12.x\n",
    "Python: {sys.version.split()[0]}\n",
    "\n",
    "## Components\n",
    "\n",
    "llama.cpp:\n",
    "  - Branch: main\n",
    "  - Features: FlashAttention, CUDA Graphs, Tensor Cores\n",
    "  - Compute: SM 7.5\n",
    "\n",
    "llcuda:\n",
    "  - Version: 2.0.1\n",
    "  - Repository: https://github.com/waqasm86/llcuda\n",
    "\n",
    "## Build Options\n",
    "\n",
    "CMAKE_BUILD_TYPE: Release\n",
    "GGML_CUDA: ON\n",
    "GGML_CUDA_FA: ON (FlashAttention)\n",
    "GGML_CUDA_FA_ALL_QUANTS: ON\n",
    "GGML_CUDA_GRAPHS: ON\n",
    "CMAKE_CUDA_ARCHITECTURES: 75\n",
    "BUILD_SHARED_LIBS: ON\n",
    "\"\"\"\n",
    "\n",
    "with open('BUILD_INFO.txt', 'w') as f:\n",
    "    f.write(metadata)\n",
    "\n",
    "print(\"âœ… Metadata file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show package contents and sizes\n",
    "os.chdir('/content/llcuda-complete-t4')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“¦ Package Contents\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!echo \"\\nðŸ“ Directory structure:\"\n",
    "!tree -L 2 -h || find . -maxdepth 2 -type f -exec ls -lh {} \\;\n",
    "\n",
    "!echo \"\\nðŸ’¾ Size breakdown:\"\n",
    "!du -sh .\n",
    "!du -sh bin/\n",
    "!du -sh lib/\n",
    "!du -sh python/\n",
    "\n",
    "!echo \"\\nðŸ“Š File counts:\"\n",
    "!echo \"Binaries: $(ls bin/ | wc -l)\"\n",
    "!echo \"Libraries: $(ls lib/*.so* | wc -l)\"\n",
    "!echo \"Python packages: $(ls python/*.whl | wc -l)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Tar Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content')\n",
    "\n",
    "print(\"\\nðŸ“¦ Creating tar archive...\")\n",
    "print(\"   This may take 2-3 minutes...\\n\")\n",
    "\n",
    "!tar -czf llcuda-complete-cuda12-t4.tar.gz llcuda-complete-t4/\n",
    "\n",
    "print(\"\\nâœ… Tar archive created!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "!ls -lh llcuda-complete-cuda12-t4.tar.gz\n",
    "!du -h llcuda-complete-cuda12-t4.tar.gz\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHA256 checksum\n",
    "!sha256sum llcuda-complete-cuda12-t4.tar.gz > llcuda-complete-cuda12-t4.tar.gz.sha256\n",
    "!cat llcuda-complete-cuda12-t4.tar.gz.sha256\n",
    "\n",
    "print(\"\\nâœ… Checksum generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Package\n",
    "\n",
    "Download the unified tar file to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“¥ DOWNLOAD READY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nFile: llcuda-complete-cuda12-t4.tar.gz\")\n",
    "print(\"Size: ~350-400 MB (compressed)\")\n",
    "print(\"\\nDownloading...\\n\")\n",
    "\n",
    "files.download('/content/llcuda-complete-cuda12-t4.tar.gz')\n",
    "\n",
    "print(\"\\nâœ… Download started!\")\n",
    "print(\"\\nOptionally download checksum:\")\n",
    "files.download('/content/llcuda-complete-cuda12-t4.tar.gz.sha256')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Quick Test (Optional)\n",
    "\n",
    "Test the build directly in Colab before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llcuda from the built wheel\n",
    "!pip install -q /content/llcuda-complete-t4/python/*.whl --force-reinstall\n",
    "\n",
    "import llcuda\n",
    "print(f\"\\nâœ… llcuda version: {llcuda.__version__}\")\n",
    "\n",
    "# Set environment\n",
    "import os\n",
    "os.environ['LD_LIBRARY_PATH'] = '/content/llcuda-complete-t4/lib:/usr/local/cuda/lib64'\n",
    "os.environ['LLAMA_SERVER_PATH'] = '/content/llcuda-complete-t4/bin/llama-server'\n",
    "\n",
    "# Test GPU detection\n",
    "cuda_info = llcuda.detect_cuda()\n",
    "print(f\"\\nCUDA available: {cuda_info['available']}\")\n",
    "print(f\"GPU: {cuda_info['gpus'][0]['name']}\")\n",
    "print(f\"Compute: SM {cuda_info['gpus'][0]['compute_capability']}\")\n",
    "\n",
    "print(\"\\nâœ… llcuda works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Build Complete!\n",
    "\n",
    "### What You Have\n",
    "\n",
    "**File**: `llcuda-complete-cuda12-t4.tar.gz` (~350-400 MB)\n",
    "\n",
    "**Contents**:\n",
    "- âœ… llama.cpp binaries (llama-server with FlashAttention)\n",
    "- âœ… CUDA libraries (libggml-cuda.so, etc.)\n",
    "- âœ… llcuda Python package (wheel)\n",
    "- âœ… Installation script\n",
    "- âœ… Documentation\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Extract on target system**:\n",
    "   ```bash\n",
    "   tar -xzf llcuda-complete-cuda12-t4.tar.gz\n",
    "   cd llcuda-complete-t4\n",
    "   bash install.sh\n",
    "   ```\n",
    "\n",
    "2. **Use with Unsloth**:\n",
    "   ```python\n",
    "   import llcuda\n",
    "   engine = llcuda.InferenceEngine()\n",
    "   engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
    "   result = engine.infer(\"Hello!\", max_tokens=50)\n",
    "   print(result.text)\n",
    "   ```\n",
    "\n",
    "3. **Upload to GitHub Releases** (optional):\n",
    "   ```bash\n",
    "   gh release upload v2.0.1 llcuda-complete-cuda12-t4.tar.gz\n",
    "   ```\n",
    "\n",
    "### Features\n",
    "\n",
    "âœ… Tesla T4 optimized (SM 7.5)\n",
    "âœ… FlashAttention enabled (2-3x faster)\n",
    "âœ… Tensor Core optimization\n",
    "âœ… CUDA 12.x compatible\n",
    "âœ… All quantization formats\n",
    "âœ… Unsloth integration ready\n",
    "\n",
    "---\n",
    "\n",
    "**Built with**: Google Colab | Tesla T4 | CUDA 12 | Python 3.10+\n",
    "**For**: llcuda v2.0.1 - Unsloth CUDA Backend\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
