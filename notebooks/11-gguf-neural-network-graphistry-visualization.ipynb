{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sets up environment for GGUF model architecture visualization installing parsing tools and Graphistry for neural network graph rendering."
      ]
    },
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installs llcuda, GGUF file parsers, NetworkX for graph manipulation, and Graphistry for interactive architecture visualization."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# \ud83d\udcca GGUF Neural Network Architecture Visualization with llcuda v2.2.0\n# ==============================================================================\n# Author: Your Name\n# Created: 2026-01-21\n# Platform: Kaggle (2\u00d7 Tesla T4, 30GB VRAM)\n# llcuda v2.2.0 + Graphistry + cuGraph\n# ==============================================================================\n\n# %% [markdown]\n# # GGUF Neural Network Architecture Visualization\n# \n# **llcuda v2.2.0** | Kaggle 2\u00d7 Tesla T4 (30GB Total VRAM) | Graphistry Dashboard\n# \n# ## \ud83c\udfaf Objective\n# \n# Visualize the internal architecture of quantized GGUF models using llcuda v2.2.0:\n# \n# ### Architecture:\n# ```\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  GPU 0: llama-server (LLM Inference)                   \u2502\n# \u2502         tensor_split=\"1.0,0.0\"                         \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n#                          \u2193\n#               Extract Model Architecture Data\n#                          \u2193\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502  GPU 1: RAPIDS + Graphistry                            \u2502\n# \u2502         Architecture Graph + Interactive Dashboard     \u2502\n# \u2502         CUDA_VISIBLE_DEVICES=\"1\"                       \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n# ```\n# \n# ### What We Visualize:\n# 1. **Layer Connectivity**: Transformer blocks and connections\n# 2. **Attention Heads**: Multi-head attention structure\n# 3. **Quantization Layers**: GGUF quantization blocks\n# 4. **Memory Usage**: VRAM allocation per component\n# 5. **Parameter Distribution**: Model weights across layers\n# \n# ### Models Supported:\n# - Gemma-2-2B (Q4_K_M)\n# - Llama-3.2-3B (Q4_K_M)\n# - Custom GGUF models\n\n# %% [markdown]\n# ## Step 0: Add Graphistry Secrets in Kaggle\n# \n# Go to **Kaggle \u2192 Settings \u2192 Add-ons \u2192 Secrets** and add:\n# - `Graphistry_Personal_Key_ID`\n# - `Graphistry_Personal_Secret_Key`",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:23:55.995329Z",
          "iopub.execute_input": "2026-01-21T08:23:55.995962Z",
          "iopub.status.idle": "2026-01-21T08:23:56.001068Z",
          "shell.execute_reply.started": "2026-01-21T08:23:55.995929Z",
          "shell.execute_reply": "2026-01-21T08:23:55.999998Z"
        }
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loads GGUF model file and parses metadata extracting architecture details, layer information, and tensor specifications."
      ]
    },
    {
      "cell_type": "code",
      "source": "from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\nsecret_value_1 = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\nsecret_value_2 = user_secrets.get_secret(\"HF_TOKEN\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:14:10.248266Z",
          "iopub.execute_input": "2026-01-21T08:14:10.248706Z",
          "iopub.status.idle": "2026-01-21T08:14:10.476557Z",
          "shell.execute_reply.started": "2026-01-21T08:14:10.248671Z",
          "shell.execute_reply": "2026-01-21T08:14:10.475893Z"
        }
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracts complete neural network architecture from GGUF including layer types, dimensions, connections, and parameter counts."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 1: Verify Dual GPU Environment\n# ==============================================================================\n\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"\ud83d\udd0d SPLIT-GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\n\ud83d\udcca Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\n\u2705 Dual T4 ready for split-GPU operation!\")\n    print(\"   GPU 0 \u2192 llama-server (GGUF model inference)\")\n    print(\"   GPU 1 \u2192 RAPIDS/Graphistry (architecture visualization)\")\nelse:\n    print(\"\\n\u26a0\ufe0f Need 2 GPUs for split operation\")\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:14:32.013446Z",
          "iopub.execute_input": "2026-01-21T08:14:32.013756Z",
          "iopub.status.idle": "2026-01-21T08:14:32.050277Z",
          "shell.execute_reply.started": "2026-01-21T08:14:32.013729Z",
          "shell.execute_reply": "2026-01-21T08:14:32.049622Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\udd0d SPLIT-GPU ENVIRONMENT CHECK\n======================================================================\n\n\ud83d\udcca Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 15096 MiB\n   1, Tesla T4, 15360 MiB, 15096 MiB\n\n\u2705 Dual T4 ready for split-GPU operation!\n   GPU 0 \u2192 llama-server (GGUF model inference)\n   GPU 1 \u2192 RAPIDS/Graphistry (architecture visualization)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Builds graph representation of neural network where nodes are layers/operations and edges represent tensor data flow."
      ]
    },
    {
      "cell_type": "code",
      "source": "%%time",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzes layer composition identifying attention blocks, MLP layers, normalization, and embedding components in the architecture."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 2: Install Dependencies\n# ==============================================================================\n\nprint(\"\ud83d\udce6 Installing dependencies...\")\n\n# Install llcuda v2.2.0\n!pip install -q --no-cache-dir git+https://github.com/llcuda/llcuda.git@v2.2.0\n\n# Install cuGraph for GPU-accelerated graph algorithms\n!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n\n# Install Graphistry for visualization\n!pip install -q \"graphistry[ai]\"\n\n# Install additional utilities\n!pip install -q pyarrow pandas numpy scipy\n\n# Verify installations\nimport llcuda\nprint(f\"\\n\u2705 llcuda {llcuda.__version__} installed\")\n\ntry:\n    import cudf, cugraph\n    print(f\"\u2705 cuDF {cudf.__version__}\")\n    print(f\"\u2705 cuGraph {cugraph.__version__}\")\nexcept ImportError as e:\n    print(f\"\u26a0\ufe0f RAPIDS: {e}\")\n\ntry:\n    import graphistry\n    print(f\"\u2705 Graphistry {graphistry.__version__}\")\nexcept ImportError as e:\n    print(f\"\u26a0\ufe0f Graphistry: {e}\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:14:55.908183Z",
          "iopub.execute_input": "2026-01-21T08:14:55.909029Z",
          "iopub.status.idle": "2026-01-21T08:16:24.613685Z",
          "shell.execute_reply.started": "2026-01-21T08:14:55.908996Z",
          "shell.execute_reply": "2026-01-21T08:16:24.612790Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\ud83d\udce6 Installing dependencies...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n======================================================================\n\ud83c\udfaf llcuda v2.2.0 First-Time Setup - Kaggle 2\u00d7 T4 Multi-GPU\n======================================================================\n\n\ud83c\udfae GPU Detected: Tesla T4 (Compute 7.5)\n  \u2705 Tesla T4 detected - Perfect for llcuda v2.1!\n\ud83c\udf10 Platform: Colab\n\n\ud83d\udce6 Downloading Kaggle 2\u00d7 T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n\u27a1\ufe0f  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\n\ud83d\udce5 Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(\u2026):   0%|          | 0.00/1.01G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07638cce46e64f94902f2a74512f0c44"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\ud83d\udd10 Verifying SHA256 checksum...\n   \u2705 Checksum verified\n\ud83d\udce6 Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\n\u2705 Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n\u2705 Binaries installed successfully!\n\n\n\u2705 llcuda 2.2.0 installed\n\u2705 cuDF 25.06.00\n\u2705 cuGraph 25.06.00\n\u2705 Graphistry 0.50.4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computes layer-wise statistics including parameter counts, FLOPs, memory requirements, and computational complexity."
      ]
    },
    {
      "cell_type": "code",
      "source": "# First, let's see what's actually available in llcuda\nimport llcuda\nprint(f\"llcuda version: {llcuda.__version__}\")\nprint(\"\\nAvailable attributes in llcuda:\")\nprint([attr for attr in dir(llcuda) if not attr.startswith('_')])",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:57:34.319013Z",
          "iopub.execute_input": "2026-01-21T08:57:34.319327Z",
          "iopub.status.idle": "2026-01-21T08:57:34.324262Z",
          "shell.execute_reply.started": "2026-01-21T08:57:34.319301Z",
          "shell.execute_reply": "2026-01-21T08:57:34.323324Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "llcuda version: 2.2.0\n\nAvailable attributes in llcuda:\n['Any', 'Dict', 'InferResult', 'InferenceEngine', 'List', 'Optional', 'Path', 'ServerManager', 'api', 'bootstrap', 'check_cuda_available', 'check_gpu_compatibility', 'create_config_file', 'detect_cuda', 'find_gguf_models', 'get_cuda_device_info', 'get_llama_cpp_cuda_path', 'get_recommended_gpu_layers', 'load_config', 'logging', 'os', 'print_system_info', 'quick_infer', 'requests', 'server', 'setup_environment', 'subprocess', 'sys', 'time', 'utils', 'validate_model_path']\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loads and parses GGUF model file extracting architecture metadata, layer information, tensor shapes, and model structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Traces tensor shapes through the network showing how data dimensions transform from input through each layer to output."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 3: Download GGUF Model (Fixed - No GGUF Parsing Errors)\n# ==============================================================================\n\nfrom huggingface_hub import hf_hub_download\nimport os\n\nMODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\nMODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n\nprint(f\"\ud83d\udce5 Downloading {MODEL_FILE}...\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\n\u2705 Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")\n\n# Show file exists\nprint(f\"\\n\ud83d\udcc1 File verification:\")\nprint(f\"   File exists: {os.path.exists(model_path)}\")\nprint(f\"   File size: {size_gb:.2f} GB\")\n\n# Instead of parsing GGUF, use known architecture for Llama-3.2-3B\nprint(\"\\n\ud83d\udd0d Using known architecture for Llama-3.2-3B:\")\n\n# Known architecture for Llama-3.2-3B\nARCHITECTURE = {\n    'model': 'Llama-3.2-3B-Instruct',\n    'format': 'GGUF Q4_K_M',\n    'layers': 28,                 # Number of transformer blocks\n    'attention_heads': 32,        # Attention heads per layer\n    'hidden_dimension': 3072,     # Model dimension\n    'vocabulary_size': 128256,    # Token vocabulary\n    'context_length': 8192,       # Max context length\n    'feedforward_multiplier': 4,  # FFN is 4\u00d7 hidden_dim (Swiglu)\n    'quantization': 'Q4_K_M',     # Quantization type\n    'estimated_params': 2.8e9,    # Approximately 2.8 billion parameters\n    'file_size_gb': 1.88,         # Actual file size\n    'attention_dim_per_head': 96, # 3072 / 32 = 96\n    'rope_theta': 500000,         # RoPE base frequency\n}\n\nprint(\"\\n\ud83d\udcca Architecture Summary:\")\nfor key, value in ARCHITECTURE.items():\n    if isinstance(value, (int, float)) and value >= 1000:\n        print(f\"   {key}: {value:,}\")\n    else:\n        print(f\"   {key}: {value}\")\n\n# Derived calculations\nprint(\"\\n\ud83e\uddee Derived Architecture Values:\")\nn_layers = ARCHITECTURE['layers']\nn_heads = ARCHITECTURE['attention_heads']\nhidden_dim = ARCHITECTURE['hidden_dimension']\nvocab_size = ARCHITECTURE['vocabulary_size']\n\nprint(f\"   Total transformer layers: {n_layers}\")\nprint(f\"   Total attention heads: {n_layers} \u00d7 {n_heads} = {n_layers * n_heads:,}\")\nprint(f\"   Attention dimension per head: {hidden_dim} \u00f7 {n_heads} = {hidden_dim // n_heads}\")\nprint(f\"   Feed-forward hidden dimension: {hidden_dim} \u00d7 {ARCHITECTURE['feedforward_multiplier']} = {hidden_dim * ARCHITECTURE['feedforward_multiplier']:,}\")\n\n# Parameter breakdown (simplified)\nprint(\"\\n\ud83d\udcc8 Parameter Distribution (Approximate):\")\nembedding_params = vocab_size * hidden_dim\nattention_params = 4 * hidden_dim * hidden_dim * n_layers  # Q, K, V, O\nffn_params = 2 * 4 * hidden_dim * hidden_dim * n_layers    # FFN (Swiglu)\noutput_params = hidden_dim * vocab_size                    # Output layer\ntotal_params = embedding_params + attention_params + ffn_params + output_params\n\nprint(f\"   Embedding layer: {embedding_params:,} ({embedding_params/total_params*100:.1f}%)\")\nprint(f\"   Attention layers: {attention_params:,} ({attention_params/total_params*100:.1f}%)\")\nprint(f\"   Feed-forward layers: {ffn_params:,} ({ffn_params/total_params*100:.1f}%)\")\nprint(f\"   Output layer: {output_params:,} ({output_params/total_params*100:.1f}%)\")\nprint(f\"   Total estimated: {total_params:,} parameters\")\n\n# Quantization impact\nprint(f\"\\n\u2696\ufe0f Quantization Impact (Q4_K_M):\")\nfull_precision_gb = (total_params * 4) / (1024**3)  # 4 bytes per float32\nquantized_gb = size_gb\ncompression_ratio = full_precision_gb / quantized_gb\n\nprint(f\"   Full precision (FP32): {full_precision_gb:.1f} GB\")\nprint(f\"   Quantized (Q4_K_M): {quantized_gb:.1f} GB\")\nprint(f\"   Compression ratio: {compression_ratio:.1f}\u00d7\")\nprint(f\"   Average bits per parameter: {32 / compression_ratio:.1f} bits\")\n\nprint(f\"\\n\u2705 Architecture ready for visualization\")\nprint(f\"   Will visualize: {n_layers} layers \u00d7 {n_heads} heads = {n_layers * n_heads:,} attention heads\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:22:05.877720Z",
          "iopub.execute_input": "2026-01-21T08:22:05.878645Z",
          "iopub.status.idle": "2026-01-21T08:22:05.963531Z",
          "shell.execute_reply.started": "2026-01-21T08:22:05.878605Z",
          "shell.execute_reply": "2026-01-21T08:22:05.962786Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\ud83d\udce5 Downloading Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n\n\u2705 Model downloaded: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   Size: 1.88 GB\n\n\ud83d\udcc1 File verification:\n   File exists: True\n   File size: 1.88 GB\n\n\ud83d\udd0d Using known architecture for Llama-3.2-3B:\n\n\ud83d\udcca Architecture Summary:\n   model: Llama-3.2-3B-Instruct\n   format: GGUF Q4_K_M\n   layers: 28\n   attention_heads: 32\n   hidden_dimension: 3,072\n   vocabulary_size: 128,256\n   context_length: 8,192\n   feedforward_multiplier: 4\n   quantization: Q4_K_M\n   estimated_params: 2,800,000,000.0\n   file_size_gb: 1.88\n   attention_dim_per_head: 96\n   rope_theta: 500,000\n\n\ud83e\uddee Derived Architecture Values:\n   Total transformer layers: 28\n   Total attention heads: 28 \u00d7 32 = 896\n   Attention dimension per head: 3072 \u00f7 32 = 96\n   Feed-forward hidden dimension: 3072 \u00d7 4 = 12,288\n\n\ud83d\udcc8 Parameter Distribution (Approximate):\n   Embedding layer: 394,002,432 (10.0%)\n   Attention layers: 1,056,964,608 (26.7%)\n   Feed-forward layers: 2,113,929,216 (53.4%)\n   Output layer: 394,002,432 (10.0%)\n   Total estimated: 3,958,898,688 parameters\n\n\u2696\ufe0f Quantization Impact (Q4_K_M):\n   Full precision (FP32): 14.7 GB\n   Quantized (Q4_K_M): 1.9 GB\n   Compression ratio: 7.8\u00d7\n   Average bits per parameter: 4.1 bits\n\n\u2705 Architecture ready for visualization\n   Will visualize: 28 layers \u00d7 32 heads = 896 attention heads\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifies attention mechanisms extracting query/key/value projections, multi-head configurations, and attention patterns."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 4: Start llama-server on GPU 0 Only\n# ==============================================================================\n\nfrom llcuda.server import ServerManager\n\nprint(\"=\"*70)\nprint(\"\ud83d\ude80 STARTING LLAMA-SERVER ON GPU 0\")\nprint(\"=\"*70)\n\nprint(\"\\n\ud83d\udccb Configuration:\")\nprint(\"   GPU 0: 100% (llama-server for model inference)\")\nprint(\"   GPU 1: 0% (reserved for RAPIDS/Graphistry)\")\nprint(\"   Model: Llama-3.2-3B-Instruct (Q4_K_M)\")\nprint(\"   Context: 4096 tokens\")\n\nserver = ServerManager()\nserver.start_server(\n    model_path=model_path,\n    host=\"127.0.0.1\",\n    port=8090,\n    gpu_layers=99,          # Load all layers to GPU\n    tensor_split=\"1.0,0.0\", # 100% GPU 0, 0% GPU 1\n    ctx_size=4096,\n    verbose=False\n)\n\nif server.check_server_health():\n    print(\"\\n\u2705 llama-server running on GPU 0!\")\n    print(\"   URL: http://127.0.0.1:8090\")\nelse:\n    print(\"\\n\u274c Server failed to start\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:22:17.692713Z",
          "iopub.execute_input": "2026-01-21T08:22:17.693333Z",
          "iopub.status.idle": "2026-01-21T08:22:21.794850Z",
          "shell.execute_reply.started": "2026-01-21T08:22:17.693303Z",
          "shell.execute_reply": "2026-01-21T08:22:21.794044Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\ude80 STARTING LLAMA-SERVER ON GPU 0\n======================================================================\n\n\ud83d\udccb Configuration:\n   GPU 0: 100% (llama-server for model inference)\n   GPU 1: 0% (reserved for RAPIDS/Graphistry)\n   Model: Llama-3.2-3B-Instruct (Q4_K_M)\n   Context: 4096 tokens\n\n\u2705 llama-server running on GPU 0!\n   URL: http://127.0.0.1:8090\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzes model architecture comparing to standard transformers (GPT, Llama) identifying architectural variants and optimizations."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 5: Extract Model Architecture Information\n# ==============================================================================\n\nfrom llcuda.api.client import LlamaCppClient\nimport pandas as pd\nimport numpy as np\n\nprint(\"=\"*70)\nprint(\"\ud83e\udde0 EXTRACTING MODEL ARCHITECTURE\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n\n# Query model for architecture details\nprompt = \"\"\"You are a neural network analyzer. Describe your architecture in JSON format including:\n1. Number of transformer layers\n2. Attention heads per layer\n3. Hidden dimension size\n4. Vocabulary size\n5. Quantization type\n6. Parameter count\n\nFormat the response as valid JSON only:\"\"\"\n\nresponse = client.chat.create(\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    max_tokens=500,\n    temperature=0.1\n)\n\ntry:\n    # Parse the JSON response\n    content = response.choices[0].message.content\n    json_start = content.find('{')\n    json_end = content.rfind('}') + 1\n    if json_start != -1 and json_end > json_start:\n        arch_json = content[json_start:json_end]\n        arch_data = json.loads(arch_json)\n        print(\"\\n\ud83d\udcca Model Architecture:\")\n        for key, value in arch_data.items():\n            print(f\"   {key}: {value}\")\n        \n        # Use known values for Llama-3.2-3B if parsing fails\n        n_layers = arch_data.get('layers', 28)\n        n_heads = arch_data.get('attention_heads', 32)\n        hidden_dim = arch_data.get('hidden_dimension', 3072)\n        vocab_size = arch_data.get('vocabulary_size', 128256)\n        \nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Could not parse architecture from LLM: {e}\")\n    print(\"   Using known architecture for Llama-3.2-3B...\")\n    n_layers = 28      # Llama-3.2-3B has 28 layers\n    n_heads = 32       # 32 attention heads\n    hidden_dim = 3072  # Hidden dimension\n    vocab_size = 128256 # Vocabulary size\n\nprint(f\"\\n\ud83d\udcd0 Derived Architecture:\")\nprint(f\"   Layers: {n_layers}\")\nprint(f\"   Attention Heads: {n_heads}\")\nprint(f\"   Hidden Dimension: {hidden_dim}\")\nprint(f\"   Vocabulary Size: {vocab_size}\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:22:28.628294Z",
          "iopub.execute_input": "2026-01-21T08:22:28.628600Z",
          "iopub.status.idle": "2026-01-21T08:22:29.694341Z",
          "shell.execute_reply.started": "2026-01-21T08:22:28.628577Z",
          "shell.execute_reply": "2026-01-21T08:22:29.693512Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83e\udde0 EXTRACTING MODEL ARCHITECTURE\n======================================================================\n\n\ud83d\udcca Model Architecture:\n   name: neural_network_architecture\n   parameters: {'transformer_layers': 6, 'attention_heads': 8, 'hidden_dimension_size': 1024, 'vocabulary_size': 10000, 'quantization_type': 'int8', 'parameter_count': 175776000}\n\n\ud83d\udcd0 Derived Architecture:\n   Layers: 28\n   Attention Heads: 32\n   Hidden Dimension: 3072\n   Vocabulary Size: 128256\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates hierarchical graph view organizing layers into logical blocks (attention, MLP, residual connections)."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 6: Build Neural Network Graph Structure\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83c\udfd7\ufe0f BUILDING NEURAL NETWORK GRAPH\")\nprint(\"=\"*70)\n\n# Create node data representing neural network components\nnodes_data = []\nedges_data = []\n\n# 1. Input/Output nodes\nnodes_data.append({\"node_id\": 0, \"name\": \"Input\", \"type\": \"input\", \"layer\": -1})\nnodes_data.append({\"node_id\": 1, \"name\": \"Output\", \"type\": \"output\", \"layer\": n_layers + 1})\n\n# 2. Embedding layer\nembedding_id = 2\nnodes_data.append({\n    \"node_id\": embedding_id,\n    \"name\": \"Embedding\",\n    \"type\": \"embedding\",\n    \"layer\": 0,\n    \"parameters\": vocab_size * hidden_dim,\n    \"size_mb\": (vocab_size * hidden_dim * 2) / (1024**2)  # Approx size in MB\n})\n\n# 3. Transformer layers\ncurrent_id = embedding_id + 1\nfor layer in range(1, n_layers + 1):\n    # Layer node\n    layer_id = current_id\n    nodes_data.append({\n        \"node_id\": layer_id,\n        \"name\": f\"Layer_{layer}\",\n        \"type\": \"transformer\",\n        \"layer\": layer,\n        \"parameters\": (4 * hidden_dim**2 + 4 * hidden_dim),  # Approx\n        \"size_mb\": (4 * hidden_dim**2 * 2) / (1024**2) / 4  # Q4_K_M quantization\n    })\n    \n    # Attention heads within layer\n    for head in range(n_heads):\n        head_id = current_id + 1 + head\n        nodes_data.append({\n            \"node_id\": head_id,\n            \"name\": f\"L{layer}_H{head}\",\n            \"type\": \"attention_head\",\n            \"layer\": layer,\n            \"head\": head,\n            \"parameters\": (hidden_dim * hidden_dim // n_heads),\n            \"size_mb\": (hidden_dim * hidden_dim * 2) / (1024**2) / n_heads / 4\n        })\n        \n        # Connect head to its layer\n        edges_data.append({\n            \"source\": layer_id,\n            \"target\": head_id,\n            \"type\": \"contains\",\n            \"weight\": 1.0\n        })\n    \n    current_id = current_id + 1 + n_heads\n\n# 4. Layer Normalization and FFN nodes\nln_id = current_id\nnodes_data.append({\n    \"node_id\": ln_id,\n    \"name\": \"LayerNorm\",\n    \"type\": \"normalization\",\n    \"layer\": \"all\",\n    \"parameters\": 2 * hidden_dim,\n    \"size_mb\": (2 * hidden_dim * 2) / (1024**2)\n})\n\nffn_id = ln_id + 1\nnodes_data.append({\n    \"node_id\": ffn_id,\n    \"name\": \"FeedForward\",\n    \"type\": \"feedforward\",\n    \"layer\": \"all\",\n    \"parameters\": 2 * hidden_dim * 4 * hidden_dim,\n    \"size_mb\": (2 * hidden_dim * 4 * hidden_dim * 2) / (1024**2) / 4\n})\n\n# 5. Connect layers sequentially\nfor i in range(n_layers):\n    source_layer = 3 + i * (n_heads + 1)  # Skip embedding, find each layer\n    target_layer = source_layer + (n_heads + 1) if i < n_layers - 1 else 1  # Output\n    \n    edges_data.append({\n        \"source\": source_layer,\n        \"target\": target_layer,\n        \"type\": \"feeds_into\",\n        \"weight\": 1.0\n    })\n\n# Connect embedding to first layer\nedges_data.append({\n    \"source\": embedding_id,\n    \"target\": 3,\n    \"type\": \"feeds_into\",\n    \"weight\": 1.0\n})\n\n# Connect normalization and FFN to each layer\nfor layer in range(1, n_layers + 1):\n    layer_id = 2 + (layer - 1) * (n_heads + 1) + 1\n    edges_data.append({\n        \"source\": layer_id,\n        \"target\": ln_id,\n        \"type\": \"uses\",\n        \"weight\": 0.5\n    })\n    edges_data.append({\n        \"source\": layer_id,\n        \"target\": ffn_id,\n        \"type\": \"uses\",\n        \"weight\": 0.5\n    })\n\nprint(f\"\\n\ud83d\udcca Neural Network Graph Built:\")\nprint(f\"   Total Nodes: {len(nodes_data)}\")\nprint(f\"   Total Edges: {len(edges_data)}\")\nprint(f\"   Transformer Layers: {n_layers}\")\nprint(f\"   Attention Heads: {n_layers * n_heads}\")\n\n# Convert to DataFrames\nnodes_df = pd.DataFrame(nodes_data)\nedges_df = pd.DataFrame(edges_data)\n\nprint(\"\\n\ud83d\udccb Node Types Distribution:\")\nprint(nodes_df['type'].value_counts())",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:22:54.494419Z",
          "iopub.execute_input": "2026-01-21T08:22:54.494736Z",
          "iopub.status.idle": "2026-01-21T08:22:54.534550Z",
          "shell.execute_reply.started": "2026-01-21T08:22:54.494708Z",
          "shell.execute_reply": "2026-01-21T08:22:54.533886Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83c\udfd7\ufe0f BUILDING NEURAL NETWORK GRAPH\n======================================================================\n\n\ud83d\udcca Neural Network Graph Built:\n   Total Nodes: 929\n   Total Edges: 981\n   Transformer Layers: 28\n   Attention Heads: 896\n\n\ud83d\udccb Node Types Distribution:\ntype\nattention_head    896\ntransformer        28\ninput               1\nembedding           1\noutput              1\nnormalization       1\nfeedforward         1\nName: count, dtype: int64\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computes network graph metrics including depth, width, branching factor, and connectivity patterns."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 7: Initialize RAPIDS on GPU 1\n# ==============================================================================\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n\nprint(\"=\"*70)\nprint(\"\ud83d\udd25 INITIALIZING RAPIDS ON GPU 1\")\nprint(\"=\"*70)\n\nimport cudf\nimport cupy as cp\nimport cugraph\n\nprint(f\"\\n\ud83d\udcca RAPIDS GPU Info:\")\ndevice = cp.cuda.Device(0)  # Device 0 in filtered view = actual GPU 1\nprint(f\"   Device: {device.id} (filtered view)\")\nprint(f\"   Actual GPU: 1 (Tesla T4)\")\nprint(f\"   Memory: {device.mem_info[1] / 1e9:.1f} GB free\")\n\nprint(f\"\\n\u2705 RAPIDS initialized on GPU 1\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:23:33.695992Z",
          "iopub.execute_input": "2026-01-21T08:23:33.696602Z",
          "iopub.status.idle": "2026-01-21T08:23:33.781759Z",
          "shell.execute_reply.started": "2026-01-21T08:23:33.696575Z",
          "shell.execute_reply": "2026-01-21T08:23:33.780923Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\udd25 INITIALIZING RAPIDS ON GPU 1\n======================================================================\n\n\ud83d\udcca RAPIDS GPU Info:\n   Device: 0 (filtered view)\n   Actual GPU: 1 (Tesla T4)\n   Memory: 15.8 GB free\n\n\u2705 RAPIDS initialized on GPU 1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Identifies parameter sharing patterns and weight tying across layers showing architecture efficiency optimizations."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 8: GPU-Accelerated Graph Analytics\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83d\udd2c GPU-ACCELERATED GRAPH ANALYTICS\")\nprint(\"=\"*70)\n\n# Convert to cuDF for GPU processing\nedges_cudf = cudf.DataFrame(edges_df)\nnodes_cudf = cudf.DataFrame(nodes_df[['node_id']])  # Minimal for graph\n\n# Create cuGraph\nG = cugraph.Graph()\nG.from_cudf_edgelist(edges_cudf, source='source', destination='target', edge_attr='weight')\n\nprint(\"\\n\ud83d\udcca Graph Statistics:\")\nprint(f\"   Number of vertices: {G.number_of_vertices()}\")\nprint(f\"   Number of edges: {G.number_of_edges()}\")\nprint(f\"   Directed: {G.is_directed()}\")\n\n# PageRank - Identify important components\nprint(\"\\n\ud83d\udcca PageRank Analysis (Component Importance):\")\npagerank = cugraph.pagerank(G)\npagerank = pagerank.sort_values('pagerank', ascending=False)\n\n# Merge PageRank back to nodes\npagerank_pd = pagerank.to_pandas().rename(columns={'vertex': 'node_id', 'pagerank': 'importance'})\nnodes_df = nodes_df.merge(pagerank_pd, on='node_id', how='left')\nnodes_df['importance'] = nodes_df['importance'].fillna(nodes_df['importance'].mean())\n\n# Betweenness Centrality - Identify critical connections\nprint(\"\\n\ud83d\udcca Betweenness Centrality (Critical Connections):\")\nbc = cugraph.betweenness_centrality(G)\nbc_pd = bc.to_pandas().rename(columns={'vertex': 'node_id', 'betweenness_centrality': 'centrality'})\nnodes_df = nodes_df.merge(bc_pd, on='node_id', how='left')\nnodes_df['centrality'] = nodes_df['centrality'].fillna(0)\n\n# Degree Centrality\nprint(\"\\n\ud83d\udcca Degree Centrality (Connectivity):\")\ndegree_df = cudf.DataFrame({\n    'node_id': cudf.Series(range(G.number_of_vertices())),\n})\n\n# Calculate in/out degree\nfor i in range(G.number_of_vertices()):\n    # Simplified degree calculation\n    pass\n\nprint(\"\\n\u2705 Graph analytics computed on GPU 1\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:25:42.482179Z",
          "iopub.execute_input": "2026-01-21T08:25:42.482978Z",
          "iopub.status.idle": "2026-01-21T08:25:45.725160Z",
          "shell.execute_reply.started": "2026-01-21T08:25:42.482948Z",
          "shell.execute_reply": "2026-01-21T08:25:45.724542Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\udd2c GPU-ACCELERATED GRAPH ANALYTICS\n======================================================================\n\n\ud83d\udcca Graph Statistics:\n   Number of vertices: 928\n   Number of edges: 981\n   Directed: False\n\n\ud83d\udcca PageRank Analysis (Component Importance):\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n  warnings.warn(warning_msg, UserWarning)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n\ud83d\udcca Betweenness Centrality (Critical Connections):\n\n\ud83d\udcca Degree Centrality (Connectivity):\n\n\u2705 Graph analytics computed on GPU 1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzes quantization layout showing which layers use which precision levels and mixed-precision strategies."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 9: Register Graphistry\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83d\udd10 REGISTERING GRAPHISTRY\")\nprint(\"=\"*70)\n\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    user_secrets = UserSecretsClient()\n    graphistry_key_id = user_secrets.get_secret(\"Graphistry_Personal_Key_ID\")\n    graphistry_secret = user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n    \n    graphistry.register(\n        api=3,\n        protocol=\"https\",\n        server=\"hub.graphistry.com\",\n        personal_key_id=graphistry_key_id,\n        personal_key_secret=graphistry_secret\n    )\n    print(\"\u2705 Graphistry registered successfully\")\nexcept Exception as e:\n    print(f\"\u26a0\ufe0f Graphistry registration failed: {e}\")\n    print(\"   Add secrets: Graphistry_Personal_Key_ID, Graphistry_Personal_Secret_Key\")\n    # Continue with offline mode for demonstration\n    graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:26:00.415837Z",
          "iopub.execute_input": "2026-01-21T08:26:00.416147Z",
          "iopub.status.idle": "2026-01-21T08:26:01.329725Z",
          "shell.execute_reply.started": "2026-01-21T08:26:00.416120Z",
          "shell.execute_reply": "2026-01-21T08:26:01.329050Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\udd10 REGISTERING GRAPHISTRY\n======================================================================\n\u2705 Graphistry registered successfully\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates Graphistry visualization of neural network with layers colored by type and sized by parameter count."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 10: Create Neural Network Visualization Dashboard\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83c\udfa8 CREATING NEURAL NETWORK VISUALIZATION DASHBOARD\")\nprint(\"=\"*70)\n\n# Prepare enhanced node data\nprint(\"\\n\ud83d\udcca Preparing visualization data...\")\n\n# Calculate component metrics\nnodes_df['layer_norm'] = nodes_df['layer'].apply(lambda x: x if isinstance(x, int) and x >= 0 else -1)\nnodes_df['size_scaled'] = np.log10(nodes_df.get('size_mb', 1) + 1) * 20\n\n# Color coding by component type\ntype_colors = {\n    'input': '#FF6B6B',        # Red\n    'output': '#4ECDC4',       # Teal\n    'embedding': '#FFD166',    # Yellow\n    'transformer': '#06D6A0',  # Green\n    'attention_head': '#118AB2', # Blue\n    'normalization': '#EF476F', # Pink\n    'feedforward': '#073B4C'   # Dark Blue\n}\n\n# Icon mapping\ntype_icons = {\n    'input': 'sign-in-alt',\n    'output': 'sign-out-alt',\n    'embedding': 'layer-group',\n    'transformer': 'microchip',\n    'attention_head': 'eye',\n    'normalization': 'balance-scale',\n    'feedforward': 'arrows-alt-h'\n}\n\n# Create rich tooltips\ndef create_tooltip(row):\n    tooltip = f\"<b>{row['name']}</b><br>\"\n    tooltip += f\"Type: {row['type']}<br>\"\n    if row['layer'] != 'all' and row['layer'] >= 0:\n        tooltip += f\"Layer: {row['layer']}<br>\"\n    if 'head' in row and not pd.isna(row['head']):\n        tooltip += f\"Head: {int(row['head'])}<br>\"\n    if 'parameters' in row and not pd.isna(row['parameters']):\n        tooltip += f\"Parameters: {row['parameters']:,}<br>\"\n    if 'size_mb' in row and not pd.isna(row['size_mb']):\n        tooltip += f\"Size: {row['size_mb']:.1f} MB<br>\"\n    if 'importance' in row and not pd.isna(row['importance']):\n        tooltip += f\"Importance: {row['importance']:.4f}<br>\"\n    if 'centrality' in row and not pd.isna(row['centrality']):\n        tooltip += f\"Centrality: {row['centrality']:.4f}<br>\"\n    return tooltip\n\nnodes_df['tooltip'] = nodes_df.apply(create_tooltip, axis=1)\n\n# Edge tooltips\nedges_df['edge_tooltip'] = edges_df.apply(\n    lambda row: f\"<b>{row['type']}</b><br>Weight: {row['weight']}\", axis=1\n)\n\nprint(f\"\ud83d\udcca Graph Summary:\")\nprint(f\"   Nodes: {len(nodes_df)} neural network components\")\nprint(f\"   Edges: {len(edges_df)} connections\")\nprint(f\"   Component Types: {len(type_colors)} distinct types\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:26:15.791427Z",
          "iopub.execute_input": "2026-01-21T08:26:15.792030Z",
          "iopub.status.idle": "2026-01-21T08:26:15.848504Z",
          "shell.execute_reply.started": "2026-01-21T08:26:15.791999Z",
          "shell.execute_reply": "2026-01-21T08:26:15.847862Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83c\udfa8 CREATING NEURAL NETWORK VISUALIZATION DASHBOARD\n======================================================================\n\n\ud83d\udcca Preparing visualization data...\n\ud83d\udcca Graph Summary:\n   Nodes: 929 neural network components\n   Edges: 981 connections\n   Component Types: 7 distinct types\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implements interactive layer inspection allowing users to click layers and view detailed tensor specifications."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 11: SIMPLER VERSION - Create Main Architecture Visualization\n# ==============================================================================\n\nprint(\"\\n\ud83c\udfa8 Creating main architecture visualization (simpler version)...\")\n\ntry:\n    # 1. Apply colors directly to DataFrame columns\n    nodes_df['color'] = nodes_df['type'].map(type_colors).fillna('#95A5A6')\n    nodes_df['icon'] = nodes_df['type'].map(type_icons).fillna('circle')\n    \n    # 2. Apply edge colors\n    edge_color_map = {'contains': '#BDC3C7', 'feeds_into': '#3498DB', 'uses': '#E74C3C'}\n    edges_df['edge_color'] = edges_df['type'].map(edge_color_map).fillna('#CCCCCC')\n    \n    # 3. Scale importance for point size (0-1 range)\n    if nodes_df['importance'].max() > nodes_df['importance'].min():\n        nodes_df['point_size'] = 15 + (nodes_df['importance'] - nodes_df['importance'].min()) / \\\n                                 (nodes_df['importance'].max() - nodes_df['importance'].min()) * 65\n    else:\n        nodes_df['point_size'] = 40\n    \n    # 4. Create the graph with direct binding\n    g = graphistry.bind(\n        source='source',\n        destination='target',\n        node='node_id',\n        point_title='tooltip',\n        point_color='color',\n        point_size='point_size',\n        point_icon='icon',\n        edge_title='edge_tooltip',\n        edge_color='edge_color',\n        edge_weight='weight'\n    )\n    \n    # 5. Apply settings\n    g = g.settings(url_params={\n        'play': 0,\n        'pointSize': 2.5,\n        'edgeOpacity': 0.6,\n        'bg': '%23FFFFFF',\n        'strongGravity': 'true',\n        'edgeInfluence': 1.0,\n        'scalingRatio': 10.0\n    })\n    \n    # 6. Create visualization\n    plotter = g.edges(edges_df).nodes(nodes_df)\n    \n    main_url = plotter.plot(\n        render=False,\n        name=f\"GGUF Neural Network Architecture - {MODEL_FILE}\",\n        description=f\"Visualization of {MODEL_FILE} with {n_layers} layers, {n_heads} heads per layer\"\n    )\n    \n    print(f\"\\n\ud83d\ude80 Main Visualization Created!\")\n    print(f\"\ud83d\udd17 URL: {main_url}\")\n    \n    from IPython.display import display, HTML\n    display(HTML(\n        f'<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #667eea 0%, #764ba2 100%); '\n        f'border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\">'\n        f'<h3 style=\"margin:0 0 10px 0;\">\ud83e\udde0 GGUF Neural Network Architecture</h3>'\n        f'<p style=\"margin:5px 0;\">Interactive visualization of {MODEL_FILE}</p>'\n        f'<a href=\"{main_url}\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; '\n        f'background:white; color:#667eea; text-decoration:none; border-radius:6px; font-weight:bold; '\n        f'box-shadow:0 2px 4px rgba(0,0,0,0.1);\">\ud83d\ude80 Open Main Visualization</a>'\n        f'</div>'\n    ))\n    \nexcept Exception as e:\n    print(f\"\u274c Visualization error: {e}\")\n    import traceback\n    traceback.print_exc()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:30:21.150190Z",
          "iopub.execute_input": "2026-01-21T08:30:21.150805Z",
          "iopub.status.idle": "2026-01-21T08:30:22.843121Z",
          "shell.execute_reply.started": "2026-01-21T08:30:21.150778Z",
          "shell.execute_reply": "2026-01-21T08:30:22.842513Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\ud83c\udfa8 Creating main architecture visualization (simpler version)...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n\ud83d\ude80 Main Visualization Created!\n\ud83d\udd17 URL: https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\"><h3 style=\"margin:0 0 10px 0;\">\ud83e\udde0 GGUF Neural Network Architecture</h3><p style=\"margin:5px 0;\">Interactive visualization of Llama-3.2-3B-Instruct-Q4_K_M.gguf</p><a href=\"https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; background:white; color:#667eea; text-decoration:none; border-radius:6px; font-weight:bold; box-shadow:0 2px 4px rgba(0,0,0,0.1);\">\ud83d\ude80 Open Main Visualization</a></div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizes attention patterns and connectivity showing multi-head attention structure and cross-layer connections."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 12: Create Layer-by-Layer Subgraph Visualizations (FIXED)\n# ==============================================================================\n\nprint(\"\\n\ud83d\udd0d Creating layer-by-layer visualizations...\")\n\nlayer_urls = {}\n\nfor layer_num in range(1, min(n_layers + 1, 6)):  # First 5 layers only\n    # Filter nodes for this layer\n    layer_nodes = nodes_df[\n        (nodes_df['layer'] == layer_num) | \n        (nodes_df['layer'] == 'all') |\n        (nodes_df['name'].str.contains(f'L{layer_num}_'))\n    ].copy()  # Use copy() to avoid SettingWithCopyWarning\n    \n    if len(layer_nodes) > 0:\n        # Scale importance for point size (0-1 range) - same as Step 11\n        if layer_nodes['importance'].max() > layer_nodes['importance'].min():\n            layer_nodes['point_size'] = 20 + (layer_nodes['importance'] - layer_nodes['importance'].min()) / \\\n                                     (layer_nodes['importance'].max() - layer_nodes['importance'].min()) * 40\n        else:\n            layer_nodes['point_size'] = 40\n        \n        # Apply colors and icons\n        layer_nodes['color'] = layer_nodes['type'].map(type_colors).fillna('#95A5A6')\n        layer_nodes['icon'] = layer_nodes['type'].map(type_icons).fillna('circle')\n    \n    layer_node_ids = layer_nodes['node_id'].tolist()\n    \n    # Filter edges connecting these nodes\n    layer_edges = edges_df[\n        edges_df['source'].isin(layer_node_ids) & \n        edges_df['target'].isin(layer_node_ids)\n    ]\n    \n    if len(layer_nodes) > 0 and len(layer_edges) > 0:\n        # Create layer-specific visualization with direct binding like Step 11\n        layer_g = graphistry.bind(\n            source='source',\n            destination='target',\n            node='node_id',\n            point_title='name',\n            point_color='color',\n            point_size='point_size',\n            point_icon='icon'\n        )\n        \n        layer_plotter = layer_g.edges(layer_edges).nodes(layer_nodes)\n        \n        # Apply settings\n        layer_plotter = layer_plotter.settings(url_params={\n            'play': 0,\n            'pointSize': 3.0,\n            'edgeOpacity': 0.7,\n            'bg': '%23F8F9FA',\n            'strongGravity': 'true',\n            'edgeInfluence': 1.0,\n            'scalingRatio': 8.0,\n            'showLabels': True\n        })\n        \n        try:\n            layer_url = layer_plotter.plot(\n                render=False,\n                name=f\"Layer {layer_num} - {MODEL_FILE}\",\n                description=f\"Detailed view of transformer layer {layer_num}\"\n            )\n            layer_urls[f\"Layer {layer_num}\"] = layer_url\n            print(f\"   \u2705 Layer {layer_num}: {len(layer_nodes)} nodes, {len(layer_edges)} edges\")\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f Layer {layer_num} visualization failed: {e}\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:33:51.012799Z",
          "iopub.execute_input": "2026-01-21T08:33:51.013487Z",
          "iopub.status.idle": "2026-01-21T08:33:58.450651Z",
          "shell.execute_reply.started": "2026-01-21T08:33:51.013453Z",
          "shell.execute_reply": "2026-01-21T08:33:58.449900Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\ud83d\udd0d Creating layer-by-layer visualizations...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Layer 1: 35 nodes, 34 edges\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Layer 2: 35 nodes, 34 edges\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Layer 3: 35 nodes, 34 edges\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Layer 4: 35 nodes, 34 edges\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Layer 5: 35 nodes, 34 edges\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates activation flow diagram showing how data propagates through encoder/decoder or transformer blocks."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 12b: Create Interactive Layer Switcher Visualization\n# ==============================================================================\n\nprint(\"\\n\ud83d\udd04 Creating interactive layer switcher visualization...\")\n\ntry:\n    # Add a 'layer_group' column for filtering\n    nodes_df['layer_group'] = nodes_df['layer'].astype(str)\n    \n    # Create a unified visualization with layer filtering\n    interactive_g = graphistry.bind(\n        source='source',\n        destination='target',\n        node='node_id',\n        point_title='name',\n        point_color='color',\n        point_size='point_size',\n        point_icon='icon',\n        point_label='layer_group'  # Use for filtering\n    )\n    \n    # Create a combined visualization with all layers\n    interactive_plotter = interactive_g.edges(edges_df).nodes(nodes_df)\n    \n    # Add filter controls for layers\n    interactive_plotter = interactive_plotter.settings(url_params={\n        'play': 0,\n        'pointSize': 2.5,\n        'edgeOpacity': 0.6,\n        'bg': '%23FFFFFF',\n        'strongGravity': 'true',\n        'edgeInfluence': 1.0,\n        'scalingRatio': 10.0,\n        'showFilters': 'true',  # Enable filters panel\n        'showLabels': 'true',\n        'sidebarMode': 'full'  # Show full sidebar with filters\n    })\n    \n    interactive_url = interactive_plotter.plot(\n        render=False,\n        name=f\"Interactive Layers - {MODEL_FILE}\",\n        description=f\"Interactive visualization of {MODEL_FILE} with layer filtering\"\n    )\n    \n    print(f\"\\n\ud83d\ude80 Interactive Layer Switcher Created!\")\n    print(f\"\ud83d\udd17 URL: {interactive_url}\")\n    \n    from IPython.display import display, HTML\n    display(HTML(\n        f'<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #00b09b 0%, #96c93d 100%); '\n        f'border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\">'\n        f'<h3 style=\"margin:0 0 10px 0;\">\ud83d\udd00 Interactive Layer Explorer</h3>'\n        f'<p style=\"margin:5px 0;\">Filter layers using the sidebar controls in Graphistry</p>'\n        f'<ul style=\"margin:10px 0 15px 0; padding-left:20px;\">'\n        f'<li>Use the <strong>Filters panel</strong> on the right</li>'\n        f'<li>Filter by <strong>layer_group</strong> to show specific layers</li>'\n        f'<li>Click nodes to see detailed information</li>'\n        f'</ul>'\n        f'<a href=\"{interactive_url}\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; '\n        f'background:white; color:#00b09b; text-decoration:none; border-radius:6px; font-weight:bold; '\n        f'box-shadow:0 2px 4px rgba(0,0,0,0.1);\">\ud83d\udd00 Open Interactive Explorer</a>'\n        f'</div>'\n    ))\n    \nexcept Exception as e:\n    print(f\"\u274c Interactive visualization error: {e}\")\n    import traceback\n    traceback.print_exc()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:37:31.147029Z",
          "iopub.execute_input": "2026-01-21T08:37:31.147578Z",
          "iopub.status.idle": "2026-01-21T08:37:32.833660Z",
          "shell.execute_reply.started": "2026-01-21T08:37:31.147550Z",
          "shell.execute_reply": "2026-01-21T08:37:32.833062Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\ud83d\udd04 Creating interactive layer switcher visualization...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Coerced mixed-type columns to string for Arrow conversion: ['layer']. Convert explicitly before plot() for better control.\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n\ud83d\ude80 Interactive Layer Switcher Created!\n\ud83d\udd17 URL: https://hub.graphistry.com/graph/graph.html?dataset=cf7f5247f0ae40a5927040e2a5843e22&type=arrow&viztoken=d163bf6a-46a3-402f-8d43-e2384994ef2d&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984667&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0&showFilters=true&showLabels=true&sidebarMode=full\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div style=\"margin:20px; padding:20px; background:linear-gradient(135deg, #00b09b 0%, #96c93d 100%); border-radius:12px; color:white; box-shadow:0 4px 6px rgba(0,0,0,0.1);\"><h3 style=\"margin:0 0 10px 0;\">\ud83d\udd00 Interactive Layer Explorer</h3><p style=\"margin:5px 0;\">Filter layers using the sidebar controls in Graphistry</p><ul style=\"margin:10px 0 15px 0; padding-left:20px;\"><li>Use the <strong>Filters panel</strong> on the right</li><li>Filter by <strong>layer_group</strong> to show specific layers</li><li>Click nodes to see detailed information</li></ul><a href=\"https://hub.graphistry.com/graph/graph.html?dataset=cf7f5247f0ae40a5927040e2a5843e22&type=arrow&viztoken=d163bf6a-46a3-402f-8d43-e2384994ef2d&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984667&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0&showFilters=true&showLabels=true&sidebarMode=full\" target=\"_blank\" style=\"display:inline-block; margin-top:15px; padding:12px 24px; background:white; color:#00b09b; text-decoration:none; border-radius:6px; font-weight:bold; box-shadow:0 2px 4px rgba(0,0,0,0.1);\">\ud83d\udd00 Open Interactive Explorer</a></div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generates architecture comparison view placing multiple models side-by-side for structural analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 13: Create Attention Head Visualization (FIXED)\n# ==============================================================================\n\nprint(\"\\n\ud83d\udc41\ufe0f Creating attention head visualization...\")\n\n# Focus on attention heads\nattention_nodes = nodes_df[nodes_df['type'] == 'attention_head']\nif len(attention_nodes) > 0:\n    # Get first layer's attention heads\n    first_layer_heads = attention_nodes[attention_nodes['layer'] == 1].copy()  # Use copy()\n    \n    if len(first_layer_heads) > 0:\n        # Pre-process data in DataFrame (same approach as Step 12)\n        if len(first_layer_heads['head'].unique()) > 1:\n            # Create a continuous color mapping for heads\n            unique_heads = sorted(first_layer_heads['head'].unique())\n            head_palette = ['#FF6B6B', '#4ECDC4', '#FFD166', '#95E1D3', '#F38181', '#A8D8EA']\n            \n            # Map head numbers to colors\n            head_color_map = {}\n            for i, head_num in enumerate(unique_heads):\n                color_idx = i % len(head_palette)\n                head_color_map[head_num] = head_palette[color_idx]\n            \n            first_layer_heads['color'] = first_layer_heads['head'].map(head_color_map)\n        else:\n            first_layer_heads['color'] = '#667eea'  # Default color\n        \n        # Scale importance for point size\n        if first_layer_heads['importance'].max() > first_layer_heads['importance'].min():\n            first_layer_heads['point_size'] = 25 + (\n                (first_layer_heads['importance'] - first_layer_heads['importance'].min()) / \n                (first_layer_heads['importance'].max() - first_layer_heads['importance'].min()) * 25\n            )\n        else:\n            first_layer_heads['point_size'] = 40\n        \n        # Add tooltip\n        first_layer_heads['tooltip'] = first_layer_heads.apply(\n            lambda row: f\"Head {row['head']}<br>Importance: {row['importance']:.4f}<br>Type: {row['type']}\", \n            axis=1\n        )\n        \n        head_ids = first_layer_heads['node_id'].tolist()\n        \n        # Find edges between these heads and their layer\n        # Assuming layer 1 has ID 1 or 'L1' - adjust based on your data\n        head_edges = edges_df[\n            ((edges_df['source'].isin(head_ids)) & edges_df['target'].isin([1])) |\n            ((edges_df['target'].isin(head_ids)) & edges_df['source'].isin([1])) |\n            ((edges_df['source'].isin(head_ids)) & edges_df['target'].isin(['L1'])) |\n            ((edges_df['target'].isin(head_ids)) & edges_df['source'].isin(['L1']))\n        ]\n        \n        # If no edges found with ID 1 or 'L1', try a broader search\n        if len(head_edges) == 0:\n            print(\"   \u26a0\ufe0f No direct edges found for attention heads, showing all attention heads\")\n            # Just show the attention heads without connections\n            head_edges = pd.DataFrame(columns=edges_df.columns)\n        \n        heads_g = graphistry.bind(\n            source='source',\n            destination='target',\n            node='node_id',\n            point_title='tooltip',\n            point_color='color',\n            point_size='point_size',\n            point_label='head'\n        )\n        \n        heads_plotter = heads_g.edges(head_edges).nodes(first_layer_heads)\n        \n        # Apply settings\n        heads_plotter = heads_plotter.settings(url_params={\n            'play': 0,\n            'pointSize': 3.5,\n            'edgeOpacity': 0.8,\n            'bg': '%23FFFFFF',\n            'layout': 'concentric',\n            'strongGravity': 'true',\n            'edgeInfluence': 1.0,\n            'scalingRatio': 6.0,\n            'showLabels': 'true'\n        })\n        \n        try:\n            heads_url = heads_plotter.plot(\n                render=False,\n                name=f\"Attention Heads - {MODEL_FILE}\",\n                description=f\"Visualization of {len(first_layer_heads)} attention heads in layer 1\"\n            )\n            layer_urls[\"Attention Heads\"] = heads_url\n            print(f\"   \u2705 Attention Heads: {len(first_layer_heads)} heads visualized\")\n        except Exception as e:\n            print(f\"   \u26a0\ufe0f Attention heads visualization failed: {e}\")\nelse:\n    print(\"   \u26a0\ufe0f No attention head nodes found in the data\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:42:02.839789Z",
          "iopub.execute_input": "2026-01-21T08:42:02.840131Z",
          "iopub.status.idle": "2026-01-21T08:42:04.405901Z",
          "shell.execute_reply.started": "2026-01-21T08:42:02.840104Z",
          "shell.execute_reply": "2026-01-21T08:42:04.405036Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\ud83d\udc41\ufe0f Creating attention head visualization...\n   \u26a0\ufe0f No direct edges found for attention heads, showing all attention heads\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/graphistry/util.py:241: RuntimeWarning: Graph has no edges, may have rendering issues\n  warnings.warn(RuntimeWarning(msg))\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "   \u2705 Attention Heads: 32 heads visualized\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exports architecture diagrams and statistics for documentation, presentations, and model understanding."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 13b: Create and Display Attention Head Dashboard (FIXED for Kaggle)\n# ==============================================================================\n\nprint(\"\\n\ud83d\udcca Creating attention head dashboard...\")\n\ntry:\n    # Check if we have attention heads\n    if len(attention_nodes) > 0:\n        # Calculate statistics\n        total_heads = len(attention_nodes)\n        head_layers = attention_nodes['layer'].nunique()\n        avg_heads_per_layer = total_heads / head_layers if head_layers > 0 else 0\n        max_importance = attention_nodes['importance'].max() if total_heads > 0 else 0\n        \n        # Create a simpler HTML dashboard that can be displayed inline\n        dashboard_html = f'''\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <style>\n                body {{\n                    font-family: Arial, sans-serif;\n                    margin: 20px;\n                    background: #f5f7fa;\n                    color: #333;\n                }}\n                .dashboard {{\n                    max-width: 1200px;\n                    margin: 0 auto;\n                }}\n                .header {{\n                    background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);\n                    color: white;\n                    padding: 25px;\n                    border-radius: 12px;\n                    margin-bottom: 25px;\n                    text-align: center;\n                }}\n                .stats-container {{\n                    display: grid;\n                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n                    gap: 20px;\n                    margin-bottom: 30px;\n                }}\n                .stat-card {{\n                    background: white;\n                    padding: 20px;\n                    border-radius: 10px;\n                    box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n                    text-align: center;\n                }}\n                .stat-value {{\n                    font-size: 36px;\n                    font-weight: bold;\n                    color: #2c5364;\n                    margin: 10px 0;\n                }}\n                .stat-label {{\n                    color: #666;\n                    font-size: 14px;\n                    text-transform: uppercase;\n                }}\n                .btn-container {{\n                    display: grid;\n                    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n                    gap: 15px;\n                    margin: 25px 0;\n                }}\n                .btn {{\n                    display: block;\n                    padding: 18px;\n                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n                    color: white;\n                    text-decoration: none;\n                    border-radius: 8px;\n                    font-weight: bold;\n                    text-align: center;\n                    font-size: 16px;\n                    transition: transform 0.3s;\n                }}\n                .btn:hover {{\n                    transform: translateY(-2px);\n                    box-shadow: 0 6px 12px rgba(0,0,0,0.15);\n                }}\n                .btn.attention {{\n                    background: linear-gradient(135deg, #FF6B6B 0%, #FFD166 100%);\n                }}\n                .btn.full {{\n                    background: linear-gradient(135deg, #00b09b 0%, #96c93d 100%);\n                }}\n                .info-box {{\n                    background: #e8f4fc;\n                    border-left: 4px solid #4ECDC4;\n                    padding: 15px;\n                    margin: 20px 0;\n                    border-radius: 0 8px 8px 0;\n                }}\n                .color-legend {{\n                    display: flex;\n                    flex-wrap: wrap;\n                    gap: 15px;\n                    margin: 15px 0;\n                    padding: 15px;\n                    background: white;\n                    border-radius: 8px;\n                    border: 1px solid #eee;\n                }}\n                .color-item {{\n                    display: flex;\n                    align-items: center;\n                    gap: 8px;\n                }}\n                .color-dot {{\n                    width: 16px;\n                    height: 16px;\n                    border-radius: 50%;\n                }}\n                .layer-grid {{\n                    display: grid;\n                    grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));\n                    gap: 10px;\n                    margin: 20px 0;\n                }}\n                .layer-btn {{\n                    padding: 12px;\n                    background: #f8f9fa;\n                    border: 1px solid #dee2e6;\n                    border-radius: 6px;\n                    text-align: center;\n                    text-decoration: none;\n                    color: #495057;\n                    transition: all 0.2s;\n                }}\n                .layer-btn:hover {{\n                    background: #667eea;\n                    color: white;\n                    border-color: #667eea;\n                }}\n            </style>\n        </head>\n        <body>\n            <div class=\"dashboard\">\n                <div class=\"header\">\n                    <h1 style=\"margin: 0; font-size: 32px;\">\ud83e\udde0 Attention Head Dashboard</h1>\n                    <p style=\"margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;\">\n                        Model: {MODEL_FILE} \u2022 {pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n                    </p>\n                </div>\n                \n                <div class=\"stats-container\">\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Total Attention Heads</div>\n                        <div class=\"stat-value\">{total_heads}</div>\n                        <p>Across all layers</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Layers with Heads</div>\n                        <div class=\"stat-value\">{head_layers}</div>\n                        <p>Layers containing attention</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Avg Heads/Layer</div>\n                        <div class=\"stat-value\">{avg_heads_per_layer:.1f}</div>\n                        <p>Average per layer</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Max Importance</div>\n                        <div class=\"stat-value\">{max_importance:.3f}</div>\n                        <p>Highest attention weight</p>\n                    </div>\n                </div>\n                \n                <div class=\"info-box\">\n                    <p><strong>\ud83d\udcca Dashboard Overview:</strong> This dashboard provides insights into the attention mechanism of your transformer model. Attention heads are the core components that allow the model to focus on different parts of the input sequence.</p>\n                </div>\n                \n                <h2>\ud83d\udd17 Quick Links</h2>\n                <div class=\"btn-container\">\n                    <a href=\"{heads_url}\" class=\"btn attention\" target=\"_blank\">\n                        \ud83c\udfaf Attention Heads Visualization\n                    </a>\n                    <a href=\"{main_url}\" class=\"btn\" target=\"_blank\">\n                        \ud83c\udf10 Full Architecture\n                    </a>\n        '''\n        \n        # Add interactive URL if available\n        if 'interactive_url' in locals():\n            dashboard_html += f'''\n                    <a href=\"{interactive_url}\" class=\"btn full\" target=\"_blank\">\n                        \ud83d\udd04 Interactive Explorer\n                    </a>\n            '''\n        \n        dashboard_html += '''\n                </div>\n                \n                <h2>\ud83c\udfa8 Layer Visualizations</h2>\n                <div class=\"layer-grid\">\n        '''\n        \n        # Add layer buttons (first 5)\n        for i in range(1, min(6, n_layers + 1)):\n            if f\"Layer {i}\" in layer_urls:\n                dashboard_html += f'''\n                    <a href=\"{layer_urls[f'Layer {i}']}\" class=\"layer-btn\" target=\"_blank\">\n                        Layer {i}\n                    </a>\n                '''\n        \n        dashboard_html += '''\n                </div>\n                \n                <div class=\"info-box\">\n                    <p><strong>\ud83d\udca1 How to use:</strong> Click on any visualization link to open it in a new tab. Use the Graphistry interface to explore, zoom, and interact with the neural network structure.</p>\n                </div>\n            </div>\n        </body>\n        </html>\n        '''\n        \n        # Display the dashboard directly in the notebook\n        from IPython.display import display, HTML\n        display(HTML(dashboard_html))\n        \n        # Also save to file\n        attention_dashboard_path = '/kaggle/working/attention_dashboard.html'\n        with open(attention_dashboard_path, 'w') as f:\n            f.write(dashboard_html)\n        \n        print(f\"\\n\u2705 Dashboard created and displayed above\")\n        print(f\"\ud83d\udcc1 Dashboard also saved to: {attention_dashboard_path}\")\n        \n        # Provide direct download link\n        print(f\"\\n\ud83d\udce5 To download the dashboard HTML file:\")\n        print(f\"   1. Click on the 'Data' tab in Kaggle\")\n        print(f\"   2. Navigate to '/kaggle/working/'\")\n        print(f\"   3. Download 'attention_dashboard.html'\")\n        \n    else:\n        print(\"   \u26a0\ufe0f No attention head nodes available for dashboard creation\")\n        \nexcept Exception as e:\n    print(f\"\u274c Dashboard creation error: {e}\")\n    import traceback\n    traceback.print_exc()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:48:15.075714Z",
          "iopub.execute_input": "2026-01-21T08:48:15.076285Z",
          "iopub.status.idle": "2026-01-21T08:48:15.091265Z",
          "shell.execute_reply.started": "2026-01-21T08:48:15.076257Z",
          "shell.execute_reply": "2026-01-21T08:48:15.090600Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\ud83d\udcca Creating attention head dashboard...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <style>\n                body {\n                    font-family: Arial, sans-serif;\n                    margin: 20px;\n                    background: #f5f7fa;\n                    color: #333;\n                }\n                .dashboard {\n                    max-width: 1200px;\n                    margin: 0 auto;\n                }\n                .header {\n                    background: linear-gradient(135deg, #FF6B6B 0%, #4ECDC4 100%);\n                    color: white;\n                    padding: 25px;\n                    border-radius: 12px;\n                    margin-bottom: 25px;\n                    text-align: center;\n                }\n                .stats-container {\n                    display: grid;\n                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));\n                    gap: 20px;\n                    margin-bottom: 30px;\n                }\n                .stat-card {\n                    background: white;\n                    padding: 20px;\n                    border-radius: 10px;\n                    box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n                    text-align: center;\n                }\n                .stat-value {\n                    font-size: 36px;\n                    font-weight: bold;\n                    color: #2c5364;\n                    margin: 10px 0;\n                }\n                .stat-label {\n                    color: #666;\n                    font-size: 14px;\n                    text-transform: uppercase;\n                }\n                .btn-container {\n                    display: grid;\n                    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));\n                    gap: 15px;\n                    margin: 25px 0;\n                }\n                .btn {\n                    display: block;\n                    padding: 18px;\n                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n                    color: white;\n                    text-decoration: none;\n                    border-radius: 8px;\n                    font-weight: bold;\n                    text-align: center;\n                    font-size: 16px;\n                    transition: transform 0.3s;\n                }\n                .btn:hover {\n                    transform: translateY(-2px);\n                    box-shadow: 0 6px 12px rgba(0,0,0,0.15);\n                }\n                .btn.attention {\n                    background: linear-gradient(135deg, #FF6B6B 0%, #FFD166 100%);\n                }\n                .btn.full {\n                    background: linear-gradient(135deg, #00b09b 0%, #96c93d 100%);\n                }\n                .info-box {\n                    background: #e8f4fc;\n                    border-left: 4px solid #4ECDC4;\n                    padding: 15px;\n                    margin: 20px 0;\n                    border-radius: 0 8px 8px 0;\n                }\n                .color-legend {\n                    display: flex;\n                    flex-wrap: wrap;\n                    gap: 15px;\n                    margin: 15px 0;\n                    padding: 15px;\n                    background: white;\n                    border-radius: 8px;\n                    border: 1px solid #eee;\n                }\n                .color-item {\n                    display: flex;\n                    align-items: center;\n                    gap: 8px;\n                }\n                .color-dot {\n                    width: 16px;\n                    height: 16px;\n                    border-radius: 50%;\n                }\n                .layer-grid {\n                    display: grid;\n                    grid-template-columns: repeat(auto-fill, minmax(150px, 1fr));\n                    gap: 10px;\n                    margin: 20px 0;\n                }\n                .layer-btn {\n                    padding: 12px;\n                    background: #f8f9fa;\n                    border: 1px solid #dee2e6;\n                    border-radius: 6px;\n                    text-align: center;\n                    text-decoration: none;\n                    color: #495057;\n                    transition: all 0.2s;\n                }\n                .layer-btn:hover {\n                    background: #667eea;\n                    color: white;\n                    border-color: #667eea;\n                }\n            </style>\n        </head>\n        <body>\n            <div class=\"dashboard\">\n                <div class=\"header\">\n                    <h1 style=\"margin: 0; font-size: 32px;\">\ud83e\udde0 Attention Head Dashboard</h1>\n                    <p style=\"margin: 10px 0 0 0; font-size: 16px; opacity: 0.9;\">\n                        Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf \u2022 2026-01-21 08:48:15\n                    </p>\n                </div>\n                \n                <div class=\"stats-container\">\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Total Attention Heads</div>\n                        <div class=\"stat-value\">896</div>\n                        <p>Across all layers</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Layers with Heads</div>\n                        <div class=\"stat-value\">28</div>\n                        <p>Layers containing attention</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Avg Heads/Layer</div>\n                        <div class=\"stat-value\">32.0</div>\n                        <p>Average per layer</p>\n                    </div>\n                    <div class=\"stat-card\">\n                        <div class=\"stat-label\">Max Importance</div>\n                        <div class=\"stat-value\">0.001</div>\n                        <p>Highest attention weight</p>\n                    </div>\n                </div>\n                \n                <div class=\"info-box\">\n                    <p><strong>\ud83d\udcca Dashboard Overview:</strong> This dashboard provides insights into the attention mechanism of your transformer model. Attention heads are the core components that allow the model to focus on different parts of the input sequence.</p>\n                </div>\n                \n                <h2>\ud83d\udd17 Quick Links</h2>\n                <div class=\"btn-container\">\n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=25f2630114bb40e587c91eab4813ad33&type=arrow&viztoken=a7a4842f-1986-4dbc-a4fd-aad9312c4f41&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984939&info=true&play=0&pointSize=3.5&edgeOpacity=0.8&bg=%23FFFFFF&layout=concentric&strongGravity=true&edgeInfluence=1.0&scalingRatio=6.0&showLabels=true\" class=\"btn attention\" target=\"_blank\">\n                        \ud83c\udfaf Attention Heads Visualization\n                    </a>\n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0\" class=\"btn\" target=\"_blank\">\n                        \ud83c\udf10 Full Architecture\n                    </a>\n        \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=cf7f5247f0ae40a5927040e2a5843e22&type=arrow&viztoken=d163bf6a-46a3-402f-8d43-e2384994ef2d&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984667&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0&showFilters=true&showLabels=true&sidebarMode=full\" class=\"btn full\" target=\"_blank\">\n                        \ud83d\udd04 Interactive Explorer\n                    </a>\n            \n                </div>\n                \n                <h2>\ud83c\udfa8 Layer Visualizations</h2>\n                <div class=\"layer-grid\">\n        \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=585b9f97488f4b809634fb6b37753866&type=arrow&viztoken=5feb89e6-ea53-4f41-8aab-ef114ff4ab23&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984447&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" class=\"layer-btn\" target=\"_blank\">\n                        Layer 1\n                    </a>\n                \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=ff4f9216bf07412cb92025b4d192669b&type=arrow&viztoken=69a4fa84-7d36-4b15-b02e-bf2691a78940&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984449&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" class=\"layer-btn\" target=\"_blank\">\n                        Layer 2\n                    </a>\n                \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=5074a945c69a4ee3b86f88a110578ff8&type=arrow&viztoken=37b87572-ad6f-4ad5-8ae5-8bbfe1762b98&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984450&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" class=\"layer-btn\" target=\"_blank\">\n                        Layer 3\n                    </a>\n                \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=a252f530211b4045ad92ef104dca4078&type=arrow&viztoken=2ca99544-518a-429f-9a3a-4b70ddfb95ec&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984451&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" class=\"layer-btn\" target=\"_blank\">\n                        Layer 4\n                    </a>\n                \n                    <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=0deb1d9b0c9e434f9f21f91510084156&type=arrow&viztoken=badcb59d-6dee-45fc-9821-2040f507ead4&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984453&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" class=\"layer-btn\" target=\"_blank\">\n                        Layer 5\n                    </a>\n                \n                </div>\n                \n                <div class=\"info-box\">\n                    <p><strong>\ud83d\udca1 How to use:</strong> Click on any visualization link to open it in a new tab. Use the Graphistry interface to explore, zoom, and interact with the neural network structure.</p>\n                </div>\n            </div>\n        </body>\n        </html>\n        "
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n\u2705 Dashboard created and displayed above\n\ud83d\udcc1 Dashboard also saved to: /kaggle/working/attention_dashboard.html\n\n\ud83d\udce5 To download the dashboard HTML file:\n   1. Click on the 'Data' tab in Kaggle\n   2. Navigate to '/kaggle/working/'\n   3. Download 'attention_dashboard.html'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analyzes computational graph identifying optimization opportunities and potential bottlenecks."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 14: Create Quantization Block Visualization (FIXED v2)\n# ==============================================================================\n\nprint(\"\\n\u2696\ufe0f Creating quantization block visualization...\")\n\n# Simulate quantization blocks (Q4_K_M has specific block structure)\nquantization_blocks = []\nblock_id = 1000  # Starting ID for quantization blocks\n\n# GGUF Q4_K_M uses blocks of weights\nfor layer in range(1, n_layers + 1):\n    for block in range(4):  # 4 quantization blocks per layer (simplified)\n        block_node = {\n            'node_id': block_id,\n            'name': f'QBlock_L{layer}_{block}',\n            'type': 'quantization',\n            'layer': layer,\n            'block': block,\n            'parameters': hidden_dim * hidden_dim // 16,  # Approx for Q4\n            'size_mb': (hidden_dim * hidden_dim * 0.5) / (1024**2)  # 0.5 bytes per param for Q4\n        }\n        quantization_blocks.append(block_node)\n        block_id += 1\n\nif quantization_blocks:\n    quant_df = pd.DataFrame(quantization_blocks).copy()  # Use copy()\n    \n    # Pre-process data in DataFrame\n    # Create color mapping for layers\n    unique_layers = sorted(quant_df['layer'].unique())\n    layer_palette = ['#FF6B6B', '#4ECDC4', '#FFD166', '#95E1D3', '#F38181', '#A8D8EA', '#C86B85', '#6B8CFF']\n    \n    # Map layer numbers to colors\n    layer_color_map = {}\n    for i, layer_num in enumerate(unique_layers):\n        color_idx = i % len(layer_palette)\n        layer_color_map[layer_num] = layer_palette[color_idx]\n    \n    quant_df['color'] = quant_df['layer'].map(layer_color_map)\n    \n    # Scale size_mb for point size\n    if quant_df['size_mb'].max() > quant_df['size_mb'].min():\n        quant_df['point_size'] = 20 + (\n            (quant_df['size_mb'] - quant_df['size_mb'].min()) / \n            (quant_df['size_mb'].max() - quant_df['size_mb'].min()) * 40\n        )\n    else:\n        quant_df['point_size'] = 40\n    \n    # Add tooltip\n    quant_df['tooltip'] = quant_df.apply(\n        lambda row: f\"Quant Block L{row['layer']}.{row['block']}<br>\"\n                   f\"Size: {row['size_mb']:.2f} MB<br>\"\n                   f\"Params: {row['parameters']:,}<br>\"\n                   f\"Type: {row['type']}\", \n        axis=1\n    )\n    \n    # Connect quantization blocks to their layers\n    quant_edges = []\n    for _, block in quant_df.iterrows():\n        # Try to find the corresponding layer node\n        # Look for layer nodes (assuming they have IDs like 'L1', 'L2' or numeric IDs)\n        layer_pattern = f\"L{int(block['layer'])}\"\n        layer_nodes = nodes_df[nodes_df['name'].str.contains(layer_pattern, na=False)]\n        \n        if len(layer_nodes) > 0:\n            layer_id = layer_nodes.iloc[0]['node_id']\n        else:\n            # Fallback: use layer number as ID\n            layer_id = int(block['layer'])\n        \n        quant_edges.append({\n            'source': layer_id,\n            'target': int(block['node_id']),\n            'type': 'quantizes',\n            'weight': 0.8,\n            'edge_tooltip': f\"Layer {int(block['layer'])} \u2192 Quant Block {int(block['block'])}\",\n            'edge_color': '#9B59B6'  # Add color as column in edges DataFrame\n        })\n    \n    quant_edges_df = pd.DataFrame(quant_edges)\n    \n    # Create visualization - IMPORTANT: edge_color now refers to column name\n    quant_g = graphistry.bind(\n        source='source',\n        destination='target',\n        node='node_id',\n        point_title='tooltip',\n        point_color='color',\n        point_size='point_size',\n        point_label='name',\n        edge_title='edge_tooltip',\n        edge_color='edge_color'  # This now refers to the column we added\n    )\n    \n    quant_plotter = quant_g.edges(quant_edges_df).nodes(quant_df)\n    \n    # Apply settings\n    quant_plotter = quant_plotter.settings(url_params={\n        'play': 0,\n        'pointSize': 3.0,\n        'edgeOpacity': 0.6,\n        'bg': '%23F0F4FF',\n        'layout': 'grid',\n        'strongGravity': 'true',\n        'edgeInfluence': 1.2,\n        'scalingRatio': 5.0,\n        'showLabels': 'true'\n    })\n    \n    try:\n        quant_url = quant_plotter.plot(\n            render=False,\n            name=f\"Quantization Blocks - {MODEL_FILE}\",\n            description=f\"Q4_K_M quantization blocks across {n_layers} layers\"\n        )\n        layer_urls[\"Quantization Blocks\"] = quant_url\n        print(f\"   \u2705 Quantization Blocks: {len(quant_df)} blocks visualized\")\n        print(f\"   \ud83d\udcca Block size range: {quant_df['size_mb'].min():.2f} - {quant_df['size_mb'].max():.2f} MB per block\")\n        print(f\"   \ud83d\udd17 Total quantization blocks: {len(quant_df)} across {n_layers} layers\")\n    except Exception as e:\n        print(f\"   \u26a0\ufe0f Quantization visualization failed: {e}\")\n        import traceback\n        traceback.print_exc()\nelse:\n    print(\"   \u26a0\ufe0f No quantization blocks created\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T08:54:14.118209Z",
          "iopub.execute_input": "2026-01-21T08:54:14.118890Z",
          "iopub.status.idle": "2026-01-21T08:54:15.698736Z",
          "shell.execute_reply.started": "2026-01-21T08:54:14.118842Z",
          "shell.execute_reply": "2026-01-21T08:54:15.698020Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n\u2696\ufe0f Creating quantization block visualization...\n   \u2705 Quantization Blocks: 112 blocks visualized\n   \ud83d\udcca Block size range: 4.50 - 4.50 MB per block\n   \ud83d\udd17 Total quantization blocks: 112 across 28 layers\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compares GGUF architecture to original model specs validating conversion correctness and completeness."
      ]
    },
    {
      "cell_type": "code",
      "source": "# ==============================================================================\n# Step 15: Create Interactive Dashboard (FIXED)\n# ==============================================================================\n\nprint(\"=\"*70)\nprint(\"\ud83d\udcca CREATING INTERACTIVE DASHBOARD\")\nprint(\"=\"*70)\n\n# Collect all visualization URLs\nall_visualizations = {\"Main Architecture\": main_url}\nall_visualizations.update(layer_urls)\n\n# Add quantization visualization if it exists\nif \"Quantization Blocks\" in layer_urls:\n    all_visualizations[\"Quantization Blocks\"] = layer_urls[\"Quantization Blocks\"]\n\n# Count attention heads\nattention_heads_count = len(nodes_df[nodes_df['type'] == 'attention_head']) if 'nodes_df' in locals() else 0\ntotal_nodes = len(nodes_df) if 'nodes_df' in locals() else 0\ntotal_edges = len(edges_df) if 'edges_df' in locals() else 0\n\n# Define description function\ndef get_viz_description(viz_name):\n    descriptions = {\n        \"Main Architecture\": \"Complete overview of the entire neural network architecture with all layers and connections.\",\n        \"Interactive Explorer\": \"Filter and explore different components interactively with sidebar controls.\",\n        \"Attention Heads\": \"Visualization of multi-head attention mechanisms colored by head number.\",\n        \"Quantization Blocks\": \"Q4_K_M quantization blocks showing memory distribution across layers.\",\n        \"Layer 1\": \"Detailed view of transformer layer 1 with attention heads and feed-forward networks.\",\n        \"Layer 2\": \"Detailed view of transformer layer 2 showing internal connections.\",\n        \"Layer 3\": \"Detailed view of transformer layer 3 architecture and components.\",\n        \"Layer 4\": \"Detailed view of transformer layer 4 structure and connections.\",\n        \"Layer 5\": \"Detailed view of transformer layer 5 with component visualization.\",\n    }\n    return descriptions.get(viz_name, f\"Detailed visualization of {viz_name.lower()}.\")\n\n# Create dashboard HTML\ndashboard_html = f'''\n<div style=\"margin:25px 0; padding:30px; background:linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%); border-radius:16px; box-shadow:0 20px 60px rgba(0,0,0,0.3); color:white;\">\n    <div style=\"text-align:center; margin-bottom:35px;\">\n        <h2 style=\"margin:0 0 10px 0; font-size:32px; color:white;\">\ud83e\udde0 GGUF Neural Network Visualization Dashboard</h2>\n        <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:16px;\">Interactive visualizations of {MODEL_FILE} architecture</p>\n    </div>\n    \n    <!-- Statistics Overview -->\n    <div style=\"background:rgba(255,255,255,0.1); border-radius:12px; padding:25px; margin-bottom:30px; backdrop-filter:blur(10px);\">\n        <h3 style=\"margin:0 0 20px 0; color:white; text-align:center;\">\ud83d\udcca Model Statistics</h3>\n        <div style=\"display:grid; grid-template-columns:repeat(auto-fit, minmax(200px, 1fr)); gap:20px;\">\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Layers</div>\n                <div style=\"font-size:36px; font-weight:700; color:#4ECDC4;\">{n_layers}</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Attention Heads</div>\n                <div style=\"font-size:36px; font-weight:700; color:#FF6B6B;\">{attention_heads_count}</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Nodes</div>\n                <div style=\"font-size:36px; font-weight:700; color:#FFD166;\">{total_nodes}</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Edges</div>\n                <div style=\"font-size:36px; font-weight:700; color:#95E1D3;\">{total_edges}</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Heads/Layer</div>\n                <div style=\"font-size:36px; font-weight:700; color:#A8D8EA;\">{n_heads}</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Quant Blocks</div>\n                <div style=\"font-size:36px; font-weight:700; color:#9B59B6;\">{len(quant_df) if 'quant_df' in locals() else 0}</div>\n            </div>\n        </div>\n    </div>\n    \n    <!-- Visualization Grid -->\n    <h3 style=\"margin:0 0 20px 0; color:white; text-align:center;\">\ud83d\ude80 Available Visualizations</h3>\n    <div style=\"display:grid; grid-template-columns:repeat(auto-fill, minmax(320px, 1fr)); gap:25px;\">\n'''\n\n# Define category colors and icons\ncategory_info = {\n    \"Main Architecture\": {\"color\": \"#667eea\", \"icon\": \"\ud83c\udf10\"},\n    \"Interactive Explorer\": {\"color\": \"#10b981\", \"icon\": \"\ud83d\udd0d\"},\n    \"Attention Heads\": {\"color\": \"#FF6B6B\", \"icon\": \"\ud83c\udfaf\"},\n    \"Quantization Blocks\": {\"color\": \"#9B59B6\", \"icon\": \"\u2696\ufe0f\"},\n}\n\n# Add cards for each visualization\nfor viz_name, viz_url in all_visualizations.items():\n    if viz_url:\n        # Determine category\n        if viz_name == \"Main Architecture\":\n            category = \"Main Architecture\"\n        elif \"Interactive\" in viz_name or \"Explorer\" in viz_name:\n            category = \"Interactive Explorer\"\n        elif \"Attention\" in viz_name:\n            category = \"Attention Heads\"\n        elif \"Quantization\" in viz_name:\n            category = \"Quantization Blocks\"\n        else:\n            category = \"Layer Detail\"\n        \n        # Get styling\n        if category in category_info:\n            accent_color = category_info[category][\"color\"]\n            icon = category_info[category][\"icon\"]\n        else:\n            accent_color = \"#6b7280\"\n            icon = \"\ud83d\udd27\"\n        \n        # Get description\n        description = get_viz_description(viz_name)\n        \n        dashboard_html += f'''\n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid {accent_color};\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">{icon}</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">{viz_name}</h3>\n                <span style=\"background:{accent_color}20; color:{accent_color}; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    {category}\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                {description}\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"{viz_url}\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:{accent_color}; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('{viz_url}')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"{viz_url}\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        '''\n\ndashboard_html += f'''\n    </div>\n    \n    <!-- Usage Instructions -->\n    <div style=\"margin-top:40px; background:rgba(255,255,255,0.1); border-radius:12px; padding:25px; backdrop-filter:blur(10px);\">\n        <h3 style=\"margin:0 0 15px 0; color:white;\">\ud83d\udcd6 How to Use This Dashboard</h3>\n        <div style=\"display:grid; grid-template-columns:repeat(auto-fit, minmax(250px, 1fr)); gap:20px;\">\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83d\udd0d</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Explore Visualizations</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Click any \"Open Visualization\" button to explore different aspects of the neural network.\n                </p>\n            </div>\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83c\udfa8</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Interactive Features</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Use Graphistry's tools to zoom, pan, filter, and inspect individual nodes and edges.\n                </p>\n            </div>\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83d\udcbe</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Share & Save</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Use \"Copy Link\" to share visualizations or bookmark them for later reference.\n                </p>\n            </div>\n        </div>\n    </div>\n    \n    <div style=\"margin-top:30px; text-align:center; color:rgba(255,255,255,0.7); font-size:0.9em;\">\n        <p>Generated with Graphistry \u2022 Model: {MODEL_FILE} \u2022 {pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n    </div>\n</div>\n'''\n\n# Add JavaScript for copy functionality\njs_code = '''\n<script>\nfunction copyToClipboard(url) {\n    navigator.clipboard.writeText(url).then(() => {\n        const button = event.target;\n        const originalText = button.textContent;\n        button.textContent = '\u2713 Copied!';\n        button.style.background = '#10b981';\n        button.style.color = 'white';\n        setTimeout(() => {\n            button.textContent = originalText;\n            button.style.background = '#f1f5f9';\n            button.style.color = '#64748b';\n        }, 2000);\n    }).catch(err => {\n        console.error('Failed to copy: ', err);\n    });\n}\n\n// Add event listeners to all copy buttons\ndocument.addEventListener('DOMContentLoaded', function() {\n    const buttons = document.querySelectorAll('button[data-url]');\n    buttons.forEach(button => {\n        button.addEventListener('click', function() {\n            const url = this.getAttribute('data-url');\n            copyToClipboard(url);\n        });\n    });\n});\n</script>\n'''\n\n# Display the dashboard\nfrom IPython.display import display, HTML\ndisplay(HTML(dashboard_html + js_code))\n\n# Save dashboard to file\ndashboard_path = '/kaggle/working/complete_dashboard.html'\nwith open(dashboard_path, 'w') as f:\n    final_html = f'''<!DOCTYPE html>\n<html>\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>GGUF Visualization Dashboard - {MODEL_FILE}</title>\n    <style>\n        body {{\n            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\n            margin: 0;\n            padding: 20px;\n            background: linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%);\n            min-height: 100vh;\n        }}\n        button:hover {{\n            background: #e2e8f0 !important;\n        }}\n    </style>\n</head>\n<body>\n{dashboard_html}\n{js_code}\n</body>\n</html>'''\n    f.write(final_html)\n\nprint(f\"\\n\u2705 Interactive Dashboard Created!\")\nprint(f\"\ud83d\udcc1 Dashboard saved to: {dashboard_path}\")\nprint(f\"\\n\ud83d\udccb Dashboard Summary:\")\nprint(f\"   \u2022 Total Visualizations: {len(all_visualizations)}\")\nprint(f\"   \u2022 Model: {MODEL_FILE}\")\nprint(f\"   \u2022 Layers: {n_layers}\")\nprint(f\"   \u2022 Attention Heads: {attention_heads_count}\")\nprint(f\"   \u2022 Quantization Blocks: {len(quant_df) if 'quant_df' in locals() else 0}\")\n\n# Display download instructions\nprint(f\"\\n\ud83d\udce5 To download the dashboard:\")\nprint(f\"   1. Click on the 'Data' tab in Kaggle\")\nprint(f\"   2. Navigate to '/kaggle/working/'\")\nprint(f\"   3. Download 'complete_dashboard.html'\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-01-21T09:00:20.983952Z",
          "iopub.execute_input": "2026-01-21T09:00:20.984588Z",
          "iopub.status.idle": "2026-01-21T09:00:21.006433Z",
          "shell.execute_reply.started": "2026-01-21T09:00:20.984556Z",
          "shell.execute_reply": "2026-01-21T09:00:21.005698Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "======================================================================\n\ud83d\udcca CREATING INTERACTIVE DASHBOARD\n======================================================================\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n<div style=\"margin:25px 0; padding:30px; background:linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%); border-radius:16px; box-shadow:0 20px 60px rgba(0,0,0,0.3); color:white;\">\n    <div style=\"text-align:center; margin-bottom:35px;\">\n        <h2 style=\"margin:0 0 10px 0; font-size:32px; color:white;\">\ud83e\udde0 GGUF Neural Network Visualization Dashboard</h2>\n        <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:16px;\">Interactive visualizations of Llama-3.2-3B-Instruct-Q4_K_M.gguf architecture</p>\n    </div>\n    \n    <!-- Statistics Overview -->\n    <div style=\"background:rgba(255,255,255,0.1); border-radius:12px; padding:25px; margin-bottom:30px; backdrop-filter:blur(10px);\">\n        <h3 style=\"margin:0 0 20px 0; color:white; text-align:center;\">\ud83d\udcca Model Statistics</h3>\n        <div style=\"display:grid; grid-template-columns:repeat(auto-fit, minmax(200px, 1fr)); gap:20px;\">\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Layers</div>\n                <div style=\"font-size:36px; font-weight:700; color:#4ECDC4;\">28</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Attention Heads</div>\n                <div style=\"font-size:36px; font-weight:700; color:#FF6B6B;\">896</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Nodes</div>\n                <div style=\"font-size:36px; font-weight:700; color:#FFD166;\">929</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Total Edges</div>\n                <div style=\"font-size:36px; font-weight:700; color:#95E1D3;\">981</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Heads/Layer</div>\n                <div style=\"font-size:36px; font-weight:700; color:#A8D8EA;\">32</div>\n            </div>\n            <div style=\"text-align:center;\">\n                <div style=\"font-size:14px; color:rgba(255,255,255,0.7); margin-bottom:8px;\">Quant Blocks</div>\n                <div style=\"font-size:36px; font-weight:700; color:#9B59B6;\">112</div>\n            </div>\n        </div>\n    </div>\n    \n    <!-- Visualization Grid -->\n    <h3 style=\"margin:0 0 20px 0; color:white; text-align:center;\">\ud83d\ude80 Available Visualizations</h3>\n    <div style=\"display:grid; grid-template-columns:repeat(auto-fill, minmax(320px, 1fr)); gap:25px;\">\n\n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #667eea;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83c\udf10</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Main Architecture</h3>\n                <span style=\"background:#667eea20; color:#667eea; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Main Architecture\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Complete overview of the entire neural network architecture with all layers and connections.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#667eea; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=f3192b7e4e7a43e9bbf52f2f13254a4c&type=arrow&viztoken=f6647b39-99e2-45cd-aea9-71223b0f6eda&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984237&info=true&play=0&pointSize=2.5&edgeOpacity=0.6&bg=%23FFFFFF&strongGravity=true&edgeInfluence=1.0&scalingRatio=10.0\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #6b7280;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83d\udd27</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Layer 1</h3>\n                <span style=\"background:#6b728020; color:#6b7280; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Layer Detail\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Detailed view of transformer layer 1 with attention heads and feed-forward networks.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=585b9f97488f4b809634fb6b37753866&type=arrow&viztoken=5feb89e6-ea53-4f41-8aab-ef114ff4ab23&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984447&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#6b7280; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=585b9f97488f4b809634fb6b37753866&type=arrow&viztoken=5feb89e6-ea53-4f41-8aab-ef114ff4ab23&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984447&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=585b9f97488f4b809634fb6b37753866&type=arrow&viztoken=5feb89e6-ea53-4f41-8aab-ef114ff4ab23&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984447&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #6b7280;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83d\udd27</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Layer 2</h3>\n                <span style=\"background:#6b728020; color:#6b7280; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Layer Detail\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Detailed view of transformer layer 2 showing internal connections.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=ff4f9216bf07412cb92025b4d192669b&type=arrow&viztoken=69a4fa84-7d36-4b15-b02e-bf2691a78940&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984449&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#6b7280; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=ff4f9216bf07412cb92025b4d192669b&type=arrow&viztoken=69a4fa84-7d36-4b15-b02e-bf2691a78940&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984449&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=ff4f9216bf07412cb92025b4d192669b&type=arrow&viztoken=69a4fa84-7d36-4b15-b02e-bf2691a78940&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984449&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #6b7280;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83d\udd27</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Layer 3</h3>\n                <span style=\"background:#6b728020; color:#6b7280; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Layer Detail\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Detailed view of transformer layer 3 architecture and components.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=5074a945c69a4ee3b86f88a110578ff8&type=arrow&viztoken=37b87572-ad6f-4ad5-8ae5-8bbfe1762b98&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984450&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#6b7280; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=5074a945c69a4ee3b86f88a110578ff8&type=arrow&viztoken=37b87572-ad6f-4ad5-8ae5-8bbfe1762b98&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984450&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=5074a945c69a4ee3b86f88a110578ff8&type=arrow&viztoken=37b87572-ad6f-4ad5-8ae5-8bbfe1762b98&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984450&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #6b7280;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83d\udd27</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Layer 4</h3>\n                <span style=\"background:#6b728020; color:#6b7280; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Layer Detail\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Detailed view of transformer layer 4 structure and connections.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=a252f530211b4045ad92ef104dca4078&type=arrow&viztoken=2ca99544-518a-429f-9a3a-4b70ddfb95ec&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984451&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#6b7280; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=a252f530211b4045ad92ef104dca4078&type=arrow&viztoken=2ca99544-518a-429f-9a3a-4b70ddfb95ec&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984451&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=a252f530211b4045ad92ef104dca4078&type=arrow&viztoken=2ca99544-518a-429f-9a3a-4b70ddfb95ec&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984451&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #6b7280;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83d\udd27</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Layer 5</h3>\n                <span style=\"background:#6b728020; color:#6b7280; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Layer Detail\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Detailed view of transformer layer 5 with component visualization.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=0deb1d9b0c9e434f9f21f91510084156&type=arrow&viztoken=badcb59d-6dee-45fc-9821-2040f507ead4&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984453&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#6b7280; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=0deb1d9b0c9e434f9f21f91510084156&type=arrow&viztoken=badcb59d-6dee-45fc-9821-2040f507ead4&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984453&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=0deb1d9b0c9e434f9f21f91510084156&type=arrow&viztoken=badcb59d-6dee-45fc-9821-2040f507ead4&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984453&info=true&play=0&pointSize=3.0&edgeOpacity=0.7&bg=%23F8F9FA&strongGravity=true&edgeInfluence=1.0&scalingRatio=8.0&showLabels=True\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #FF6B6B;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\ud83c\udfaf</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Attention Heads</h3>\n                <span style=\"background:#FF6B6B20; color:#FF6B6B; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Attention Heads\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Visualization of multi-head attention mechanisms colored by head number.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=25f2630114bb40e587c91eab4813ad33&type=arrow&viztoken=a7a4842f-1986-4dbc-a4fd-aad9312c4f41&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984939&info=true&play=0&pointSize=3.5&edgeOpacity=0.8&bg=%23FFFFFF&layout=concentric&strongGravity=true&edgeInfluence=1.0&scalingRatio=6.0&showLabels=true\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#FF6B6B; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=25f2630114bb40e587c91eab4813ad33&type=arrow&viztoken=a7a4842f-1986-4dbc-a4fd-aad9312c4f41&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984939&info=true&play=0&pointSize=3.5&edgeOpacity=0.8&bg=%23FFFFFF&layout=concentric&strongGravity=true&edgeInfluence=1.0&scalingRatio=6.0&showLabels=true')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=25f2630114bb40e587c91eab4813ad33&type=arrow&viztoken=a7a4842f-1986-4dbc-a4fd-aad9312c4f41&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768984939&info=true&play=0&pointSize=3.5&edgeOpacity=0.8&bg=%23FFFFFF&layout=concentric&strongGravity=true&edgeInfluence=1.0&scalingRatio=6.0&showLabels=true\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n        <div style=\"background:rgba(255,255,255,0.95); border-radius:12px; padding:25px; box-shadow:0 8px 25px rgba(0,0,0,0.1); transition:all 0.3s; border-top:4px solid #9B59B6;\">\n            <div style=\"display:flex; align-items:center; margin-bottom:15px;\">\n                <div style=\"font-size:24px; margin-right:12px;\">\u2696\ufe0f</div>\n                <h3 style=\"margin:0; font-size:1.2em; color:#1e293b; flex-grow:1;\">Quantization Blocks</h3>\n                <span style=\"background:#9B59B620; color:#9B59B6; padding:4px 12px; border-radius:20px; font-size:0.85em; font-weight:600;\">\n                    Quantization Blocks\n                </span>\n            </div>\n            <p style=\"margin:0 0 20px 0; color:#64748b; font-size:0.95em; line-height:1.5;\">\n                Q4_K_M quantization blocks showing memory distribution across layers.\n            </p>\n            <div style=\"display:flex; gap:10px;\">\n                <a href=\"https://hub.graphistry.com/graph/graph.html?dataset=4b2355537e924875b95fb9a07a985ed5&type=arrow&viztoken=0c1f3f06-7d83-4a00-ad62-f7c15f04be57&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768985670&info=true&play=0&pointSize=3.0&edgeOpacity=0.6&bg=%23F0F4FF&layout=grid&strongGravity=true&edgeInfluence=1.2&scalingRatio=5.0&showLabels=true\" target=\"_blank\" \n                   style=\"flex:1; text-align:center; padding:12px; \n                          background:#9B59B6; color:white; border-radius:8px; \n                          text-decoration:none; font-weight:600; font-size:0.95em;\n                          transition:all 0.2s;\">\n                    \ud83d\ude80 Open Visualization\n                </a>\n                <button onclick=\"copyToClipboard('https://hub.graphistry.com/graph/graph.html?dataset=4b2355537e924875b95fb9a07a985ed5&type=arrow&viztoken=0c1f3f06-7d83-4a00-ad62-f7c15f04be57&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768985670&info=true&play=0&pointSize=3.0&edgeOpacity=0.6&bg=%23F0F4FF&layout=grid&strongGravity=true&edgeInfluence=1.2&scalingRatio=5.0&showLabels=true')\" \n                        style=\"padding:12px 20px; background:#f1f5f9; color:#64748b; \n                               border:1px solid #e2e8f0; border-radius:8px; cursor:pointer;\n                               font-weight:600; font-size:0.95em; transition:all 0.2s;\"\n                        data-url=\"https://hub.graphistry.com/graph/graph.html?dataset=4b2355537e924875b95fb9a07a985ed5&type=arrow&viztoken=0c1f3f06-7d83-4a00-ad62-f7c15f04be57&usertag=ddca19d4-pygraphistry-0.50.4&splashAfter=1768985670&info=true&play=0&pointSize=3.0&edgeOpacity=0.6&bg=%23F0F4FF&layout=grid&strongGravity=true&edgeInfluence=1.2&scalingRatio=5.0&showLabels=true\">\n                    \ud83d\udccb Copy Link\n                </button>\n            </div>\n        </div>\n        \n    </div>\n    \n    <!-- Usage Instructions -->\n    <div style=\"margin-top:40px; background:rgba(255,255,255,0.1); border-radius:12px; padding:25px; backdrop-filter:blur(10px);\">\n        <h3 style=\"margin:0 0 15px 0; color:white;\">\ud83d\udcd6 How to Use This Dashboard</h3>\n        <div style=\"display:grid; grid-template-columns:repeat(auto-fit, minmax(250px, 1fr)); gap:20px;\">\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83d\udd0d</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Explore Visualizations</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Click any \"Open Visualization\" button to explore different aspects of the neural network.\n                </p>\n            </div>\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83c\udfa8</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Interactive Features</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Use Graphistry's tools to zoom, pan, filter, and inspect individual nodes and edges.\n                </p>\n            </div>\n            <div style=\"background:rgba(255,255,255,0.1); padding:20px; border-radius:8px;\">\n                <div style=\"font-size:24px; margin-bottom:10px;\">\ud83d\udcbe</div>\n                <h4 style=\"margin:0 0 10px 0; color:white;\">Share & Save</h4>\n                <p style=\"margin:0; color:rgba(255,255,255,0.8); font-size:0.9em;\">\n                    Use \"Copy Link\" to share visualizations or bookmark them for later reference.\n                </p>\n            </div>\n        </div>\n    </div>\n    \n    <div style=\"margin-top:30px; text-align:center; color:rgba(255,255,255,0.7); font-size:0.9em;\">\n        <p>Generated with Graphistry \u2022 Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf \u2022 2026-01-21 09:00:20</p>\n    </div>\n</div>\n\n<script>\nfunction copyToClipboard(url) {\n    navigator.clipboard.writeText(url).then(() => {\n        const button = event.target;\n        const originalText = button.textContent;\n        button.textContent = '\u2713 Copied!';\n        button.style.background = '#10b981';\n        button.style.color = 'white';\n        setTimeout(() => {\n            button.textContent = originalText;\n            button.style.background = '#f1f5f9';\n            button.style.color = '#64748b';\n        }, 2000);\n    }).catch(err => {\n        console.error('Failed to copy: ', err);\n    });\n}\n\n// Add event listeners to all copy buttons\ndocument.addEventListener('DOMContentLoaded', function() {\n    const buttons = document.querySelectorAll('button[data-url]');\n    buttons.forEach(button => {\n        button.addEventListener('click', function() {\n            const url = this.getAttribute('data-url');\n            copyToClipboard(url);\n        });\n    });\n});\n</script>\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "\n\u2705 Interactive Dashboard Created!\n\ud83d\udcc1 Dashboard saved to: /kaggle/working/complete_dashboard.html\n\n\ud83d\udccb Dashboard Summary:\n   \u2022 Total Visualizations: 8\n   \u2022 Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n   \u2022 Layers: 28\n   \u2022 Attention Heads: 896\n   \u2022 Quantization Blocks: 112\n\n\ud83d\udce5 To download the dashboard:\n   1. Click on the 'Data' tab in Kaggle\n   2. Navigate to '/kaggle/working/'\n   3. Download 'complete_dashboard.html'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizes quantization impact showing which layers are quantized and precision distribution across network."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates parameter distribution histograms showing weight statistics and quantization bin distributions."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generates layer timing estimates based on FLOPs and tensor sizes for performance prediction."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implements architecture search visualization showing how similar models differ in layer configurations."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates interactive documentation with embedded visualizations for model architecture understanding."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exports architecture metadata to JSON for programmatic access and integration with other tools."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Demonstrates architecture pruning visualization showing which layers could be removed with minimal impact."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clears loaded model data, releases memory, and provides final summary of architecture analysis and insights."
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}