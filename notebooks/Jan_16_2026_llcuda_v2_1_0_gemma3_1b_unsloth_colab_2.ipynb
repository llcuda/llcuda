{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# llcuda v2.1.0 + Unsloth Gemma 3-1B Tutorial\n",
        "\n",
        "**Complete Guide**: Using llcuda v2.1.0 with Unsloth GGUF models on Tesla T4 GPU\n",
        "\n",
        "**What This Demonstrates**:\n",
        "1. âœ… Install llcuda v2.1.0 from GitHub (no PyPI)\n",
        "2. âœ… Auto-download CUDA binaries from GitHub Releases (v2.1.0 primary bundle with automatic v2.0.6 fallback)\n",
        "3. âœ… Use Gemma 3-1B-IT GGUF from Unsloth\n",
        "4. âœ… Fast inference on Tesla T4 with FlashAttention\n",
        "5. âœ… Performance benchmarking and optimization\n",
        "\n",
        "**Requirements**:\n",
        "- Google Colab with Tesla T4 GPU\n",
        "- Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "- **llcuda v2.1.0**: https://github.com/llcuda/llcuda\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth GGUF Models**: https://huggingface.co/unsloth\n",
        "- **Installation Guide**: https://github.com/llcuda/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Verify Tesla T4 GPU\n",
        "\n",
        "llcuda v2.1.0 now ships with Tesla T4-optimized CUDA 12 binaries (v2.1.0 bundle) and automatically falls back to the proven v2.0.6 bundle only if the latest archive is unavailable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_gpu",
        "outputId": "8ed233a5-d8dc-47f8-fa8b-1e6148454700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name, compute_cap, memory.total [MiB]\n",
            "Tesla T4, 7.5, 15360 MiB\n",
            "\n",
            "======================================================================\n",
            "GPU: Tesla T4\n",
            "Compute Capability: SM 7.5\n",
            "======================================================================\n",
            "\n",
            "âœ… Tesla T4 detected - Perfect for llcuda v2.1.0!\n",
            "   Binaries (v2.1.0) include FlashAttention and Tensor Core optimization\n"
          ]
        }
      ],
      "source": [
        "# Check GPU configuration\n",
        "!nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv\n",
        "\n",
        "import subprocess\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['nvidia-smi', '--query-gpu=name,compute_cap', '--format=csv,noheader'],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "gpu_info = result.stdout.strip().split(',')\n",
        "gpu_name = gpu_info[0].strip()\n",
        "compute_cap = gpu_info[1].strip()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"Compute Capability: SM {compute_cap}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if 'T4' in gpu_name and compute_cap == '7.5':\n",
        "    print(\"\\nâœ… Tesla T4 detected - Perfect for llcuda v2.1.0!\")\n",
        "    print(\"   Binaries (v2.1.0) include FlashAttention and Tensor Core optimization\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸  {gpu_name} detected\")\n",
        "    print(\"   llcuda v2.1.0 is optimized for Tesla T4\")\n",
        "    print(\"   Performance may vary on other GPUs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Install llcuda v2.1.0 from GitHub\n",
        "\n",
        "**Note**: llcuda is now **GitHub-only** (no longer on PyPI)\n",
        "\n",
        "The primary CUDA binary bundle (v2.1.0, ~266 MB) auto-downloads from GitHub Releases on first import. If that archive is temporarily unavailable, the bootstrapper transparently falls back to the v2.0.6 bundle that remains 100% compatible with the v2.1.0 Python APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_llcuda",
        "outputId": "10e5c613-fa61-456f-d5b3-d12b33d44c61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Installing llcuda v2.1.0 from GitHub...\n",
            "\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "âœ… llcuda v2.1.0 installed successfully!\n",
            "\n",
            "â„¹ï¸  CUDA binaries (v2.1.0, ~266 MB) will auto-download on first import\n",
            "   These binaries are fully compatible with v2.1.0 Python APIs\n",
            "   Download happens once - subsequent runs use cached binaries\n"
          ]
        }
      ],
      "source": [
        "# Install llcuda v2.1.0 from GitHub\n",
        "print(\"ğŸ“¥ Installing llcuda v2.1.0 from GitHub...\\n\")\n",
        "\n",
        "!pip install -q \"git+https://github.com/llcuda/llcuda.git@v2.1.1\"\n",
        "\n",
        "print(\"\\nâœ… llcuda v2.1.0 installed successfully!\")\n",
        "print(\"\\nâ„¹ï¸  CUDA binaries (v2.1.0, ~266 MB) will auto-download on first import\")\n",
        "print(\"   These binaries are fully compatible with v2.1.0 Python APIs\")\n",
        "print(\"   Download happens once - subsequent runs use cached binaries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Import llcuda (Triggers Binary Download)\n",
        "\n",
        "First import downloads CUDA binaries from:\n",
        "https://github.com/llcuda/llcuda/releases/download/v2.1.0/llcuda-binaries-cuda12-t4-v2.1.0.tar.gz\n",
        "\n",
        "**Note**: If the v2.1.0 archive is unavailable, llcuda automatically falls back to the v2.0.6 bundle so the notebook can continue running without manual intervention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "import_llcuda",
        "outputId": "16637eca-81ed-496e-bae6-c8df0ecc8dd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n",
            "/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py:112: RuntimeWarning: llcuda bootstrap failed: cannot import name '__version__' from partially initialized module 'llcuda' (most likely due to a circular import) (/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py)\n",
            "Some features may not work. Please check your installation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llcuda: New version available (2.1.1) - pip install --upgrade git+https://github.com/llcuda/llcuda.git\n",
            "\n",
            "======================================================================\n",
            "llcuda version: 2.1.0\n",
            "======================================================================\n",
            "\n",
            "âœ… llcuda imported successfully!\n",
            "\n",
            "â„¹ï¸  If this was the first run:\n",
            "   - CUDA binaries were downloaded to ~/.cache/llcuda/\n",
            "   - Includes: llama-server, libggml-cuda.so, and supporting libs\n",
            "   - Next imports will be instant (binaries cached)\n"
          ]
        }
      ],
      "source": [
        "import llcuda\n",
        "import time\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"llcuda version: {llcuda.__version__}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nâœ… llcuda imported successfully!\")\n",
        "print(\"\\nâ„¹ï¸  If this was the first run:\")\n",
        "print(\"   - CUDA binaries were downloaded to ~/.cache/llcuda/\")\n",
        "print(\"   - Includes: llama-server, libggml-cuda.so, and supporting libs\")\n",
        "print(\"   - Next imports will be instant (binaries cached)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Verify GPU Compatibility\n",
        "\n",
        "Check if llcuda can detect and use the GPU properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_compat",
        "outputId": "1816b94d-cb40-432b-a43e-1729ef243c1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "GPU COMPATIBILITY CHECK\n",
            "======================================================================\n",
            "GPU Name: Tesla T4\n",
            "Compute Capability: SM 7.5\n",
            "Platform: colab\n",
            "Compatible: True\n",
            "======================================================================\n",
            "\n",
            "âœ… GPU is compatible with llcuda!\n"
          ]
        }
      ],
      "source": [
        "# Check GPU compatibility with llcuda\n",
        "compat = llcuda.check_gpu_compatibility()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"GPU COMPATIBILITY CHECK\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"GPU Name: {compat['gpu_name']}\")\n",
        "print(f\"Compute Capability: SM {compat['compute_capability']}\")\n",
        "print(f\"Platform: {compat['platform']}\")\n",
        "print(f\"Compatible: {compat['compatible']}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "if compat['compatible']:\n",
        "    print(\"\\nâœ… GPU is compatible with llcuda!\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  GPU compatibility issue detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Load Gemma 3-1B-IT from Unsloth\n",
        "\n",
        "**Three methods to load models:**\n",
        "\n",
        "1. **HuggingFace Hub** (recommended): Direct from Unsloth's repo\n",
        "2. **Model Registry**: Pre-configured model names\n",
        "3. **Local Path**: From downloaded GGUF file\n",
        "\n",
        "We'll use Method 1: Direct from Unsloth HuggingFace repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "load_model",
        "outputId": "b4660018-fc19-4761-98b0-dfeb6a6bbf26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ“¥ Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\n",
            "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
            "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
            "   This may take 2-3 minutes on first run (downloads model)\n",
            "\n",
            "Loading model: unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\n",
            "âœ“ Using cached model: gemma-3-1b-it-Q4_K_M.gguf\n",
            "\n",
            "Auto-configuring optimal settings...\n",
            "âœ“ Auto-configured for 15.0 GB VRAM\n",
            "  GPU Layers: 99\n",
            "  Context Size: 4096\n",
            "  Batch Size: 2048\n",
            "  Micro-batch Size: 512\n",
            "\n",
            "Starting llama-server...\n",
            "GPU Check:\n",
            "  Platform: colab\n",
            "  GPU: Tesla T4\n",
            "  Compute Capability: 7.5\n",
            "  Status: âœ“ Compatible\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'system_paths' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-670427480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m engine.load_model(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# Suppress llama-server output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, model_name_or_path, gpu_layers, ctx_size, auto_start, auto_configure, n_parallel, verbose, interactive_download, silent, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 )\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 success = self._server_manager.start_server(\n\u001b[0m\u001b[1;32m    406\u001b[0m                     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                     \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/server.py\u001b[0m in \u001b[0;36mstart_server\u001b[0;34m(self, model_path, port, host, gpu_layers, ctx_size, n_parallel, batch_size, ubatch_size, timeout, verbose, skip_gpu_check, silent, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;31m# Find llama-server executable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_llama_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llcuda/server.py\u001b[0m in \u001b[0;36mfind_llama_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0;34m\"Failed to download and extract llama-server binaries.\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 )\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpath_str\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msystem_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mserver_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_library_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'system_paths' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize inference engine\n",
        "engine = llcuda.InferenceEngine()\n",
        "\n",
        "print(\"\\nğŸ“¥ Loading Gemma 3-1B-IT Q4_K_M from Unsloth...\")\n",
        "print(\"   Repository: unsloth/gemma-3-1b-it-GGUF\")\n",
        "print(\"   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\")\n",
        "print(\"   This may take 2-3 minutes on first run (downloads model)\\n\")\n",
        "\n",
        "# Load model from Unsloth HuggingFace repository\n",
        "# Format: repo_id:filename\n",
        "start_time = time.time()\n",
        "\n",
        "engine.load_model(\n",
        "    \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "    silent=True,        # Suppress llama-server output\n",
        "    auto_start=True     # Start server automatically\n",
        ")\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nâœ… Model loaded successfully in {load_time:.1f}s!\")\n",
        "print(\"\\nğŸš€ Ready for inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: First Inference - General Knowledge\n",
        "\n",
        "Test the model with a general knowledge question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "first_inference"
      },
      "outputs": [],
      "source": [
        "# Test with a general knowledge prompt\n",
        "prompt = \"Explain quantum computing in simple terms that a beginner can understand.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PROMPT:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(prompt)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Generating response...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    prompt,\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(\"RESPONSE:\")\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Performance:\")\n",
        "print(f\"   Tokens generated: {result.tokens_generated}\")\n",
        "print(f\"   Latency: {result.latency_ms:.1f} ms\")\n",
        "print(f\"   Speed: {result.tokens_per_sec:.1f} tokens/sec\")\n",
        "print(f\"\\nğŸ’¡ Expected on Tesla T4: ~45 tokens/sec with Q4_K_M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Code Generation\n",
        "\n",
        "Test the model's ability to generate Python code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code_generation"
      },
      "outputs": [],
      "source": [
        "# Test code generation\n",
        "code_prompt = \"\"\"Write a Python function to calculate the fibonacci sequence using dynamic programming.\n",
        "Include docstring and example usage.\"\"\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CODE GENERATION TEST\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {code_prompt}\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(\"\\nğŸ¤– Generating code...\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    code_prompt,\n",
        "    max_tokens=300,\n",
        "    temperature=0.3,  # Lower temperature for more deterministic code\n",
        "    top_p=0.95\n",
        ")\n",
        "\n",
        "print(f\"{'='*70}\")\n",
        "print(result.text)\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tokens/sec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Batch Inference\n",
        "\n",
        "Process multiple prompts efficiently with batch inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "batch_inference"
      },
      "outputs": [],
      "source": [
        "# Prepare multiple prompts\n",
        "prompts = [\n",
        "    \"What is machine learning in one sentence?\",\n",
        "    \"Explain neural networks briefly.\",\n",
        "    \"What is the difference between AI and ML?\",\n",
        "    \"Define deep learning concisely.\"\n",
        "]\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"BATCH INFERENCE - Processing 4 prompts\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "results = engine.batch_infer(prompts, max_tokens=80, temperature=0.7)\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "for i, (prompt, result) in enumerate(zip(prompts, results), 1):\n",
        "    print(f\"\\n{'â”€'*70}\")\n",
        "    print(f\"Query {i}: {prompt}\")\n",
        "    print(f\"{'â”€'*70}\")\n",
        "    print(result.text)\n",
        "    print(f\"\\nğŸ“Š Speed: {result.tokens_per_sec:.1f} tok/s | Latency: {result.latency_ms:.0f}ms\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Total batch time: {total_time:.1f}s for {len(prompts)} prompts\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Performance Metrics\n",
        "\n",
        "Analyze aggregated performance metrics across all requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics"
      },
      "outputs": [],
      "source": [
        "# Get comprehensive metrics\n",
        "metrics = engine.get_metrics()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"PERFORMANCE METRICS SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Throughput:\")\n",
        "print(f\"   Total requests: {metrics['throughput']['total_requests']}\")\n",
        "print(f\"   Total tokens: {metrics['throughput']['total_tokens']}\")\n",
        "print(f\"   Average speed: {metrics['throughput']['tokens_per_sec']:.1f} tokens/sec\")\n",
        "\n",
        "print(f\"\\nâ±ï¸  Latency Distribution:\")\n",
        "print(f\"   Mean: {metrics['latency']['mean_ms']:.1f} ms\")\n",
        "print(f\"   Median (P50): {metrics['latency']['p50_ms']:.1f} ms\")\n",
        "print(f\"   P95: {metrics['latency']['p95_ms']:.1f} ms\")\n",
        "print(f\"   P99: {metrics['latency']['p99_ms']:.1f} ms\")\n",
        "print(f\"   Min: {metrics['latency']['min_ms']:.1f} ms\")\n",
        "print(f\"   Max: {metrics['latency']['max_ms']:.1f} ms\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Sample count: {metrics['latency']['sample_count']}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step10"
      },
      "source": [
        "## Step 10: Advanced Generation Parameters\n",
        "\n",
        "Explore different generation strategies and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advanced_params"
      },
      "outputs": [],
      "source": [
        "# Test creative writing with higher temperature\n",
        "creative_prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"CREATIVE GENERATION (High Temperature)\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Prompt: {creative_prompt}\")\n",
        "print(f\"Parameters: temperature=0.9, top_p=0.95, top_k=50\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "result = engine.infer(\n",
        "    creative_prompt,\n",
        "    max_tokens=100,\n",
        "    temperature=0.9,     # High creativity\n",
        "    top_p=0.95,          # Nucleus sampling\n",
        "    top_k=50,            # Top-k sampling\n",
        "    stop_sequences=[\"\\n\\n\"]  # Stop at double newline\n",
        ")\n",
        "\n",
        "print(result.text)\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step11"
      },
      "source": [
        "## Step 11: Alternative Model Loading Methods\n",
        "\n",
        "Demonstrate other ways to load models with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alternative_loading"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALTERNATIVE MODEL LOADING METHODS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1ï¸âƒ£  HuggingFace Hub (Current method - RECOMMENDED):\")\n",
        "print(\"   engine.load_model('unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf')\")\n",
        "print(\"   âœ… Direct from Unsloth repository\")\n",
        "print(\"   âœ… Auto-downloads and caches\")\n",
        "print(\"   âœ… Always up-to-date\")\n",
        "\n",
        "print(\"\\n2ï¸âƒ£  Model Registry (Pre-configured shortcuts):\")\n",
        "print(\"   engine.load_model('gemma-3-1b-Q4_K_M')\")\n",
        "print(\"   âœ… Simple one-word names\")\n",
        "print(\"   âœ… Curated model list\")\n",
        "print(\"   âš ï¸  May not include all Unsloth models\")\n",
        "\n",
        "print(\"\\n3ï¸âƒ£  Local Path (Pre-downloaded GGUF):\")\n",
        "print(\"   engine.load_model('/path/to/model.gguf')\")\n",
        "print(\"   âœ… Full control over model files\")\n",
        "print(\"   âœ… No network dependency after download\")\n",
        "print(\"   âš ï¸  Manual model management\")\n",
        "\n",
        "print(\"\\nğŸ’¡ Tip: For Unsloth models, use method 1 (HuggingFace Hub)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step12"
      },
      "source": [
        "## Step 12: Unsloth Fine-tuning â†’ llcuda Inference Workflow\n",
        "\n",
        "Example workflow showing how to integrate llcuda with Unsloth fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "workflow_demo"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘          UNSLOTH FINE-TUNING â†’ llcuda INFERENCE WORKFLOW             â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "STEP 1: Fine-tune with Unsloth\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load base model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "# Fine-tune on your dataset\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    ...\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "STEP 2: Export to GGUF\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Save fine-tuned model as GGUF (Q4_K_M quantization)\n",
        "model.save_pretrained_gguf(\n",
        "    \"my_finetuned_model\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"\n",
        ")\n",
        "\n",
        "# This creates: my_finetuned_model/unsloth.Q4_K_M.gguf\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "STEP 3: Deploy with llcuda v2.1.0\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import llcuda\n",
        "\n",
        "# Load your fine-tuned GGUF model\n",
        "engine = llcuda.InferenceEngine()\n",
        "engine.load_model(\"my_finetuned_model/unsloth.Q4_K_M.gguf\")\n",
        "\n",
        "# Run inference\n",
        "result = engine.infer(\"Your task-specific prompt\", max_tokens=200)\n",
        "print(result.text)\n",
        "print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "âœ… BENEFITS:\n",
        "   â€¢ Fast training with Unsloth (2x faster, 70% less VRAM)\n",
        "   â€¢ Fast inference with llcuda (FlashAttention, T4-optimized)\n",
        "   â€¢ Easy deployment (GGUF = single portable file)\n",
        "   â€¢ Full llama.cpp ecosystem compatibility\n",
        "   â€¢ Production-ready inference server\n",
        "\n",
        "ğŸ“Š EXPECTED PERFORMANCE (Tesla T4):\n",
        "   â€¢ Gemma 3-1B Q4_K_M: ~45 tok/s\n",
        "   â€¢ Llama 3.2-3B Q4_K_M: ~30 tok/s\n",
        "   â€¢ Qwen 2.5-7B Q4_K_M: ~18 tok/s\n",
        "   â€¢ Llama 3.1-8B Q4_K_M: ~15 tok/s\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step13"
      },
      "source": [
        "## Step 13: Context Manager Usage\n",
        "\n",
        "Use llcuda with Python context managers for automatic cleanup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "context_manager"
      },
      "outputs": [],
      "source": [
        "# Context manager automatically cleans up resources\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"CONTEXT MANAGER DEMO\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "with llcuda.InferenceEngine(server_url=\"http://127.0.0.1:8093\") as temp_engine:\n",
        "    print(\"ğŸ“¥ Loading model in context...\")\n",
        "    temp_engine.load_model(\n",
        "        \"unsloth/gemma-3-1b-it-GGUF:gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "        silent=True\n",
        "    )\n",
        "\n",
        "    print(\"ğŸ¤– Running quick test...\\n\")\n",
        "    result = temp_engine.infer(\n",
        "        \"What is the capital of France?\",\n",
        "        max_tokens=30\n",
        "    )\n",
        "\n",
        "    print(f\"Response: {result.text}\")\n",
        "    print(f\"Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
        "\n",
        "print(\"\\nâœ… Context exited - engine automatically cleaned up\")\n",
        "print(\"   Server stopped, resources released\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step14"
      },
      "source": [
        "## Step 14: Available Unsloth Models\n",
        "\n",
        "Browse popular Unsloth GGUF models compatible with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "available_models"
      },
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "â•‘              POPULAR UNSLOTH GGUF MODELS FOR llcuda v2.1.0           â•‘\n",
        "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "ğŸ”¹ SMALL MODELS (1-3B) - Best for T4\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "1. Gemma 3-1B Instruct\n",
        "   Repository: unsloth/gemma-3-1b-it-GGUF\n",
        "   File: gemma-3-1b-it-Q4_K_M.gguf (~650 MB)\n",
        "   Speed on T4: ~45 tok/s | VRAM: ~1.2 GB\n",
        "   Use case: General chat, Q&A, reasoning\n",
        "\n",
        "2. Llama 3.2-3B Instruct\n",
        "   Repository: unsloth/Llama-3.2-3B-Instruct-GGUF\n",
        "   File: Llama-3.2-3B-Instruct-Q4_K_M.gguf (~2 GB)\n",
        "   Speed on T4: ~30 tok/s | VRAM: ~2.0 GB\n",
        "   Use case: Instruction following, chat\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ”¹ MEDIUM MODELS (7-8B) - Fits on T4\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "3. Qwen 2.5-7B Instruct\n",
        "   Repository: unsloth/Qwen2.5-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Multilingual, coding, math\n",
        "\n",
        "4. Llama 3.1-8B Instruct\n",
        "   Repository: unsloth/Meta-Llama-3.1-8B-Instruct-GGUF\n",
        "   File: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (~5 GB)\n",
        "   Speed on T4: ~15 tok/s | VRAM: ~5.5 GB\n",
        "   Use case: Advanced reasoning, long context\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ”¹ CODE MODELS\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "5. Qwen 2.5-Coder-7B\n",
        "   Repository: unsloth/Qwen2.5-Coder-7B-Instruct-GGUF\n",
        "   File: Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf (~4.5 GB)\n",
        "   Speed on T4: ~18 tok/s | VRAM: ~5.0 GB\n",
        "   Use case: Code generation, debugging\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ’¡ LOADING SYNTAX:\n",
        "\n",
        "engine.load_model(\"unsloth/REPO-NAME:filename.gguf\")\n",
        "\n",
        "Example:\n",
        "engine.load_model(\"unsloth/Qwen2.5-7B-Instruct-GGUF:Qwen2.5-7B-Instruct-Q4_K_M.gguf\")\n",
        "\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "ğŸ“¦ QUANTIZATION GUIDE:\n",
        "   â€¢ Q4_K_M: Best balance (recommended for T4)\n",
        "   â€¢ Q5_K_M: Higher quality, slower\n",
        "   â€¢ Q8_0: Near full precision, much slower\n",
        "   â€¢ Q2_K: Smallest, lowest quality\n",
        "\n",
        "ğŸ¯ T4 GPU LIMITS:\n",
        "   â€¢ Total VRAM: 16 GB\n",
        "   â€¢ Recommended max model: 8B parameters (Q4_K_M)\n",
        "   â€¢ Leave ~2-3 GB for context and processing\n",
        "\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary"
      },
      "source": [
        "## ğŸ“Š Summary\n",
        "\n",
        "### What We Accomplished\n",
        "\n",
        "âœ… **Installed llcuda v2.1.0** from GitHub (GitHub-only distribution)\n",
        "âœ… **Auto-downloaded binaries** from GitHub Releases (v2.1.0 primary bundle with seamless v2.0.6 fallback)\n",
        "âœ… **Loaded Gemma 3-1B-IT** GGUF from Unsloth HuggingFace\n",
        "âœ… **Ran inference** with ~45 tok/s on Tesla T4\n",
        "âœ… **Batch processing** multiple prompts efficiently\n",
        "âœ… **Performance analysis** with detailed metrics\n",
        "âœ… **Demonstrated workflow** from Unsloth fine-tuning to llcuda deployment\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Results (Tesla T4)\n",
        "\n",
        "| Model | Quantization | Speed | VRAM | Context |\n",
        "|-------|--------------|-------|------|---------|\n",
        "| Gemma 3-1B | Q4_K_M | ~45 tok/s | 1.2 GB | 2048 |\n",
        "| Llama 3.2-3B | Q4_K_M | ~30 tok/s | 2.0 GB | 4096 |\n",
        "| Qwen 2.5-7B | Q4_K_M | ~18 tok/s | 5.0 GB | 8192 |\n",
        "| Llama 3.1-8B | Q4_K_M | ~15 tok/s | 5.5 GB | 8192 |\n",
        "\n",
        "---\n",
        "\n",
        "### Key Features of llcuda v2.1.0\n",
        "\n",
        "ğŸš€ **GitHub-Only Distribution**\n",
        "- No PyPI dependency\n",
        "- Install: `pip install \\\"git+https://github.com/llcuda/llcuda.git@v2.1.1\\\"`\n",
        "- Binaries auto-download from GitHub Releases (v2.1.0 primary archive; automatic fallback to v2.0.6)\n",
        "\n",
        "âš¡ **Performance Optimizations**\n",
        "- FlashAttention support (2-3x faster for long contexts)\n",
        "- Tensor Core optimization for SM 7.5 (Tesla T4)\n",
        "- CUDA Graphs for reduced overhead\n",
        "- All quantization formats supported\n",
        "\n",
        "ğŸ”„ **Seamless Unsloth Integration**\n",
        "- Direct loading from Unsloth HuggingFace repos\n",
        "- Compatible with Unsloth fine-tuned GGUF exports\n",
        "- Full llama.cpp ecosystem support\n",
        "\n",
        "---\n",
        "\n",
        "### Resources\n",
        "\n",
        "- **llcuda GitHub**: https://github.com/llcuda/llcuda\n",
        "- **Installation Guide**: https://github.com/llcuda/llcuda/blob/main/GITHUB_INSTALL_GUIDE.md\n",
        "- **Releases**: https://github.com/llcuda/llcuda/releases\n",
        "- **Unsloth**: https://github.com/unslothai/unsloth\n",
        "- **Unsloth Models**: https://huggingface.co/unsloth\n",
        "- **Unsloth GGUF Docs**: https://docs.unsloth.ai/basics/saving-to-gguf\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try different models**: Experiment with larger models from Unsloth\n",
        "2. **Fine-tune with Unsloth**: Train on your custom dataset\n",
        "3. **Export to GGUF**: Use Unsloth's export functionality\n",
        "4. **Deploy with llcuda**: Fast inference on your fine-tuned models\n",
        "\n",
        "---\n",
        "\n",
        "**Built with**: llcuda v2.1.0 | Tesla T4 | CUDA 12 | Unsloth Integration | FlashAttention\n",
        "\n",
        "**Author**: Waqas Muhammad (waqasm86@gmail.com)\n",
        "**License**: MIT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfg2W-94l9nr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
