{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Build llcuda v2.1.0 Binaries for Tesla T4 (Google Colab)\n",
        "\n",
        "**Purpose**: Build complete CUDA 12 binaries for llcuda v2.1.0 on Google Colab Tesla T4 GPU\n",
        "\n",
        "**Output**:\n",
        "1. llama.cpp binaries (264 MB) - HTTP server mode with FlashAttention\n",
        "2. llcuda_cpp.so - The native extension for the Tensor API\n",
        "3. Complete package: `llcuda-binaries-cuda12-t4-v2.1.0.tar.gz`\n",
        "\n",
        "**Important Notes**:\n",
        "- These binaries are optimized for Tesla T4 (SM 7.5)\n",
        "- Includes FlashAttention v2, CUDA Graphs, and Tensor Core optimizations\n",
        "- Compatible with v2.1.0 Python APIs and Unsloth integration\n",
        "\n",
        "**Requirements**:\n",
        "- Google Colab with Tesla T4 GPU\n",
        "- CUDA 12.x (pre-installed in Colab)\n",
        "- Python 3.10+\n",
        "\n",
        "**Estimated Time**: ~20 minutes\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step1",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Verify GPU and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-gpu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-gpu",
        "outputId": "7bf4279c-df41-41f2-cffe-5eba7430a96f"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi --query-gpu=name,compute_cap,driver_version,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-cuda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-cuda",
        "outputId": "8dd062ae-b131-42cf-8ab1-24de40157768"
      },
      "outputs": [],
      "source": [
        "# Verify CUDA version\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "check-python",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check-python",
        "outputId": "22cd221d-95c9-441f-9e91-f5793f03993b"
      },
      "outputs": [],
      "source": [
        "# Check Python version\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Expected: 3.10+ (Colab default)")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step2",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Clone llama.cpp Repository\n",
        "\n",
        "We'll build llama.cpp with CUDA 12 support, FlashAttention, and optimizations for Tesla T4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clone-llamacpp",
      "metadata": {
        "id": "clone-llamacpp"
      },
      "outputs": [],
      "source": [
        "# Clone llama.cpp\n",
        "%cd /content\n",
        "!git clone https://github.com/ggml-org/llama.cpp.git\n",
        "%cd llama.cpp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step3",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Configure and Build llama.cpp for Tesla T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "configure-cmake",
      "metadata": {
        "id": "configure-cmake"
      },
      "outputs": [],
      "source": [
        "# Configure llama.cpp for Tesla T4 with all optimizations\n",
        "!cmake -B build_cuda12_t4 \\\\n",
        "    -DCMAKE_BUILD_TYPE=Release \\\\n",
        "    -DGGML_CUDA=ON \\\\n",
        "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\\n",
        "    -DCMAKE_CUDA_COMPILER=/usr/local/cuda/bin/nvcc \\\\n",
        "    -DGGML_NATIVE=OFF \\\\n",
        "    -DGGML_CUDA_FORCE_MMQ=OFF \\\\n",
        "    -DGGML_CUDA_FORCE_CUBLAS=OFF \\\\n",
        "    -DGGML_CUDA_FA=ON \\\\n",
        "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\\n",
        "    -DGGML_CUDA_GRAPHS=ON \\\\n",
        "    -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \\\\n",
        "    -DLLAMA_BUILD_SERVER=ON \\\\n",
        "    -DLLAMA_BUILD_TOOLS=ON \\\\n",
        "    -DBUILD_SHARED_LIBS=ON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "build-llamacpp",
      "metadata": {
        "id": "build-llamacpp"
      },
      "outputs": [],
      "source": [
        "# Build llama.cpp (takes ~10 minutes)\n",
        "import time\n",
        "\n",
        