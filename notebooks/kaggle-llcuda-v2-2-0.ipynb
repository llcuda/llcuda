{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a36ca5",
   "metadata": {},
   "source": [
    "# llcuda v2.2.0 - Kaggle 2Ã— T4 Build Notebook\n",
    "\n",
    "## Architecture: Split-GPU Workload\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         GPU 0             â”‚            GPU 1              â”‚\n",
    "â”‚  llama-server (GGUF)      â”‚  RAPIDS + Graphistry          â”‚\n",
    "â”‚  LLM Inference            â”‚  Graph Visualization (cuGraph)â”‚\n",
    "â”‚  15GB VRAM                â”‚  15GB VRAM                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "This notebook builds llcuda binaries for **split-GPU** operation:\n",
    "- **GPU 0**: llama-server with GGUF model (LLM inference)\n",
    "- **GPU 1**: RAPIDS/Graphistry with cuDF/cuGraph (graph simulation)\n",
    "\n",
    "## Step 1: Verify Kaggle GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b107f3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:57:14.762529Z",
     "iopub.status.busy": "2026-01-16T18:57:14.762220Z",
     "iopub.status.idle": "2026-01-16T18:57:15.104195Z",
     "shell.execute_reply": "2026-01-16T18:57:15.103358Z",
     "shell.execute_reply.started": "2026-01-16T18:57:14.762499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "KAGGLE GPU ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Detected GPUs: 2\n",
      "   GPU 0: Tesla T4 (UUID: GPU-b786a961-a605-2e9c-6c9b-142d40448580)\n",
      "   GPU 1: Tesla T4 (UUID: GPU-5b353acb-1468-b12c-7935-a40ac74777c8)\n",
      "\n",
      "ğŸ“Š CUDA Version:\n",
      "Cuda compilation tools, release 12.5, V12.5.82\n",
      "\n",
      "ğŸ“Š VRAM Summary:\n",
      "index, name, memory.total [MiB]\n",
      "0, Tesla T4, 15360 MiB\n",
      "1, Tesla T4, 15360 MiB\n",
      "\n",
      "âœ… Multi-GPU environment confirmed! Ready for dual-T4 build.\n"
     ]
    }
   ],
   "source": [
    "# Verify we have 2Ã— T4 GPUs\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KAGGLE GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check nvidia-smi\n",
    "result = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\n",
    "gpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\n",
    "print(f\"\\nğŸ“Š Detected GPUs: {len(gpu_lines)}\")\n",
    "for line in gpu_lines:\n",
    "    print(f\"   {line}\")\n",
    "\n",
    "# Check CUDA version\n",
    "print(\"\\nğŸ“Š CUDA Version:\")\n",
    "!nvcc --version | grep release\n",
    "\n",
    "# Check total VRAM\n",
    "print(\"\\nğŸ“Š VRAM Summary:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
    "\n",
    "# Verify we have 2 GPUs\n",
    "if len(gpu_lines) >= 2:\n",
    "    print(\"\\nâœ… Multi-GPU environment confirmed! Ready for dual-T4 build.\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ WARNING: Less than 2 GPUs detected!\")\n",
    "    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334985f0",
   "metadata": {},
   "source": [
    "## Step 2: Verify/Install Build Dependencies\n",
    "\n",
    "**Note:** Kaggle 2Ã— T4 comes with cmake 3.31.6 and ninja 1.13.0 pre-installed.\n",
    "We only install what's missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3ce1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:57:19.063003Z",
     "iopub.status.busy": "2026-01-16T18:57:19.062272Z",
     "iopub.status.idle": "2026-01-16T18:57:35.271862Z",
     "shell.execute_reply": "2026-01-16T18:57:35.270918Z",
     "shell.execute_reply.started": "2026-01-16T18:57:19.062964Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking build dependencies...\n",
      "âœ… cmake version 3.31.6\n",
      "âœ… Ninja 1.13.0.git.kitware.jobserver-pipe-1\n",
      "ğŸ“¦ Installing ccache...\n",
      "Selecting previously unselected package libhiredis0.14:amd64.\n",
      "(Reading database ... 129073 files and directories currently installed.)\n",
      "Preparing to unpack .../libhiredis0.14_0.14.1-2_amd64.deb ...\n",
      "Unpacking libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Selecting previously unselected package ccache.\n",
      "Preparing to unpack .../ccache_4.5.1-1_amd64.deb ...\n",
      "Unpacking ccache (4.5.1-1) ...\n",
      "Setting up libhiredis0.14:amd64 (0.14.1-2) ...\n",
      "Setting up ccache (4.5.1-1) ...\n",
      "Updating symlinks in /usr/lib/ccache ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "\n",
      "ğŸ“¦ Checking Python packages...\n",
      "   âœ… huggingface_hub\n",
      "   ğŸ“¦ Installing sseclient-py...\n",
      "\n",
      "âœ… Build dependencies ready\n",
      "cmake version 3.31.6\n",
      "1.13.0.git.kitware.jobserver-pipe-1\n",
      "CPU times: user 244 ms, sys: 66.9 ms, total: 311 ms\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check pre-installed build tools (Kaggle 2Ã— T4 has cmake/ninja)\n",
    "import subprocess\n",
    "\n",
    "print(\"Checking build dependencies...\")\n",
    "\n",
    "# Check CMake\n",
    "cmake_result = subprocess.run([\"cmake\", \"--version\"], capture_output=True, text=True)\n",
    "if cmake_result.returncode == 0:\n",
    "    cmake_ver = cmake_result.stdout.split(\"\\n\")[0]\n",
    "    print(f\"âœ… {cmake_ver}\")\n",
    "else:\n",
    "    print(\"âš ï¸  CMake not found, installing...\")\n",
    "    !apt-get update -qq && apt-get install -y -qq cmake\n",
    "\n",
    "# Check Ninja  \n",
    "ninja_result = subprocess.run([\"ninja\", \"--version\"], capture_output=True, text=True)\n",
    "if ninja_result.returncode == 0:\n",
    "    print(f\"âœ… Ninja {ninja_result.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"âš ï¸  Ninja not found, installing...\")\n",
    "    !apt-get install -y -qq ninja-build\n",
    "\n",
    "# Check ccache (optional but speeds up rebuilds)\n",
    "ccache_result = subprocess.run([\"which\", \"ccache\"], capture_output=True, text=True)\n",
    "if ccache_result.returncode != 0:\n",
    "    print(\"ğŸ“¦ Installing ccache...\")\n",
    "    !apt-get install -y -qq ccache\n",
    "\n",
    "# Install Python dependencies (minimal - most are pre-installed on Kaggle)\n",
    "print(\"\\nğŸ“¦ Checking Python packages...\")\n",
    "required_py = [\"huggingface_hub\", \"sseclient-py\"]\n",
    "for pkg in required_py:\n",
    "    try:\n",
    "        __import__(pkg.replace(\"-\", \"_\"))\n",
    "        print(f\"   âœ… {pkg}\")\n",
    "    except ImportError:\n",
    "        print(f\"   ğŸ“¦ Installing {pkg}...\")\n",
    "        !pip install -q {pkg}\n",
    "\n",
    "print(\"\\nâœ… Build dependencies ready\")\n",
    "!cmake --version | head -1\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eee12f",
   "metadata": {},
   "source": [
    "## Step 2b: Install cuGraph (GPU 1 Workload) - RAPIDS 25.6.0 Compatible\n",
    "\n",
    "**Important:** Kaggle has RAPIDS 25.6.0 pre-installed (cudf-cu12, cuml-cu12, pylibraft-cu12, etc.).\n",
    "\n",
    "**DO NOT** upgrade cuda-python or numba-cuda - this breaks the pre-installed RAPIDS packages!\n",
    "\n",
    "**Solution:** Install cugraph-cu12==25.6.* to match Kaggle's pre-installed RAPIDS version.\n",
    "\n",
    "**Note:** Some pip dependency warnings are expected but the packages will work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097adecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:57:40.743100Z",
     "iopub.status.busy": "2026-01-16T18:57:40.742335Z",
     "iopub.status.idle": "2026-01-16T19:01:20.151079Z",
     "shell.execute_reply": "2026-01-16T19:01:20.150341Z",
     "shell.execute_reply.started": "2026-01-16T18:57:40.743057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VERIFYING RAPIDS + INSTALLING GRAPHISTRY FOR GPU 1\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¦ Checking pre-installed RAPIDS packages:\n",
      "   âœ… cudf-cu12: 25.6.0\n",
      "   âœ… cuml-cu12: 25.6.0\n",
      "   âœ… pylibraft-cu12: 25.6.0\n",
      "   âœ… raft-dask-cu12: 25.6.0\n",
      "\n",
      "ğŸ“¦ Checking cuGraph:\n",
      "   âš ï¸  cugraph not directly available, checking pylibcugraph...\n",
      "   âœ… pylibcugraph available\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m161.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m165.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m157.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m880.4/880.4 kB\u001b[0m \u001b[31m188.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m198.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m518.5/518.5 kB\u001b[0m \u001b[31m177.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m642.3/642.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m25.0/25.0 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m204.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m529.7/529.7 kB\u001b[0m \u001b[31m184.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m141.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m127.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m195.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m581.2/581.2 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.3/68.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m338.1/338.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m366.5/366.5 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.0/202.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.9/112.9 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m743.3/743.3 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcuml-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.12.0 which is incompatible.\n",
      "libcuml-cu12 25.6.0 requires rapids-logger==0.1.*, but you have rapids-logger 0.2.3 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires cudf-cu12==25.6.*, but you have cudf-cu12 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires dask-cuda==25.6.*, but you have dask-cuda 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires dask-cudf-cu12==25.6.*, but you have dask-cudf-cu12 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires raft-dask-cu12==25.6.*, but you have raft-dask-cu12 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires rapids-dask-dependency==25.6.*, but you have rapids-dask-dependency 25.12.0 which is incompatible.\n",
      "cuml-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.12.0 which is incompatible.\n",
      "libcuvs-cu12 25.6.1 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.12.0 which is incompatible.\n",
      "libcuvs-cu12 25.6.1 requires librmm-cu12==25.6.*, but you have librmm-cu12 25.12.0 which is incompatible.\n",
      "nx-cugraph-cu12 25.6.0 requires pylibcugraph-cu12==25.6.*, but you have pylibcugraph-cu12 25.12.2 which is incompatible.\n",
      "cuvs-cu12 25.6.1 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.12.0 which is incompatible.\n",
      "torch 2.8.0+cu126 requires nvidia-cublas-cu12==12.6.4.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.1.4 which is incompatible.\n",
      "torch 2.8.0+cu126 requires nvidia-cuda-nvrtc-cu12==12.6.77; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.9.86 which is incompatible.\n",
      "torch 2.8.0+cu126 requires nvidia-curand-cu12==10.3.7.77; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.8.0+cu126 requires nvidia-cusolver-cu12==11.7.1.2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.5.82 which is incompatible.\n",
      "torch 2.8.0+cu126 requires nvidia-cusparse-cu12==12.5.4.2; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.10.65 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m   âŒ Error: /usr/local/lib/python3.12/dist-packages/cugraph/structure/graph_primtypes_wrapper.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN3rmm13out_of_memoryC1ERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "\n",
      "ğŸ“¦ Installing Graphistry (minimal):\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ“¦ Final verification:\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'nvrtc' from 'cuda.bindings' (/usr/local/lib/python3.12/dist-packages/cuda/bindings/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/cudf/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcupy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmm_cupy_allocator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msimulator_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdevice_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_auto_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/device_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Re export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from .stubs import (\n\u001b[1;32m      8\u001b[0m     \u001b[0mthreadIdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cg.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextending\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnvvmutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextending\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mintrinsic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrid_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridGroup\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mGridGroupClass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/nvvmutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtargetconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcgutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcudadrv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnvvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/nvvm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNvvmError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNvvmSupportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNvvmWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_libdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_libdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_cudalib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcgutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/libs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_paths\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_cuda_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudadrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocate_driver_and_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudadrv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCudaSupportError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinkable_code\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinkableCode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLTOIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFatbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_file_read\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudadrv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnvrtc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mUSE_NV_BINDING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA_USE_NVIDIA_BINDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/cudadrv/nvrtc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCUDA_USE_NVIDIA_BINDING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbindings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnvrtc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbindings_nvrtc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProgramOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'nvrtc' from 'cuda.bindings' (/usr/local/lib/python3.12/dist-packages/cuda/bindings/__init__.py)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Install cuGraph matching Kaggle's pre-installed RAPIDS 25.6.0\n",
    "# CRITICAL: Do NOT upgrade cuda-python or numba-cuda - this breaks RAPIDS!\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"INSTALLING CUGRAPH FOR GPU 1 (RAPIDS 25.6.0 COMPATIBLE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Step 1: Check pre-installed RAPIDS versions\n",
    "import subprocess\n",
    "print(\"\\nğŸ“¦ Pre-installed RAPIDS packages on Kaggle:\")\n",
    "for pkg in [\"cudf-cu12\", \"cuml-cu12\", \"pylibraft-cu12\", \"cuda-python\", \"numba-cuda\"]:\n",
    "    result = subprocess.run([\"pip\", \"show\", pkg], capture_output=True, text=True)\n",
    "    if \"Version:\" in result.stdout:\n",
    "        version = [l for l in result.stdout.split(\"\\n\") if l.startswith(\"Version:\")][0]\n",
    "        print(f\"   {pkg}: {version.split(': ')[1]}\")\n",
    "    else:\n",
    "        print(f\"   {pkg}: NOT INSTALLED\")\n",
    "\n",
    "# Step 2: Install cugraph-cu12 matching RAPIDS 25.6.* (Kaggle's version)\n",
    "# Using pypi.nvidia.com for RAPIDS packages\n",
    "print(\"\\nğŸ“¦ Installing cugraph-cu12==25.6.* (matching Kaggle's RAPIDS)...\")\n",
    "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n",
    "\n",
    "# Step 3: Install graphistry (minimal, no [ai] extras to avoid conflicts)\n",
    "print(\"\\nğŸ“¦ Installing graphistry...\")\n",
    "!pip install -q graphistry\n",
    "\n",
    "# Step 4: Verify RAPIDS imports work\n",
    "print(\"\\nğŸ“¦ Final verification:\")\n",
    "try:\n",
    "    import cudf\n",
    "    print(f\"   âœ… cuDF: {cudf.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   âŒ cuDF: {e}\")\n",
    "\n",
    "try:\n",
    "    import cugraph\n",
    "    print(f\"   âœ… cuGraph: {cugraph.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"   âŒ cuGraph: {e}\")\n",
    "    print(\"   ğŸ’¡ If cuGraph fails, try: Runtime â†’ Restart runtime, then re-run this cell\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"   âœ… Graphistry: {graphistry.__version__}\")\n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Graphistry: {e}\")\n",
    "\n",
    "print(\"\\nâœ… RAPIDS packages installed! If imports fail, restart runtime and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c69ed34",
   "metadata": {},
   "source": [
    "## Step 3: Clone llama.cpp (Latest Stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fa6016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T19:01:20.154012Z",
     "iopub.status.busy": "2026-01-16T19:01:20.152809Z",
     "iopub.status.idle": "2026-01-16T19:01:24.687255Z",
     "shell.execute_reply": "2026-01-16T19:01:24.686446Z",
     "shell.execute_reply.started": "2026-01-16T19:01:20.153975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning llama.cpp...\n",
      "Cloning into 'llama.cpp'...\n",
      "remote: Enumerating objects: 2395, done.\u001b[K\n",
      "remote: Counting objects: 100% (2395/2395), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1875/1875), done.\u001b[K\n",
      "remote: Total 2395 (delta 518), reused 1568 (delta 448), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (2395/2395), 27.25 MiB | 16.74 MiB/s, done.\n",
      "Resolving deltas: 100% (518/518), done.\n",
      "\n",
      "ğŸ“¦ llama.cpp Version:\n",
      "\u001b[33m388ce82\u001b[m\u001b[33m (\u001b[m\u001b[1;34mgrafted\u001b[m\u001b[33m, \u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmaster\u001b[m\u001b[33m, \u001b[m\u001b[1;33mtag: b7760\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/master\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m ggml : extend ggml_pool_1d + metal (#16429)\n",
      "b7760\n",
      "CPU times: user 96.3 ms, sys: 54 ms, total: 150 ms\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = \"/kaggle/working\"\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Clean any previous build\n",
    "!rm -rf llama.cpp\n",
    "\n",
    "# Clone llama.cpp\n",
    "print(\"Cloning llama.cpp...\")\n",
    "!git clone --depth 1 https://github.com/ggml-org/llama.cpp.git\n",
    "\n",
    "os.chdir(\"llama.cpp\")\n",
    "\n",
    "# Get commit info\n",
    "print(\"\\nğŸ“¦ llama.cpp Version:\")\n",
    "!git log -1 --oneline\n",
    "!git describe --tags --always 2>/dev/null || echo \"(no tag)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa16d20",
   "metadata": {},
   "source": [
    "## Step 4: Configure CMake with CUDA (with VMM disabled)\n",
    "\n",
    "**Critical:** Kaggle has no `libcuda.so` and `/usr/local/cuda/` is read-only. We create a minimal stub in `/kaggle/working/` AND disable CUDA Virtual Memory Management (`-DGGML_CUDA_VMM=OFF`) to avoid needing advanced driver APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf67503",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-16T18:39:53.005197Z",
     "iopub.status.busy": "2026-01-16T18:39:53.004764Z",
     "iopub.status.idle": "2026-01-16T18:39:59.964145Z",
     "shell.execute_reply": "2026-01-16T18:39:59.963309Z",
     "shell.execute_reply.started": "2026-01-16T18:39:53.005161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CMake for Kaggle 2Ã— Tesla T4...\n",
      "\n",
      "-- The C compiler identification is GNU 11.4.0\n",
      "-- The CXX compiler identification is GNU 11.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
      "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
      "-- The ASM compiler identification is GNU\n",
      "-- Found assembler: /usr/bin/gcc\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "-- Found Threads: TRUE\n",
      "-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- GGML_SYSTEM_ARCH: x86\n",
      "-- Including CPU backend\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "-- Found OpenMP: TRUE (found version \"4.5\")\n",
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -msse4.2;-mf16c;-mfma;-mbmi2;-mavx;-mavx2 GGML_SSE42;GGML_F16C;GGML_FMA;GGML_BMI2;GGML_AVX;GGML_AVX2\n",
      "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.5.82\")\n",
      "-- CUDA Toolkit found\n",
      "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
      "-- Detecting CUDA compiler ABI info\n",
      "-- Detecting CUDA compiler ABI info - done\n",
      "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "-- Detecting CUDA compile features\n",
      "-- Detecting CUDA compile features - done\n",
      "-- Using CMAKE_CUDA_ARCHITECTURES=75 CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n",
      "-- CUDA host compiler is GNU 11.4.0\n",
      "-- Including CUDA backend\n",
      "-- ggml version: 0.9.5\n",
      "-- ggml commit:  388ce82\n",
      "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
      "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
      "-- OpenSSL found: 3.0.2\n",
      "-- Generating embedded license file for target: common\n",
      "-- Configuring done (6.3s)\n",
      "\u001b[31mCMake Error at ggml/src/ggml-cuda/CMakeLists.txt:182 (target_link_libraries):\n",
      "  Target \"ggml-cuda\" links to:\n",
      "\n",
      "    CUDA::cuda_driver\n",
      "\n",
      "  but the target was not found.  Possible reasons include:\n",
      "\n",
      "    * There is a typo in the target name.\n",
      "    * A find_package call is missing for an IMPORTED target.\n",
      "    * An ALIAS target is missing.\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "-- Generating done (0.2s)\n",
      "\u001b[0mCMake Generate step failed.  Build files cannot be regenerated correctly.\u001b[0m\n",
      "\n",
      "âœ… CMake configuration complete!\n",
      "   Target: SM 7.5 (Tesla T4)\n",
      "   FlashAttention: All quantization types\n",
      "   Static linking: Enabled\n",
      "CPU times: user 103 ms, sys: 47.3 ms, total: 150 ms\n",
      "Wall time: 6.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Clean previous build\n",
    "!rm -rf build\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STEP 4: CREATE CUDA DRIVER STUB + CONFIGURE CMAKE (VMM DISABLED)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL FIX: Create libcuda.so stub in WRITABLE location\n",
    "# Kaggle's /usr/local/cuda is read-only, so we use /kaggle/working/\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ”§ Creating CUDA driver stub library...\")\n",
    "\n",
    "STUBS_DIR = \"/kaggle/working/cuda_stubs\"\n",
    "os.makedirs(STUBS_DIR, exist_ok=True)\n",
    "\n",
    "# Create a minimal C file that provides empty symbols for libcuda.so\n",
    "# NOTE: We disable VMM via -DGGML_CUDA_NO_VMM compile flag so we don't need\n",
    "# the advanced memory management APIs (cuMemCreate, cuMemMap, etc.)\n",
    "stub_code = '''\n",
    "// Minimal CUDA driver stub for linking purposes only\n",
    "// At runtime, the real driver is used\n",
    "\n",
    "void* cuGetErrorString = 0;\n",
    "void* cuGetErrorName = 0;\n",
    "void* cuInit = 0;\n",
    "void* cuDriverGetVersion = 0;\n",
    "void* cuDeviceGet = 0;\n",
    "void* cuDeviceGetCount = 0;\n",
    "void* cuDeviceGetName = 0;\n",
    "void* cuDeviceGetAttribute = 0;\n",
    "void* cuDeviceTotalMem = 0;\n",
    "void* cuDeviceGetUuid = 0;\n",
    "void* cuCtxCreate = 0;\n",
    "void* cuCtxDestroy = 0;\n",
    "void* cuCtxGetCurrent = 0;\n",
    "void* cuCtxSetCurrent = 0;\n",
    "void* cuCtxPushCurrent = 0;\n",
    "void* cuCtxPopCurrent = 0;\n",
    "void* cuCtxSynchronize = 0;\n",
    "void* cuMemAlloc = 0;\n",
    "void* cuMemFree = 0;\n",
    "void* cuMemcpy = 0;\n",
    "void* cuMemcpyHtoD = 0;\n",
    "void* cuMemcpyDtoH = 0;\n",
    "void* cuMemcpyDtoD = 0;\n",
    "void* cuMemsetD8 = 0;\n",
    "void* cuMemsetD32 = 0;\n",
    "void* cuModuleLoad = 0;\n",
    "void* cuModuleUnload = 0;\n",
    "void* cuModuleGetFunction = 0;\n",
    "void* cuLaunchKernel = 0;\n",
    "void* cuStreamCreate = 0;\n",
    "void* cuStreamDestroy = 0;\n",
    "void* cuStreamSynchronize = 0;\n",
    "void* cuEventCreate = 0;\n",
    "void* cuEventDestroy = 0;\n",
    "void* cuEventRecord = 0;\n",
    "void* cuEventSynchronize = 0;\n",
    "void* cuEventElapsedTime = 0;\n",
    "'''\n",
    "\n",
    "# Write stub source\n",
    "stub_c_path = f\"{STUBS_DIR}/cuda_stub.c\"\n",
    "with open(stub_c_path, \"w\") as f:\n",
    "    f.write(stub_code)\n",
    "\n",
    "# Compile to shared library\n",
    "stub_so_path = f\"{STUBS_DIR}/libcuda.so\"\n",
    "!gcc -shared -fPIC -o {stub_so_path} {stub_c_path}\n",
    "\n",
    "# Also create libcuda.so.1 symlink (some builds look for this)\n",
    "!ln -sf {stub_so_path} {STUBS_DIR}/libcuda.so.1\n",
    "\n",
    "# Verify the stub was created\n",
    "if os.path.exists(stub_so_path):\n",
    "    size = os.path.getsize(stub_so_path)\n",
    "    print(f\"   âœ… Created libcuda.so stub ({size} bytes) in {STUBS_DIR}\")\n",
    "    !ls -la {STUBS_DIR}\n",
    "else:\n",
    "    print(\"   âŒ Failed to create stub!\")\n",
    "\n",
    "# Set environment variables for linker\n",
    "os.environ[\"LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LIBRARY_PATH\", \"\")\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"{STUBS_DIR}:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "\n",
    "# ============================================================================\n",
    "# CMake Configuration with explicit stub path + VMM DISABLED via compile flag\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ“¦ CMake Configuration:\")\n",
    "print(\"   Target: SM 7.5 (Tesla T4)\")\n",
    "print(\"   FlashAttention: All quantization types\")\n",
    "print(\"   CUDA VMM: DISABLED via -DGGML_CUDA_NO_VMM compile flag\")\n",
    "print(\"   Static linking: Enabled\")\n",
    "print(f\"   CUDA stub path: {STUBS_DIR}\")\n",
    "print(\"\")\n",
    "\n",
    "# Pass the stubs directory to CMake\n",
    "# CRITICAL: -DGGML_CUDA_NO_VMM disables Virtual Memory Management at compile time\n",
    "# This avoids needing cuMemCreate, cuMemMap, cuMemUnmap, cuMemAddressReserve, etc.\n",
    "cmake_cmd = f\"\"\"\n",
    "cmake -B build -G Ninja \\\n",
    "    -DGGML_CUDA=ON \\\n",
    "    -DCMAKE_CUDA_ARCHITECTURES=\"75\" \\\n",
    "    -DGGML_CUDA_FA_ALL_QUANTS=ON \\\n",
    "    -DGGML_NATIVE=OFF \\\n",
    "    -DBUILD_SHARED_LIBS=OFF \\\n",
    "    -DLLAMA_BUILD_EXAMPLES=ON \\\n",
    "    -DLLAMA_BUILD_TESTS=OFF \\\n",
    "    -DLLAMA_BUILD_SERVER=ON \\\n",
    "    -DCMAKE_BUILD_TYPE=Release \\\n",
    "    -DCMAKE_C_COMPILER=gcc \\\n",
    "    -DCMAKE_CXX_COMPILER=g++ \\\n",
    "    -DCMAKE_C_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_CXX_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_CUDA_FLAGS=\"-DGGML_CUDA_NO_VMM\" \\\n",
    "    -DCMAKE_LIBRARY_PATH=\"{STUBS_DIR}\" \\\n",
    "    -DCUDAToolkit_LIBRARY_DIR=\"{STUBS_DIR}\"\n",
    "\"\"\"\n",
    "\n",
    "!{cmake_cmd}\n",
    "\n",
    "# Verify configuration succeeded\n",
    "import subprocess\n",
    "result = subprocess.run([\"test\", \"-f\", \"build/build.ninja\"], capture_output=True)\n",
    "if result.returncode == 0:\n",
    "    print(\"\\nâœ… CMake configuration complete!\")\n",
    "else:\n",
    "    print(\"\\nâŒ CMake configuration failed - check errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267abd9c",
   "metadata": {},
   "source": [
    "## Step 5: Build llama.cpp (This takes ~8-12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d6ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import multiprocessing\n",
    "import sys\n",
    "\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "\n",
    "# Get CPU count for parallel build\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f\"Building with {cpu_count} parallel jobs...\")\n",
    "print(\"This will take approximately 8-12 minutes.\\n\")\n",
    "\n",
    "# Build\n",
    "build_result = os.system(f\"cmake --build build --config Release -j{cpu_count}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verify build succeeded\n",
    "if build_result == 0 and os.path.exists(\"build/bin/llama-server\"):\n",
    "    print(\"âœ… BUILD COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    !ls -lh build/bin/llama-server\n",
    "else:\n",
    "    print(\"âŒ BUILD FAILED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Check the build output above for errors.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dcf23",
   "metadata": {},
   "source": [
    "## Step 6: Verify Built Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d961d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Built binaries:\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh llama-* 2>/dev/null | head -20\n",
    "\n",
    "print(\"\\nKey binary sizes:\")\n",
    "!du -h llama-server llama-cli llama-quantize 2>/dev/null\n",
    "\n",
    "print(\"\\nChecking CUDA support in llama-server:\")\n",
    "!./llama-server --help 2>&1 | grep -i \"cuda\\|gpu\\|ngl\" | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6abd2f",
   "metadata": {},
   "source": [
    "## Step 7: Test Multi-GPU Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working/llama.cpp/build/bin\")\n",
    "\n",
    "print(\"Testing multi-GPU CLI flags:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for multi-GPU flags\n",
    "print(\"\\nğŸ“Œ --tensor-split (VRAM distribution):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"tensor-split\"\n",
    "\n",
    "print(\"\\nğŸ“Œ --split-mode (layer/row splitting):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"split-mode\"\n",
    "\n",
    "print(\"\\nğŸ“Œ --main-gpu (primary GPU selection):\")\n",
    "!./llama-server --help 2>&1 | grep -A2 \"main-gpu\"\n",
    "\n",
    "print(\"\\nâœ… Multi-GPU support confirmed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0b99",
   "metadata": {},
   "source": [
    "## Step 8: Create llcuda v2.2.0 Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb728ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "# Package info\n",
    "VERSION = \"2.2.0\"\n",
    "BUILD_DATE = datetime.now().strftime(\"%Y%m%d\")\n",
    "PACKAGE_NAME = f\"llcuda-v{VERSION}-cuda12-kaggle-t4x2\"\n",
    "PACKAGE_DIR = f\"/kaggle/working/{PACKAGE_NAME}\"\n",
    "\n",
    "print(f\"Creating package: {PACKAGE_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directory structure\n",
    "os.makedirs(f\"{PACKAGE_DIR}/bin\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/lib\", exist_ok=True)\n",
    "os.makedirs(f\"{PACKAGE_DIR}/include\", exist_ok=True)\n",
    "\n",
    "# Binaries to include\n",
    "BUILD_BIN = \"/kaggle/working/llama.cpp/build/bin\"\n",
    "binaries = [\n",
    "    # Core server\n",
    "    \"llama-server\",\n",
    "    \"llama-cli\",\n",
    "    # Quantization & conversion\n",
    "    \"llama-quantize\",\n",
    "    \"llama-gguf\",\n",
    "    \"llama-gguf-hash\",\n",
    "    \"llama-gguf-split\",\n",
    "    \"llama-imatrix\",\n",
    "    # LoRA & embedding\n",
    "    \"llama-export-lora\",\n",
    "    \"llama-embedding\",\n",
    "    # Utilities\n",
    "    \"llama-tokenize\",\n",
    "    \"llama-infill\",\n",
    "    \"llama-perplexity\",\n",
    "    \"llama-bench\",\n",
    "    \"llama-cvector-generator\",\n",
    "]\n",
    "\n",
    "# Copy binaries\n",
    "copied = []\n",
    "for binary in binaries:\n",
    "    src = f\"{BUILD_BIN}/{binary}\"\n",
    "    if os.path.exists(src):\n",
    "        shutil.copy2(src, f\"{PACKAGE_DIR}/bin/{binary}\")\n",
    "        os.chmod(f\"{PACKAGE_DIR}/bin/{binary}\", 0o755)\n",
    "        copied.append(binary)\n",
    "        print(f\"  âœ… {binary}\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  {binary} (not found)\")\n",
    "\n",
    "print(f\"\\nğŸ“¦ Copied {len(copied)}/{len(binaries)} binaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c7058",
   "metadata": {},
   "source": [
    "## Step 9: Create Package Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Get llama.cpp info\n",
    "os.chdir(\"/kaggle/working/llama.cpp\")\n",
    "commit_hash = subprocess.getoutput(\"git rev-parse HEAD\")\n",
    "commit_date = subprocess.getoutput(\"git log -1 --format=%ci\")\n",
    "commit_msg = subprocess.getoutput(\"git log -1 --format=%s\")\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = subprocess.getoutput(\"nvcc --version | grep release | sed 's/.*release //' | cut -d, -f1\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"package\": \"llcuda\",\n",
    "    \"version\": VERSION,\n",
    "    \"build_date\": datetime.now().isoformat(),\n",
    "    \"platform\": {\n",
    "        \"name\": \"kaggle\",\n",
    "        \"gpu_count\": 2,\n",
    "        \"gpu_model\": \"Tesla T4\",\n",
    "        \"vram_per_gpu_gb\": 15,\n",
    "        \"total_vram_gb\": 30,\n",
    "        \"compute_capability\": \"7.5\",\n",
    "        \"architecture\": \"Turing\"\n",
    "    },\n",
    "    \"cuda\": {\n",
    "        \"version\": cuda_version,\n",
    "        \"architectures\": [\"sm_75\"],\n",
    "        \"flash_attention\": True,\n",
    "        \"flash_attention_all_quants\": True\n",
    "    },\n",
    "    \"llama_cpp\": {\n",
    "        \"commit\": commit_hash,\n",
    "        \"commit_date\": commit_date,\n",
    "        \"commit_message\": commit_msg,\n",
    "        \"repo\": \"https://github.com/ggml-org/llama.cpp\"\n",
    "    },\n",
    "    \"multi_gpu\": {\n",
    "        \"supported\": True,\n",
    "        \"method\": \"native_cuda\",\n",
    "        \"modes\": {\n",
    "            \"tensor_split\": {\n",
    "                \"description\": \"Split model across both GPUs for larger models\",\n",
    "                \"flags\": [\"--tensor-split 0.5,0.5\", \"--split-mode layer\"],\n",
    "                \"use_case\": \"Large GGUF models (>15GB)\"\n",
    "            },\n",
    "            \"split_workload\": {\n",
    "                \"description\": \"Dedicated GPU assignment: GPU 0 for LLM, GPU 1 for graphs\",\n",
    "                \"method\": \"CUDA_VISIBLE_DEVICES environment variable\",\n",
    "                \"use_case\": \"LLM inference + RAPIDS/Graphistry graph simulation\"\n",
    "            }\n",
    "        },\n",
    "        \"recommended_config\": {\n",
    "            \"tensor_split\": \"0.5,0.5\",\n",
    "            \"split_mode\": \"layer\",\n",
    "            \"n_gpu_layers\": -1\n",
    "        }\n",
    "    },\n",
    "    \"split_workload\": {\n",
    "        \"description\": \"Split-GPU architecture for combined LLM + Graph workloads\",\n",
    "        \"gpu_0\": \"llama-server with GGUF model (LLM inference)\",\n",
    "        \"gpu_1\": \"RAPIDS + Graphistry (cuDF, cuGraph for graph visualization)\",\n",
    "        \"rapids_packages\": [\"cudf-cu12\", \"cuml-cu12\", \"cugraph-cu12\"],\n",
    "        \"graphistry_packages\": [\"graphistry[ai]\"],\n",
    "        \"usage\": {\n",
    "            \"llm_gpu\": \"CUDA_VISIBLE_DEVICES=0 ./llama-server -m model.gguf -ngl 99\",\n",
    "            \"graph_gpu\": \"import os; os.environ['CUDA_VISIBLE_DEVICES']='1'; import cudf, cugraph\"\n",
    "        }\n",
    "    },\n",
    "    \"binaries\": copied,\n",
    "    \"features\": [\n",
    "        \"multi-gpu-tensor-split\",\n",
    "        \"split-workload-architecture\",\n",
    "        \"flash-attention-all-quants\",\n",
    "        \"openai-compatible-api\",\n",
    "        \"anthropic-compatible-api\",\n",
    "        \"29-quantization-formats\",\n",
    "        \"lora-adapters\",\n",
    "        \"grammar-constraints\",\n",
    "        \"json-schema-output\",\n",
    "        \"embeddings-reranking\",\n",
    "        \"streaming-sse\",\n",
    "        \"kv-cache-slots\",\n",
    "        \"speculative-decoding\"\n",
    "    ],\n",
    "    \"unsloth_integration\": {\n",
    "        \"description\": \"CUDA 12 inference backend for Unsloth fine-tuned models\",\n",
    "        \"workflow\": \"Unsloth (training) â†’ GGUF (conversion) â†’ llcuda (inference)\",\n",
    "        \"supported_exports\": [\"f16\", \"q8_0\", \"q4_k_m\", \"q5_k_m\", \"iq4_xs\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write metadata\n",
    "os.chdir(\"/kaggle/working\")\n",
    "with open(f\"{PACKAGE_DIR}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"ğŸ“‹ Package Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b1740",
   "metadata": {},
   "source": [
    "## Step 10: Create README and Usage Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme_content = f'''# llcuda v{VERSION} - Kaggle 2Ã— Tesla T4 Build\n",
    "\n",
    "Pre-built CUDA 12 binaries for **Kaggle dual Tesla T4** multi-GPU inference.\n",
    "\n",
    "## ğŸ¯ Unsloth Integration\n",
    "\n",
    "llcuda is the **CUDA 12 inference backend for Unsloth**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   UNSLOTH   â”‚â”€â”€â”€â–¶â”‚   LLCUDA    â”‚â”€â”€â”€â–¶â”‚  llama-server   â”‚\n",
    "â”‚  Training   â”‚    â”‚  GGUF Conv  â”‚    â”‚  Multi-GPU Inf  â”‚\n",
    "â”‚  Fine-tune  â”‚    â”‚  Quantize   â”‚    â”‚  2Ã— T4 (30GB)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ğŸš€ Quick Start\n",
    "\n",
    "### 1. Extract Package\n",
    "```bash\n",
    "tar -xzf llcuda-v{VERSION}-cuda12-kaggle-t4x2.tar.gz\n",
    "cd llcuda-v{VERSION}-cuda12-kaggle-t4x2\n",
    "chmod +x bin/*\n",
    "```\n",
    "\n",
    "### 2. Start Multi-GPU Server\n",
    "```bash\n",
    "./bin/llama-server \\\\\n",
    "    -m /path/to/model.gguf \\\\\n",
    "    -ngl 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    -fa \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port 8080 \\\\\n",
    "    -c 8192\n",
    "```\n",
    "\n",
    "### 3. Use with Python\n",
    "```python\n",
    "from llcuda.api import LlamaCppClient, kaggle_t4_dual_config\n",
    "\n",
    "# Get optimal config for Kaggle\n",
    "config = kaggle_t4_dual_config()\n",
    "print(config.to_cli_args())\n",
    "\n",
    "# Connect to server\n",
    "client = LlamaCppClient(\"http://localhost:8080\")\n",
    "\n",
    "# OpenAI-compatible chat\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{{\"role\": \"user\", \"content\": \"Hello!\"}}],\n",
    "    max_tokens=100\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "## ğŸ“Š Multi-GPU Flags\n",
    "\n",
    "| Flag | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| `-ngl 99` | Offload all layers to GPU | Required |\n",
    "| `--tensor-split` | VRAM ratio per GPU | `0.5,0.5` |\n",
    "| `--split-mode` | Split strategy | `layer` or `row` |\n",
    "| `--main-gpu` | Primary GPU ID | `0` |\n",
    "| `-fa` | FlashAttention | Recommended |\n",
    "\n",
    "## ğŸ“¦ Recommended Models for 30GB VRAM\n",
    "\n",
    "| Model | Quant | Size | Context | Fits? |\n",
    "|-------|-------|------|---------|-------|\n",
    "| Llama 3.1 70B | IQ3_XS | ~25GB | 4K | âœ… |\n",
    "| Qwen2.5 32B | Q4_K_M | ~19GB | 8K | âœ… |\n",
    "| Gemma 2 27B | Q4_K_M | ~16GB | 8K | âœ… |\n",
    "| Llama 3.1 8B | Q8_0 | ~9GB | 16K | âœ… |\n",
    "| Mistral 7B | Q8_0 | ~8GB | 32K | âœ… |\n",
    "\n",
    "## ğŸ”§ Unsloth â†’ llcuda Workflow\n",
    "\n",
    "```python\n",
    "# 1. Fine-tune with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(...)\n",
    "# ... training ...\n",
    "\n",
    "# 2. Export to GGUF (Unsloth built-in)\n",
    "model.save_pretrained_gguf(\"my_model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "\n",
    "# 3. Run with llcuda\n",
    "# ./bin/llama-server -m my_model-Q4_K_M.gguf -ngl 99 --tensor-split 0.5,0.5\n",
    "```\n",
    "\n",
    "## ğŸ“‹ Build Info\n",
    "\n",
    "- **llcuda Version:** {VERSION}\n",
    "- **CUDA Version:** 12.4\n",
    "- **Target GPU:** Tesla T4 Ã— 2\n",
    "- **Compute Capability:** SM 7.5 (Turing)\n",
    "- **FlashAttention:** All quantization types\n",
    "- **Build Date:** {BUILD_DATE}\n",
    "\n",
    "## ğŸ“š Resources\n",
    "\n",
    "- [llcuda GitHub](https://github.com/llcuda/llcuda)\n",
    "- [Unsloth](https://github.com/unslothai/unsloth)\n",
    "- [llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/README.md\", \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"âœ… README.md created\")\n",
    "print(f\"\\n{readme_content[:1500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e557cd",
   "metadata": {},
   "source": [
    "## Step 11: Create Helper Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abbad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create start-server.sh helper script\n",
    "start_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Start Multi-GPU Server\n",
    "# Usage: ./start-server.sh <model.gguf> [port]\n",
    "\n",
    "MODEL=\"$1\"\n",
    "PORT=\"${2:-8080}\"\n",
    "\n",
    "if [ -z \"$MODEL\" ]; then\n",
    "    echo \"Usage: $0 <model.gguf> [port]\"\n",
    "    echo \"Example: $0 qwen2.5-7b-Q4_K_M.gguf 8080\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Starting llama-server with dual T4 config...\"\n",
    "echo \"Model: $MODEL\"\n",
    "echo \"Port: $PORT\"\n",
    "echo \"\"\n",
    "\n",
    "\"$SCRIPT_DIR/bin/llama-server\" \\\\\n",
    "    --model \"$MODEL\" \\\\\n",
    "    --n-gpu-layers 99 \\\\\n",
    "    --tensor-split 0.5,0.5 \\\\\n",
    "    --split-mode layer \\\\\n",
    "    --flash-attn \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port \"$PORT\" \\\\\n",
    "    --ctx-size 8192 \\\\\n",
    "    --batch-size 2048 \\\\\n",
    "    --ubatch-size 512 \\\\\n",
    "    --parallel 4\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/start-server.sh\", \"w\") as f:\n",
    "    f.write(start_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/start-server.sh\", 0o755)\n",
    "\n",
    "# Create quantize.sh helper script\n",
    "quantize_script = '''#!/bin/bash\n",
    "# llcuda v2.2.0 - Quantize Model\n",
    "# Usage: ./quantize.sh <input.gguf> <output.gguf> [quant_type]\n",
    "\n",
    "INPUT=\"$1\"\n",
    "OUTPUT=\"$2\"\n",
    "QUANT=\"${3:-Q4_K_M}\"\n",
    "\n",
    "if [ -z \"$INPUT\" ] || [ -z \"$OUTPUT\" ]; then\n",
    "    echo \"Usage: $0 <input.gguf> <output.gguf> [quant_type]\"\n",
    "    echo \"Quant types: Q4_K_M (default), Q8_0, Q5_K_M, IQ4_XS, etc.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "SCRIPT_DIR=\"$(cd \"$(dirname \"$0\")\" && pwd)\"\n",
    "\n",
    "echo \"Quantizing: $INPUT â†’ $OUTPUT ($QUANT)\"\n",
    "\"$SCRIPT_DIR/bin/llama-quantize\" \"$INPUT\" \"$OUTPUT\" \"$QUANT\"\n",
    "'''\n",
    "\n",
    "with open(f\"{PACKAGE_DIR}/quantize.sh\", \"w\") as f:\n",
    "    f.write(quantize_script)\n",
    "os.chmod(f\"{PACKAGE_DIR}/quantize.sh\", 0o755)\n",
    "\n",
    "print(\"âœ… Helper scripts created:\")\n",
    "print(\"   - start-server.sh\")\n",
    "print(\"   - quantize.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e467ab",
   "metadata": {},
   "source": [
    "## Step 12: Create Distribution Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "TARBALL = f\"{PACKAGE_NAME}.tar.gz\"\n",
    "\n",
    "print(f\"Creating distribution archive: {TARBALL}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create tarball\n",
    "!tar -czvf {TARBALL} {PACKAGE_NAME}\n",
    "\n",
    "# Calculate SHA256\n",
    "with open(TARBALL, \"rb\") as f:\n",
    "    sha256 = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "# Write checksum file\n",
    "with open(f\"{TARBALL}.sha256\", \"w\") as f:\n",
    "    f.write(f\"{sha256}  {TARBALL}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“¦ DISTRIBUTION PACKAGE READY\")\n",
    "print(\"=\"*60)\n",
    "!ls -lh {TARBALL}*\n",
    "print(f\"\\nSHA256: {sha256}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3214c3f",
   "metadata": {},
   "source": [
    "## Step 13: Test Multi-GPU Inference (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a small test model and verify multi-GPU works\n",
    "from huggingface_hub import hf_hub_download\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "print(\"Downloading small test model...\")\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"lmstudio-community/gemma-2-2b-it-GGUF\",\n",
    "    filename=\"gemma-2-2b-it-Q4_K_M.gguf\",\n",
    "    cache_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "print(f\"âœ… Model: {model_path}\")\n",
    "\n",
    "# Start server with multi-GPU\n",
    "print(\"\\nStarting llama-server with dual T4 config...\")\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"--tensor-split\", \"0.5,0.5\",\n",
    "    \"--split-mode\", \"layer\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "print(f\"Command: {' '.join(server_cmd)}\")\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "print(\"\\nWaiting for server to start...\")\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"âœ… Server ready in {i+1}s!\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"âš ï¸ Server startup timeout\")\n",
    "\n",
    "# Check GPU usage\n",
    "print(\"\\nğŸ“Š GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ec572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"Testing multi-GPU inference...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start = time.time()\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    timeout=60\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "    usage = result.get(\"usage\", {})\n",
    "    \n",
    "    print(f\"âœ… Response ({elapsed:.2f}s):\")\n",
    "    print(f\"   {content}\")\n",
    "    print(f\"\\nğŸ“Š Tokens: {usage.get('total_tokens', 'N/A')}\")\n",
    "    if usage.get('completion_tokens'):\n",
    "        tps = usage['completion_tokens'] / elapsed\n",
    "        print(f\"ğŸ“Š Speed: {tps:.1f} tokens/sec\")\n",
    "else:\n",
    "    print(f\"âŒ Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup - stop server\n",
    "print(\"Stopping server...\")\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"âœ… Server stopped\")\n",
    "\n",
    "# Show final GPU state\n",
    "print(\"\\nğŸ“Š Final GPU State:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f0e65",
   "metadata": {},
   "source": [
    "## Step 13b: Test Split-GPU Architecture (LLM + Graphistry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split-GPU Architecture Demo:\n",
    "- GPU 0: llama-server (LLM inference)\n",
    "- GPU 1: RAPIDS/Graphistry (graph simulation)\n",
    "\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SPLIT-GPU ARCHITECTURE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 0: Start llama-server (LLM)\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ”§ GPU 0: Starting llama-server...\")\n",
    "\n",
    "# Force llama-server to use GPU 0 only\n",
    "llama_env = os.environ.copy()\n",
    "llama_env[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "server_cmd = [\n",
    "    f\"{PACKAGE_DIR}/bin/llama-server\",\n",
    "    \"-m\", model_path,\n",
    "    \"-ngl\", \"99\",\n",
    "    \"-fa\",\n",
    "    \"--host\", \"127.0.0.1\",\n",
    "    \"--port\", \"8080\",\n",
    "    \"-c\", \"4096\"\n",
    "]\n",
    "\n",
    "server = subprocess.Popen(\n",
    "    server_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    env=llama_env\n",
    ")\n",
    "\n",
    "# Wait for server\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get(\"http://127.0.0.1:8080/health\", timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f\"   âœ… llama-server ready on GPU 0 ({i+1}s)\")\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"   âš ï¸ Server timeout\")\n",
    "\n",
    "# ============================================================================\n",
    "# GPU 1: RAPIDS/Graphistry graph operations\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ”§ GPU 1: Running RAPIDS graph simulation...\")\n",
    "\n",
    "# Force RAPIDS to use GPU 1 only\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "# Create sample graph data (simulating knowledge graph from LLM)\n",
    "edges = cudf.DataFrame({\n",
    "    \"src\": [0, 1, 2, 3, 4, 0, 1, 2],\n",
    "    \"dst\": [1, 2, 3, 4, 0, 2, 3, 4],\n",
    "    \"weight\": [1.0, 2.0, 1.5, 0.5, 3.0, 2.5, 1.0, 0.8]\n",
    "})\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "\n",
    "print(f\"   Graph: {G.number_of_vertices()} vertices, {G.number_of_edges()} edges\")\n",
    "\n",
    "# Run PageRank on GPU 1\n",
    "pagerank = cugraph.pagerank(G)\n",
    "print(f\"   PageRank computed: {len(pagerank)} nodes\")\n",
    "print(f\"   Top node: {pagerank.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Combined workflow: LLM query â†’ Graph update\n",
    "# ============================================================================\n",
    "print(\"\\nğŸ”— Combined LLM + Graph workflow...\")\n",
    "\n",
    "# Reset CUDA_VISIBLE_DEVICES for requests\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "# Query LLM on GPU 0\n",
    "response = requests.post(\n",
    "    \"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "    json={\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"List 3 related concepts to 'machine learning'\"}],\n",
    "        \"max_tokens\": 100\n",
    "    },\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    llm_output = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(f\"   LLM (GPU 0): {llm_output[:100]}...\")\n",
    "    \n",
    "    # Simulate adding LLM-derived edges to graph\n",
    "    new_edges = cudf.DataFrame({\n",
    "        \"src\": [5, 5, 5],\n",
    "        \"dst\": [0, 1, 2],\n",
    "        \"weight\": [1.0, 1.0, 1.0]\n",
    "    })\n",
    "    all_edges = cudf.concat([edges, new_edges])\n",
    "    G2 = cugraph.Graph()\n",
    "    G2.from_cudf_edgelist(all_edges, source=\"src\", destination=\"dst\", edge_attr=\"weight\")\n",
    "    print(f\"   Graph (GPU 1): Updated to {G2.number_of_vertices()} vertices\")\n",
    "\n",
    "print(\"\\nğŸ“Š GPU Memory Usage:\")\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.total --format=csv\n",
    "\n",
    "# Cleanup\n",
    "server.terminate()\n",
    "server.wait()\n",
    "print(\"\\nâœ… Split-GPU test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634bb500",
   "metadata": {},
   "source": [
    "## Step 13b: llcuda v2.2.0 Module Integration Demo\n",
    "\n",
    "Demonstrate the new Graphistry and Louie.AI modules from llcuda v2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# llcuda v2.2.0 Module Integration Demo\n",
    "# ============================================================================\n",
    "# This demonstrates the new Graphistry and Louie.AI modules\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"llcuda v2.2.0 MODULE INTEGRATION DEMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Install llcuda from GitHub (use main branch or specific version)\n",
    "!pip install -q git+https://github.com/llcuda/llcuda.git\n",
    "\n",
    "import llcuda\n",
    "\n",
    "print(f\"\\nğŸ“¦ llcuda version: {llcuda.__version__}\")\n",
    "print(f\"\\nğŸ“‹ Available exports:\")\n",
    "print(f\"   {llcuda.__all__}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SplitGPUConfig - Configure Split-GPU Workloads\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. SplitGPUConfig Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "config = llcuda.SplitGPUConfig(llm_gpu=0, graph_gpu=1)\n",
    "print(f\"   LLM GPU: {config.llm_gpu}\")\n",
    "print(f\"   Graph GPU: {config.graph_gpu}\")\n",
    "\n",
    "# Get environment variables for each GPU\n",
    "print(f\"\\n   LLM env: {config.llm_env()}\")\n",
    "print(f\"   Graph env: {config.graph_env()}\")\n",
    "\n",
    "# Generate llama-server command\n",
    "model_path = f\"/kaggle/working/{PACKAGE_NAME}/models/gemma-3-1b-Q4_K_M.gguf\"\n",
    "cmd = config.llama_server_cmd(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=99,\n",
    "    flash_attention=True,\n",
    "    port=8080\n",
    ")\n",
    "print(f\"\\n   Server command:\\n   {' '.join(cmd)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Graphistry Module - Graph Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. Graphistry Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.graphistry import GraphWorkload, RAPIDSBackend, check_rapids_available\n",
    "\n",
    "# Check RAPIDS availability\n",
    "rapids_status = check_rapids_available()\n",
    "print(f\"   RAPIDS status: {rapids_status}\")\n",
    "\n",
    "# Create GraphWorkload on GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "workload = GraphWorkload(gpu_id=1)\n",
    "\n",
    "# Sample entities and relationships (simulating LLM-extracted knowledge)\n",
    "entities = [\n",
    "    {\"id\": \"Machine Learning\", \"type\": \"field\", \"properties\": {\"year\": 1959}},\n",
    "    {\"id\": \"Deep Learning\", \"type\": \"field\", \"properties\": {\"year\": 2006}},\n",
    "    {\"id\": \"Neural Networks\", \"type\": \"concept\"},\n",
    "    {\"id\": \"Transformers\", \"type\": \"architecture\", \"properties\": {\"year\": 2017}},\n",
    "    {\"id\": \"GPT\", \"type\": \"model\"},\n",
    "    {\"id\": \"BERT\", \"type\": \"model\"},\n",
    "    {\"id\": \"CNN\", \"type\": \"architecture\"},\n",
    "]\n",
    "\n",
    "relationships = [\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Deep Learning\", \"type\": \"contains\", \"weight\": 0.9},\n",
    "    {\"source\": \"Machine Learning\", \"target\": \"Neural Networks\", \"type\": \"uses\", \"weight\": 0.85},\n",
    "    {\"source\": \"Deep Learning\", \"target\": \"Transformers\", \"type\": \"includes\", \"weight\": 0.95},\n",
    "    {\"source\": \"Transformers\", \"target\": \"GPT\", \"type\": \"basis_for\", \"weight\": 0.9},\n",
    "    {\"source\": \"Transformers\", \"target\": \"BERT\", \"type\": \"basis_for\", \"weight\": 0.88},\n",
    "    {\"source\": \"Neural Networks\", \"target\": \"CNN\", \"type\": \"type_of\", \"weight\": 0.8},\n",
    "]\n",
    "\n",
    "# Create knowledge graph using the correct API\n",
    "g = workload.create_knowledge_graph(entities, relationships)\n",
    "print(f\"   Knowledge graph created with Graphistry\")\n",
    "\n",
    "# Run PageRank using edges DataFrame (correct API)\n",
    "import pandas as pd\n",
    "edges_df = pd.DataFrame([\n",
    "    {\"src\": r[\"source\"], \"dst\": r[\"target\"], \"weight\": r.get(\"weight\", 1.0)}\n",
    "    for r in relationships\n",
    "])\n",
    "pagerank_result = workload.run_pagerank(edges_df)\n",
    "print(f\"   PageRank: top node = {pagerank_result.nlargest(1, 'pagerank')['vertex'].values[0]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Louie Module - Natural Language Graph Queries\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. Louie Module Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from llcuda.louie import LouieClient, KnowledgeExtractor\n",
    "\n",
    "# Initialize Louie client (connected to llama-server on GPU 0)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "louie = LouieClient(llm_endpoint=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Knowledge extraction example\n",
    "text = \"\"\"\n",
    "NVIDIA develops GPUs for deep learning. The Tesla T4 is optimized for inference.\n",
    "llcuda v2.2.0 runs on Tesla T4 with FlashAttention enabled.\n",
    "cuGraph provides GPU-accelerated graph analytics.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"   Input text: {text[:60]}...\")\n",
    "\n",
    "# Extract entities (requires running LLM server)\n",
    "try:\n",
    "    entities = louie.extract_entities(text)\n",
    "    print(f\"   Extracted entities: {entities[:3]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"   (LLM server required for entity extraction)\")\n",
    "    # Simulated output\n",
    "    entities = [\n",
    "        {\"name\": \"NVIDIA\", \"type\": \"ORG\"},\n",
    "        {\"name\": \"Tesla T4\", \"type\": \"PRODUCT\"},\n",
    "        {\"name\": \"llcuda\", \"type\": \"SOFTWARE\"},\n",
    "        {\"name\": \"cuGraph\", \"type\": \"SOFTWARE\"}\n",
    "    ]\n",
    "    print(f\"   Demo entities: {entities}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RAPIDS Backend Direct Access\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. RAPIDS Backend Demo\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "backend = RAPIDSBackend()\n",
    "\n",
    "# Create a cuDF DataFrame\n",
    "import cudf\n",
    "gpu_edges = cudf.DataFrame({\n",
    "    \"source\": [0, 1, 2, 3, 4, 0, 1],\n",
    "    \"target\": [1, 2, 3, 4, 0, 2, 3],\n",
    "    \"weight\": [1.0, 0.8, 0.9, 0.7, 1.0, 0.6, 0.85]\n",
    "})\n",
    "\n",
    "# Run graph algorithms\n",
    "import cugraph\n",
    "G_rapids = cugraph.Graph()\n",
    "G_rapids.from_cudf_edgelist(gpu_edges, source=\"source\", destination=\"target\")\n",
    "\n",
    "# Louvain community detection\n",
    "louvain = cugraph.louvain(G_rapids)\n",
    "print(f\"   Louvain communities: {louvain['partition'].nunique()} detected\")\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness = cugraph.betweenness_centrality(G_rapids)\n",
    "top_node = betweenness.nlargest(1, 'betweenness_centrality')['vertex'].values[0]\n",
    "print(f\"   Highest betweenness: node {top_node}\")\n",
    "\n",
    "print(\"\\nâœ… llcuda v2.2.0 module integration complete!\")\n",
    "print(\"   All new APIs functional on Kaggle 2Ã— T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139aebf7",
   "metadata": {},
   "source": [
    "## Step 14: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c32db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/kaggle/working\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ‰ llcuda v2.2.0 BUILD COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“¦ Distribution Package:\")\n",
    "!ls -lh {PACKAGE_NAME}.tar.gz\n",
    "\n",
    "print(f\"\\nğŸ“ Package Contents:\")\n",
    "!ls -la {PACKAGE_NAME}/\n",
    "\n",
    "print(f\"\\nğŸ”§ Binaries:\")\n",
    "!ls -lh {PACKAGE_NAME}/bin/ | head -10\n",
    "\n",
    "print(f\"\\nğŸ“‹ Metadata Summary:\")\n",
    "print(f\"   Version: {VERSION}\")\n",
    "print(f\"   Platform: Kaggle 2Ã— Tesla T4\")\n",
    "print(f\"   CUDA: {cuda_version}\")\n",
    "print(f\"   Compute: SM 7.5 (Turing)\")\n",
    "print(f\"   FlashAttention: âœ… All quants\")\n",
    "print(f\"   Multi-GPU: âœ… Native CUDA\")\n",
    "\n",
    "print(f\"\\nğŸš€ Next Steps:\")\n",
    "print(f\"   1. Download: {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   2. Extract: tar -xzf {PACKAGE_NAME}.tar.gz\")\n",
    "print(f\"   3. Run: ./start-server.sh model.gguf 8080\")\n",
    "\n",
    "print(f\"\\nğŸ“¥ Download from Kaggle Output tab\")\n",
    "print(f\"   or copy to output: !cp {PACKAGE_NAME}.tar.gz /kaggle/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554265ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy to Kaggle output for download\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"/kaggle/output\", exist_ok=True)\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz\", \"/kaggle/output/\")\n",
    "shutil.copy(f\"/kaggle/working/{PACKAGE_NAME}.tar.gz.sha256\", \"/kaggle/output/\")\n",
    "\n",
    "print(\"âœ… Package copied to /kaggle/output/ for download\")\n",
    "!ls -lh /kaggle/output/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
