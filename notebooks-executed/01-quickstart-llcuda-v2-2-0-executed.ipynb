{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b394706d","cell_type":"markdown","source":"## Step 1: Verify Kaggle GPU Environment\n\nFirst, let's confirm we have 2Ã— Tesla T4 GPUs available.","metadata":{}},{"id":"e5e56c1a","cell_type":"code","source":"# Verify we have 2Ã— T4 GPUs\nimport subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"ðŸ” KAGGLE GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check nvidia-smi\nresult = subprocess.run([\"nvidia-smi\", \"-L\"], capture_output=True, text=True)\ngpu_lines = [l for l in result.stdout.strip().split(\"\\n\") if l.startswith(\"GPU\")]\nprint(f\"\\nðŸ“Š Detected GPUs: {len(gpu_lines)}\")\nfor line in gpu_lines:\n    print(f\"   {line}\")\n\n# Check CUDA version\nprint(\"\\nðŸ“Š CUDA Version:\")\n!nvcc --version | grep release\n\n# Check total VRAM\nprint(\"\\nðŸ“Š VRAM Summary:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n\n# Verify we have 2 GPUs\nif len(gpu_lines) >= 2:\n    print(\"\\nâœ… Multi-GPU environment confirmed! Ready for llcuda v2.2.0.\")\nelse:\n    print(\"\\nâš ï¸ WARNING: Less than 2 GPUs detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle notebook settings.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:13:44.157608Z","iopub.execute_input":"2026-01-19T01:13:44.157986Z","iopub.status.idle":"2026-01-19T01:13:44.498406Z","shell.execute_reply.started":"2026-01-19T01:13:44.157955Z","shell.execute_reply":"2026-01-19T01:13:44.497486Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸ” KAGGLE GPU ENVIRONMENT CHECK\n======================================================================\n\nðŸ“Š Detected GPUs: 2\n   GPU 0: Tesla T4 (UUID: GPU-74d2d9a7-45a3-1bb5-a448-e5052f97d46c)\n   GPU 1: Tesla T4 (UUID: GPU-683b0de7-3158-c330-7e4d-a86251f10d5f)\n\nðŸ“Š CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\nðŸ“Š VRAM Summary:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\n\nâœ… Multi-GPU environment confirmed! Ready for llcuda v2.2.0.\n","output_type":"stream"}],"execution_count":1},{"id":"b734856a","cell_type":"markdown","source":"## Step 2: Install llcuda v2.2.0\n\nInstall from GitHub with pre-built binaries for Kaggle T4Ã—2.","metadata":{}},{"id":"8f5d3990","cell_type":"code","source":"%%time\n# Install llcuda v2.2.0 from GitHub (force fresh install, no cache)\nprint(\"ðŸ“¦ Installing llcuda v2.2.0...\")\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n\n# Verify installation\nimport llcuda\nprint(f\"\\nâœ… llcuda {llcuda.__version__} installed!\")\n\n# Check llcuda status using available APIs\nfrom llcuda import check_cuda_available, get_cuda_device_info\nfrom llcuda.api.multigpu import gpu_count\n\ncuda_info = get_cuda_device_info()\nprint(f\"\\nðŸ“Š llcuda Status:\")\nprint(f\"   CUDA Available: {check_cuda_available()}\")\nprint(f\"   GPUs: {gpu_count()}\")\nif cuda_info:\n    print(f\"   CUDA Version: {cuda_info.get('cuda_version', 'N/A')}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:15:21.392717Z","iopub.execute_input":"2026-01-19T01:15:21.393073Z","iopub.status.idle":"2026-01-19T01:15:45.521533Z","shell.execute_reply.started":"2026-01-19T01:15:21.393044Z","shell.execute_reply":"2026-01-19T01:15:45.520790Z"}},"outputs":[{"name":"stdout","text":"ðŸ“¦ Installing llcuda v2.2.0...\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m534.5/534.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m302.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m338.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m368.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m335.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m341.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m320.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m313.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m342.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m256.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m145.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m308.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m293.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m242.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m285.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m330.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\nâœ… llcuda 2.2.0 installed!\n\nðŸ“Š llcuda Status:\n   CUDA Available: True\n   GPUs: 2\n   CUDA Version: 12.5\nCPU times: user 249 ms, sys: 65.6 ms, total: 314 ms\nWall time: 24.1 s\n","output_type":"stream"}],"execution_count":3},{"id":"7892d082","cell_type":"markdown","source":"## Step 3: Download a GGUF Model\n\nWe'll use Gemma 3n 4B in Q4_K_M quantization - perfect for Kaggle T4 GPUs.","metadata":{}},{"id":"fd35a857","cell_type":"code","source":"%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Model selection - optimized for 15GB VRAM\nMODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\nMODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n\nprint(f\"ðŸ“¥ Downloading {MODEL_FILE}...\")\nprint(f\"   Repository: {MODEL_REPO}\")\nprint(f\"   Expected size: ~2.5GB\")\n\n# Download to Kaggle working directory\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nprint(f\"\\nâœ… Model downloaded: {model_path}\")\n\n# Show model size\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"   Size: {size_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:15:45.523002Z","iopub.execute_input":"2026-01-19T01:15:45.523252Z","iopub.status.idle":"2026-01-19T01:15:50.491576Z","shell.execute_reply.started":"2026-01-19T01:15:45.523221Z","shell.execute_reply":"2026-01-19T01:15:50.491003Z"}},"outputs":[{"name":"stdout","text":"ðŸ“¥ Downloading gemma-3-4b-it-Q4_K_M.gguf...\n   Repository: unsloth/gemma-3-4b-it-GGUF\n   Expected size: ~2.5GB\n","output_type":"stream"},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\nWARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc688049fe540c1ba36ad4b4cd4eff5"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Model downloaded: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   Size: 2.32 GB\nCPU times: user 5.18 s, sys: 8.06 s, total: 13.2 s\nWall time: 4.96 s\n","output_type":"stream"}],"execution_count":4},{"id":"c8710abc","cell_type":"markdown","source":"## Step 4: Start llama-server\n\nStart the inference server on GPU 0 with optimal settings for T4.","metadata":{}},{"id":"d5bbe24c","cell_type":"code","source":"from llcuda.server import ServerManager\nfrom llcuda.api.multigpu import kaggle_t4_dual_config\n\n# Get optimized configuration for Kaggle T4Ã—2\nconfig = kaggle_t4_dual_config()\n\nprint(\"ðŸš€ Starting llama-server with Multi-GPU configuration...\")\nprint(f\"   Model: {model_path}\")\nprint(f\"   GPU Layers: {config.n_gpu_layers} (all layers)\")\nprint(f\"   Context Size: {config.ctx_size}\")\nprint(f\"   Tensor Split: {config.tensor_split} (equal across 2 GPUs)\")\nprint(f\"   Flash Attention: {config.flash_attention}\")\n\n# Create server manager\nserver = ServerManager(server_url=\"http://127.0.0.1:8080\")\n\n# Start server with multi-GPU configuration\n# Pass tensor_split as comma-separated string for --tensor-split flag\ntensor_split_str = \",\".join(str(x) for x in config.tensor_split) if config.tensor_split else None\n\ntry:\n    server.start_server(\n        model_path=model_path,\n        host=\"127.0.0.1\",\n        port=8080,\n        gpu_layers=config.n_gpu_layers,\n        ctx_size=config.ctx_size,\n        timeout=120,\n        verbose=True,\n        # Multi-GPU parameters (passed via **kwargs)\n        flash_attn=1 if config.flash_attention else 0,\n        split_mode=\"layer\",\n        tensor_split=tensor_split_str,\n    )\n    print(\"\\nâœ… llama-server is ready with dual T4 GPUs!\")\n    print(f\"   API endpoint: http://127.0.0.1:8080\")\nexcept Exception as e:\n    print(f\"\\nâŒ Server failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:15:54.523638Z","iopub.execute_input":"2026-01-19T01:15:54.524366Z","iopub.status.idle":"2026-01-19T01:15:59.575043Z","shell.execute_reply.started":"2026-01-19T01:15:54.524338Z","shell.execute_reply":"2026-01-19T01:15:59.574362Z"}},"outputs":[{"name":"stdout","text":"ðŸš€ Starting llama-server with Multi-GPU configuration...\n   Model: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   GPU Layers: -1 (all layers)\n   Context Size: 8192\n   Tensor Split: [0.5, 0.5] (equal across 2 GPUs)\n   Flash Attention: True\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: -1\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready........ âœ“ Ready in 5.0s\n\nâœ… llama-server is ready with dual T4 GPUs!\n   API endpoint: http://127.0.0.1:8080\n","output_type":"stream"}],"execution_count":5},{"id":"3ad24c79","cell_type":"markdown","source":"## Step 5: Run Your First Inference\n\nUse the OpenAI-compatible API to chat with the model.","metadata":{}},{"id":"249c0176","cell_type":"code","source":"from llcuda.api.client import LlamaCppClient\n\n# Create client\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Test simple completion using OpenAI-compatible API\nprint(\"ðŸ’¬ Testing inference...\\n\")\n\nresponse = client.chat.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is CUDA? Explain in 2 sentences.\"}\n    ],\n    max_tokens=100,\n    temperature=0.7\n)\n\nprint(\"ðŸ“ Response:\")\nprint(response.choices[0].message.content)\n\nprint(f\"\\nðŸ“Š Stats:\")\nprint(f\"   Tokens generated: {response.usage.completion_tokens}\")\nprint(f\"   Total tokens: {response.usage.total_tokens}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:24:50.609088Z","iopub.execute_input":"2026-01-19T01:24:50.609895Z","iopub.status.idle":"2026-01-19T01:24:52.621321Z","shell.execute_reply.started":"2026-01-19T01:24:50.609864Z","shell.execute_reply":"2026-01-19T01:24:52.620687Z"}},"outputs":[{"name":"stdout","text":"ðŸ’¬ Testing inference...\n\nðŸ“ Response:\nCUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA that allows software developers to leverage the power of NVIDIA GPUs for general-purpose processing tasks â€“ essentially, using graphics cards for more than just displaying images. It enables developers to write code that can run concurrently across thousands of GPU cores, dramatically speeding up computationally intensive applications like machine learning and scientific simulations.\n\nðŸ“Š Stats:\n   Tokens generated: 78\n   Total tokens: 97\n","output_type":"stream"}],"execution_count":7},{"id":"cab76caa","cell_type":"markdown","source":"## Step 6: Streaming Response Example\n\nStream responses for real-time output.","metadata":{}},{"id":"8f6c7c3e","cell_type":"code","source":"# Streaming example using OpenAI-compatible API\nprint(\"ðŸ’¬ Streaming response...\\n\")\n\nfor chunk in client.chat.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a Python function to calculate factorial.\"}\n    ],\n    max_tokens=200,\n    temperature=0.3,\n    stream=True  # Enable streaming\n):\n    if hasattr(chunk, 'choices') and chunk.choices:\n        delta = chunk.choices[0].delta\n        if hasattr(delta, 'content') and delta.content:\n            print(delta.content, end=\"\", flush=True)\n\nprint(\"\\n\\nâœ… Streaming complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:25:02.814605Z","iopub.execute_input":"2026-01-19T01:25:02.815316Z","iopub.status.idle":"2026-01-19T01:25:07.431021Z","shell.execute_reply.started":"2026-01-19T01:25:02.815282Z","shell.execute_reply":"2026-01-19T01:25:07.430281Z"}},"outputs":[{"name":"stdout","text":"ðŸ’¬ Streaming response...\n\n\n\nâœ… Streaming complete!\n","output_type":"stream"}],"execution_count":8},{"id":"7eb40cde","cell_type":"markdown","source":"## Step 7: Check GPU Memory Usage\n\nMonitor VRAM usage to understand resource consumption.","metadata":{}},{"id":"7f3b2939","cell_type":"code","source":"# Check GPU memory usage\nprint(\"ðŸ“Š GPU Memory Usage:\")\nprint(\"=\"*60)\n!nvidia-smi --query-gpu=index,name,memory.used,memory.total,utilization.gpu --format=csv\n\nprint(\"\\nðŸ’¡ Note:\")\nprint(\"   GPU 0: llama-server (LLM inference)\")\nprint(\"   GPU 1: Available for RAPIDS/Graphistry\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:25:27.271229Z","iopub.execute_input":"2026-01-19T01:25:27.271526Z","iopub.status.idle":"2026-01-19T01:25:27.432551Z","shell.execute_reply.started":"2026-01-19T01:25:27.271501Z","shell.execute_reply":"2026-01-19T01:25:27.431893Z"}},"outputs":[{"name":"stdout","text":"ðŸ“Š GPU Memory Usage:\n============================================================\nindex, name, memory.used [MiB], memory.total [MiB], utilization.gpu [%]\n0, Tesla T4, 1305 MiB, 15360 MiB, 0 %\n1, Tesla T4, 1795 MiB, 15360 MiB, 0 %\n\nðŸ’¡ Note:\n   GPU 0: llama-server (LLM inference)\n   GPU 1: Available for RAPIDS/Graphistry\n","output_type":"stream"}],"execution_count":9},{"id":"dd0b4582","cell_type":"markdown","source":"## Step 8: Cleanup\n\nStop the server when done.","metadata":{}},{"id":"5c3cd97d","cell_type":"code","source":"# Stop the server\nprint(\"ðŸ›‘ Stopping llama-server...\")\nserver.stop_server()\nprint(\"\\nâœ… Server stopped. Resources freed.\")\n\n# Verify GPU memory is released\nprint(\"\\nðŸ“Š GPU Memory After Cleanup:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:25:37.551597Z","iopub.execute_input":"2026-01-19T01:25:37.552152Z","iopub.status.idle":"2026-01-19T01:25:38.224028Z","shell.execute_reply.started":"2026-01-19T01:25:37.552117Z","shell.execute_reply":"2026-01-19T01:25:38.223157Z"}},"outputs":[{"name":"stdout","text":"ðŸ›‘ Stopping llama-server...\n\nâœ… Server stopped. Resources freed.\n\nðŸ“Š GPU Memory After Cleanup:\nindex, memory.used [MiB], memory.total [MiB]\n0, 0 MiB, 15360 MiB\n1, 0 MiB, 15360 MiB\n","output_type":"stream"}],"execution_count":10},{"id":"2ad536df","cell_type":"markdown","source":"## ðŸŽ‰ Quick Start Complete!\n\nYou've successfully:\n1. âœ… Verified Kaggle GPU environment\n2. âœ… Installed llcuda v2.2.0\n3. âœ… Downloaded a GGUF model\n4. âœ… Started llama-server\n5. âœ… Ran inference with chat completion\n6. âœ… Used streaming responses\n\n## Next Steps\n\nExplore more tutorials:\n- ðŸ“˜ [02-llama-server-setup](02-llama-server-setup-llcuda-v2.2.0.ipynb) - Advanced server configuration\n- ðŸ“˜ [03-multi-gpu-inference](03-multi-gpu-inference-llcuda-v2.2.0.ipynb) - Dual T4 inference\n- ðŸ“˜ [04-gguf-quantization](04-gguf-quantization-llcuda-v2.2.0.ipynb) - Quantization guide\n- ðŸ“˜ [05-unsloth-integration](05-unsloth-integration-llcuda-v2.2.0.ipynb) - Unsloth training â†’ llcuda\n- ðŸ“˜ [06-split-gpu-graphistry](06-split-gpu-graphistry-llcuda-v2.2.0.ipynb) - LLM + Graph visualization\n\n---\n\n**llcuda v2.2.0** | CUDA 12 Inference Backend for Unsloth","metadata":{}},{"id":"6964f4d2-a159-4410-bb1c-13f3427abec2","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}