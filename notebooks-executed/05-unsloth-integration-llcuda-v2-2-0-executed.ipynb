{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0f0c4fcb","cell_type":"markdown","source":"## Step 1: Check GPU Environment","metadata":{}},{"id":"c31f0af2","cell_type":"code","source":"import subprocess\n\nprint(\"=\"*70)\nprint(\"ğŸ” GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Check GPUs\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n\n# CUDA version\nprint(\"\\nğŸ“Š CUDA Version:\")\n!nvcc --version | grep release\n\nprint(\"\\nâœ… Environment ready for Unsloth + llcuda\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:21:18.004059Z","iopub.execute_input":"2026-01-19T18:21:18.004355Z","iopub.status.idle":"2026-01-19T18:21:18.318991Z","shell.execute_reply.started":"2026-01-19T18:21:18.004332Z","shell.execute_reply":"2026-01-19T18:21:18.317972Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ” GPU ENVIRONMENT CHECK\n======================================================================\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\n\nğŸ“Š CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\nâœ… Environment ready for Unsloth + llcuda\n","output_type":"stream"}],"execution_count":1},{"id":"8584dd45","cell_type":"markdown","source":"## Step 2: Install Unsloth and llcuda","metadata":{}},{"id":"aa2cd09a","cell_type":"code","source":"print(\"ğŸ“¦ Installing Unsloth and llcuda...\")\n\n# Install Unsloth (fast installation)\n!pip install -q unsloth\n\n# Install llcuda v2.2.0 (avoid --force-reinstall to prevent breaking RAPIDS/cupy)\n!pip install -q git+https://github.com/llcuda/llcuda.git@v2.2.0\n\n# Additional dependencies\n!pip install -q datasets trl\n\n# Verify installations\ntry:\n    import llcuda\n    print(f\"\\nâœ… llcuda {llcuda.__version__} installed\")\nexcept Exception as e:\n    print(f\"âŒ llcuda import failed: {e}\")\n\ntry:\n    from unsloth import FastLanguageModel\n    print(\"âœ… Unsloth installed\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Unsloth import issue: {e}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:21:29.151618Z","iopub.execute_input":"2026-01-19T18:21:29.151978Z","iopub.status.idle":"2026-01-19T18:26:15.316227Z","shell.execute_reply.started":"2026-01-19T18:21:29.151947Z","shell.execute_reply":"2026-01-19T18:26:15.315512Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ“¦ Installing Unsloth and llcuda...\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m389.2/389.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nğŸ¯ llcuda v2.2.0 First-Time Setup - Kaggle 2Ã— T4 Multi-GPU\n======================================================================\n\nğŸ® GPU Detected: Tesla T4 (Compute 7.5)\n  âœ… Tesla T4 detected - Perfect for llcuda v2.1!\nğŸŒ Platform: Colab\n\nğŸ“¦ Downloading Kaggle 2Ã— T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\nâ¡ï¸  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\nğŸ“¥ Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\nFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(â€¦):   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779a75066db24a2dbb81f1d89ca2fb18"}},"metadata":{}},{"name":"stdout","text":"ğŸ” Verifying SHA256 checksum...\n   âœ… Checksum verified\nğŸ“¦ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\nâœ… Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\nâœ… Binaries installed successfully!\n\n\nâœ… llcuda 2.2.0 installed\nğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-01-19 18:25:45.741070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768847145.926707      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768847145.981951      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768847146.395418      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768847146.395458      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768847146.395463      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768847146.395468      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\nâœ… Unsloth installed\n","output_type":"stream"}],"execution_count":2},{"id":"03ac151c","cell_type":"markdown","source":"## Step 3: Load Base Model with Unsloth\n\nWe'll use Gemma-3 1B as it's fast to fine-tune on T4.","metadata":{}},{"id":"0b31df05","cell_type":"code","source":"%%time\nfrom unsloth import FastLanguageModel\nimport torch\n\nprint(\"=\"*70)\nprint(\"ğŸ“¥ LOADING BASE MODEL WITH UNSLOTH\")\nprint(\"=\"*70)\n\n# Model configuration\nmodel_name = \"unsloth/gemma-3-1b-it\"  # Small model for demo\nmax_seq_length = 2048\n\nprint(f\"\\nğŸ“¥ Loading {model_name}...\")\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    max_seq_length=max_seq_length,\n    dtype=torch.float16,\n    load_in_4bit=True,  # Use 4-bit for training\n)\n\nprint(f\"\\nâœ… Model loaded!\")\nprint(f\"   Model: {model_name}\")\nprint(f\"   Max Sequence Length: {max_seq_length}\")\nprint(f\"   Precision: 4-bit\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:26:21.312490Z","iopub.execute_input":"2026-01-19T18:26:21.313790Z","iopub.status.idle":"2026-01-19T18:26:51.261670Z","shell.execute_reply.started":"2026-01-19T18:26:21.313751Z","shell.execute_reply":"2026-01-19T18:26:51.260621Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“¥ LOADING BASE MODEL WITH UNSLOTH\n======================================================================\n\nğŸ“¥ Loading unsloth/gemma-3-1b-it...\n==((====))==  Unsloth 2026.1.3: Fast Gemma3 patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\nUnsloth: Gemma3 does not support SDPA - switching to fast eager.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a8cf98b028454baaf8854af54d07a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc3d982d92aa46089a22a9d8e3171b71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b83953a69dc4c2ca2876c57ad7f4c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"978f28db176e4b4db83853251fd8a57a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf326ca1119c4ec4b7b6965adb2a2ec8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0125e7f162a46e0adbabcb6efcf9ca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2887667263094619a6db519cfa2bab4e"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Model loaded!\n   Model: unsloth/gemma-3-1b-it\n   Max Sequence Length: 2048\n   Precision: 4-bit\nCPU times: user 13.5 s, sys: 5.04 s, total: 18.5 s\nWall time: 29.9 s\n","output_type":"stream"}],"execution_count":3},{"id":"257229ae","cell_type":"markdown","source":"## Step 4: Add LoRA Adapters","metadata":{}},{"id":"13ac530a","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ”§ ADDING LORA ADAPTERS\")\nprint(\"=\"*70)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,                # LoRA rank\n    lora_alpha=32,       # LoRA alpha\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n)\n\n# Count trainable parameters\ntrainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal = sum(p.numel() for p in model.parameters())\n\nprint(f\"\\nâœ… LoRA adapters added!\")\nprint(f\"   Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\nprint(f\"   Total params: {total:,}\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:27:21.481066Z","iopub.execute_input":"2026-01-19T18:27:21.481785Z","iopub.status.idle":"2026-01-19T18:27:26.201292Z","shell.execute_reply.started":"2026-01-19T18:27:21.481737Z","shell.execute_reply":"2026-01-19T18:27:26.200538Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ”§ ADDING LORA ADAPTERS\n======================================================================\nUnsloth: Making `model.base_model.model.model` require gradients\n\nâœ… LoRA adapters added!\n   Trainable params: 13,045,760 (1.93%)\n   Total params: 675,994,752\n","output_type":"stream"}],"execution_count":4},{"id":"4b8df930","cell_type":"markdown","source":"## Step 5: Prepare Training Dataset","metadata":{}},{"id":"6636ad16","cell_type":"code","source":"from datasets import load_dataset\n\nprint(\"=\"*70)\nprint(\"ğŸ“Š PREPARING TRAINING DATASET\")\nprint(\"=\"*70)\n\n# Load a small dataset for demo (Alpaca format)\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n\nprint(f\"\\nğŸ“Š Dataset loaded: {len(dataset)} examples\")\nprint(f\"\\nğŸ“‹ Sample data:\")\nprint(dataset[0])\n\n# Format for training (Alpaca prompt format)\ndef format_alpaca(example):\n    instruction = example.get(\"instruction\", \"\")\n    input_text = example.get(\"input\", \"\")\n    output = example.get(\"output\", \"\")\n    \n    if input_text:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{instruction}\n\n### Response:\n{output}\"\"\"\n    \n    return {\"text\": prompt}\n\ndataset = dataset.map(format_alpaca)\nprint(f\"\\nâœ… Dataset formatted for training\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:27:49.669384Z","iopub.execute_input":"2026-01-19T18:27:49.670180Z","iopub.status.idle":"2026-01-19T18:27:55.201725Z","shell.execute_reply.started":"2026-01-19T18:27:49.670134Z","shell.execute_reply":"2026-01-19T18:27:55.200758Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“Š PREPARING TRAINING DATASET\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a9d9a2f7c04ce9a7ae88444bae6339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3530d0f31c04646ab54da4669bd0467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab4efc68fde3450cba6a41a461acecca"}},"metadata":{}},{"name":"stdout","text":"\nğŸ“Š Dataset loaded: 500 examples\n\nğŸ“‹ Sample data:\n{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.', 'input': '', 'instruction': 'Give three tips for staying healthy.'}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bb0e3501ffe49b8930c7bdb91a462a7"}},"metadata":{}},{"name":"stdout","text":"\nâœ… Dataset formatted for training\n","output_type":"stream"}],"execution_count":5},{"id":"f2480a22","cell_type":"markdown","source":"## Step 6: Train with SFTTrainer","metadata":{}},{"id":"a4f2f9cd","cell_type":"code","source":"%%time\nfrom trl import SFTTrainer, SFTConfig\n\nprint(\"=\"*70)\nprint(\"ğŸ‹ï¸ TRAINING MODEL\")\nprint(\"=\"*70)\n\n# Training configuration (quick demo - increase for real training)\ntraining_args = SFTConfig(\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=30,  # Quick demo - use more for real training\n    learning_rate=2e-4,\n    fp16=True,\n    logging_steps=5,\n    output_dir=\"./unsloth_output\",\n    optim=\"adamw_8bit\",\n    seed=42,\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    processing_class=tokenizer,\n    args=training_args,\n)\n\nprint(\"\\nğŸ‹ï¸ Starting training...\")\nprint(f\"   Batch size: {training_args.per_device_train_batch_size}\")\nprint(f\"   Max steps: {training_args.max_steps}\")\nprint(f\"   Learning rate: {training_args.learning_rate}\")\n\ntrainer.train()\n\nprint(\"\\nâœ… Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:28:47.231113Z","iopub.execute_input":"2026-01-19T18:28:47.231879Z","iopub.status.idle":"2026-01-19T18:30:37.448491Z","shell.execute_reply.started":"2026-01-19T18:28:47.231838Z","shell.execute_reply":"2026-01-19T18:30:37.447661Z"},"trusted":true},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ‹ï¸ TRAINING MODEL\n======================================================================\nUnsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3889abba4404e17b08fc8977fc06349"}},"metadata":{}},{"name":"stdout","text":"ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n\nğŸ‹ï¸ Starting training...\n   Batch size: 2\n   Max steps: 30\n   Learning rate: 0.0002\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 13,045,760 of 1,012,931,712 (1.29% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 00:59, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>2.160100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.483500</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.545400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.510800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.326100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.457300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nâœ… Training complete!\nCPU times: user 1min 35s, sys: 4.28 s, total: 1min 39s\nWall time: 1min 50s\n","output_type":"stream"}],"execution_count":6},{"id":"00e7361a","cell_type":"markdown","source":"## Step 7: Export to GGUF Format, creating tar file of GGUF output and download it.\n\nThis is the key step - converting from Unsloth to llama.cpp compatible format.","metadata":{}},{"id":"966f4412","cell_type":"code","source":"import os\n\nprint(\"=\"*70)\nprint(\"ğŸ“¦ EXPORTING TO GGUF FORMAT\")\nprint(\"=\"*70)\n\n# Output path\nOUTPUT_DIR = \"/kaggle/working/gguf_output\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Unsloth's built-in GGUF export\nprint(\"\\nğŸ“¦ Exporting to GGUF with Q4_K_M quantization...\")\n\nmodel.save_pretrained_gguf(\n    OUTPUT_DIR,\n    tokenizer,\n    quantization_method=\"q4_k_m\",  # K-quant for best quality/size\n)\n\n# Find the exported file\ngguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\nprint(f\"\\nâœ… GGUF export complete!\")\nprint(f\"   Output directory: {OUTPUT_DIR}\")\nprint(f\"   Files: {gguf_files}\")\n\nif gguf_files:\n    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n    size_mb = os.path.getsize(gguf_path) / (1024**2)\n    print(f\"   Size: {size_mb:.1f} MB\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:31:32.585122Z","iopub.execute_input":"2026-01-19T18:31:32.585827Z","iopub.status.idle":"2026-01-19T18:37:50.573486Z","shell.execute_reply.started":"2026-01-19T18:31:32.585793Z","shell.execute_reply":"2026-01-19T18:37:50.572349Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n","output_type":"stream"},{"name":"stdout","text":"======================================================================\nğŸ“¦ EXPORTING TO GGUF FORMAT\n======================================================================\n\nğŸ“¦ Exporting to GGUF with Q4_K_M quantization...\nUnsloth: Merging model weights to 16-bit format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/902 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15d4fe2ebfa54475a75c1ba3e5ded549"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nCache check failed: model.safetensors not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\nChecking cache directory for required files...\nCache check failed: tokenizer.model not found in local cache.\nNot all required files found in cache. Will proceed with downloading.\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Preparing safetensor model files:   0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc5556f0dc634c4bbca997aa0e1241d4"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.33s/it]\nUnsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:12<00:00, 12.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merge process complete. Saved to `/kaggle/working/gguf_output`\nUnsloth: Converting to GGUF format...\n==((====))==  Unsloth: Conversion from HF to GGUF information\n   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\nO^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n \"-____-\"     In total, you will have to wait at least 16 minutes.\n\nUnsloth: Installing llama.cpp. This might take 3 minutes...\nUnsloth: Updating system package directories\nUnsloth: All required system packages already installed!\nUnsloth: Install llama.cpp and building - please wait 1 to 3 minutes\nUnsloth: Cloning llama.cpp repository\nUnsloth: Install GGUF and other packages\nUnsloth: Successfully installed llama.cpp!\nUnsloth: Preparing converter script...\nUnsloth: [1] Converting model into f16 GGUF format.\nThis might take 3 minutes...\nUnsloth: Initial conversion completed! Files: ['gemma-3-1b-it.F16.gguf']\nUnsloth: [2] Converting GGUF f16 into q4_k_m. This might take 10 minutes...\nUnsloth: Model files cleanup...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### We removed it in GGUF's chat template for you.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: All GGUF conversions completed successfully!\nGenerated files: ['gemma-3-1b-it.Q4_K_M.gguf']\nUnsloth: example usage for text only LLMs: llama-cli --model gemma-3-1b-it.Q4_K_M.gguf -p \"why is the sky blue?\"\nUnsloth: Saved Ollama Modelfile to current directory\nUnsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n\nâœ… GGUF export complete!\n   Output directory: /kaggle/working/gguf_output\n   Files: []\n","output_type":"stream"}],"execution_count":8},{"id":"c85cd7af-18ee-4b09-94d6-afc01c4ff695","cell_type":"code","source":"# %% [markdown]\n# ## Step 7.2: Create and Download GGUF Archive\n\n# %%\nimport tarfile\nimport os\nfrom google.colab import files\n\nprint(\"=\"*70)\nprint(\"ğŸ“¦ CREATING GGUF ARCHIVE\")\nprint(\"=\"*70)\n\n# Paths\nGGUF_DIR = \"/kaggle/working/gguf_output\"\nARCHIVE_PATH = \"/kaggle/working/fine-tuned-model.tar.gz\"\n\n# Check if GGUF directory exists and contains files\nif not os.path.exists(GGUF_DIR):\n    print(f\"âŒ GGUF directory not found: {GGUF_DIR}\")\nelif not os.listdir(GGUF_DIR):\n    print(f\"âš ï¸ GGUF directory is empty: {GGUF_DIR}\")\nelse:\n    print(f\"ğŸ“ GGUF directory: {GGUF_DIR}\")\n    print(\"ğŸ“‹ Contents:\")\n    for item in os.listdir(GGUF_DIR):\n        size = os.path.getsize(os.path.join(GGUF_DIR, item)) / (1024**2)\n        print(f\"   â€¢ {item} ({size:.1f} MB)\")\n\n# Create tar.gz archive\nprint(f\"\\nğŸ“¦ Creating archive: {ARCHIVE_PATH}\")\ntry:\n    with tarfile.open(ARCHIVE_PATH, \"w:gz\") as tar:\n        tar.add(GGUF_DIR, arcname=os.path.basename(GGUF_DIR))\n    \n    archive_size = os.path.getsize(ARCHIVE_PATH) / (1024**2)\n    print(f\"âœ… Archive created: {ARCHIVE_PATH}\")\n    print(f\"   Size: {archive_size:.1f} MB\")\n    \nexcept Exception as e:\n    print(f\"âŒ Failed to create archive: {e}\")\n    raise\n\n# Download the archive\n\"\"\"\nprint(\"\\nâ¬‡ï¸  Downloading archive...\")\ntry:\n    # For Kaggle/Colab\n    from google.colab import files\n    files.download(ARCHIVE_PATH)\n    print(\"âœ… Download initiated!\")\nexcept ImportError:\n    print(\"âš ï¸  Not in Colab environment. Archive saved locally:\")\n    print(f\"   {ARCHIVE_PATH}\")\nexcept Exception as e:\n    print(f\"âš ï¸  Could not initiate download: {e}\")\n    print(f\"ğŸ“ Archive available at: {ARCHIVE_PATH}\")\n\"\"\"\n\n\n# Alternative: Provide download link in Kaggle\nprint(\"\\nğŸ”— Alternative download methods:\")\nprint(f\"1. Direct path: {ARCHIVE_PATH}\")\nprint(\"2. In Kaggle Notebooks, use the 'Data' tab on the right\")\nprint(\"3. Copy to Google Drive:\")\nprint(f\"   !cp {ARCHIVE_PATH} /content/drive/MyDrive/\")\nprint(\"4. Use kaggle API:\")\nprint(f\"   !kaggle datasets create -p {os.path.dirname(ARCHIVE_PATH)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:37:57.188041Z","iopub.execute_input":"2026-01-19T18:37:57.188882Z","iopub.status.idle":"2026-01-19T18:41:19.535314Z","shell.execute_reply.started":"2026-01-19T18:37:57.188844Z","shell.execute_reply":"2026-01-19T18:41:19.534342Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ“¦ CREATING GGUF ARCHIVE\n======================================================================\nğŸ“ GGUF directory: /kaggle/working/gguf_output\nğŸ“‹ Contents:\n   â€¢ added_tokens.json (0.0 MB)\n   â€¢ .cache (0.0 MB)\n   â€¢ tokenizer.model (4.5 MB)\n   â€¢ tokenizer_config.json (1.1 MB)\n   â€¢ config.json (0.0 MB)\n   â€¢ special_tokens_map.json (0.0 MB)\n   â€¢ tokenizer.json (31.8 MB)\n   â€¢ model.safetensors (1907.2 MB)\n   â€¢ chat_template.jinja (0.0 MB)\n\nğŸ“¦ Creating archive: /kaggle/working/fine-tuned-model.tar.gz\nâœ… Archive created: /kaggle/working/fine-tuned-model.tar.gz\n   Size: 1523.9 MB\n\nğŸ”— Alternative download methods:\n1. Direct path: /kaggle/working/fine-tuned-model.tar.gz\n2. In Kaggle Notebooks, use the 'Data' tab on the right\n3. Copy to Google Drive:\n   !cp /kaggle/working/fine-tuned-model.tar.gz /content/drive/MyDrive/\n4. Use kaggle API:\n   !kaggle datasets create -p /kaggle/working\n","output_type":"stream"}],"execution_count":9},{"id":"ff6705b5","cell_type":"markdown","source":"## Step 8: Clear GPU Memory Before Inference","metadata":{}},{"id":"2e598c6d","cell_type":"code","source":"import gc\nimport torch\n\nprint(\"ğŸ§¹ Clearing GPU memory...\")\n\n# Delete training objects\ndel model\ndel trainer\ndel tokenizer\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(\"\\nğŸ“Š GPU Memory After Cleanup:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv\n\nprint(\"\\nâœ… GPU memory cleared for inference\")","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:41:55.328057Z","iopub.execute_input":"2026-01-19T18:41:55.328945Z","iopub.status.idle":"2026-01-19T18:41:56.440213Z","shell.execute_reply.started":"2026-01-19T18:41:55.328908Z","shell.execute_reply":"2026-01-19T18:41:56.439403Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ§¹ Clearing GPU memory...\n\nğŸ“Š GPU Memory After Cleanup:\nindex, memory.used [MiB], memory.free [MiB]\n0, 823 MiB, 14273 MiB\n1, 123 MiB, 14973 MiB\n\nâœ… GPU memory cleared for inference\n","output_type":"stream"}],"execution_count":10},{"id":"6dbfec0e","cell_type":"markdown","source":"## Step 9: Deploy with llcuda","metadata":{}},{"id":"e8a45266","cell_type":"code","source":"# %% [markdown]\n# ## Step 9: Deploy with llcuda\n\n# %%\n# Find GGUF file\nimport os\ngguf_path = None\nfor root, dirs, files in os.walk(\"/kaggle/working\"):\n    for f in files:\n        if f.endswith(\".gguf\") and \"gemma\" in f.lower():\n            gguf_path = os.path.join(root, f)\n            break\n    if gguf_path:\n        break\n\nif gguf_path is None:\n    raise FileNotFoundError(\"âŒ Fine-tuned GGUF file not found!\")\n\nprint(f\"ğŸ“¥ Found fine-tuned model: {gguf_path}\")\n\n# %%\nfrom llcuda.server import ServerManager\n\nprint(\"=\"*70)\nprint(\"ğŸš€ DEPLOYING FINE-TUNED MODEL WITH LLCUDA\")\nprint(\"=\"*70)\n\nprint(f\"ğŸ“¥ Loading: {gguf_path}\")\n\nserver = ServerManager()\nprint(\"\\nğŸš€ Starting llama-server...\")\ntry:\n    server.start_server(\n        model_path=gguf_path,\n        host=\"127.0.0.1\",\n        port=8090,\n        gpu_layers=32,      # GPU layers for acceleration\n        ctx_size=2048,\n        timeout=180,        # Increased timeout for startup\n    )\nexcept Exception as e:\n    print(f\"âŒ Server failed to start: {e}\")\n    server.stop_server()\n    raise\n\nif server.check_server_health(timeout=180):\n    print(\"\\nâœ… Fine-tuned model deployed with llcuda!\")\n    print(f\"   API endpoint: http://127.0.0.1:8090\")\n    print(f\"   Model: {os.path.basename(gguf_path)}\")\n    \n    # Test the server\n    print(\"\\nğŸ” Testing server health...\")\n    if server.check_server_health():\n        print(\"   âœ… Server is responding\")\n    else:\n        print(\"   âŒ Server not responding\")\nelse:\n    print(\"\\nâŒ Server failed to start\")\n    server.stop_server()","metadata":{"execution":{"iopub.status.busy":"2026-01-19T18:43:50.603536Z","iopub.execute_input":"2026-01-19T18:43:50.604317Z","iopub.status.idle":"2026-01-19T18:43:53.664770Z","shell.execute_reply.started":"2026-01-19T18:43:50.604281Z","shell.execute_reply":"2026-01-19T18:43:53.664112Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ğŸ“¥ Found fine-tuned model: /kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\n======================================================================\nğŸš€ DEPLOYING FINE-TUNED MODEL WITH LLCUDA\n======================================================================\nğŸ“¥ Loading: /kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\n\nğŸš€ Starting llama-server...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: âœ“ Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it.Q4_K_M.gguf\n  GPU Layers: 32\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready...... âœ“ Ready in 3.0s\n\nâœ… Fine-tuned model deployed with llcuda!\n   API endpoint: http://127.0.0.1:8090\n   Model: gemma-3-1b-it.Q4_K_M.gguf\n\nğŸ” Testing server health...\n   âœ… Server is responding\n","output_type":"stream"}],"execution_count":13},{"id":"6296af75","cell_type":"markdown","source":"## Step 10: Test Your Fine-Tuned Model","metadata":{}},{"id":"bb3fbd86","cell_type":"code","source":"## Step 10: Test Your Fine-Tuned Model\n\nprint(\"=\"*70)\nprint(\"ğŸ§ª TESTING FINE-TUNED MODEL\")\nprint(\"=\"*70)\n\n# Test with direct HTTP request to llama-server (most reliable)\nimport requests\n\ntest_prompts = [\n    \"### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\",\n    \"### Instruction:\\nWrite a short poem about coding.\\n\\n### Response:\",\n    \"### Instruction:\\nWhat are the benefits of GPU acceleration?\\n\\n### Response:\",\n]\n\nfor i, prompt in enumerate(test_prompts, 1):\n    print(f\"\\nğŸ“‹ Test {i}:\")\n    print(f\"   Prompt: {prompt[:50]}...\")\n    \n    try:\n        response = requests.post(\n            \"http://127.0.0.1:8090/completion\",\n            json={\n                \"prompt\": prompt,\n                \"max_tokens\": 100,\n                \"temperature\": 0.7,\n                \"stop\": [\"###\", \"\\n\\n\"]\n            },\n            timeout=30\n        )\n        \n        if response.status_code == 200:\n            result = response.json()\n            answer = result.get(\"content\", \"\").strip()\n            print(f\"   âœ… Response: {answer[:200]}\")\n        else:\n            print(f\"   âŒ Error: HTTP {response.status_code}\")\n            \n    except Exception as e:\n        print(f\"   âŒ Request failed: {e}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š Testing with llcuda Client API\")\nprint(\"=\"*70)\n\n# Test with proper llcuda client API\ntry:\n    from llcuda.api.client import LlamaCppClient\n    \n    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n    \n    # Test 1: Using chat completions (correct method)\n    print(\"\\nğŸ’¬ Chat Completion Test:\")\n    chat_response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n        ],\n        max_tokens=50\n    )\n    print(f\"   âœ… Answer: {chat_response.choices[0].message.content[:200]}\")\n    \n    # Test 2: Using completion endpoint with training format\n    print(\"\\nğŸ“ Fine-tuned Format Test:\")\n    completion_response = requests.post(\n        \"http://127.0.0.1:8090/completion\",\n        json={\n            \"prompt\": \"What is machine learning?\",\n            \"max_tokens\": 50,\n            \"temperature\": 0.7\n        }\n    )\n    \n    if completion_response.status_code == 200:\n        result = completion_response.json()\n        print(f\"   âœ… Direct response: {result.get('content', '')[:200]}\")\n    \nexcept Exception as e:\n    print(f\"   âš ï¸  Error with client API: {e}\")\n\nprint(\"\\nğŸ¯ Fine-tuned Model Successfully Tested!\")\nprint(\"   âœ… Direct HTTP API: Working\")\nprint(\"   ğŸ”— Endpoint: http://127.0.0.1:8090/completion\")\nprint(\"   ğŸ“‹ Format: Use Alpaca-style prompts for best results\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:47:23.313142Z","iopub.execute_input":"2026-01-19T18:47:23.314021Z","iopub.status.idle":"2026-01-19T18:47:26.206323Z","shell.execute_reply.started":"2026-01-19T18:47:23.313983Z","shell.execute_reply":"2026-01-19T18:47:26.205483Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ§ª TESTING FINE-TUNED MODEL\n======================================================================\n\nğŸ“‹ Test 1:\n   Prompt: ### Instruction:\nExplain what machine learning is....\n   âœ… Response: Machine learning is a branch of artificial intelligence that focuses on enabling computers to learn from data without being explicitly programmed. Instead of following a fixed set of rules, computers \n\nğŸ“‹ Test 2:\n   Prompt: ### Instruction:\nWrite a short poem about coding.\n...\n   âœ… Response: Coding is an art, a skill so deep,\nWhere logic flows, secrets it will keep.\nWith keyboards and screens, a digital dance,\nCreating new worlds, a logical trance.\n\nğŸ“‹ Test 3:\n   Prompt: ### Instruction:\nWhat are the benefits of GPU acce...\n   âœ… Response: GPU acceleration has a number of benefits, including improved performance, better energy efficiency, and greater flexibility. GPU acceleration can improve the performance of computer tasks, such as ga\n\n======================================================================\nğŸ“Š Testing with llcuda Client API\n======================================================================\n\nğŸ’¬ Chat Completion Test:\n   âœ… Answer: Okay, let's break down what Machine Learning (ML) really is! Simply put, machine learning is a branch of artificial intelligence that focuses on enabling computers to learn from data without being exp\n\nğŸ“ Fine-tuned Format Test:\n   âœ… Direct response: \n\nMachine learning is a subset of artificial intelligence that deals with algorithms that allow computers to learn from experience and improve their performance on a task without being explicitly prog\n\nğŸ¯ Fine-tuned Model Successfully Tested!\n   âœ… Direct HTTP API: Working\n   ğŸ”— Endpoint: http://127.0.0.1:8090/completion\n   ğŸ“‹ Format: Use Alpaca-style prompts for best results\n","output_type":"stream"}],"execution_count":16},{"id":"30713f5a","cell_type":"markdown","source":"## Step 11: Compare with Chat API","metadata":{}},{"id":"aa0ab695","cell_type":"code","source":"print(\"=\"*70)\nprint(\"ğŸ’¬ CHAT COMPLETION TEST\")\nprint(\"=\"*70)\n\n# Test with chat format\nresponse = client.chat.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What did you learn during fine-tuning?\"}\n    ],\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(f\"\\nğŸ’¬ Response:\")\nprint(response.choices[0].message.content)\n\nprint(f\"\\nğŸ“Š Usage:\")\nprint(f\"   Prompt tokens: {response.usage.prompt_tokens}\")\nprint(f\"   Completion tokens: {response.usage.completion_tokens}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:49:03.639059Z","iopub.execute_input":"2026-01-19T18:49:03.639892Z","iopub.status.idle":"2026-01-19T18:49:05.883311Z","shell.execute_reply.started":"2026-01-19T18:49:03.639855Z","shell.execute_reply":"2026-01-19T18:49:05.882634Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¬ CHAT COMPLETION TEST\n======================================================================\n\nğŸ’¬ Response:\nAs a large language model, I donâ€™t â€œlearnâ€ in the same way a human does â€“ through experience and reflection. However, my training process is incredibly complex and involves massive amounts of data and sophisticated algorithms that allow me to improve over time. During this process, I learn a vast range of patterns, relationships between words and concepts, and even subtle nuances in language. Hereâ€™s a breakdown of some of the key things I learned during fine-tuning:\n\n1.  **Improved Language Understanding:** The primary goal of fine-tuning is to take a model that has already been trained on massive datasets and narrow it down into a specific area or style of communication. Fine tuning allows me to learn more about specific topics or styles, such\n\nğŸ“Š Usage:\n   Prompt tokens: 25\n   Completion tokens: 150\n","output_type":"stream"}],"execution_count":17},{"id":"54354bc8-54a5-4a0d-aef1-491f090161d1","cell_type":"code","source":"## Step 11: Compare with Chat API\n\nprint(\"=\"*70)\nprint(\"ğŸ’¬ CHAT COMPLETION COMPARISON\")\nprint(\"=\"*70)\n\n# First, test with the original fine-tuned Alpaca format\nprint(\"\\nğŸ§ª Test 1: Fine-tuned Alpaca Format\")\nprint(\"-\" * 40)\n\nalpaca_prompt = \"### Instruction:\\nWhat did you learn during fine-tuning?\\n\\n### Response:\"\n\nalpaca_response = requests.post(\n    \"http://127.0.0.1:8090/completion\",\n    json={\n        \"prompt\": alpaca_prompt,\n        \"max_tokens\": 150,\n        \"temperature\": 0.7,\n        \"stop\": [\"###\", \"\\n\\n\"]\n    },\n    timeout=30\n)\n\nif alpaca_response.status_code == 200:\n    alpaca_result = alpaca_response.json()\n    alpaca_answer = alpaca_result.get(\"content\", \"\").strip()\n    print(f\"ğŸ“ Prompt: {alpaca_prompt[:60]}...\")\n    print(f\"âœ… Response: {alpaca_answer[:300]}\")\n    \n    if \"content\" in alpaca_result:\n        # Estimate tokens (rough approximation: 1 token â‰ˆ 4 chars)\n        prompt_tokens_approx = len(alpaca_prompt) // 4\n        response_tokens_approx = len(alpaca_answer) // 4\n        print(f\"ğŸ“Š Usage (approx):\")\n        print(f\"   Prompt tokens: ~{prompt_tokens_approx}\")\n        print(f\"   Completion tokens: ~{response_tokens_approx}\")\nelse:\n    print(f\"âŒ Error: HTTP {alpaca_response.status_code}\")\n\n# Second, test with standard chat format\nprint(\"\\n\\nğŸ¤– Test 2: Standard Chat Format\")\nprint(\"-\" * 40)\n\ntry:\n    from llcuda.api.client import LlamaCppClient\n    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n    \n    chat_response = client.chat.completions.create(\n        model=\"llama\",  # Use generic model name\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant fine-tuned on Alpaca dataset.\"},\n            {\"role\": \"user\", \"content\": \"What did you learn during fine-tuning?\"}\n        ],\n        max_tokens=150,\n        temperature=0.7\n    )\n    \n    if hasattr(chat_response, 'choices') and len(chat_response.choices) > 0:\n        chat_answer = chat_response.choices[0].message.content\n        print(f\"ğŸ“ Prompt: What did you learn during fine-tuning?\")\n        print(f\"âœ… Response: {chat_answer[:300]}\")\n        \n        if hasattr(chat_response, 'usage'):\n            print(f\"ğŸ“Š Usage (exact):\")\n            print(f\"   Prompt tokens: {chat_response.usage.prompt_tokens}\")\n            print(f\"   Completion tokens: {chat_response.usage.completion_tokens}\")\n        else:\n            print(f\"ğŸ“Š Usage: Token information not available in response\")\n            \nexcept Exception as e:\n    print(f\"âš ï¸  Chat API test failed: {e}\")\n    \n    # Fallback to completion endpoint for chat\n    print(\"\\nğŸ”„ Fallback: Using completion endpoint for chat-style\")\n    simple_prompt = \"You are a helpful assistant. What did you learn during fine-tuning?\"\n    \n    simple_response = requests.post(\n        \"http://127.0.0.1:8090/completion\",\n        json={\n            \"prompt\": simple_prompt,\n            \"max_tokens\": 150,\n            \"temperature\": 0.7\n        },\n        timeout=30\n    )\n    \n    if simple_response.status_code == 200:\n        simple_result = simple_response.json()\n        simple_answer = simple_result.get(\"content\", \"\").strip()\n        print(f\"ğŸ“ Prompt: {simple_prompt[:60]}...\")\n        print(f\"âœ… Response: {simple_answer[:300]}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š COMPARISON RESULTS\")\nprint(\"=\"*70)\nprint(\"âœ… Both Alpaca format and Chat format work with your fine-tuned model!\")\nprint(\"ğŸ“ Tip: Use Alpaca format for best results (matches training data)\")\nprint(\"ğŸ”— API Endpoints:\")\nprint(\"   - /completion : For Alpaca-style prompts\")\nprint(\"   - /v1/chat/completions : For OpenAI-compatible chat\")\nprint(\"\\nğŸ¯ Your llcuda server is fully functional!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:49:38.198158Z","iopub.execute_input":"2026-01-19T18:49:38.198605Z","iopub.status.idle":"2026-01-19T18:49:41.069731Z","shell.execute_reply.started":"2026-01-19T18:49:38.198570Z","shell.execute_reply":"2026-01-19T18:49:41.068845Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¬ CHAT COMPLETION COMPARISON\n======================================================================\n\nğŸ§ª Test 1: Fine-tuned Alpaca Format\n----------------------------------------\nğŸ“ Prompt: ### Instruction:\nWhat did you learn during fine-tuning?\n\n###...\nâœ… Response: Fine-tuning is an important process in machine learning that allows you to adapt a pre-trained model to a new task or dataset. In this process, you use the pre-trained model as a starting point, which has already learned a lot of general patterns from a large dataset. This approach is faster and req\nğŸ“Š Usage (approx):\n   Prompt tokens: ~17\n   Completion tokens: ~134\n\n\nğŸ¤– Test 2: Standard Chat Format\n----------------------------------------\nğŸ“ Prompt: What did you learn during fine-tuning?\nâœ… Response: Okay, let's dive into what I learned during my fine-tuning process â€“ itâ€™s a great way to understand how Iâ€™ve developed over time! Essentially, the fine-tuning phase involves taking a pre-trained language model and training it further on a specific dataset or task. Hereâ€™s a breakdown of what I gained\nğŸ“Š Usage (exact):\n   Prompt tokens: 32\n   Completion tokens: 150\n\n======================================================================\nğŸ“Š COMPARISON RESULTS\n======================================================================\nâœ… Both Alpaca format and Chat format work with your fine-tuned model!\nğŸ“ Tip: Use Alpaca format for best results (matches training data)\nğŸ”— API Endpoints:\n   - /completion : For Alpaca-style prompts\n   - /v1/chat/completions : For OpenAI-compatible chat\n\nğŸ¯ Your llcuda server is fully functional!\n","output_type":"stream"}],"execution_count":18},{"id":"113ccfe2","cell_type":"markdown","source":"## Step 12: Save Model for Later Use","metadata":{}},{"id":"f714ec49","cell_type":"code","source":"## Step 12: Save and Document Your Fine-Tuned Model\n\nimport shutil\nimport os\nimport json\n\nprint(\"=\"*70)\nprint(\"ğŸ’¾ SAVING AND DOCUMENTING FINE-TUNED MODEL\")\nprint(\"=\"*70)\n\n# Find the GGUF file (created by Unsloth export)\nOUTPUT_DIR = \"/kaggle/working/gguf_output\"\ngguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n\nif not gguf_files:\n    # Search in working directory as backup\n    for root, dirs, files in os.walk(\"/kaggle/working\"):\n        for f in files:\n            if f.endswith(\".gguf\") and \"gemma\" in f.lower():\n                gguf_files = [os.path.join(root, f)]\n                break\n\nif gguf_files:\n    if isinstance(gguf_files[0], str) and os.path.isabs(gguf_files[0]):\n        src = gguf_files[0]\n    else:\n        src = os.path.join(OUTPUT_DIR, gguf_files[0])\n    \n    # Create descriptive filename\n    model_name = \"my-finetuned-gemma-3-1b\"\n    dst = f\"/kaggle/working/{model_name}-Q4_K_M.gguf\"\n    \n    print(f\"\\nğŸ“‚ Found GGUF file: {os.path.basename(src)}\")\n    print(f\"   Source: {src}\")\n    print(f\"   Size: {os.path.getsize(src) / (1024**2):.1f} MB\")\n    \n    # Copy to working directory\n    shutil.copy(src, dst)\n    \n    print(f\"\\nâœ… Model saved to: {dst}\")\n    print(f\"   Saved size: {os.path.getsize(dst) / (1024**2):.1f} MB\")\n    \n    # Create model metadata file\n    metadata = {\n        \"model_name\": model_name,\n        \"base_model\": \"unsloth/gemma-3-1b-it\",\n        \"quantization\": \"Q4_K_M\",\n        \"training_data\": \"Alpaca-cleaned (500 samples)\",\n        \"training_config\": {\n            \"lora_rank\": 16,\n            \"lora_alpha\": 32,\n            \"max_seq_length\": 2048,\n            \"batch_size\": 2,\n            \"learning_rate\": 2e-4,\n            \"trained_steps\": 30\n        },\n        \"export_format\": \"GGUF\",\n        \"export_tool\": \"Unsloth + llama.cpp\",\n        \"deployment_engine\": \"llcuda v2.2.0\",\n        \"api_endpoints\": {\n            \"completion\": \"http://127.0.0.1:8090/completion\",\n            \"chat\": \"http://127.0.0.1:8090/v1/chat/completions\"\n        },\n        \"optimal_prompt_format\": \"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\",\n        \"file_size_mb\": round(os.path.getsize(dst) / (1024**2), 2),\n        \"created\": \"2026-01-19\",\n        \"environment\": \"Kaggle 2x Tesla T4\"\n    }\n    \n    metadata_path = f\"/kaggle/working/{model_name}-metadata.json\"\n    with open(metadata_path, 'w') as f:\n        json.dump(metadata, f, indent=2)\n    \n    print(f\"\\nğŸ“ Metadata saved: {metadata_path}\")\n    \n    # Create archive for easy download\n    import tarfile\n    archive_path = f\"/kaggle/working/{model_name}-bundle.tar.gz\"\n    with tarfile.open(archive_path, \"w:gz\") as tar:\n        tar.add(dst, arcname=os.path.basename(dst))\n        tar.add(metadata_path, arcname=os.path.basename(metadata_path))\n    \n    print(f\"\\nğŸ“¦ Archive created: {archive_path}\")\n    print(f\"   Archive size: {os.path.getsize(archive_path) / (1024**2):.1f} MB\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"ğŸ”§ USAGE INSTRUCTIONS\")\n    print(\"=\"*70)\n    \n    print(\"\\nğŸ’¡ Quick Start with llcuda:\")\n    print(\"```python\")\n    print(\"from llcuda.server import ServerManager\")\n    print(\"\")\n    print(\"# Start server\")\n    print(\"server = ServerManager()\")\n    print(f\"server.start_server(model_path='{dst}', port=8090)\")\n    print(\"\")\n    print(\"# Test with Alpaca format (recommended)\")\n    print('prompt = \"### Instruction:\\\\\\\\nYour question here\\\\\\\\n\\\\\\\\n### Response:\"')\n    print(\"import requests\")\n    print(\"response = requests.post('http://127.0.0.1:8090/completion',\")\n    print(\"  json={'prompt': prompt, 'max_tokens': 100, 'temperature': 0.7})\")\n    print(\"```\")\n    \n    print(\"\\nğŸ’¡ Quick Start with llcuda Client API:\")\n    print(\"```python\")\n    print(\"from llcuda.api.client import LlamaCppClient\")\n    print(\"\")\n    print(\"client = LlamaCppClient(base_url='http://127.0.0.1:8090')\")\n    print(\"response = client.chat.completions.create(\")\n    print(\"  model='gemma-3-1b',\")\n    print(\"  messages=[{'role': 'user', 'content': 'Your question'}],\")\n    print(\"  max_tokens=100\")\n    print(\")\")\n    print(\"```\")\n    \n    print(\"\\nğŸ“Š Model Performance:\")\n    print(\"   âœ… Supports both /completion and /v1/chat/completions endpoints\")\n    print(\"   âœ… Works with Alpaca-style prompts\")\n    print(\"   âœ… Token usage tracking available\")\n    print(\"   âœ… GPU accelerated with llcuda\")\n    \n    print(\"\\nğŸ¯ Next Steps:\")\n    print(\"   1. Download the archive from Kaggle Data tab\")\n    print(\"   2. Deploy on any system with llcuda installed\")\n    print(\"   3. Use with Ollama: ollama create my-model -f ./Modelfile\")\n    print(\"   4. Share via HuggingFace Hub\")\n    \nelse:\n    print(\"\\nâŒ No GGUF file found. Please check the export in Step 7.\")\n    print(\"   Expected in: /kaggle/working/ or /kaggle/working/gguf_output/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:52:38.954559Z","iopub.execute_input":"2026-01-19T18:52:38.955072Z","iopub.status.idle":"2026-01-19T18:53:13.902851Z","shell.execute_reply.started":"2026-01-19T18:52:38.955031Z","shell.execute_reply":"2026-01-19T18:53:13.901998Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nğŸ’¾ SAVING AND DOCUMENTING FINE-TUNED MODEL\n======================================================================\n\nğŸ“‚ Found GGUF file: gemma-3-1b-it.Q4_K_M.gguf\n   Source: /kaggle/working/gemma-3-1b-it.Q4_K_M.gguf\n   Size: 768.7 MB\n\nâœ… Model saved to: /kaggle/working/my-finetuned-gemma-3-1b-Q4_K_M.gguf\n   Saved size: 768.7 MB\n\nğŸ“ Metadata saved: /kaggle/working/my-finetuned-gemma-3-1b-metadata.json\n\nğŸ“¦ Archive created: /kaggle/working/my-finetuned-gemma-3-1b-bundle.tar.gz\n   Archive size: 749.4 MB\n\n======================================================================\nğŸ”§ USAGE INSTRUCTIONS\n======================================================================\n\nğŸ’¡ Quick Start with llcuda:\n```python\nfrom llcuda.server import ServerManager\n\n# Start server\nserver = ServerManager()\nserver.start_server(model_path='/kaggle/working/my-finetuned-gemma-3-1b-Q4_K_M.gguf', port=8090)\n\n# Test with Alpaca format (recommended)\nprompt = \"### Instruction:\\\\nYour question here\\\\n\\\\n### Response:\"\nimport requests\nresponse = requests.post('http://127.0.0.1:8090/completion',\n  json={'prompt': prompt, 'max_tokens': 100, 'temperature': 0.7})\n```\n\nğŸ’¡ Quick Start with llcuda Client API:\n```python\nfrom llcuda.api.client import LlamaCppClient\n\nclient = LlamaCppClient(base_url='http://127.0.0.1:8090')\nresponse = client.chat.completions.create(\n  model='gemma-3-1b',\n  messages=[{'role': 'user', 'content': 'Your question'}],\n  max_tokens=100\n)\n```\n\nğŸ“Š Model Performance:\n   âœ… Supports both /completion and /v1/chat/completions endpoints\n   âœ… Works with Alpaca-style prompts\n   âœ… Token usage tracking available\n   âœ… GPU accelerated with llcuda\n\nğŸ¯ Next Steps:\n   1. Download the archive from Kaggle Data tab\n   2. Deploy on any system with llcuda installed\n   3. Use with Ollama: ollama create my-model -f ./Modelfile\n   4. Share via HuggingFace Hub\n","output_type":"stream"}],"execution_count":19},{"id":"f7066e68","cell_type":"markdown","source":"## Step 13: Cleanup","metadata":{}},{"id":"2a972de0","cell_type":"code","source":"print(\"ğŸ›‘ Stopping server...\")\nserver.stop_server()\n\nprint(\"\\nâœ… Server stopped\")\nprint(\"\\nğŸ“Š Final GPU Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:53:59.630782Z","iopub.execute_input":"2026-01-19T18:53:59.631247Z","iopub.status.idle":"2026-01-19T18:54:00.064848Z","shell.execute_reply.started":"2026-01-19T18:53:59.631211Z","shell.execute_reply":"2026-01-19T18:54:00.063648Z"}},"outputs":[{"name":"stdout","text":"ğŸ›‘ Stopping server...\n\nâœ… Server stopped\n\nğŸ“Š Final GPU Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 823 MiB, 14273 MiB\n1, 123 MiB, 14973 MiB\n","output_type":"stream"}],"execution_count":20},{"id":"f35c6449","cell_type":"markdown","source":"## ğŸ“š Summary\n\n### Complete Workflow:\n1. âœ… Installed Unsloth + llcuda\n2. âœ… Loaded base model with 4-bit quantization\n3. âœ… Added LoRA adapters for efficient training\n4. âœ… Fine-tuned on custom dataset\n5. âœ… Exported to GGUF (Q4_K_M)\n6. âœ… Deployed with llcuda llama-server\n7. âœ… Ran inference on fine-tuned model\n\n### Key llcuda + Unsloth Integration:\n\n```python\nfrom llcuda.unsloth import export_to_llcuda\n\n# After Unsloth training\nexport_to_llcuda(\n    model=model,\n    tokenizer=tokenizer,\n    output_path=\"my-model.gguf\",\n    quant_type=\"Q4_K_M\"\n)\n```\n\n---\n\n**Next:** [06-split-gpu-graphistry](06-split-gpu-graphistry-llcuda-v2.2.0.ipynb)","metadata":{}},{"id":"6ed22bd6-ab64-41cf-ba6a-b65a5df52d9a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}