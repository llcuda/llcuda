{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a8cf5402","cell_type":"markdown","source":"## Step 1: Verify Dual GPU Environment","metadata":{}},{"id":"aa5253aa","cell_type":"code","source":"import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"üîç DUAL GPU ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# Get GPU info\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\ngpus = result.stdout.strip().split('\\n')\nprint(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\nfor gpu in gpus:\n    print(f\"   {gpu}\")\n\nif len(gpus) >= 2:\n    print(\"\\n‚úÖ Dual T4 environment confirmed!\")\n    print(\"   Total VRAM: 30GB (15GB √ó 2)\")\nelse:\n    print(\"\\n‚ö†Ô∏è Only 1 GPU detected!\")\n    print(\"   Enable 'GPU T4 x2' in Kaggle settings.\")\n\n# CUDA version\nprint(\"\\nüìä CUDA Version:\")\n!nvcc --version | grep release","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:25:04.032131Z","iopub.execute_input":"2026-01-19T03:25:04.032651Z","iopub.status.idle":"2026-01-19T03:25:04.240138Z","shell.execute_reply.started":"2026-01-19T03:25:04.032621Z","shell.execute_reply":"2026-01-19T03:25:04.239397Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîç DUAL GPU ENVIRONMENT CHECK\n======================================================================\n\nüìä Detected 2 GPU(s):\n   0, Tesla T4, 15360 MiB, 7.5\n   1, Tesla T4, 15360 MiB, 7.5\n\n‚úÖ Dual T4 environment confirmed!\n   Total VRAM: 30GB (15GB √ó 2)\n\nüìä CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n","output_type":"stream"}],"execution_count":1},{"id":"cfa5ca7c","cell_type":"markdown","source":"## Step 2: Install llcuda v2.2.0","metadata":{}},{"id":"2fb3eff5","cell_type":"code","source":"%%time\n# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n!pip install -q huggingface_hub sseclient-py\n\nimport llcuda\nprint(f\"‚úÖ llcuda {llcuda.__version__} installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:25:07.916826Z","iopub.execute_input":"2026-01-19T03:25:07.917169Z","iopub.status.idle":"2026-01-19T03:26:29.772802Z","shell.execute_reply.started":"2026-01-19T03:25:07.917124Z","shell.execute_reply":"2026-01-19T03:26:29.771971Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m534.5/534.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m299.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m240.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m370.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m321.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m283.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m280.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m327.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m346.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m330.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m314.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m398.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m295.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m343.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m298.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m367.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m289.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llcuda v2.2.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llcuda v2.1!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28b55cd5385249ad966b5207a5cba84f"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n‚úÖ Binaries installed successfully!\n\n‚úÖ llcuda 2.2.0 installed\nCPU times: user 37.7 s, sys: 8.47 s, total: 46.1 s\nWall time: 1min 21s\n","output_type":"stream"}],"execution_count":2},{"id":"0a2febcb","cell_type":"markdown","source":"## Step 3: Understanding Multi-GPU Options\n\nllama.cpp provides several flags for multi-GPU configuration:","metadata":{}},{"id":"c722131f","cell_type":"code","source":"from llcuda.api.multigpu import MultiGPUConfig, SplitMode, GPUInfo\n\nprint(\"=\"*70)\nprint(\"üìã MULTI-GPU CONFIGURATION OPTIONS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nüîπ --tensor-split, -ts\n   Distributes VRAM usage across GPUs.\n   Example: --tensor-split 0.5,0.5 (50% each GPU)\n   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n\nüîπ --split-mode, -sm  \n   How to split the model across GPUs:\n   ‚Ä¢ 'layer' - Split by transformer layers (default, recommended)\n   ‚Ä¢ 'row'   - Split by matrix rows (can be slower)\n   ‚Ä¢ 'none'  - Disable multi-GPU (single GPU only)\n\nüîπ --main-gpu, -mg\n   Primary GPU for small tensors and scratch buffers.\n   Default: 0 (first GPU)\n\nüîπ --n-gpu-layers, -ngl\n   Number of layers to offload to GPU(s).\n   Use 99 to offload all layers.\n\"\"\")\n\n# Show split mode enum\nprint(\"\\nüìã Split Modes:\")\nfor mode in SplitMode:\n    print(f\"   {mode.name}: {mode.value}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:26:39.180139Z","iopub.execute_input":"2026-01-19T03:26:39.180481Z","iopub.status.idle":"2026-01-19T03:26:39.203910Z","shell.execute_reply.started":"2026-01-19T03:26:39.180447Z","shell.execute_reply":"2026-01-19T03:26:39.203371Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìã MULTI-GPU CONFIGURATION OPTIONS\n======================================================================\n\nüîπ --tensor-split, -ts\n   Distributes VRAM usage across GPUs.\n   Example: --tensor-split 0.5,0.5 (50% each GPU)\n   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n\nüîπ --split-mode, -sm  \n   How to split the model across GPUs:\n   ‚Ä¢ 'layer' - Split by transformer layers (default, recommended)\n   ‚Ä¢ 'row'   - Split by matrix rows (can be slower)\n   ‚Ä¢ 'none'  - Disable multi-GPU (single GPU only)\n\nüîπ --main-gpu, -mg\n   Primary GPU for small tensors and scratch buffers.\n   Default: 0 (first GPU)\n\nüîπ --n-gpu-layers, -ngl\n   Number of layers to offload to GPU(s).\n   Use 99 to offload all layers.\n\n\nüìã Split Modes:\n   NONE: none\n   LAYER: layer\n   ROW: row\n","output_type":"stream"}],"execution_count":3},{"id":"bdfed953","cell_type":"markdown","source":"## Step 4: GPU Info Utility","metadata":{}},{"id":"058365ea","cell_type":"code","source":"from llcuda.api.multigpu import detect_gpus, get_free_vram\n\nprint(\"=\"*70)\nprint(\"üìä GPU INFORMATION\")\nprint(\"=\"*70)\n\n# Get detailed GPU info using detect_gpus (actual API function)\ngpus = detect_gpus()\n\nfor gpu in gpus:\n    print(f\"\\nüîπ GPU {gpu.id}: {gpu.name}\")\n    print(f\"   Total VRAM: {gpu.memory_total_gb:.1f} GB\")\n    print(f\"   Free VRAM: {gpu.memory_free_gb:.1f} GB\")\n    if gpu.compute_capability:\n        print(f\"   Compute Capability: {gpu.compute_capability}\")\n\n# Calculate total available VRAM\ntotal_vram = sum(gpu.memory_free_gb for gpu in gpus)\nprint(f\"\\nüìä Total Available VRAM: {total_vram:.1f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:26:42.415255Z","iopub.execute_input":"2026-01-19T03:26:42.416200Z","iopub.status.idle":"2026-01-19T03:26:42.463468Z","shell.execute_reply.started":"2026-01-19T03:26:42.416162Z","shell.execute_reply":"2026-01-19T03:26:42.462553Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä GPU INFORMATION\n======================================================================\n\nüîπ GPU 0: Tesla T4\n   Total VRAM: 15.0 GB\n   Free VRAM: 14.7 GB\n   Compute Capability: 7.5\n\nüîπ GPU 1: Tesla T4\n   Total VRAM: 15.0 GB\n   Free VRAM: 14.7 GB\n   Compute Capability: 7.5\n\nüìä Total Available VRAM: 29.5 GB\n","output_type":"stream"}],"execution_count":4},{"id":"9140cd97","cell_type":"markdown","source":"## Step 5: Download a Larger Model for Multi-GPU Testing\n\nWe'll use Gemma-3-4B which benefits from dual-GPU distribution.","metadata":{}},{"id":"3a4b9bc3","cell_type":"code","source":"%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# For multi-GPU testing, use a 4B model\nMODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\nMODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n\nprint(f\"üì• Downloading {MODEL_FILE}...\")\nprint(f\"   This ~2.5GB model will be split across both GPUs.\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\n‚úÖ Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:26:45.703756Z","iopub.execute_input":"2026-01-19T03:26:45.704404Z","iopub.status.idle":"2026-01-19T03:26:51.206869Z","shell.execute_reply.started":"2026-01-19T03:26:45.704374Z","shell.execute_reply":"2026-01-19T03:26:51.206322Z"}},"outputs":[{"name":"stdout","text":"üì• Downloading gemma-3-4b-it-Q4_K_M.gguf...\n   This ~2.5GB model will be split across both GPUs.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb7cc5a2c6b443ca52f3052b9e4dadb"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Model downloaded: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n   Size: 2.32 GB\nCPU times: user 5.21 s, sys: 7.88 s, total: 13.1 s\nWall time: 4.88 s\n","output_type":"stream"}],"execution_count":5},{"id":"1d35c0e9","cell_type":"markdown","source":"## Step 6: Tensor-Split Configurations\n\nDifferent ways to distribute the model across GPUs.","metadata":{}},{"id":"f1a16bd0","cell_type":"code","source":"from llcuda.api.multigpu import MultiGPUConfig, SplitMode\n\nprint(\"=\"*70)\nprint(\"üìã TENSOR-SPLIT CONFIGURATIONS\")\nprint(\"=\"*70)\n\nconfigs = [\n    {\n        \"name\": \"Equal Split (50/50)\",\n        \"tensor_split\": [0.5, 0.5],\n        \"description\": \"Equal distribution across both GPUs\",\n        \"use_case\": \"Default, balanced workload\"\n    },\n    {\n        \"name\": \"GPU 0 Heavy (70/30)\",\n        \"tensor_split\": [0.7, 0.3],\n        \"description\": \"More VRAM on GPU 0\",\n        \"use_case\": \"When GPU 1 needed for other tasks\"\n    },\n    {\n        \"name\": \"GPU 0 Only (100/0)\",\n        \"tensor_split\": [1.0, 0.0],\n        \"description\": \"Single GPU mode\",\n        \"use_case\": \"When GPU 1 reserved for RAPIDS/Graphistry\"\n    },\n]\n\nfor i, config in enumerate(configs, 1):\n    print(f\"\\nüîπ Config {i}: {config['name']}\")\n    print(f\"   Tensor Split: {config['tensor_split']}\")\n    print(f\"   Description: {config['description']}\")\n    print(f\"   Use Case: {config['use_case']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:26:55.735056Z","iopub.execute_input":"2026-01-19T03:26:55.735353Z","iopub.status.idle":"2026-01-19T03:26:55.741723Z","shell.execute_reply.started":"2026-01-19T03:26:55.735330Z","shell.execute_reply":"2026-01-19T03:26:55.741110Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìã TENSOR-SPLIT CONFIGURATIONS\n======================================================================\n\nüîπ Config 1: Equal Split (50/50)\n   Tensor Split: [0.5, 0.5]\n   Description: Equal distribution across both GPUs\n   Use Case: Default, balanced workload\n\nüîπ Config 2: GPU 0 Heavy (70/30)\n   Tensor Split: [0.7, 0.3]\n   Description: More VRAM on GPU 0\n   Use Case: When GPU 1 needed for other tasks\n\nüîπ Config 3: GPU 0 Only (100/0)\n   Tensor Split: [1.0, 0.0]\n   Description: Single GPU mode\n   Use Case: When GPU 1 reserved for RAPIDS/Graphistry\n","output_type":"stream"}],"execution_count":6},{"id":"d58e3e2c","cell_type":"markdown","source":"## Step 7: Start Server with Dual-GPU Configuration","metadata":{}},{"id":"433d461a","cell_type":"code","source":"from llcuda.server import ServerManager\nfrom llcuda.api.multigpu import SplitMode\n\nprint(\"=\"*70)\nprint(\"üöÄ STARTING DUAL-GPU SERVER\")\nprint(\"=\"*70)\n\n# Dual-GPU configuration parameters\ndual_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \n    # Multi-GPU settings\n    \"gpu_layers\": 99,              # Offload all layers\n    \"tensor_split\": \"0.5,0.5\",     # Equal split (as comma-separated string)\n    \n    # Performance\n    \"ctx_size\": 8192,\n    \"batch_size\": 1024,\n    \n    # Parallelism\n    \"n_parallel\": 4,\n}\n\nprint(f\"\\nüìã Dual-GPU Configuration:\")\nprint(f\"   Model: {model_path.split('/')[-1]}\")\nprint(f\"   Tensor Split: GPU0=50%, GPU1=50%\")\nprint(f\"   Context Size: {dual_config['ctx_size']}\")\n\n# Start server\nserver = ServerManager(server_url=f\"http://{dual_config['host']}:{dual_config['port']}\")\nprint(\"\\nüöÄ Starting server...\")\n\ntry:\n    server.start_server(\n        model_path=dual_config['model_path'],\n        host=dual_config['host'],\n        port=dual_config['port'],\n        gpu_layers=dual_config['gpu_layers'],\n        ctx_size=dual_config['ctx_size'],\n        batch_size=dual_config['batch_size'],\n        n_parallel=dual_config['n_parallel'],\n        timeout=120,\n        verbose=True,\n        # Multi-GPU tensor split\n        tensor_split=dual_config['tensor_split']\n    )\n    print(\"\\n‚úÖ Dual-GPU server started successfully!\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:26:59.443941Z","iopub.execute_input":"2026-01-19T03:26:59.444569Z","iopub.status.idle":"2026-01-19T03:27:04.501206Z","shell.execute_reply.started":"2026-01-19T03:26:59.444537Z","shell.execute_reply":"2026-01-19T03:27:04.500408Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüöÄ STARTING DUAL-GPU SERVER\n======================================================================\n\nüìã Dual-GPU Configuration:\n   Model: gemma-3-4b-it-Q4_K_M.gguf\n   Tensor Split: GPU0=50%, GPU1=50%\n   Context Size: 8192\n\nüöÄ Starting server...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready........ ‚úì Ready in 5.0s\n\n‚úÖ Dual-GPU server started successfully!\n","output_type":"stream"}],"execution_count":7},{"id":"18c50fb0","cell_type":"markdown","source":"## Step 8: Verify Multi-GPU Distribution","metadata":{}},{"id":"299a7e2a","cell_type":"code","source":"import subprocess\n\nprint(\"=\"*70)\nprint(\"üìä GPU MEMORY DISTRIBUTION\")\nprint(\"=\"*70)\n\n# Check memory usage on both GPUs\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,name,memory.used,memory.total,utilization.gpu\", \n     \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nprint(\"\\nüìä GPU Memory After Model Load:\")\nfor line in result.stdout.strip().split('\\n'):\n    parts = line.split(', ')\n    if len(parts) >= 4:\n        idx, name, used, total = parts[0], parts[1], parts[2], parts[3]\n        print(f\"   GPU {idx}: {used} / {total}\")\n\nprint(\"\\nüí° Both GPUs should show VRAM usage if tensor-split is working.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:27:09.250776Z","iopub.execute_input":"2026-01-19T03:27:09.251422Z","iopub.status.idle":"2026-01-19T03:27:09.287765Z","shell.execute_reply.started":"2026-01-19T03:27:09.251394Z","shell.execute_reply":"2026-01-19T03:27:09.286983Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä GPU MEMORY DISTRIBUTION\n======================================================================\n\nüìä GPU Memory After Model Load:\n   GPU 0: 1513 MiB / 15360 MiB\n   GPU 1: 1997 MiB / 15360 MiB\n\nüí° Both GPUs should show VRAM usage if tensor-split is working.\n","output_type":"stream"}],"execution_count":8},{"id":"fceb983b","cell_type":"markdown","source":"## Step 9: Benchmark Multi-GPU Performance","metadata":{}},{"id":"a3690e5e","cell_type":"code","source":"import time\nfrom llcuda.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"üìä MULTI-GPU PERFORMANCE BENCHMARK\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Longer prompts to test multi-GPU throughput\nprompts = [\n    \"Write a detailed explanation of how GPU parallelism works in deep learning.\",\n    \"Explain the architecture of a transformer model step by step.\",\n    \"Describe the CUDA programming model and its key concepts.\",\n    \"What are the advantages of using multiple GPUs for inference?\",\n    \"Explain tensor parallelism vs pipeline parallelism.\",\n]\n\nprint(f\"\\nüèÉ Running benchmark with {len(prompts)} prompts...\\n\")\n\ntotal_input_tokens = 0\ntotal_output_tokens = 0\ntotal_time = 0\n\nfor i, prompt in enumerate(prompts, 1):\n    start = time.time()\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=150,\n        temperature=0.7\n    )\n    \n    elapsed = time.time() - start\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n    \n    total_input_tokens += input_tokens\n    total_output_tokens += output_tokens\n    total_time += elapsed\n    \n    tok_per_sec = output_tokens / elapsed\n    print(f\"   Prompt {i}: {output_tokens} tokens in {elapsed:.2f}s ({tok_per_sec:.1f} tok/s)\")\n\nprint(f\"\\nüìä Benchmark Results:\")\nprint(f\"   Total Input Tokens: {total_input_tokens}\")\nprint(f\"   Total Output Tokens: {total_output_tokens}\")\nprint(f\"   Total Time: {total_time:.2f}s\")\nprint(f\"   Average Generation Speed: {total_output_tokens/total_time:.1f} tokens/second\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:27:16.928012Z","iopub.execute_input":"2026-01-19T03:27:16.928344Z","iopub.status.idle":"2026-01-19T03:27:33.779344Z","shell.execute_reply.started":"2026-01-19T03:27:16.928317Z","shell.execute_reply":"2026-01-19T03:27:33.778571Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä MULTI-GPU PERFORMANCE BENCHMARK\n======================================================================\n\nüèÉ Running benchmark with 5 prompts...\n\n   Prompt 1: 150 tokens in 3.65s (41.1 tok/s)\n   Prompt 2: 150 tokens in 3.41s (44.0 tok/s)\n   Prompt 3: 150 tokens in 3.38s (44.4 tok/s)\n   Prompt 4: 150 tokens in 3.30s (45.4 tok/s)\n   Prompt 5: 150 tokens in 3.11s (48.3 tok/s)\n\nüìä Benchmark Results:\n   Total Input Tokens: 97\n   Total Output Tokens: 750\n   Total Time: 16.84s\n   Average Generation Speed: 44.5 tokens/second\n","output_type":"stream"}],"execution_count":9},{"id":"06b558e2","cell_type":"markdown","source":"## Step 10: Test Different Split Configurations","metadata":{}},{"id":"2e7db403","cell_type":"code","source":"# Stop current server\nprint(\"üõë Stopping server for reconfiguration...\")\nserver.stop_server()\n\nimport time\ntime.sleep(2)\nprint(\"‚úÖ Server stopped\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:27:36.890696Z","iopub.execute_input":"2026-01-19T03:27:36.890995Z","iopub.status.idle":"2026-01-19T03:27:39.309994Z","shell.execute_reply.started":"2026-01-19T03:27:36.890969Z","shell.execute_reply":"2026-01-19T03:27:39.309256Z"}},"outputs":[{"name":"stdout","text":"üõë Stopping server for reconfiguration...\n‚úÖ Server stopped\n","output_type":"stream"}],"execution_count":10},{"id":"8741132f","cell_type":"code","source":"print(\"=\"*70)\nprint(\"üîß TESTING 70/30 SPLIT CONFIGURATION\")\nprint(\"=\"*70)\n\n# 70/30 split - more on GPU 0\nconfig_70_30 = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,\n    \"tensor_split\": \"0.7,0.3\",  # 70% GPU0, 30% GPU1\n    \"ctx_size\": 8192,\n}\n\nprint(f\"\\nüìã Configuration:\")\nprint(f\"   Tensor Split: GPU0=70%, GPU1=30%\")\nprint(f\"   Use Case: When GPU1 needs memory for other tasks\")\n\ntry:\n    server.start_server(\n        model_path=config_70_30['model_path'],\n        host=config_70_30['host'],\n        port=config_70_30['port'],\n        gpu_layers=config_70_30['gpu_layers'],\n        ctx_size=config_70_30['ctx_size'],\n        timeout=60,\n        verbose=True,\n        tensor_split=config_70_30['tensor_split']\n    )\n    print(\"\\n‚úÖ 70/30 split server started!\")\n    \n    # Check memory distribution\n    print(\"\\nüìä Memory Distribution:\")\n    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\nexcept Exception as e:\n    print(f\"\\n‚ùå Failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:27:40.907285Z","iopub.execute_input":"2026-01-19T03:27:40.907841Z","iopub.status.idle":"2026-01-19T03:27:45.142294Z","shell.execute_reply.started":"2026-01-19T03:27:40.907815Z","shell.execute_reply":"2026-01-19T03:27:45.141288Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîß TESTING 70/30 SPLIT CONFIGURATION\n======================================================================\n\nüìã Configuration:\n   Tensor Split: GPU0=70%, GPU1=30%\n   Use Case: When GPU1 needs memory for other tasks\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready....... ‚úì Ready in 4.0s\n\n‚úÖ 70/30 split server started!\n\nüìä Memory Distribution:\nindex, memory.used [MiB], memory.total [MiB]\n0, 1741 MiB, 15360 MiB\n1, 1355 MiB, 15360 MiB\n","output_type":"stream"}],"execution_count":11},{"id":"d5da3269","cell_type":"markdown","source":"## Step 11: Split-GPU Mode for LLM + RAPIDS\n\nConfigure for running LLM on GPU 0 while reserving GPU 1 for RAPIDS/Graphistry.","metadata":{}},{"id":"4ff9bd02","cell_type":"code","source":"# Stop current server\nserver.stop_server()\nimport time\ntime.sleep(2)\n\nprint(\"=\"*70)\nprint(\"üéØ SPLIT-GPU MODE: LLM + RAPIDS\")\nprint(\"=\"*70)\n\nprint(\"\"\"\nThis configuration runs the LLM entirely on GPU 0,\nleaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       GPU 0 (15GB)      ‚îÇ        GPU 1 (15GB)           ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ  llama-server   ‚îÇ    ‚îÇ   ‚îÇ  RAPIDS / Graphistry    ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ  (Full Model)   ‚îÇ    ‚îÇ   ‚îÇ  (Graph Visualization)  ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\"\"\")\n\n# Single GPU configuration (GPU 0 only)\nsplit_gpu_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,\n    \"tensor_split\": \"1.0,0.0\",  # 100% on GPU 0\n    \"ctx_size\": 4096,  # Smaller context to fit in single GPU\n}\n\ntry:\n    server.start_server(\n        model_path=split_gpu_config['model_path'],\n        host=split_gpu_config['host'],\n        port=split_gpu_config['port'],\n        gpu_layers=split_gpu_config['gpu_layers'],\n        ctx_size=split_gpu_config['ctx_size'],\n        timeout=60,\n        verbose=True,\n        tensor_split=split_gpu_config['tensor_split']\n    )\n    print(\"\\n‚úÖ Split-GPU mode server started!\")\n    print(\"   GPU 0: llama-server\")\n    print(\"   GPU 1: Available for RAPIDS/Graphistry\")\n    \n    print(\"\\nüìä Memory Distribution:\")\n    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\nexcept Exception as e:\n    print(f\"\\n‚ùå Failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:27:45.433696Z","iopub.execute_input":"2026-01-19T03:27:45.434008Z","iopub.status.idle":"2026-01-19T03:27:50.881578Z","shell.execute_reply.started":"2026-01-19T03:27:45.433977Z","shell.execute_reply":"2026-01-19T03:27:50.880827Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüéØ SPLIT-GPU MODE: LLM + RAPIDS\n======================================================================\n\nThis configuration runs the LLM entirely on GPU 0,\nleaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ       GPU 0 (15GB)      ‚îÇ        GPU 1 (15GB)           ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n‚îÇ  ‚îÇ  llama-server   ‚îÇ    ‚îÇ   ‚îÇ  RAPIDS / Graphistry    ‚îÇ ‚îÇ\n‚îÇ  ‚îÇ  (Full Model)   ‚îÇ    ‚îÇ   ‚îÇ  (Graph Visualization)  ‚îÇ ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-4b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n\n‚úÖ Split-GPU mode server started!\n   GPU 0: llama-server\n   GPU 1: Available for RAPIDS/Graphistry\n\nüìä Memory Distribution:\nindex, memory.used [MiB], memory.total [MiB]\n0, 2857 MiB, 15360 MiB\n1, 103 MiB, 15360 MiB\n","output_type":"stream"}],"execution_count":12},{"id":"fa4e1778","cell_type":"markdown","source":"## Step 12: Verify GPU 1 is Free for RAPIDS","metadata":{}},{"id":"fe283bc0","cell_type":"code","source":"import subprocess\n\nprint(\"=\"*70)\nprint(\"üìä GPU 1 AVAILABILITY CHECK\")\nprint(\"=\"*70)\n\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,memory.used,memory.free\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nlines = result.stdout.strip().split('\\n')\nif len(lines) >= 2:\n    gpu1_info = lines[1].split(', ')\n    used = gpu1_info[1].strip()\n    free = gpu1_info[2].strip()\n    \n    print(f\"\\nüìä GPU 1 Status:\")\n    print(f\"   Memory Used: {used}\")\n    print(f\"   Memory Free: {free}\")\n    \n    # Parse free memory\n    free_mb = int(free.replace(' MiB', ''))\n    if free_mb > 14000:  # > 14GB free\n        print(f\"\\n‚úÖ GPU 1 has {free_mb/1024:.1f} GB free - Ready for RAPIDS!\")\n    else:\n        print(f\"\\n‚ö†Ô∏è GPU 1 has limited free memory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:28:00.582802Z","iopub.execute_input":"2026-01-19T03:28:00.583152Z","iopub.status.idle":"2026-01-19T03:28:00.619355Z","shell.execute_reply.started":"2026-01-19T03:28:00.583088Z","shell.execute_reply":"2026-01-19T03:28:00.618735Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä GPU 1 AVAILABILITY CHECK\n======================================================================\n\nüìä GPU 1 Status:\n   Memory Used: 103 MiB\n   Memory Free: 14993 MiB\n\n‚úÖ GPU 1 has 14.6 GB free - Ready for RAPIDS!\n","output_type":"stream"}],"execution_count":13},{"id":"40c23da2","cell_type":"markdown","source":"## Step 13: Cleanup","metadata":{}},{"id":"b2bdf5c1","cell_type":"code","source":"print(\"üõë Stopping server...\")\nserver.stop_server()\n\nprint(\"\\n‚úÖ Server stopped\")\nprint(\"\\nüìä Final GPU Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:38:24.999902Z","iopub.execute_input":"2026-01-19T03:38:25.000266Z","iopub.status.idle":"2026-01-19T03:38:25.166776Z","shell.execute_reply.started":"2026-01-19T03:38:25.000239Z","shell.execute_reply":"2026-01-19T03:38:25.166086Z"}},"outputs":[{"name":"stdout","text":"üõë Stopping server...\n\n‚úÖ Server stopped\n\nüìä Final GPU Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 0 MiB, 15096 MiB\n1, 3 MiB, 15093 MiB\n","output_type":"stream"}],"execution_count":18},{"id":"15e9a1f2","cell_type":"markdown","source":"## üìö Summary\n\nYou've learned:\n1. ‚úÖ Multi-GPU configuration options (tensor-split, split-mode)\n2. ‚úÖ Equal split (50/50) for maximum model size\n3. ‚úÖ Asymmetric split (70/30) for mixed workloads\n4. ‚úÖ Single-GPU mode (100/0) for LLM + RAPIDS\n5. ‚úÖ Performance benchmarking across GPUs\n\n## Configuration Quick Reference\n\n| Use Case | Tensor Split | GPU 0 | GPU 1 |\n|----------|--------------|-------|-------|\n| Max Model Size | 0.5, 0.5 | LLM (50%) | LLM (50%) |\n| LLM + Light Task | 0.7, 0.3 | LLM (70%) | LLM (30%) |\n| LLM + RAPIDS | 1.0, 0.0 | LLM (100%) | RAPIDS (100%) |\n\n---\n\n**Next:** [04-gguf-quantization](04-gguf-quantization-llcuda-v2.2.0.ipynb)","metadata":{}},{"id":"92cda2ef-e686-4d5e-a430-63a5c8e881ae","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}