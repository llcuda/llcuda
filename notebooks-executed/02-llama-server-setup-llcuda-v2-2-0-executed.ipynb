{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"32eed8ef","cell_type":"markdown","source":"## Step 1: Environment Verification","metadata":{}},{"id":"cbf1e6ab","cell_type":"code","source":"import subprocess\nimport os\n\nprint(\"=\"*70)\nprint(\"üîç ENVIRONMENT CHECK\")\nprint(\"=\"*70)\n\n# GPU check\nresult = subprocess.run([\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \n                         \"--format=csv,noheader\"], capture_output=True, text=True)\nprint(\"\\nüìä GPUs Available:\")\nfor line in result.stdout.strip().split('\\n'):\n    print(f\"   {line}\")\n\n# CUDA version\nprint(\"\\nüìä CUDA Version:\")\n!nvcc --version | grep release\n\nprint(\"\\n‚úÖ Environment ready for llama-server configuration\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:38:31.900810Z","iopub.execute_input":"2026-01-19T01:38:31.901015Z","iopub.status.idle":"2026-01-19T01:38:32.099113Z","shell.execute_reply.started":"2026-01-19T01:38:31.900992Z","shell.execute_reply":"2026-01-19T01:38:32.098377Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîç ENVIRONMENT CHECK\n======================================================================\n\nüìä GPUs Available:\n   0, Tesla T4, 15360 MiB, 7.5\n   1, Tesla T4, 15360 MiB, 7.5\n\nüìä CUDA Version:\nCuda compilation tools, release 12.5, V12.5.82\n\n‚úÖ Environment ready for llama-server configuration\n","output_type":"stream"}],"execution_count":1},{"id":"8b2c6025","cell_type":"markdown","source":"## Step 2: Install llcuda and Dependencies","metadata":{}},{"id":"5fb85191","cell_type":"code","source":"%%time\n# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n!pip install -q huggingface_hub sseclient-py\n\nimport llcuda\nprint(f\"‚úÖ llcuda {llcuda.__version__} installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:38:38.217399Z","iopub.execute_input":"2026-01-19T01:38:38.218129Z","iopub.status.idle":"2026-01-19T01:39:56.871698Z","shell.execute_reply.started":"2026-01-19T01:38:38.218097Z","shell.execute_reply":"2026-01-19T01:39:56.870695Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m534.5/534.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m316.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m270.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m326.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m376.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m195.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m309.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m328.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m305.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m219.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m320.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m239.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m348.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m302.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llcuda v2.2.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llcuda v2.1!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92410124b0a9433e93f23b1301654a6d"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n‚úÖ Binaries installed successfully!\n\n‚úÖ llcuda 2.2.0 installed\nCPU times: user 38 s, sys: 8.78 s, total: 46.8 s\nWall time: 1min 18s\n","output_type":"stream"}],"execution_count":2},{"id":"bff35470","cell_type":"markdown","source":"## Step 3: Understanding Server Configuration Options\n\nllama-server has many configuration options. Here's a comprehensive overview:","metadata":{}},{"id":"17b81ee0","cell_type":"code","source":"from llcuda.server import ServerManager\nfrom llcuda.api.multigpu import MultiGPUConfig, SplitMode\n\n# Display all configuration options\nprint(\"=\"*70)\nprint(\"üìã LLAMA-SERVER CONFIGURATION OPTIONS\")\nprint(\"=\"*70)\n\nconfig_options = {\n    \"Model Settings\": {\n        \"--model, -m\": \"Path to GGUF model file\",\n        \"--alias, -a\": \"Model alias for API responses\",\n        \"--ctx-size, -c\": \"Context size (default: 4096)\",\n        \"--batch-size, -b\": \"Batch size for prompt processing\",\n        \"--ubatch-size\": \"Physical batch size (default: 512)\",\n    },\n    \"GPU Settings\": {\n        \"--n-gpu-layers, -ngl\": \"Layers to offload to GPU (99 = all)\",\n        \"--main-gpu, -mg\": \"Main GPU for computations\",\n        \"--tensor-split, -ts\": \"VRAM distribution across GPUs\",\n        \"--split-mode, -sm\": \"Split mode: layer, row, none\",\n    },\n    \"Performance\": {\n        \"--flash-attn, -fa\": \"Enable FlashAttention (faster)\",\n        \"--threads, -t\": \"CPU threads for generation\",\n        \"--threads-batch, -tb\": \"CPU threads for batch processing\",\n        \"--cont-batching\": \"Enable continuous batching\",\n        \"--parallel, -np\": \"Number of parallel sequences\",\n    },\n    \"Server Settings\": {\n        \"--host\": \"Host address (default: 127.0.0.1)\",\n        \"--port\": \"Port number (default: 8080)\",\n        \"--timeout\": \"Server timeout in seconds\",\n        \"--embeddings\": \"Enable embeddings endpoint\",\n    },\n}\n\nfor category, options in config_options.items():\n    print(f\"\\nüìå {category}:\")\n    for flag, desc in options.items():\n        print(f\"   {flag:25} {desc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:39:56.873662Z","iopub.execute_input":"2026-01-19T01:39:56.874017Z","iopub.status.idle":"2026-01-19T01:39:56.920636Z","shell.execute_reply.started":"2026-01-19T01:39:56.873978Z","shell.execute_reply":"2026-01-19T01:39:56.920078Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìã LLAMA-SERVER CONFIGURATION OPTIONS\n======================================================================\n\nüìå Model Settings:\n   --model, -m               Path to GGUF model file\n   --alias, -a               Model alias for API responses\n   --ctx-size, -c            Context size (default: 4096)\n   --batch-size, -b          Batch size for prompt processing\n   --ubatch-size             Physical batch size (default: 512)\n\nüìå GPU Settings:\n   --n-gpu-layers, -ngl      Layers to offload to GPU (99 = all)\n   --main-gpu, -mg           Main GPU for computations\n   --tensor-split, -ts       VRAM distribution across GPUs\n   --split-mode, -sm         Split mode: layer, row, none\n\nüìå Performance:\n   --flash-attn, -fa         Enable FlashAttention (faster)\n   --threads, -t             CPU threads for generation\n   --threads-batch, -tb      CPU threads for batch processing\n   --cont-batching           Enable continuous batching\n   --parallel, -np           Number of parallel sequences\n\nüìå Server Settings:\n   --host                    Host address (default: 127.0.0.1)\n   --port                    Port number (default: 8080)\n   --timeout                 Server timeout in seconds\n   --embeddings              Enable embeddings endpoint\n","output_type":"stream"}],"execution_count":3},{"id":"2b95c505","cell_type":"markdown","source":"## Step 4: Configuration Presets for Kaggle T4","metadata":{}},{"id":"12d6d414","cell_type":"code","source":"from llcuda.api.multigpu import kaggle_t4_dual_config, colab_t4_single_config\n\nprint(\"=\"*70)\nprint(\"üìã KAGGLE T4 CONFIGURATION PRESETS\")\nprint(\"=\"*70)\n\n# Single GPU configuration (use GPU 0 only)\n# Note: colab_t4_single_config works for Kaggle single T4 as well (same GPU)\nprint(\"\\nüîπ Single T4 Configuration (15GB VRAM):\")\nsingle_config = colab_t4_single_config()\nprint(f\"   GPU Layers: {single_config.n_gpu_layers}\")\nprint(f\"   Context Size: {single_config.ctx_size}\")\nprint(f\"   Batch Size: {single_config.batch_size}\")\nprint(f\"   Flash Attention: {single_config.flash_attention}\")\nprint(f\"   Best for: Models up to ~7B Q4_K_M\")\n\n# Dual GPU configuration (split across both)\nprint(\"\\nüîπ Dual T4 Configuration (30GB VRAM total):\")\ndual_config = kaggle_t4_dual_config()\nprint(f\"   GPU Layers: {dual_config.n_gpu_layers}\")\nprint(f\"   Context Size: {dual_config.ctx_size}\")\nprint(f\"   Tensor Split: {dual_config.tensor_split}\")\nprint(f\"   Split Mode: {dual_config.split_mode}\")\nprint(f\"   Flash Attention: {dual_config.flash_attention}\")\nprint(f\"   Best for: Models up to ~13B Q4_K_M\")\n\n# Split GPU configuration (LLM on GPU 0, RAPIDS on GPU 1)\nprint(\"\\nüîπ Split-GPU Configuration (Recommended):\")\nprint(\"   GPU 0: llama-server (LLM inference)\")\nprint(\"   GPU 1: RAPIDS/Graphistry (graph processing)\")\nprint(\"   Best for: Combined LLM + visualization workflows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:07.443659Z","iopub.execute_input":"2026-01-19T01:40:07.443944Z","iopub.status.idle":"2026-01-19T01:40:07.450795Z","shell.execute_reply.started":"2026-01-19T01:40:07.443921Z","shell.execute_reply":"2026-01-19T01:40:07.450125Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìã KAGGLE T4 CONFIGURATION PRESETS\n======================================================================\n\nüîπ Single T4 Configuration (15GB VRAM):\n   GPU Layers: -1\n   Context Size: 4096\n   Batch Size: 1024\n   Flash Attention: True\n   Best for: Models up to ~7B Q4_K_M\n\nüîπ Dual T4 Configuration (30GB VRAM total):\n   GPU Layers: -1\n   Context Size: 8192\n   Tensor Split: [0.5, 0.5]\n   Split Mode: SplitMode.LAYER\n   Flash Attention: True\n   Best for: Models up to ~13B Q4_K_M\n\nüîπ Split-GPU Configuration (Recommended):\n   GPU 0: llama-server (LLM inference)\n   GPU 1: RAPIDS/Graphistry (graph processing)\n   Best for: Combined LLM + visualization workflows\n","output_type":"stream"}],"execution_count":4},{"id":"da13fa6e","cell_type":"markdown","source":"## Step 5: Download Test Model","metadata":{}},{"id":"83a39360","cell_type":"code","source":"%%time\nfrom huggingface_hub import hf_hub_download\nimport os\n\n# Download a small model for testing configurations\nMODEL_REPO = \"unsloth/gemma-3-1b-it-GGUF\"\nMODEL_FILE = \"gemma-3-1b-it-Q4_K_M.gguf\"\n\nprint(f\"üì• Downloading {MODEL_FILE} for configuration testing...\")\n\nmodel_path = hf_hub_download(\n    repo_id=MODEL_REPO,\n    filename=MODEL_FILE,\n    local_dir=\"/kaggle/working/models\"\n)\n\nsize_gb = os.path.getsize(model_path) / (1024**3)\nprint(f\"\\n‚úÖ Model downloaded: {model_path}\")\nprint(f\"   Size: {size_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:12.609897Z","iopub.execute_input":"2026-01-19T01:40:12.610427Z","iopub.status.idle":"2026-01-19T01:40:15.452311Z","shell.execute_reply.started":"2026-01-19T01:40:12.610396Z","shell.execute_reply":"2026-01-19T01:40:15.451580Z"}},"outputs":[{"name":"stdout","text":"üì• Downloading gemma-3-1b-it-Q4_K_M.gguf for configuration testing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97e8060f9ecb4c86a7212966d5597f12"}},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Model downloaded: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n   Size: 0.75 GB\nCPU times: user 1.72 s, sys: 3.03 s, total: 4.75 s\nWall time: 2.84 s\n","output_type":"stream"}],"execution_count":5},{"id":"4c09c3a3","cell_type":"markdown","source":"## Step 6: Basic Server Configuration","metadata":{}},{"id":"d518b25b","cell_type":"code","source":"from llcuda.server import ServerManager\n\n# Create basic configuration settings (used as parameters to start_server)\nprint(\"=\"*70)\nprint(\"üîß BASIC SERVER CONFIGURATION\")\nprint(\"=\"*70)\n\n# Configuration parameters\nconfig = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \"gpu_layers\": 99,       # Offload all layers to GPU\n    \"ctx_size\": 4096,       # 4K context\n    \"batch_size\": 512,      # Batch size for prompt processing\n}\n\nprint(f\"\\nüìã Configuration:\")\nprint(f\"   Model: {config['model_path']}\")\nprint(f\"   Host: {config['host']}:{config['port']}\")\nprint(f\"   GPU Layers: {config['gpu_layers']}\")\nprint(f\"   Context: {config['ctx_size']}\")\n\n# Start server using ServerManager.start_server() API\nserver = ServerManager(server_url=f\"http://{config['host']}:{config['port']}\")\nprint(\"\\nüöÄ Starting server with basic configuration...\")\n\ntry:\n    server.start_server(\n        model_path=config['model_path'],\n        host=config['host'],\n        port=config['port'],\n        gpu_layers=config['gpu_layers'],\n        ctx_size=config['ctx_size'],\n        timeout=60,\n        verbose=True\n    )\n    print(\"\\n‚úÖ Server started successfully!\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:18.239803Z","iopub.execute_input":"2026-01-19T01:40:18.240391Z","iopub.status.idle":"2026-01-19T01:40:22.296611Z","shell.execute_reply.started":"2026-01-19T01:40:18.240336Z","shell.execute_reply":"2026-01-19T01:40:22.295785Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüîß BASIC SERVER CONFIGURATION\n======================================================================\n\nüìã Configuration:\n   Model: /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf\n   Host: 127.0.0.1:8080\n   GPU Layers: 99\n   Context: 4096\n\nüöÄ Starting server with basic configuration...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 4096\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready....... ‚úì Ready in 4.0s\n\n‚úÖ Server started successfully!\n","output_type":"stream"}],"execution_count":6},{"id":"68f127a0","cell_type":"markdown","source":"## Step 7: Server Health Monitoring","metadata":{}},{"id":"3037817f","cell_type":"code","source":"import requests\nimport json\n\nprint(\"=\"*70)\nprint(\"üè• SERVER HEALTH MONITORING\")\nprint(\"=\"*70)\n\n# Health check\ntry:\n    health = requests.get(\"http://127.0.0.1:8080/health\", timeout=5)\n    print(f\"\\nüìä Health Status: {health.json()}\")\nexcept Exception as e:\n    print(f\"‚ùå Health check failed: {e}\")\n\n# Get server slots info\ntry:\n    slots = requests.get(\"http://127.0.0.1:8080/slots\", timeout=5)\n    print(f\"\\nüìä Server Slots:\")\n    for slot in slots.json():\n        print(f\"   Slot {slot.get('id', 'N/A')}: {slot.get('state', 'unknown')}\")\nexcept Exception as e:\n    print(f\"   Slots endpoint not available: {e}\")\n\n# Get model info\ntry:\n    props = requests.get(\"http://127.0.0.1:8080/props\", timeout=5)\n    print(f\"\\nüìä Model Properties:\")\n    data = props.json()\n    print(f\"   Model: {data.get('default_generation_settings', {}).get('model', 'N/A')}\")\n    print(f\"   Context: {data.get('default_generation_settings', {}).get('n_ctx', 'N/A')}\")\nexcept Exception as e:\n    print(f\"   Props endpoint not available: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:25.219860Z","iopub.execute_input":"2026-01-19T01:40:25.220601Z","iopub.status.idle":"2026-01-19T01:40:25.231833Z","shell.execute_reply.started":"2026-01-19T01:40:25.220575Z","shell.execute_reply":"2026-01-19T01:40:25.231302Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüè• SERVER HEALTH MONITORING\n======================================================================\n\nüìä Health Status: {'status': 'ok'}\n\nüìä Server Slots:\n   Slot 0: unknown\n\nüìä Model Properties:\n   Model: N/A\n   Context: 4096\n","output_type":"stream"}],"execution_count":7},{"id":"8d0968c3","cell_type":"markdown","source":"## Step 8: Stop Server and Test Advanced Configuration","metadata":{}},{"id":"1b1a5cd1","cell_type":"code","source":"# Stop current server\nprint(\"üõë Stopping current server...\")\nserver.stop_server()\n\nimport time\ntime.sleep(2)  # Wait for port to be released\n\nprint(\"\\n‚úÖ Server stopped\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:29.770850Z","iopub.execute_input":"2026-01-19T01:40:29.771139Z","iopub.status.idle":"2026-01-19T01:40:32.139836Z","shell.execute_reply.started":"2026-01-19T01:40:29.771114Z","shell.execute_reply":"2026-01-19T01:40:32.139086Z"}},"outputs":[{"name":"stdout","text":"üõë Stopping current server...\n\n‚úÖ Server stopped\n","output_type":"stream"}],"execution_count":8},{"id":"54b06ed1","cell_type":"markdown","source":"## Step 9: High-Performance Configuration\n\nOptimized for maximum throughput on Kaggle T4.","metadata":{}},{"id":"d8b3ca62","cell_type":"code","source":"print(\"=\"*70)\nprint(\"‚ö° HIGH-PERFORMANCE CONFIGURATION\")\nprint(\"=\"*70)\n\n# High-performance configuration parameters\nhp_config = {\n    \"model_path\": model_path,\n    \"host\": \"127.0.0.1\",\n    \"port\": 8080,\n    \n    # GPU settings - maximize GPU utilization\n    \"gpu_layers\": 99,\n    \n    # Context and batching\n    \"ctx_size\": 8192,      # Larger context\n    \"batch_size\": 1024,    # Larger batch for prompt processing\n    \"ubatch_size\": 512,    # Physical batch size\n    \n    # Parallelism\n    \"n_parallel\": 4,       # 4 parallel request slots\n}\n\nprint(f\"\\nüìã High-Performance Settings:\")\nprint(f\"   Context Size: {hp_config['ctx_size']} tokens\")\nprint(f\"   Batch Size: {hp_config['batch_size']}\")\nprint(f\"   Parallel Slots: {hp_config['n_parallel']}\")\n\n# Create new server manager\nserver = ServerManager(server_url=f\"http://{hp_config['host']}:{hp_config['port']}\")\n\n# Start with high-performance config\nprint(\"\\nüöÄ Starting server with high-performance configuration...\")\ntry:\n    server.start_server(\n        model_path=hp_config['model_path'],\n        host=hp_config['host'],\n        port=hp_config['port'],\n        gpu_layers=hp_config['gpu_layers'],\n        ctx_size=hp_config['ctx_size'],\n        batch_size=hp_config['batch_size'],\n        ubatch_size=hp_config['ubatch_size'],\n        n_parallel=hp_config['n_parallel'],\n        timeout=60,\n        verbose=True\n    )\n    print(\"\\n‚úÖ High-performance server started!\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Server failed to start: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:40.391525Z","iopub.execute_input":"2026-01-19T01:40:40.392250Z","iopub.status.idle":"2026-01-19T01:40:43.442660Z","shell.execute_reply.started":"2026-01-19T01:40:40.392223Z","shell.execute_reply":"2026-01-19T01:40:43.441923Z"}},"outputs":[{"name":"stdout","text":"======================================================================\n‚ö° HIGH-PERFORMANCE CONFIGURATION\n======================================================================\n\nüìã High-Performance Settings:\n   Context Size: 8192 tokens\n   Batch Size: 1024\n   Parallel Slots: 4\n\nüöÄ Starting server with high-performance configuration...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 8192\n  Server URL: http://127.0.0.1:8080\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n\n‚úÖ High-performance server started!\n","output_type":"stream"}],"execution_count":9},{"id":"12a2acb0","cell_type":"markdown","source":"## Step 10: Benchmark Inference Performance","metadata":{}},{"id":"645fd1ec","cell_type":"code","source":"import time\nfrom llcuda.api.client import LlamaCppClient\n\nprint(\"=\"*70)\nprint(\"üìä INFERENCE PERFORMANCE BENCHMARK\")\nprint(\"=\"*70)\n\nclient = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n\n# Benchmark parameters\nprompts = [\n    \"Explain quantum computing in simple terms.\",\n    \"Write a haiku about machine learning.\",\n    \"What are the benefits of GPU acceleration?\",\n]\n\nprint(\"\\nüèÉ Running benchmark with 3 prompts...\\n\")\n\ntotal_tokens = 0\ntotal_time = 0\n\nfor i, prompt in enumerate(prompts, 1):\n    start = time.time()\n    \n    response = client.chat.create(\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=100,\n        temperature=0.7\n    )\n    \n    elapsed = time.time() - start\n    tokens = response.usage.completion_tokens\n    \n    total_tokens += tokens\n    total_time += elapsed\n    \n    print(f\"   Prompt {i}: {tokens} tokens in {elapsed:.2f}s ({tokens/elapsed:.1f} tok/s)\")\n\nprint(f\"\\nüìä Benchmark Results:\")\nprint(f\"   Total Tokens: {total_tokens}\")\nprint(f\"   Total Time: {total_time:.2f}s\")\nprint(f\"   Average Speed: {total_tokens/total_time:.1f} tokens/second\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:40:50.195139Z","iopub.execute_input":"2026-01-19T01:40:50.195442Z","iopub.status.idle":"2026-01-19T01:40:54.075421Z","shell.execute_reply.started":"2026-01-19T01:40:50.195416Z","shell.execute_reply":"2026-01-19T01:40:54.074700Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä INFERENCE PERFORMANCE BENCHMARK\n======================================================================\n\nüèÉ Running benchmark with 3 prompts...\n\n   Prompt 1: 100 tokens in 1.61s (62.1 tok/s)\n   Prompt 2: 48 tokens in 0.74s (64.9 tok/s)\n   Prompt 3: 100 tokens in 1.52s (65.7 tok/s)\n\nüìä Benchmark Results:\n   Total Tokens: 248\n   Total Time: 3.87s\n   Average Speed: 64.0 tokens/second\n","output_type":"stream"}],"execution_count":10},{"id":"6538c39c","cell_type":"markdown","source":"## Step 11: GPU Memory Monitoring","metadata":{}},{"id":"c72be8f0","cell_type":"code","source":"print(\"=\"*70)\nprint(\"üìä GPU MEMORY MONITORING\")\nprint(\"=\"*70)\n\n# Current memory usage\nprint(\"\\nüîπ Current GPU Memory Usage:\")\n!nvidia-smi --query-gpu=index,name,memory.used,memory.total,memory.free --format=csv\n\n# Memory usage over time (single snapshot)\nimport subprocess\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=index,memory.used,utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\n\nprint(\"\\nüîπ GPU Utilization:\")\nfor line in result.stdout.strip().split('\\n'):\n    parts = line.split(', ')\n    if len(parts) >= 3:\n        print(f\"   GPU {parts[0]}: {parts[1]} used, {parts[2]} utilization\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:41:04.030610Z","iopub.execute_input":"2026-01-19T01:41:04.031356Z","iopub.status.idle":"2026-01-19T01:41:04.217388Z","shell.execute_reply.started":"2026-01-19T01:41:04.031310Z","shell.execute_reply":"2026-01-19T01:41:04.216610Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìä GPU MEMORY MONITORING\n======================================================================\n\nüîπ Current GPU Memory Usage:\nindex, name, memory.used [MiB], memory.total [MiB], memory.free [MiB]\n0, Tesla T4, 507 MiB, 15360 MiB, 14589 MiB\n1, Tesla T4, 1227 MiB, 15360 MiB, 13869 MiB\n\nüîπ GPU Utilization:\n   GPU 0: 507 MiB used, 0 % utilization\n   GPU 1: 1227 MiB used, 0 % utilization\n","output_type":"stream"}],"execution_count":11},{"id":"f54554a7","cell_type":"markdown","source":"## Step 12: Command-Line Reference\n\nFor running llama-server directly from command line.","metadata":{}},{"id":"0de3bfdb","cell_type":"code","source":"print(\"=\"*70)\nprint(\"üìã COMMAND-LINE REFERENCE\")\nprint(\"=\"*70)\n\ncli_examples = f\"\"\"\nüîπ Basic Start:\n   llama-server -m {model_path} --host 0.0.0.0 --port 8080\n\nüîπ Single GPU (GPU 0, 15GB):\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --main-gpu 0 \\\\\n       --ctx-size 4096 --flash-attn\n\nüîπ Dual GPU (30GB total):\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 \\\\\n       --tensor-split 0.5,0.5 \\\\\n       --split-mode layer \\\\\n       --ctx-size 8192 --flash-attn\n\nüîπ High-Performance:\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --flash-attn \\\\\n       --ctx-size 8192 --batch-size 1024 \\\\\n       --parallel 4 --cont-batching \\\\\n       --threads 4 --threads-batch 4\n\nüîπ With Embeddings:\n   llama-server -m {model_path} \\\\\n       --host 0.0.0.0 --port 8080 \\\\\n       --n-gpu-layers 99 --flash-attn \\\\\n       --embeddings\n\"\"\"\n\nprint(cli_examples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:41:16.868950Z","iopub.execute_input":"2026-01-19T01:41:16.869639Z","iopub.status.idle":"2026-01-19T01:41:16.875316Z","shell.execute_reply.started":"2026-01-19T01:41:16.869607Z","shell.execute_reply":"2026-01-19T01:41:16.874594Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nüìã COMMAND-LINE REFERENCE\n======================================================================\n\nüîπ Basic Start:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf --host 0.0.0.0 --port 8080\n\nüîπ Single GPU (GPU 0, 15GB):\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --main-gpu 0 \\\n       --ctx-size 4096 --flash-attn\n\nüîπ Dual GPU (30GB total):\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 \\\n       --tensor-split 0.5,0.5 \\\n       --split-mode layer \\\n       --ctx-size 8192 --flash-attn\n\nüîπ High-Performance:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --flash-attn \\\n       --ctx-size 8192 --batch-size 1024 \\\n       --parallel 4 --cont-batching \\\n       --threads 4 --threads-batch 4\n\nüîπ With Embeddings:\n   llama-server -m /kaggle/working/models/gemma-3-1b-it-Q4_K_M.gguf \\\n       --host 0.0.0.0 --port 8080 \\\n       --n-gpu-layers 99 --flash-attn \\\n       --embeddings\n\n","output_type":"stream"}],"execution_count":12},{"id":"4b8bc155","cell_type":"markdown","source":"## Step 13: Cleanup","metadata":{}},{"id":"58236462","cell_type":"code","source":"# Stop server\nprint(\"üõë Stopping server...\")\nserver.stop_server()\n\nprint(\"\\n‚úÖ Server stopped. Resources freed.\")\n\n# Final GPU status\nprint(\"\\nüìä Final GPU Memory Status:\")\n!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T01:41:21.889807Z","iopub.execute_input":"2026-01-19T01:41:21.890496Z","iopub.status.idle":"2026-01-19T01:41:22.259044Z","shell.execute_reply.started":"2026-01-19T01:41:21.890468Z","shell.execute_reply":"2026-01-19T01:41:22.258143Z"}},"outputs":[{"name":"stdout","text":"üõë Stopping server...\n\n‚úÖ Server stopped. Resources freed.\n\nüìä Final GPU Memory Status:\nindex, memory.used [MiB], memory.free [MiB]\n0, 0 MiB, 15096 MiB\n1, 0 MiB, 15096 MiB\n","output_type":"stream"}],"execution_count":13},{"id":"9ac5776e","cell_type":"markdown","source":"## üìö Summary\n\nYou've learned:\n1. ‚úÖ Server configuration options\n2. ‚úÖ Kaggle T4 presets (single/dual GPU)\n3. ‚úÖ High-performance tuning\n4. ‚úÖ Health monitoring\n5. ‚úÖ Command-line reference\n\n## Configuration Tips for Kaggle T4\n\n| Model Size | Quantization | VRAM | Context | Config |\n|------------|--------------|------|---------|--------|\n| 1-3B | Q4_K_M | ~2GB | 8192 | Single T4 |\n| 4-7B | Q4_K_M | ~5GB | 4096 | Single T4 |\n| 8-13B | Q4_K_M | ~8GB | 4096 | Dual T4 |\n| 13-30B | IQ3_XS | ~12GB | 2048 | Dual T4 |\n\n---\n\n**Next:** [03-multi-gpu-inference](03-multi-gpu-inference-llcuda-v2.2.0.ipynb)","metadata":{}},{"id":"da1b8ea7-4e4a-40e3-b868-6dee167761f3","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}