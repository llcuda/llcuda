{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9257fb6",
   "metadata": {},
   "source": [
    "## Step 0: Add Graphistry and/or Huggginface Secrets in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5607bb47-0f1f-4b69-bc72-9fb2c5b2b3be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:28:26.652179Z",
     "iopub.status.busy": "2026-01-19T19:28:26.651371Z",
     "iopub.status.idle": "2026-01-19T19:28:27.561628Z",
     "shell.execute_reply": "2026-01-19T19:28:27.561055Z",
     "shell.execute_reply.started": "2026-01-19T19:28:26.652148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<graphistry.pygraphistry.GraphistryClient at 0x7d3e8d347d40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "graphistry.register(\n",
    "    api=3,\n",
    "    protocol=\"https\",\n",
    "    server=\"hub.graphistry.com\",\n",
    "    personal_key_id=user_secrets.get_secret(\"Graphistry_Personal_Key_ID\"),\n",
    "    personal_key_secret=user_secrets.get_secret(\"Graphistry_Personal_Secret_Key\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b072b47",
   "metadata": {},
   "source": [
    "## Step 1: Verify Dual GPU Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937f16b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:01:47.378309Z",
     "iopub.status.busy": "2026-01-19T19:01:47.378046Z",
     "iopub.status.idle": "2026-01-19T19:01:47.416802Z",
     "shell.execute_reply": "2026-01-19T19:01:47.416038Z",
     "shell.execute_reply.started": "2026-01-19T19:01:47.378287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîç SPLIT-GPU ENVIRONMENT CHECK\n",
      "======================================================================\n",
      "\n",
      "üìä Detected 2 GPU(s):\n",
      "   0, Tesla T4, 15360 MiB, 15096 MiB\n",
      "   1, Tesla T4, 15360 MiB, 15096 MiB\n",
      "\n",
      "‚úÖ Dual T4 ready for split-GPU operation!\n",
      "   GPU 0 ‚Üí llama-server (LLM)\n",
      "   GPU 1 ‚Üí RAPIDS/Graphistry\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç SPLIT-GPU ENVIRONMENT CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check GPUs\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,memory.free\", \"--format=csv,noheader\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "gpus = result.stdout.strip().split('\\n')\n",
    "print(f\"\\nüìä Detected {len(gpus)} GPU(s):\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   {gpu}\")\n",
    "\n",
    "if len(gpus) >= 2:\n",
    "    print(\"\\n‚úÖ Dual T4 ready for split-GPU operation!\")\n",
    "    print(\"   GPU 0 ‚Üí llama-server (LLM)\")\n",
    "    print(\"   GPU 1 ‚Üí RAPIDS/Graphistry\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Need 2 GPUs for split operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560cdff2",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8354f70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:01:50.538805Z",
     "iopub.status.busy": "2026-01-19T19:01:50.538524Z",
     "iopub.status.idle": "2026-01-19T19:03:13.420966Z",
     "shell.execute_reply": "2026-01-19T19:03:13.420177Z",
     "shell.execute_reply.started": "2026-01-19T19:01:50.538778Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.4.2 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m439.8/439.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m332.3/332.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üéØ llcuda v2.2.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n",
      "======================================================================\n",
      "\n",
      "üéÆ GPU Detected: Tesla T4 (Compute 7.5)\n",
      "  ‚úÖ Tesla T4 detected - Perfect for llcuda v2.1!\n",
      "üåê Platform: Colab\n",
      "\n",
      "üì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n",
      "    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n",
      "\n",
      "‚û°Ô∏è  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\n",
      "üì• Downloading v2.2.0 from HuggingFace Hub...\n",
      "   Repo: waqasm86/llcuda-binaries\n",
      "   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94199f561e44b2da96c1734908a0edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Verifying SHA256 checksum...\n",
      "   ‚úÖ Checksum verified\n",
      "üì¶ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\n",
      "Found 21 files in archive\n",
      "Extracted 21 files to /root/.cache/llcuda/extract_2.2.0\n",
      "‚úÖ Extraction complete!\n",
      "  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n",
      "  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n",
      "  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n",
      "‚úÖ Binaries installed successfully!\n",
      "\n",
      "\n",
      "‚úÖ llcuda 2.2.0 installed\n",
      "‚úÖ cuDF 25.06.00\n",
      "‚úÖ cuGraph 25.06.00\n",
      "‚úÖ Graphistry 0.50.4\n",
      "CPU times: user 44.4 s, sys: 10.2 s, total: 54.6 s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "\n",
    "# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n",
    "!pip install -q --no-cache-dir git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
    "\n",
    "# Install cuGraph (matching Kaggle RAPIDS 25.6.0)\n",
    "!pip install -q --extra-index-url=https://pypi.nvidia.com \"cugraph-cu12==25.6.*\"\n",
    "\n",
    "# Install Graphistry\n",
    "!pip install -q graphistry\n",
    "\n",
    "# Verify installations\n",
    "import llcuda\n",
    "print(f\"\\n‚úÖ llcuda {llcuda.__version__} installed\")\n",
    "\n",
    "try:\n",
    "    import cudf\n",
    "    import cugraph\n",
    "    print(f\"‚úÖ cuDF {cudf.__version__}\")\n",
    "    print(f\"‚úÖ cuGraph {cugraph.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è RAPIDS: {e}\")\n",
    "\n",
    "try:\n",
    "    import graphistry\n",
    "    print(f\"‚úÖ Graphistry {graphistry.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Graphistry: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98692cf5",
   "metadata": {},
   "source": [
    "## Step 3: Download GGUF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eabed3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:06:29.734664Z",
     "iopub.status.busy": "2026-01-19T19:06:29.733605Z",
     "iopub.status.idle": "2026-01-19T19:06:34.883842Z",
     "shell.execute_reply": "2026-01-19T19:06:34.883172Z",
     "shell.execute_reply.started": "2026-01-19T19:06:29.734629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading gemma-3-4b-it-Q4_K_M.gguf...\n",
      "   This will run on GPU 0 only.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84208581cd4f7c9937eb92c65b3e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gemma-3-4b-it-Q4_K_M.gguf:   0%|          | 0.00/2.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model downloaded: /kaggle/working/models/gemma-3-4b-it-Q4_K_M.gguf\n",
      "   Size: 2.32 GB\n",
      "CPU times: user 5.29 s, sys: 8.2 s, total: 13.5 s\n",
      "Wall time: 5.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Download a model that fits on single GPU (leaving GPU 1 free)\n",
    "MODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\n",
    "MODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"üì• Downloading {MODEL_FILE}...\")\n",
    "print(f\"   This will run on GPU 0 only.\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    filename=MODEL_FILE,\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "\n",
    "size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "print(f\"\\n‚úÖ Model downloaded: {model_path}\")\n",
    "print(f\"   Size: {size_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5debb062-5ac4-4beb-ab86-5eae7847bae3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:17:40.756718Z",
     "iopub.status.busy": "2026-01-19T19:17:40.756048Z",
     "iopub.status.idle": "2026-01-19T19:20:25.579445Z",
     "shell.execute_reply": "2026-01-19T19:20:25.578753Z",
     "shell.execute_reply.started": "2026-01-19T19:17:40.756684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading Llama-3.2-3B-Instruct-Q4_K_M.gguf...\n",
      "   Note: Check if your llcuda binary supports FlashAttention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c5ff07a3284f4cbae36928a5c2dc5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Model downloaded: /kaggle/working/models/Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
      "   Size: 1.88 GB\n",
      "   Note: You'll need to verify FlashAttention support in llcuda\n",
      "CPU times: user 4.57 s, sys: 7.4 s, total: 12 s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "\n",
    "# Try a Llama 3 GGUF model - some builds include FlashAttention\n",
    "MODEL_REPO = \"bartowski/Llama-3.2-3B-Instruct-GGUF\"\n",
    "MODEL_FILE = \"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "print(f\"üì• Downloading {MODEL_FILE}...\")\n",
    "print(f\"   Note: Check if your llcuda binary supports FlashAttention\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=MODEL_REPO,\n",
    "    filename=MODEL_FILE,\n",
    "    local_dir=\"/kaggle/working/models\"\n",
    ")\n",
    "\n",
    "size_gb = os.path.getsize(model_path) / (1024**3)\n",
    "print(f\"\\n‚úÖ Model downloaded: {model_path}\")\n",
    "print(f\"   Size: {size_gb:.2f} GB\")\n",
    "print(f\"   Note: You'll need to verify FlashAttention support in llcuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12de7fb",
   "metadata": {},
   "source": [
    "## Step 4: Start llama-server on GPU 0 Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e958374",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:21:00.501009Z",
     "iopub.status.busy": "2026-01-19T19:21:00.500654Z",
     "iopub.status.idle": "2026-01-19T19:21:03.558322Z",
     "shell.execute_reply": "2026-01-19T19:21:03.557716Z",
     "shell.execute_reply.started": "2026-01-19T19:21:00.500982Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ STARTING LLAMA-SERVER ON GPU 0\n",
      "======================================================================\n",
      "\n",
      "üìã Configuration:\n",
      "   GPU 0: 100% (llama-server)\n",
      "   GPU 1: 0% (reserved for RAPIDS)\n",
      "GPU Check:\n",
      "  Platform: kaggle\n",
      "  GPU: Tesla T4\n",
      "  Compute Capability: 7.5\n",
      "  Status: ‚úì Compatible\n",
      "Starting llama-server...\n",
      "  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n",
      "  Model: Llama-3.2-3B-Instruct-Q4_K_M.gguf\n",
      "  GPU Layers: 99\n",
      "  Context Size: 4096\n",
      "  Server URL: http://127.0.0.1:8090\n",
      "Waiting for server to be ready...... ‚úì Ready in 3.0s\n",
      "\n",
      "‚úÖ llama-server running on GPU 0!\n"
     ]
    }
   ],
   "source": [
    "from llcuda.server import ServerManager\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üöÄ STARTING LLAMA-SERVER ON GPU 0\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configuration for GPU 0 ONLY (leave GPU 1 for RAPIDS)\n",
    "print(\"\\nüìã Configuration:\")\n",
    "print(\"   GPU 0: 100% (llama-server)\")\n",
    "print(\"   GPU 1: 0% (reserved for RAPIDS)\")\n",
    "\n",
    "server = ServerManager()\n",
    "server.start_server(\n",
    "    model_path=model_path,\n",
    "    host=\"127.0.0.1\",\n",
    "    port=8090,\n",
    "    \n",
    "    # GPU 0 only configuration\n",
    "    gpu_layers=99,\n",
    "    tensor_split=\"1.0,0.0\",  # 100% on GPU 0, 0% on GPU 1\n",
    "    \n",
    "    # Optimize for single GPU\n",
    "    ctx_size=4096,\n",
    "    # Remove or set to False: flash_attention=False,\n",
    ")\n",
    "\n",
    "if server.check_server_health():\n",
    "    print(\"\\n‚úÖ llama-server running on GPU 0!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Server failed to start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280be76",
   "metadata": {},
   "source": [
    "## Step 5: Verify GPU Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ff4722b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:21:16.044719Z",
     "iopub.status.busy": "2026-01-19T19:21:16.044429Z",
     "iopub.status.idle": "2026-01-19T19:21:16.240278Z",
     "shell.execute_reply": "2026-01-19T19:21:16.239341Z",
     "shell.execute_reply.started": "2026-01-19T19:21:16.044695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä GPU MEMORY SPLIT VERIFICATION\n",
      "======================================================================\n",
      "index, name, memory.used [MiB], memory.free [MiB]\n",
      "0, Tesla T4, 2563 MiB, 12533 MiB\n",
      "1, Tesla T4, 103 MiB, 14993 MiB\n",
      "\n",
      "‚úÖ GPU 1 has 14993 MiB free for RAPIDS!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä GPU MEMORY SPLIT VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!nvidia-smi --query-gpu=index,name,memory.used,memory.free --format=csv\n",
    "\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"nvidia-smi\", \"--query-gpu=index,memory.free\", \"--format=csv,noheader,nounits\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "lines = result.stdout.strip().split('\\n')\n",
    "if len(lines) >= 2:\n",
    "    gpu1_free = int(lines[1].split(',')[1].strip())\n",
    "    print(f\"\\n‚úÖ GPU 1 has {gpu1_free} MiB free for RAPIDS!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed80c1",
   "metadata": {},
   "source": [
    "## Step 6: Initialize RAPIDS on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bccee815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:21:23.581944Z",
     "iopub.status.busy": "2026-01-19T19:21:23.581259Z",
     "iopub.status.idle": "2026-01-19T19:21:23.719692Z",
     "shell.execute_reply": "2026-01-19T19:21:23.719101Z",
     "shell.execute_reply.started": "2026-01-19T19:21:23.581906Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üî• INITIALIZING RAPIDS ON GPU 1\n",
      "======================================================================\n",
      "\n",
      "üìä RAPIDS GPU Info:\n",
      "   Device: 0 (filtered view)\n",
      "   Actual GPU: 1 (Tesla T4)\n",
      "\n",
      "‚úÖ cuDF working on GPU 1\n",
      "   Test DataFrame: (5, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Force RAPIDS to use GPU 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî• INITIALIZING RAPIDS ON GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "\n",
    "# Verify we're on the right GPU\n",
    "print(f\"\\nüìä RAPIDS GPU Info:\")\n",
    "device = cp.cuda.Device(0)  # Device 0 in filtered view = actual GPU 1\n",
    "print(f\"   Device: {device.id} (filtered view)\")\n",
    "print(f\"   Actual GPU: 1 (Tesla T4)\")\n",
    "\n",
    "# Test cuDF on GPU 1\n",
    "test_df = cudf.DataFrame({\n",
    "    'source': [0, 1, 2, 3, 4],\n",
    "    'target': [1, 2, 3, 4, 0],\n",
    "    'weight': [1.0, 2.0, 1.5, 0.5, 3.0]\n",
    "})\n",
    "\n",
    "print(f\"\\n‚úÖ cuDF working on GPU 1\")\n",
    "print(f\"   Test DataFrame: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d2aee",
   "metadata": {},
   "source": [
    "## Step 7: Create Sample Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "069395a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:21:30.574544Z",
     "iopub.status.busy": "2026-01-19T19:21:30.573897Z",
     "iopub.status.idle": "2026-01-19T19:21:30.872318Z",
     "shell.execute_reply": "2026-01-19T19:21:30.871511Z",
     "shell.execute_reply.started": "2026-01-19T19:21:30.574516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä CREATING SAMPLE GRAPH ON GPU 1\n",
      "======================================================================\n",
      "\n",
      "üìä Graph created:\n",
      "   Nodes: 10\n",
      "   Edges: 16\n",
      "\n",
      "‚úÖ cuGraph graph created on GPU 1\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import cugraph\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä CREATING SAMPLE GRAPH ON GPU 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a sample social network graph\n",
    "edges = cudf.DataFrame({\n",
    "    'source': [0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 7, 8, 9],\n",
    "    'target': [1, 2, 3, 2, 4, 3, 5, 4, 6, 5, 6, 7, 7, 8, 9, 0],\n",
    "})\n",
    "\n",
    "# Node labels\n",
    "node_names = {\n",
    "    0: \"Alice\", 1: \"Bob\", 2: \"Charlie\", 3: \"Diana\",\n",
    "    4: \"Eve\", 5: \"Frank\", 6: \"Grace\", 7: \"Henry\",\n",
    "    8: \"Ivy\", 9: \"Jack\"\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Graph created:\")\n",
    "print(f\"   Nodes: {len(node_names)}\")\n",
    "print(f\"   Edges: {len(edges)}\")\n",
    "\n",
    "# Create cuGraph graph\n",
    "G = cugraph.Graph()\n",
    "G.from_cudf_edgelist(edges, source='source', destination='target')\n",
    "\n",
    "print(f\"\\n‚úÖ cuGraph graph created on GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f562dcfd",
   "metadata": {},
   "source": [
    "## Step 8: Run Graph Analytics on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df643fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:21:34.811540Z",
     "iopub.status.busy": "2026-01-19T19:21:34.811249Z",
     "iopub.status.idle": "2026-01-19T19:21:35.255777Z",
     "shell.execute_reply": "2026-01-19T19:21:35.255000Z",
     "shell.execute_reply.started": "2026-01-19T19:21:34.811516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üî¨ GPU-ACCELERATED GRAPH ANALYTICS\n",
      "======================================================================\n",
      "\n",
      "üìä PageRank Analysis:\n",
      "   Alice: 0.1219\n",
      "   Frank: 0.1204\n",
      "   Diana: 0.1185\n",
      "   Charlie: 0.1177\n",
      "   Henry: 0.0984\n",
      "\n",
      "üìä Betweenness Centrality:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/cugraph/link_analysis/pagerank.py:232: UserWarning: Pagerank expects the 'store_transposed' flag to be set to 'True' for optimal performance during the graph creation\n",
      "  warnings.warn(warning_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Alice: 0.2093\n",
      "   Frank: 0.1824\n",
      "   Henry: 0.1389\n",
      "   Diana: 0.1324\n",
      "   Charlie: 0.1083\n",
      "\n",
      "‚úÖ Graph analytics computed on GPU 1\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üî¨ GPU-ACCELERATED GRAPH ANALYTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# PageRank\n",
    "print(\"\\nüìä PageRank Analysis:\")\n",
    "pagerank = cugraph.pagerank(G)\n",
    "pagerank = pagerank.sort_values('pagerank', ascending=False)\n",
    "\n",
    "for _, row in pagerank.to_pandas().head(5).iterrows():\n",
    "    node_id = int(row['vertex'])\n",
    "    score = row['pagerank']\n",
    "    name = node_names.get(node_id, f\"Node {node_id}\")\n",
    "    print(f\"   {name}: {score:.4f}\")\n",
    "\n",
    "# Betweenness Centrality\n",
    "print(\"\\nüìä Betweenness Centrality:\")\n",
    "bc = cugraph.betweenness_centrality(G)\n",
    "bc = bc.sort_values('betweenness_centrality', ascending=False)\n",
    "\n",
    "for _, row in bc.to_pandas().head(5).iterrows():\n",
    "    node_id = int(row['vertex'])\n",
    "    score = row['betweenness_centrality']\n",
    "    name = node_names.get(node_id, f\"Node {node_id}\")\n",
    "    print(f\"   {name}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Graph analytics computed on GPU 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddaf3d2",
   "metadata": {},
   "source": [
    "## Step 9: Use LLM to Analyze Graph Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e664414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:32:47.637059Z",
     "iopub.status.busy": "2026-01-19T19:32:47.636408Z",
     "iopub.status.idle": "2026-01-19T19:32:48.368388Z",
     "shell.execute_reply": "2026-01-19T19:32:48.367542Z",
     "shell.execute_reply.started": "2026-01-19T19:32:47.637031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ü§î LLM ANALYSIS OF GRAPH RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìã LLM Analysis (GPU 0):\n",
      "{\"name\": \"analyzing_network\", \"parameters\": {\"Alice\": 1, \"Frank\": 1, \"Diana\": 1, \"num_nodes\": 10}}\n",
      "\n",
      "‚úÖ Simultaneous GPU operation:\n",
      "   GPU 0: LLM inference\n",
      "   GPU 1: Graph analytics (previously computed)\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Use LLM to Analyze Graph Results with llcuda\n",
    "from llcuda.api.client import LlamaCppClient\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ü§î LLM ANALYSIS OF GRAPH RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get top PageRank nodes\n",
    "top_nodes = pagerank.to_pandas().head(3)\n",
    "top_names = [node_names[int(row['vertex'])] for _, row in top_nodes.iterrows()]\n",
    "\n",
    "# Create prompt for LLM\n",
    "prompt = f\"\"\"I have a social network graph with 10 people. \n",
    "The PageRank analysis shows the most influential people are: {', '.join(top_names)}.\n",
    "\n",
    "Based on this, what insights can you provide about the network structure? \n",
    "Keep your response to 3-4 sentences.\"\"\"\n",
    "\n",
    "# Connect to llama-server running on port 8090\n",
    "client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
    "\n",
    "response = client.chat.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(f\"\\nüìã LLM Analysis (GPU 0):\")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n‚úÖ Simultaneous GPU operation:\")\n",
    "print(\"   GPU 0: LLM inference\")\n",
    "print(\"   GPU 1: Graph analytics (previously computed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe99d8b",
   "metadata": {},
   "source": [
    "## Step 10: Graphistry Visualization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb89d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:33:07.799361Z",
     "iopub.status.busy": "2026-01-19T19:33:07.798776Z",
     "iopub.status.idle": "2026-01-19T19:33:07.833320Z",
     "shell.execute_reply": "2026-01-19T19:33:07.832577Z",
     "shell.execute_reply.started": "2026-01-19T19:33:07.799330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä GRAPHISTRY VISUALIZATION\n",
      "======================================================================\n",
      "\n",
      "üìä Prepared for visualization:\n",
      "   Nodes: 10\n",
      "   Edges: 16\n",
      "\n",
      "üìã Node Metrics:\n",
      "   name  pagerank  betweenness_centrality\n",
      "  Alice  0.121906                0.209259\n",
      "    Bob  0.091935                0.037037\n",
      "Charlie  0.117719                0.108333\n",
      "  Diana  0.118467                0.132407\n",
      "    Eve  0.091814                0.057407\n",
      "  Frank  0.120436                0.182407\n",
      "  Grace  0.093641                0.060185\n",
      "  Henry  0.098380                0.138889\n",
      "    Ivy  0.073543                0.064815\n",
      "   Jack  0.072161                0.092593\n"
     ]
    }
   ],
   "source": [
    "import graphistry\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä GRAPHISTRY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Convert to pandas for Graphistry (works in-notebook)\n",
    "edges_pd = edges.to_pandas()\n",
    "edges_pd['source_name'] = edges_pd['source'].map(node_names)\n",
    "edges_pd['target_name'] = edges_pd['target'].map(node_names)\n",
    "\n",
    "# Create nodes DataFrame with metrics\n",
    "pagerank_pd = pagerank.to_pandas()\n",
    "bc_pd = bc.to_pandas()\n",
    "\n",
    "nodes_pd = pd.DataFrame({\n",
    "    'node_id': list(node_names.keys()),\n",
    "    'name': list(node_names.values())\n",
    "})\n",
    "nodes_pd = nodes_pd.merge(\n",
    "    pagerank_pd.rename(columns={'vertex': 'node_id'}),\n",
    "    on='node_id'\n",
    ")\n",
    "nodes_pd = nodes_pd.merge(\n",
    "    bc_pd.rename(columns={'vertex': 'node_id'}),\n",
    "    on='node_id'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Prepared for visualization:\")\n",
    "print(f\"   Nodes: {len(nodes_pd)}\")\n",
    "print(f\"   Edges: {len(edges_pd)}\")\n",
    "\n",
    "# Note: Graphistry requires registration for full visualization\n",
    "# For demo purposes, we'll show the prepared data\n",
    "print(f\"\\nüìã Node Metrics:\")\n",
    "print(nodes_pd[['name', 'pagerank', 'betweenness_centrality']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2be2477f-ebd0-4c4b-9b08-5c6fd458fd26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:33:17.314913Z",
     "iopub.status.busy": "2026-01-19T19:33:17.314179Z",
     "iopub.status.idle": "2026-01-19T19:33:18.513379Z",
     "shell.execute_reply": "2026-01-19T19:33:18.512625Z",
     "shell.execute_reply.started": "2026-01-19T19:33:17.314856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Graphistry Visualization URL:\n",
      "   https://hub.graphistry.com/graph/graph.html?dataset=a0968cccc7b4478fb0249f3fce3b9da2&type=arrow&viztoken=b5ddcb69-f73f-4728-aba5-477328ddc545&usertag=40fe0b69-pygraphistry-0.50.4&splashAfter=1768851213&info=true\n",
      "   Open this URL in a new browser tab to view interactive visualization\n"
     ]
    }
   ],
   "source": [
    "# Create Graphistry visualization that opens in new tab\n",
    "g = graphistry.bind(source=\"source\", destination=\"target\", node=\"node_id\")\n",
    "plotter = g.edges(edges_pd).nodes(nodes_pd)\n",
    "\n",
    "# This will generate a URL that opens in a new browser tab\n",
    "url = plotter.plot(render=False)\n",
    "print(f\"\\nüîó Graphistry Visualization URL:\")\n",
    "print(f\"   {url}\")\n",
    "print(\"   Open this URL in a new browser tab to view interactive visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b480f",
   "metadata": {},
   "source": [
    "## Step 11: Interactive LLM + Graph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f501060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:34:00.139789Z",
     "iopub.status.busy": "2026-01-19T19:34:00.139221Z",
     "iopub.status.idle": "2026-01-19T19:34:01.840660Z",
     "shell.execute_reply": "2026-01-19T19:34:01.839895Z",
     "shell.execute_reply.started": "2026-01-19T19:34:00.139757Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üîÑ INTERACTIVE LLM + GRAPH WORKFLOW\n",
      "======================================================================\n",
      "\n",
      "üîç Node Analysis:\n",
      "\n",
      "üìå Alice:\n",
      "   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1219\", \"betweenness_centrality\": \"0.2093\"}}\n",
      "\n",
      "üìå Charlie:\n",
      "   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1177\", \"betweenness_centrality\": \"0.1083\"}}\n",
      "\n",
      "üìå Frank:\n",
      "   {\"name\": \"analyze_network_position\", \"parameters\": {\"pr_score\": \"0.1204\", \"betweenness_centrality\": \"0.1824\"}}\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîÑ INTERACTIVE LLM + GRAPH WORKFLOW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def analyze_node(node_name):\n",
    "    \"\"\"Use LLM to analyze a specific node's network position.\"\"\"\n",
    "    node_data = nodes_pd[nodes_pd['name'] == node_name].iloc[0]\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the network position of {node_name}:\n",
    "    - PageRank score: {node_data['pagerank']:.4f} (higher = more influential)\n",
    "    - Betweenness centrality: {node_data['betweenness_centrality']:.4f} (higher = more connections)\n",
    "    \n",
    "    What does this tell us about {node_name}'s role in the network? Answer in 2 sentences.\"\"\"\n",
    "    \n",
    "    response = client.chat.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Analyze top 3 nodes\n",
    "print(\"\\nüîç Node Analysis:\")\n",
    "for name in ['Alice', 'Charlie', 'Frank']:\n",
    "    print(f\"\\nüìå {name}:\")\n",
    "    analysis = analyze_node(name)\n",
    "    print(f\"   {analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abed4d5",
   "metadata": {},
   "source": [
    "## Step 12: Monitor Both GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "912281d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:34:14.155413Z",
     "iopub.status.busy": "2026-01-19T19:34:14.155019Z",
     "iopub.status.idle": "2026-01-19T19:34:14.407973Z",
     "shell.execute_reply": "2026-01-19T19:34:14.407130Z",
     "shell.execute_reply.started": "2026-01-19T19:34:14.155381Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä DUAL GPU MONITORING\n",
      "======================================================================\n",
      "Mon Jan 19 19:34:14 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   68C    P0             29W /   70W |    2677MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   65C    P0             28W /   70W |     103MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "üí° Split-GPU Operation:\n",
      "   GPU 0: llama-server (GGUF model loaded)\n",
      "   GPU 1: RAPIDS memory (cuDF/cuGraph data structures)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üìä DUAL GPU MONITORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "print(\"\\nüí° Split-GPU Operation:\")\n",
    "print(\"   GPU 0: llama-server (GGUF model loaded)\")\n",
    "print(\"   GPU 1: RAPIDS memory (cuDF/cuGraph data structures)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10008ddf",
   "metadata": {},
   "source": [
    "## Step 13: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36e681ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-19T19:34:26.807512Z",
     "iopub.status.busy": "2026-01-19T19:34:26.807183Z",
     "iopub.status.idle": "2026-01-19T19:34:27.310797Z",
     "shell.execute_reply": "2026-01-19T19:34:27.310134Z",
     "shell.execute_reply.started": "2026-01-19T19:34:26.807479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõë Stopping llama-server...\n",
      "\n",
      "‚úÖ Resources cleaned up\n",
      "\n",
      "üìä Final GPU Status:\n",
      "index, memory.used [MiB], memory.free [MiB]\n",
      "0, 107 MiB, 14989 MiB\n",
      "1, 3 MiB, 15093 MiB\n"
     ]
    }
   ],
   "source": [
    "print(\"üõë Stopping llama-server...\")\n",
    "server.stop_server()\n",
    "\n",
    "# Clear RAPIDS memory\n",
    "import gc\n",
    "del G, edges, pagerank, bc\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ Resources cleaned up\")\n",
    "print(\"\\nüìä Final GPU Status:\")\n",
    "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f533e7be",
   "metadata": {},
   "source": [
    "## üìö Summary\n",
    "\n",
    "### Split-GPU Architecture:\n",
    "- **GPU 0**: llama-server with `tensor_split=[1.0, 0.0]`\n",
    "- **GPU 1**: RAPIDS/cuGraph via `CUDA_VISIBLE_DEVICES=\"1\"`\n",
    "\n",
    "### Key Integration Points:\n",
    "1. ‚úÖ LLM for natural language analysis\n",
    "2. ‚úÖ cuGraph for GPU-accelerated graph algorithms\n",
    "3. ‚úÖ Graphistry for visualization\n",
    "4. ‚úÖ Combined insights from both\n",
    "\n",
    "### Code Pattern:\n",
    "```python\n",
    "# GPU 0: llama-server\n",
    "config = ServerConfig(tensor_split=[1.0, 0.0], main_gpu=0)\n",
    "\n",
    "# GPU 1: RAPIDS\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import cudf, cugraph  # Uses GPU 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** [07-openai-api-client](07-openai-api-client-llcuda-v2.2.0.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ca421b-5004-47ec-b3da-0073b885bd25",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
