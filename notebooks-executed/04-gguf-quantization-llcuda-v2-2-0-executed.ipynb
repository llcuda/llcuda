{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"81a43b95","cell_type":"markdown","source":"## Step 1: Install llcuda and Check Environment","metadata":{}},{"id":"90338ebb","cell_type":"code","source":"%%time\n# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n\nimport llcuda\nprint(f\"‚úÖ llcuda {llcuda.__version__} installed\")\n\n# Check GPU\nprint(\"\\nüìä GPU Info:\")\n!nvidia-smi --query-gpu=index,name,memory.total --format=csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:39:54.007829Z","iopub.execute_input":"2026-01-19T03:39:54.008097Z","iopub.status.idle":"2026-01-19T03:41:15.802544Z","shell.execute_reply.started":"2026-01-19T03:39:54.008066Z","shell.execute_reply":"2026-01-19T03:41:15.801233Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m534.5/534.5 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m288.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m229.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m331.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m371.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m355.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m298.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m313.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m294.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m333.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m284.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m337.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m273.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m347.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m284.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m336.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m315.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for llcuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.2 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.1 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\ntransformers 4.57.1 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"WARNING:root:llcuda: Library directory not found - shared libraries may not load correctly\n","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nüéØ llcuda v2.2.0 First-Time Setup - Kaggle 2√ó T4 Multi-GPU\n======================================================================\n\nüéÆ GPU Detected: Tesla T4 (Compute 7.5)\n  ‚úÖ Tesla T4 detected - Perfect for llcuda v2.1!\nüåê Platform: Colab\n\nüì¶ Downloading Kaggle 2√ó T4 binaries (~961 MB)...\n    Features: FlashAttention + Tensor Cores + Multi-GPU tensor-split\n\n‚û°Ô∏è  Attempt 1: HuggingFace (llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz)\nüì• Downloading v2.2.0 from HuggingFace Hub...\n   Repo: waqasm86/llcuda-binaries\n   File: v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n  warnings.warn(\nWarning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\nWARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"v2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2.(‚Ä¶):   0%|          | 0.00/1.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349ef5fde49049eba3b6fb78895410f3"}},"metadata":{}},{"name":"stdout","text":"üîê Verifying SHA256 checksum...\n   ‚úÖ Checksum verified\nüì¶ Extracting llcuda-v2.2.0-cuda12-kaggle-t4x2.tar.gz...\nFound 21 files in archive\nExtracted 21 files to /root/.cache/llcuda/extract_2.2.0\n‚úÖ Extraction complete!\n  Found bin/ and lib/ under /root/.cache/llcuda/extract_2.2.0/llcuda-v2.2.0-cuda12-kaggle-t4x2\n  Copied 13 binaries to /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12\n  Copied 0 libraries to /usr/local/lib/python3.12/dist-packages/llcuda/lib\n‚úÖ Binaries installed successfully!\n\n‚úÖ llcuda 2.2.0 installed\n\nüìä GPU Info:\nindex, name, memory.total [MiB]\n0, Tesla T4, 15360 MiB\n1, Tesla T4, 15360 MiB\nCPU times: user 38.6 s, sys: 9.08 s, total: 47.6 s\nWall time: 1min 21s\n","output_type":"stream"}],"execution_count":1},{"id":"f8008eb9","cell_type":"markdown","source":"## Step 2: Understanding Quantization Types\n\nGGUF supports multiple quantization types, organized into families:","metadata":{}},{"id":"b0e7f9b7","cell_type":"code","source":"from llcuda.api.gguf import QUANT_TYPE_INFO, QuantTypeInfo\n\nprint(\"=\"*80)\nprint(\"üìã GGUF QUANTIZATION TYPES\")\nprint(\"=\"*80)\n\n# Group by family\nfamilies = {\n    \"Legacy (Basic)\": [\"Q4_0\", \"Q4_1\", \"Q5_0\", \"Q5_1\", \"Q8_0\"],\n    \"K-Quants (Recommended)\": [\"Q2_K\", \"Q3_K_S\", \"Q3_K_M\", \"Q3_K_L\", \"Q4_K_S\", \"Q4_K_M\", \"Q5_K_S\", \"Q5_K_M\", \"Q6_K\"],\n    \"I-Quants (Ultra-Low)\": [\"IQ2_XXS\", \"IQ2_XS\", \"IQ2_S\", \"IQ3_XXS\", \"IQ3_XS\", \"IQ3_S\", \"IQ3_M\", \"IQ4_XS\", \"IQ4_NL\"],\n    \"Full Precision\": [\"F16\", \"F32\", \"BF16\"],\n}\n\nfor family_name, types in families.items():\n    print(f\"\\nüîπ {family_name}:\")\n    print(f\"   {'Type':<12} {'Bits/Weight':<12} {'Quality':<10} {'Notes'}\")\n    print(f\"   {'-'*60}\")\n    \n    for qtype in types:\n        if qtype in QUANT_TYPE_INFO:\n            info = QUANT_TYPE_INFO[qtype]\n            quality_stars = \"‚òÖ\" * int(info.quality_score * 5)\n            notes = \"Needs imatrix\" if info.requires_imatrix else \"\"\n            print(f\"   {qtype:<12} {info.bits_per_weight:<12.2f} {quality_stars:<10} {notes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:41:27.154932Z","iopub.execute_input":"2026-01-19T03:41:27.155847Z","iopub.status.idle":"2026-01-19T03:41:27.183484Z","shell.execute_reply.started":"2026-01-19T03:41:27.155810Z","shell.execute_reply":"2026-01-19T03:41:27.182899Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìã GGUF QUANTIZATION TYPES\n================================================================================\n\nüîπ Legacy (Basic):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   Q4_0         4.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q4_1         4.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q5_0         5.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q5_1         5.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q8_0         8.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n\nüîπ K-Quants (Recommended):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   Q2_K         2.60         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   Q3_K_S       3.40         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   Q3_K_M       3.90         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   Q3_K_L       4.30         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q4_K_S       4.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q4_K_M       4.80         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q5_K_S       5.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q5_K_M       5.70         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   Q6_K         6.60         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n\nüîπ I-Quants (Ultra-Low):\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n   IQ2_XXS      2.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ2_XS       2.30         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ2_S        2.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ3_XXS      3.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ3_XS       3.30         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ3_S        3.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ Needs imatrix\n   IQ4_XS       4.00         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n   IQ4_NL       4.50         ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ \n\nüîπ Full Precision:\n   Type         Bits/Weight  Quality    Notes\n   ------------------------------------------------------------\n","output_type":"stream"}],"execution_count":2},{"id":"e1b9033a","cell_type":"markdown","source":"## Step 3: Quantization Size Calculator","metadata":{}},{"id":"4634fc0f","cell_type":"code","source":"from llcuda.api.gguf import estimate_gguf_size\n\nprint(\"=\"*80)\nprint(\"üìä GGUF SIZE CALCULATOR\")\nprint(\"=\"*80)\n\n# Common model sizes\nmodel_sizes = {\n    \"Gemma-3 1B\": 1,\n    \"Gemma-3 4B\": 4,\n    \"Llama-3.2 3B\": 3,\n    \"Llama-3.1 8B\": 8,\n    \"Qwen2.5 14B\": 14,\n    \"Llama-3.1 70B\": 70,\n}\n\n# Quantization types to compare\nquant_types = [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\", \"IQ3_XS\", \"F16\"]\n\nprint(f\"\\n{'Model':<18} | \", end=\"\")\nfor qt in quant_types:\n    print(f\"{qt:<10}\", end=\"\")\nprint()\nprint(\"-\" * 80)\n\nfor model_name, params_b in model_sizes.items():\n    print(f\"{model_name:<18} | \", end=\"\")\n    for qt in quant_types:\n        size_gb = estimate_gguf_size(params_b, qt)\n        print(f\"{size_gb:<10.1f}\", end=\"\")\n    print(\" GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:41:32.109927Z","iopub.execute_input":"2026-01-19T03:41:32.110714Z","iopub.status.idle":"2026-01-19T03:41:33.528520Z","shell.execute_reply.started":"2026-01-19T03:41:32.110672Z","shell.execute_reply":"2026-01-19T03:41:33.527851Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìä GGUF SIZE CALCULATOR\n================================================================================\n\nModel              | Q4_K_M    Q5_K_M    Q6_K      Q8_0      IQ3_XS    F16       \n--------------------------------------------------------------------------------\nGemma-3 1B         | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nGemma-3 4B         | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.2 3B       | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.1 8B       | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nQwen2.5 14B        | 0.0       0.0       0.0       0.0       0.0       0.0        GB\nLlama-3.1 70B      | 0.0       0.0       0.0       0.0       0.0       0.0        GB\n","output_type":"stream"}],"execution_count":3},{"id":"607b25c2","cell_type":"markdown","source":"## Step 4: Kaggle T4 Recommendations","metadata":{}},{"id":"d565caa6","cell_type":"code","source":"from llcuda.api.gguf import recommend_quant_for_kaggle, estimate_gguf_size\n\nprint(\"=\"*80)\nprint(\"üéØ KAGGLE T4 QUANTIZATION RECOMMENDATIONS\")\nprint(\"=\"*80)\n\n# Test various model sizes (in billions)\ntest_models = [\n    (\"Gemma-3 1B\", 1_000_000_000),\n    (\"Llama-3.2 3B\", 3_000_000_000),\n    (\"Gemma-3 4B\", 4_000_000_000),\n    (\"Llama-3.1 8B\", 8_000_000_000),\n    (\"Qwen2.5 14B\", 14_000_000_000),\n    (\"Llama-3.1 70B\", 70_000_000_000),\n]\n\nprint(\"\\nüîπ Single T4 (15GB VRAM):\")\nprint(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Fits?'}\")\nprint(f\"   {'-'*55}\")\n\nfor model_name, params in test_models:\n    rec = recommend_quant_for_kaggle(params, dual_t4=False)\n    if rec['fits']:\n        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     ‚úÖ\")\n    else:\n        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     ‚ùå Too large\")\n\nprint(\"\\nüîπ Dual T4 (30GB VRAM):\")\nprint(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Fits?'}\")\nprint(f\"   {'-'*55}\")\n\nfor model_name, params in test_models:\n    rec = recommend_quant_for_kaggle(params, dual_t4=True)\n    if rec['fits']:\n        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     ‚úÖ\")\n    else:\n        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     ‚ùå Too large\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:47:07.520604Z","iopub.execute_input":"2026-01-19T03:47:07.520909Z","iopub.status.idle":"2026-01-19T03:47:07.528634Z","shell.execute_reply.started":"2026-01-19T03:47:07.520885Z","shell.execute_reply":"2026-01-19T03:47:07.528015Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüéØ KAGGLE T4 QUANTIZATION RECOMMENDATIONS\n================================================================================\n\nüîπ Single T4 (15GB VRAM):\n   Model              Recommended  Est. Size  Fits?\n   -------------------------------------------------------\n   Gemma-3 1B         Q8_0         1.1 GB     ‚úÖ\n   Llama-3.2 3B       Q8_0         3.3 GB     ‚úÖ\n   Gemma-3 4B         Q8_0         4.4 GB     ‚úÖ\n   Llama-3.1 8B       Q8_0         8.8 GB     ‚úÖ\n   Qwen2.5 14B        Q5_K_M       11.0 GB     ‚úÖ\n   Llama-3.1 70B      IQ2_XS       22.1 GB     ‚ùå Too large\n\nüîπ Dual T4 (30GB VRAM):\n   Model              Recommended  Est. Size  Fits?\n   -------------------------------------------------------\n   Gemma-3 1B         Q8_0         1.1 GB     ‚úÖ\n   Llama-3.2 3B       Q8_0         3.3 GB     ‚úÖ\n   Gemma-3 4B         Q8_0         4.4 GB     ‚úÖ\n   Llama-3.1 8B       Q8_0         8.8 GB     ‚úÖ\n   Qwen2.5 14B        Q8_0         15.4 GB     ‚úÖ\n   Llama-3.1 70B      IQ2_XS       22.1 GB     ‚ùå Too large\n","output_type":"stream"}],"execution_count":5},{"id":"7001e562","cell_type":"markdown","source":"## Step 5: K-Quants Deep Dive\n\nK-Quants are the recommended choice for most use cases.","metadata":{}},{"id":"59f961c5","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üìò K-QUANTS DETAILED GUIDE\")\nprint(\"=\"*80)\n\nk_quant_guide = \"\"\"\nK-Quants use a sophisticated mixed-precision approach:\n- Attention layers: Higher precision (more important for quality)\n- Feed-forward layers: Lower precision (less sensitive)\n\nüîπ Naming Convention:\n   Q{bits}_K_{size}\n   ‚îî‚îÄ bits: Base quantization (2,3,4,5,6)\n      ‚îî‚îÄ K: K-quant family marker\n         ‚îî‚îÄ size: S=Small, M=Medium, L=Large\n\nüîπ Recommended K-Quants:\n\n   Q4_K_M (4.85 bits/weight) ‚≠ê RECOMMENDED\n   ‚îú‚îÄ‚îÄ Best balance of size and quality\n   ‚îú‚îÄ‚îÄ ~30% smaller than FP16\n   ‚îî‚îÄ‚îÄ Minimal quality loss\n\n   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n   ‚îú‚îÄ‚îÄ Better quality than Q4_K_M\n   ‚îú‚îÄ‚îÄ Good for creative writing\n   ‚îî‚îÄ‚îÄ ~20% larger than Q4_K_M\n\n   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n   ‚îú‚îÄ‚îÄ Almost FP16 quality\n   ‚îú‚îÄ‚îÄ Good for technical tasks\n   ‚îî‚îÄ‚îÄ ~35% larger than Q4_K_M\n\n   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n   ‚îú‚îÄ‚îÄ For larger models on limited VRAM\n   ‚îú‚îÄ‚îÄ Some quality degradation\n   ‚îî‚îÄ‚îÄ ~20% smaller than Q4_K_M\n\"\"\"\nprint(k_quant_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:47:17.855854Z","iopub.execute_input":"2026-01-19T03:47:17.856155Z","iopub.status.idle":"2026-01-19T03:47:17.861392Z","shell.execute_reply.started":"2026-01-19T03:47:17.856106Z","shell.execute_reply":"2026-01-19T03:47:17.860711Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìò K-QUANTS DETAILED GUIDE\n================================================================================\n\nK-Quants use a sophisticated mixed-precision approach:\n- Attention layers: Higher precision (more important for quality)\n- Feed-forward layers: Lower precision (less sensitive)\n\nüîπ Naming Convention:\n   Q{bits}_K_{size}\n   ‚îî‚îÄ bits: Base quantization (2,3,4,5,6)\n      ‚îî‚îÄ K: K-quant family marker\n         ‚îî‚îÄ size: S=Small, M=Medium, L=Large\n\nüîπ Recommended K-Quants:\n\n   Q4_K_M (4.85 bits/weight) ‚≠ê RECOMMENDED\n   ‚îú‚îÄ‚îÄ Best balance of size and quality\n   ‚îú‚îÄ‚îÄ ~30% smaller than FP16\n   ‚îî‚îÄ‚îÄ Minimal quality loss\n\n   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n   ‚îú‚îÄ‚îÄ Better quality than Q4_K_M\n   ‚îú‚îÄ‚îÄ Good for creative writing\n   ‚îî‚îÄ‚îÄ ~20% larger than Q4_K_M\n\n   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n   ‚îú‚îÄ‚îÄ Almost FP16 quality\n   ‚îú‚îÄ‚îÄ Good for technical tasks\n   ‚îî‚îÄ‚îÄ ~35% larger than Q4_K_M\n\n   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n   ‚îú‚îÄ‚îÄ For larger models on limited VRAM\n   ‚îú‚îÄ‚îÄ Some quality degradation\n   ‚îî‚îÄ‚îÄ ~20% smaller than Q4_K_M\n\n","output_type":"stream"}],"execution_count":6},{"id":"e5a9e958","cell_type":"markdown","source":"## Step 6: I-Quants for Large Models\n\nI-Quants enable running 70B+ models on limited hardware.","metadata":{}},{"id":"c7f2704f","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üìò I-QUANTS FOR LARGE MODELS\")\nprint(\"=\"*80)\n\ni_quant_guide = \"\"\"\nI-Quants (Importance-Matrix Quants) use importance matrices\nto determine which weights are most critical for quality.\n\nüîπ Key Requirements:\n   ‚ö†Ô∏è  Require importance matrix (imatrix) for good quality\n   ‚ö†Ô∏è  Without imatrix, quality suffers significantly\n   ‚úÖ Pre-made imatrix GGUFs are available on HuggingFace\n\nüîπ I-Quant Types:\n\n   IQ3_XS (3.30 bits/weight) ‚≠ê BEST FOR 70B\n   ‚îú‚îÄ‚îÄ Fits 70B models in ~25GB VRAM\n   ‚îú‚îÄ‚îÄ Surprisingly good quality with imatrix\n   ‚îî‚îÄ‚îÄ Ideal for Kaggle dual T4 (30GB)\n\n   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n   ‚îú‚îÄ‚îÄ Better than Q4_K_M in some benchmarks\n   ‚îú‚îÄ‚îÄ Slightly smaller file size\n   ‚îî‚îÄ‚îÄ Great for 13B-34B models\n\n   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n   ‚îú‚îÄ‚îÄ For 70B+ when VRAM is very limited\n   ‚îú‚îÄ‚îÄ Noticeable quality degradation\n   ‚îî‚îÄ‚îÄ Use only when necessary\n\nüîπ 70B Model on Kaggle Dual T4:\n   Model: Llama-3.1-70B-Instruct-GGUF\n   Quant: IQ3_XS (~25GB)\n   Config: tensor-split 0.5,0.5\n   Context: 2048 (limited by VRAM)\n\"\"\"\nprint(i_quant_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:47:23.499072Z","iopub.execute_input":"2026-01-19T03:47:23.499370Z","iopub.status.idle":"2026-01-19T03:47:23.504027Z","shell.execute_reply.started":"2026-01-19T03:47:23.499348Z","shell.execute_reply":"2026-01-19T03:47:23.503284Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìò I-QUANTS FOR LARGE MODELS\n================================================================================\n\nI-Quants (Importance-Matrix Quants) use importance matrices\nto determine which weights are most critical for quality.\n\nüîπ Key Requirements:\n   ‚ö†Ô∏è  Require importance matrix (imatrix) for good quality\n   ‚ö†Ô∏è  Without imatrix, quality suffers significantly\n   ‚úÖ Pre-made imatrix GGUFs are available on HuggingFace\n\nüîπ I-Quant Types:\n\n   IQ3_XS (3.30 bits/weight) ‚≠ê BEST FOR 70B\n   ‚îú‚îÄ‚îÄ Fits 70B models in ~25GB VRAM\n   ‚îú‚îÄ‚îÄ Surprisingly good quality with imatrix\n   ‚îî‚îÄ‚îÄ Ideal for Kaggle dual T4 (30GB)\n\n   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n   ‚îú‚îÄ‚îÄ Better than Q4_K_M in some benchmarks\n   ‚îú‚îÄ‚îÄ Slightly smaller file size\n   ‚îî‚îÄ‚îÄ Great for 13B-34B models\n\n   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n   ‚îú‚îÄ‚îÄ For 70B+ when VRAM is very limited\n   ‚îú‚îÄ‚îÄ Noticeable quality degradation\n   ‚îî‚îÄ‚îÄ Use only when necessary\n\nüîπ 70B Model on Kaggle Dual T4:\n   Model: Llama-3.1-70B-Instruct-GGUF\n   Quant: IQ3_XS (~25GB)\n   Config: tensor-split 0.5,0.5\n   Context: 2048 (limited by VRAM)\n\n","output_type":"stream"}],"execution_count":7},{"id":"def3b025","cell_type":"markdown","source":"## Step 7: Interactive Quant Selector","metadata":{}},{"id":"91968e13","cell_type":"code","source":"from llcuda.api.gguf import print_quant_guide\n\nprint(\"=\"*80)\nprint(\"üéØ INTERACTIVE QUANTIZATION GUIDE\")\nprint(\"=\"*80)\n\n# Print the full guide\nprint_quant_guide()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:47:28.684430Z","iopub.execute_input":"2026-01-19T03:47:28.685156Z","iopub.status.idle":"2026-01-19T03:47:28.689696Z","shell.execute_reply.started":"2026-01-19T03:47:28.685100Z","shell.execute_reply":"2026-01-19T03:47:28.689152Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüéØ INTERACTIVE QUANTIZATION GUIDE\n================================================================================\n======================================================================\nGGUF Quantization Guide for Kaggle T4\n======================================================================\n\nLEGEND:\n  BPW = Bits Per Weight (lower = smaller file)\n  Quality = Relative quality score (1-10, higher = better)\n  Imatrix = Whether importance matrix is recommended\n\n----------------------------------------------------------------------\nType         Gen      BPW    Quality  Imatrix  Description\n----------------------------------------------------------------------\nQ4_0         legacy   4.0    5        No       4-bit symmetric quantization\nQ4_1         legacy   4.5    6        No       4-bit asymmetric quantization with offset\nQ5_0         legacy   5.0    6        No       5-bit symmetric quantization\nQ5_1         legacy   5.5    7        No       5-bit asymmetric quantization with offset\nQ8_0         legacy   8.0    9        No       8-bit symmetric quantization (near-lossless)\nQ8_1         legacy   8.5    9        No       8-bit asymmetric quantization\nQ2_K         k-quant  2.6    3        Yes      2-bit K-quant with super-blocks\nQ3_K_S       k-quant  3.4    5        Yes      3-bit K-quant, small variant\nQ3_K_M       k-quant  3.9    6        Yes      3-bit K-quant, medium variant\nQ3_K_L       k-quant  4.3    6        No       3-bit K-quant, large variant\nQ4_K_S       k-quant  4.5    7        No       4-bit K-quant, small variant\nQ4_K_M       k-quant  4.8    8        No       4-bit K-quant, medium (RECOMMENDED)\nQ5_K_S       k-quant  5.5    8        No       5-bit K-quant, small variant\nQ5_K_M       k-quant  5.7    9        No       5-bit K-quant, medium variant\nQ6_K         k-quant  6.6    9        No       6-bit K-quant (best K-quant quality)\nIQ1_S        i-quant  1.5    1        Yes      ~1.5 bpw vector quant (experimental)\nIQ2_XXS      i-quant  2.0    2        Yes      ~2.0 bpw, extra extra small\nIQ2_XS       i-quant  2.3    3        Yes      ~2.3 bpw, extra small\nIQ2_S        i-quant  2.5    4        Yes      ~2.5 bpw, small\nIQ3_XXS      i-quant  3.0    5        Yes      ~3.0 bpw, extra extra small\nIQ3_XS       i-quant  3.3    6        Yes      ~3.3 bpw, extra small (good for 70B)\nIQ3_S        i-quant  3.5    6        Yes      ~3.5 bpw, small\nIQ4_XS       i-quant  4.0    7        No       ~4.0 bpw, near Q4_K quality\nIQ4_NL       i-quant  4.5    8        No       ~4.5 bpw, non-linear\n----------------------------------------------------------------------\n\nKAGGLE RECOMMENDATIONS:\n  Single T4 (15GB): Q4_K_M for 7B, Q3_K_M for 13B\n  Dual T4 (30GB):   Q6_K for 7B, Q4_K_M for 13B, IQ3_XS for 70B\n\nSee: https://github.com/iuliaturc/gguf-docs\n======================================================================\n","output_type":"stream"}],"execution_count":8},{"id":"5936cbe2","cell_type":"markdown","source":"## Step 8: Download and Test Different Quantizations","metadata":{}},{"id":"d785df8e","cell_type":"code","source":"from huggingface_hub import hf_hub_download\nimport os\n\nprint(\"=\"*80)\nprint(\"üì• DOWNLOAD GGUF MODELS FOR COMPARISON\")\nprint(\"=\"*80)\n\n# Available Gemma-3 1B quantizations from Unsloth\nmodels_to_test = {\n    \"Q4_K_M\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n    \"Q5_K_M\": \"gemma-3-1b-it-Q5_K_M.gguf\",\n    \"Q8_0\": \"gemma-3-1b-it-Q8_0.gguf\",\n}\n\nREPO = \"unsloth/gemma-3-1b-it-GGUF\"\nMODEL_DIR = \"/kaggle/working/models\"\n\nprint(f\"\\nüì• Downloading from {REPO}...\\n\")\n\ndownloaded = {}\nfor quant, filename in models_to_test.items():\n    print(f\"   Downloading {quant}...\", end=\" \")\n    try:\n        path = hf_hub_download(\n            repo_id=REPO,\n            filename=filename,\n            local_dir=MODEL_DIR\n        )\n        size_mb = os.path.getsize(path) / (1024**2)\n        downloaded[quant] = path\n        print(f\"‚úÖ {size_mb:.0f} MB\")\n    except Exception as e:\n        print(f\"‚ùå {e}\")\n\nprint(f\"\\n‚úÖ Downloaded {len(downloaded)} models for comparison\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:47:32.792838Z","iopub.execute_input":"2026-01-19T03:47:32.793570Z","iopub.status.idle":"2026-01-19T03:47:43.066076Z","shell.execute_reply.started":"2026-01-19T03:47:32.793540Z","shell.execute_reply":"2026-01-19T03:47:43.065310Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüì• DOWNLOAD GGUF MODELS FOR COMPARISON\n================================================================================\n\nüì• Downloading from unsloth/gemma-3-1b-it-GGUF...\n\n   Downloading Q4_K_M... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q4_K_M.gguf:   0%|          | 0.00/806M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3d6e4fde404a0f948b8dbeac0636ab"}},"metadata":{}},{"name":"stdout","text":"‚úÖ 769 MB\n   Downloading Q5_K_M... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q5_K_M.gguf:   0%|          | 0.00/851M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ebe5bf21da4f9e815e3dfff55151d3"}},"metadata":{}},{"name":"stdout","text":"‚úÖ 812 MB\n   Downloading Q8_0... ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"gemma-3-1b-it-Q8_0.gguf:   0%|          | 0.00/1.07G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cadca574b2b462583a0a392f2ce46ed"}},"metadata":{}},{"name":"stdout","text":"‚úÖ 1020 MB\n\n‚úÖ Downloaded 3 models for comparison\n","output_type":"stream"}],"execution_count":9},{"id":"223a69b6","cell_type":"markdown","source":"## Step 9: Benchmark Different Quantizations","metadata":{}},{"id":"d856fda5","cell_type":"code","source":"from llcuda.server import ServerManager\nfrom llcuda.api.client import LlamaCppClient\nimport time\n\nprint(\"=\"*80)\nprint(\"üìä QUANTIZATION BENCHMARK\")\nprint(\"=\"*80)\n\ntest_prompt = \"Explain what CUDA is in exactly 3 sentences.\"\nresults = {}\n\nfor quant, model_path in downloaded.items():\n    print(f\"\\nüîπ Testing {quant}...\")\n    \n    # Start server\n    # Note: NOT passing flash_attn because Gemma-3 doesn't support flash attention\n    # (the server's default is already no flash attention)\n    server = ServerManager()\n    try:\n        server.start_server(\n            model_path=model_path,\n            host=\"127.0.0.1\",\n            port=8080,\n            gpu_layers=99,\n            ctx_size=2048,\n            # Do NOT pass flash_attn - Gemma-3 doesn't support it\n        )\n    except Exception as e:\n        print(f\"   ‚ùå Failed to start: {e}\")\n        continue\n    \n    if not server.check_server_health(timeout=60):\n        print(f\"   ‚ùå Server not healthy\")\n        server.stop_server()\n        continue\n    \n    # Benchmark\n    client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n    \n    try:\n        # Warm-up\n        client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n            max_tokens=10\n        )\n        \n        # Actual test\n        start = time.time()\n        response = client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": test_prompt}],\n            max_tokens=100,\n            temperature=0.7\n        )\n        elapsed = time.time() - start\n        \n        tokens = response.usage.completion_tokens\n        tok_per_sec = tokens / elapsed\n        \n        results[quant] = {\n            \"tokens\": tokens,\n            \"time\": elapsed,\n            \"tok_per_sec\": tok_per_sec,\n            \"response\": response.choices[0].message.content[:100]\n        }\n        \n        print(f\"   ‚úì Tokens: {tokens}, Time: {elapsed:.2f}s, Speed: {tok_per_sec:.1f} tok/s\")\n    except Exception as e:\n        print(f\"   ‚ùå Inference failed: {e}\")\n    \n    # Stop server\n    server.stop_server()\n    time.sleep(2)\n\n# Summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä BENCHMARK SUMMARY\")\nprint(\"=\"*80)\nif results:\n    for quant, data in results.items():\n        print(f\"   {quant:<12}: {data['tok_per_sec']:.1f} tok/s\")\nelse:\n    print(\"   No successful benchmarks completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T03:56:42.997882Z","iopub.execute_input":"2026-01-19T03:56:42.998595Z","iopub.status.idle":"2026-01-19T03:59:44.189915Z","shell.execute_reply.started":"2026-01-19T03:56:42.998565Z","shell.execute_reply":"2026-01-19T03:59:44.189179Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìä QUANTIZATION BENCHMARK\n================================================================================\n\nüîπ Testing Q4_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready............................................................... ‚úó Timeout\n   ‚ùå Failed to start: Server failed to start within 60 seconds\n\nüîπ Testing Q5_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q5_K_M.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready............................................................... ‚úó Timeout\n   ‚ùå Failed to start: Server failed to start within 60 seconds\n\nüîπ Testing Q8_0...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q8_0.gguf\n  GPU Layers: 99\n  Context Size: 2048\n  Server URL: http://127.0.0.1:8090\nWaiting for server to be ready............................................................... ‚úó Timeout\n   ‚ùå Failed to start: Server failed to start within 60 seconds\n\n================================================================================\nüìä BENCHMARK SUMMARY\n================================================================================\n   No successful benchmarks completed.\n","output_type":"stream"}],"execution_count":12},{"id":"4b4fb987-00da-47a2-8bbc-387fe927b2c3","cell_type":"code","source":"from llcuda.server import ServerManager\nfrom llcuda.api.client import LlamaCppClient\nimport time\nimport socket\n\ndef get_free_port():\n    \"\"\"Find a free port on localhost.\"\"\"\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind(('', 0))\n        return s.getsockname()[1]\n\nprint(\"=\"*80)\nprint(\"üìä QUANTIZATION BENCHMARK\")\nprint(\"=\"*80)\n\ntest_prompt = \"Explain what CUDA is in exactly 3 sentences.\"\nresults = {}\n\nfor quant, model_path in downloaded.items():\n    print(f\"\\nüîπ Testing {quant}...\")\n\n    # Use a random free port for each test to avoid conflicts\n    port = get_free_port()\n    server = ServerManager(server_url=f\"http://127.0.0.1:{port}\")\n    try:\n        server.start_server(\n            model_path=model_path,\n            host=\"127.0.0.1\",\n            port=port,\n            gpu_layers=32,      # Lowered for Gemma-3 1B on T4\n            ctx_size=2048,\n            timeout=120,        # Increased timeout\n        )\n    except Exception as e:\n        # Print error output if available\n        print(f\"   ‚ùå Failed to start: {e}\")\n        if hasattr(server, \"server_process\") and server.server_process:\n            try:\n                _, err = server.server_process.communicate(timeout=5)\n                if err:\n                    print(\"   [Server stderr]:\", err.decode(errors='ignore'))\n            except Exception:\n                pass\n        continue\n\n    if not server.check_server_health(timeout=120):\n        print(f\"   ‚ùå Server not healthy\")\n        server.stop_server()\n        continue\n\n    # Benchmark\n    client = LlamaCppClient(base_url=f\"http://127.0.0.1:{port}\")\n\n    try:\n        # Warm-up\n        client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n            max_tokens=10\n        )\n\n        # Actual test\n        start = time.time()\n        response = client.chat.create(\n            messages=[{\"role\": \"user\", \"content\": test_prompt}],\n            max_tokens=100,\n            temperature=0.7\n        )\n        elapsed = time.time() - start\n\n        tokens = response.usage.completion_tokens\n        tok_per_sec = tokens / elapsed\n\n        results[quant] = {\n            \"tokens\": tokens,\n            \"time\": elapsed,\n            \"tok_per_sec\": tok_per_sec,\n            \"response\": response.choices[0].message.content[:100]\n        }\n\n        print(f\"   ‚úì Tokens: {tokens}, Time: {elapsed:.2f}s, Speed: {tok_per_sec:.1f} tok/s\")\n    except Exception as e:\n        print(f\"   ‚ùå Inference failed: {e}\")\n\n    # Stop server\n    server.stop_server()\n    time.sleep(2)\n\n# Summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"üìä BENCHMARK SUMMARY\")\nprint(\"=\"*80)\nif results:\n    for quant, data in results.items():\n        print(f\"   {quant:<12}: {data['tok_per_sec']:.1f} tok/s\")\nelse:\n    print(\"   No successful benchmarks completed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T04:02:14.005630Z","iopub.execute_input":"2026-01-19T04:02:14.005966Z","iopub.status.idle":"2026-01-19T04:02:33.940641Z","shell.execute_reply.started":"2026-01-19T04:02:14.005938Z","shell.execute_reply":"2026-01-19T04:02:33.939989Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìä QUANTIZATION BENCHMARK\n================================================================================\n\nüîπ Testing Q4_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q4_K_M.gguf\n  GPU Layers: 32\n  Context Size: 2048\n  Server URL: http://127.0.0.1:34815\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n   ‚úì Tokens: 72, Time: 1.12s, Speed: 64.5 tok/s\n\nüîπ Testing Q5_K_M...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q5_K_M.gguf\n  GPU Layers: 32\n  Context Size: 2048\n  Server URL: http://127.0.0.1:39255\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n   ‚úì Tokens: 68, Time: 1.07s, Speed: 63.5 tok/s\n\nüîπ Testing Q8_0...\nGPU Check:\n  Platform: kaggle\n  GPU: Tesla T4\n  Compute Capability: 7.5\n  Status: ‚úì Compatible\nStarting llama-server...\n  Executable: /usr/local/lib/python3.12/dist-packages/llcuda/binaries/cuda12/llama-server\n  Model: gemma-3-1b-it-Q8_0.gguf\n  GPU Layers: 32\n  Context Size: 2048\n  Server URL: http://127.0.0.1:47273\nWaiting for server to be ready...... ‚úì Ready in 3.0s\n   ‚úì Tokens: 79, Time: 1.38s, Speed: 57.3 tok/s\n\n================================================================================\nüìä BENCHMARK SUMMARY\n================================================================================\n   Q4_K_M      : 64.5 tok/s\n   Q5_K_M      : 63.5 tok/s\n   Q8_0        : 57.3 tok/s\n","output_type":"stream"}],"execution_count":13},{"id":"1b2e6c06","cell_type":"markdown","source":"## Step 10: Creating Custom Quantizations\n\nUse llama-quantize to create your own quantized models.","metadata":{}},{"id":"997e52aa","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üîß CREATING CUSTOM QUANTIZATIONS\")\nprint(\"=\"*80)\n\nquantize_guide = \"\"\"\nTo quantize a model yourself, use llama-quantize:\n\nüîπ Basic Usage:\n   llama-quantize input.gguf output.gguf Q4_K_M\n\nüîπ With Importance Matrix (for I-quants):\n   # First, generate importance matrix\n   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n   \n   # Then quantize with imatrix\n   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n\nüîπ Available in llcuda:\n   from llcuda.quantization import quantize_model\n   \n   quantize_model(\n       input_path=\"model-f16.gguf\",\n       output_path=\"model-q4km.gguf\",\n       quant_type=\"Q4_K_M\"\n   )\n\nüîπ From Unsloth/HuggingFace:\n   Most models on HuggingFace are already pre-quantized.\n   Look for repos with '-GGUF' suffix:\n   - unsloth/gemma-3-4b-it-GGUF\n   - unsloth/Llama-3.2-3B-Instruct-GGUF\n   - bartowski/Qwen2.5-14B-Instruct-GGUF\n\"\"\"\nprint(quantize_guide)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T04:02:39.009341Z","iopub.execute_input":"2026-01-19T04:02:39.009624Z","iopub.status.idle":"2026-01-19T04:02:39.014650Z","shell.execute_reply.started":"2026-01-19T04:02:39.009599Z","shell.execute_reply":"2026-01-19T04:02:39.013963Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüîß CREATING CUSTOM QUANTIZATIONS\n================================================================================\n\nTo quantize a model yourself, use llama-quantize:\n\nüîπ Basic Usage:\n   llama-quantize input.gguf output.gguf Q4_K_M\n\nüîπ With Importance Matrix (for I-quants):\n   # First, generate importance matrix\n   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n   \n   # Then quantize with imatrix\n   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n\nüîπ Available in llcuda:\n   from llcuda.quantization import quantize_model\n   \n   quantize_model(\n       input_path=\"model-f16.gguf\",\n       output_path=\"model-q4km.gguf\",\n       quant_type=\"Q4_K_M\"\n   )\n\nüîπ From Unsloth/HuggingFace:\n   Most models on HuggingFace are already pre-quantized.\n   Look for repos with '-GGUF' suffix:\n   - unsloth/gemma-3-4b-it-GGUF\n   - unsloth/Llama-3.2-3B-Instruct-GGUF\n   - bartowski/Qwen2.5-14B-Instruct-GGUF\n\n","output_type":"stream"}],"execution_count":14},{"id":"3a376df4","cell_type":"markdown","source":"## Step 11: Quick Reference Table","metadata":{}},{"id":"35089e61","cell_type":"code","source":"print(\"=\"*80)\nprint(\"üìã QUICK REFERENCE: MODEL + QUANT ‚Üí KAGGLE FEASIBILITY\")\nprint(\"=\"*80)\n\nreference = \"\"\"\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Model       ‚îÇ Quant     ‚îÇ Size (GB) ‚îÇ Kaggle T4  ‚îÇ Notes                  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1B params   ‚îÇ Q4_K_M    ‚îÇ 0.6       ‚îÇ ‚úÖ Single  ‚îÇ Fast, high quality     ‚îÇ\n‚îÇ 1B params   ‚îÇ Q8_0      ‚îÇ 1.1       ‚îÇ ‚úÖ Single  ‚îÇ Best quality for 1B    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 3B params   ‚îÇ Q4_K_M    ‚îÇ 1.8       ‚îÇ ‚úÖ Single  ‚îÇ Recommended            ‚îÇ\n‚îÇ 3B params   ‚îÇ Q5_K_M    ‚îÇ 2.1       ‚îÇ ‚úÖ Single  ‚îÇ Higher quality         ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 4B params   ‚îÇ Q4_K_M    ‚îÇ 2.4       ‚îÇ ‚úÖ Single  ‚îÇ ‚≠ê Sweet spot          ‚îÇ\n‚îÇ 4B params   ‚îÇ Q6_K      ‚îÇ 3.3       ‚îÇ ‚úÖ Single  ‚îÇ Near lossless          ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 7-8B params ‚îÇ Q4_K_M    ‚îÇ 4.5       ‚îÇ ‚úÖ Single  ‚îÇ Popular choice         ‚îÇ\n‚îÇ 7-8B params ‚îÇ Q5_K_M    ‚îÇ 5.3       ‚îÇ ‚úÖ Single  ‚îÇ Better for coding      ‚îÇ\n‚îÇ 7-8B params ‚îÇ Q6_K      ‚îÇ 6.0       ‚îÇ ‚ö†Ô∏è Single  ‚îÇ Tight fit, 4K ctx      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 13-14B      ‚îÇ Q4_K_M    ‚îÇ 8.0       ‚îÇ ‚ö†Ô∏è Single  ‚îÇ 2K context max         ‚îÇ\n‚îÇ 13-14B      ‚îÇ Q4_K_M    ‚îÇ 8.0       ‚îÇ ‚úÖ Dual    ‚îÇ Split across GPUs      ‚îÇ\n‚îÇ 13-14B      ‚îÇ IQ3_XS    ‚îÇ 5.5       ‚îÇ ‚úÖ Single  ‚îÇ Quality trade-off      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 30-34B      ‚îÇ Q4_K_M    ‚îÇ 19        ‚îÇ ‚úÖ Dual    ‚îÇ tensor-split 0.5,0.5   ‚îÇ\n‚îÇ 30-34B      ‚îÇ IQ3_XS    ‚îÇ 12        ‚îÇ ‚ö†Ô∏è Single  ‚îÇ Low context            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 70B params  ‚îÇ IQ3_XS    ‚îÇ 25        ‚îÇ ‚úÖ Dual    ‚îÇ ‚≠ê 70B on Kaggle!      ‚îÇ\n‚îÇ 70B params  ‚îÇ IQ2_XS    ‚îÇ 19        ‚îÇ ‚úÖ Dual    ‚îÇ Lower quality          ‚îÇ\n‚îÇ 70B params  ‚îÇ Q4_K_M    ‚îÇ 40        ‚îÇ ‚ùå         ‚îÇ Too large              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nLegend:\n  ‚úÖ Works well    ‚ö†Ô∏è Possible with limits    ‚ùå Won't fit\n\"\"\"\nprint(reference)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T04:02:43.849313Z","iopub.execute_input":"2026-01-19T04:02:43.849978Z","iopub.status.idle":"2026-01-19T04:02:43.855669Z","shell.execute_reply.started":"2026-01-19T04:02:43.849949Z","shell.execute_reply":"2026-01-19T04:02:43.855009Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nüìã QUICK REFERENCE: MODEL + QUANT ‚Üí KAGGLE FEASIBILITY\n================================================================================\n\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Model       ‚îÇ Quant     ‚îÇ Size (GB) ‚îÇ Kaggle T4  ‚îÇ Notes                  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 1B params   ‚îÇ Q4_K_M    ‚îÇ 0.6       ‚îÇ ‚úÖ Single  ‚îÇ Fast, high quality     ‚îÇ\n‚îÇ 1B params   ‚îÇ Q8_0      ‚îÇ 1.1       ‚îÇ ‚úÖ Single  ‚îÇ Best quality for 1B    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 3B params   ‚îÇ Q4_K_M    ‚îÇ 1.8       ‚îÇ ‚úÖ Single  ‚îÇ Recommended            ‚îÇ\n‚îÇ 3B params   ‚îÇ Q5_K_M    ‚îÇ 2.1       ‚îÇ ‚úÖ Single  ‚îÇ Higher quality         ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 4B params   ‚îÇ Q4_K_M    ‚îÇ 2.4       ‚îÇ ‚úÖ Single  ‚îÇ ‚≠ê Sweet spot          ‚îÇ\n‚îÇ 4B params   ‚îÇ Q6_K      ‚îÇ 3.3       ‚îÇ ‚úÖ Single  ‚îÇ Near lossless          ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 7-8B params ‚îÇ Q4_K_M    ‚îÇ 4.5       ‚îÇ ‚úÖ Single  ‚îÇ Popular choice         ‚îÇ\n‚îÇ 7-8B params ‚îÇ Q5_K_M    ‚îÇ 5.3       ‚îÇ ‚úÖ Single  ‚îÇ Better for coding      ‚îÇ\n‚îÇ 7-8B params ‚îÇ Q6_K      ‚îÇ 6.0       ‚îÇ ‚ö†Ô∏è Single  ‚îÇ Tight fit, 4K ctx      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 13-14B      ‚îÇ Q4_K_M    ‚îÇ 8.0       ‚îÇ ‚ö†Ô∏è Single  ‚îÇ 2K context max         ‚îÇ\n‚îÇ 13-14B      ‚îÇ Q4_K_M    ‚îÇ 8.0       ‚îÇ ‚úÖ Dual    ‚îÇ Split across GPUs      ‚îÇ\n‚îÇ 13-14B      ‚îÇ IQ3_XS    ‚îÇ 5.5       ‚îÇ ‚úÖ Single  ‚îÇ Quality trade-off      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 30-34B      ‚îÇ Q4_K_M    ‚îÇ 19        ‚îÇ ‚úÖ Dual    ‚îÇ tensor-split 0.5,0.5   ‚îÇ\n‚îÇ 30-34B      ‚îÇ IQ3_XS    ‚îÇ 12        ‚îÇ ‚ö†Ô∏è Single  ‚îÇ Low context            ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ 70B params  ‚îÇ IQ3_XS    ‚îÇ 25        ‚îÇ ‚úÖ Dual    ‚îÇ ‚≠ê 70B on Kaggle!      ‚îÇ\n‚îÇ 70B params  ‚îÇ IQ2_XS    ‚îÇ 19        ‚îÇ ‚úÖ Dual    ‚îÇ Lower quality          ‚îÇ\n‚îÇ 70B params  ‚îÇ Q4_K_M    ‚îÇ 40        ‚îÇ ‚ùå         ‚îÇ Too large              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\nLegend:\n  ‚úÖ Works well    ‚ö†Ô∏è Possible with limits    ‚ùå Won't fit\n\n","output_type":"stream"}],"execution_count":15},{"id":"837c8bdb","cell_type":"markdown","source":"## üìö Summary\n\n### Key Takeaways:\n\n1. **Q4_K_M** is the recommended default - best balance of size and quality\n2. **Q5_K_M** for better quality when VRAM allows\n3. **IQ3_XS** for fitting large models (70B) on limited hardware\n4. Always check HuggingFace for pre-quantized models (faster than doing it yourself)\n\n### Kaggle T4 Guidelines:\n- Single T4 (15GB): Up to 8B Q4_K_M comfortably\n- Dual T4 (30GB): Up to 70B IQ3_XS or 34B Q4_K_M\n\n---\n\n**Next:** [05-unsloth-integration](05-unsloth-integration-llcuda-v2.2.0.ipynb)","metadata":{}},{"id":"8803e021-e09f-450e-9e31-b8eff5abab4c","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}