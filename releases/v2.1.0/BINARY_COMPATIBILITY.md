# llcuda v2.1.0 Binary Compatibility Guide

## âœ… v2.0.6 Binaries Work with v2.1.0

**TL;DR:** llcuda v2.1.0 uses the **exact same binaries** as v2.0.6. No rebuild needed!

---

## Why Binary Compatibility?

### v2.1.0 is a Pure Python API Layer

All new features in v2.1.0 are implemented as **Python wrappers** around existing CUDA infrastructure:

```
v2.1.0 Architecture:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pure Python APIs (NEW in v2.1.0)      â”‚
â”‚  - Quantization (NF4, GGUF, Dynamic)    â”‚
â”‚  - Unsloth Integration (Load, Export)   â”‚
â”‚  - CUDA Optimization (Graphs, Triton)   â”‚
â”‚  - Advanced Inference (FlashAttn, KV)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Existing CUDA Infrastructure           â”‚
â”‚  (Unchanged from v2.0.6)                â”‚
â”‚  - llama-server (llama.cpp)             â”‚
â”‚  - libggml-cuda.so (GGML library)       â”‚
â”‚  - llcuda_cpp.so (PyTorch extension)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### No C++/CUDA Code Changes

The C++/CUDA core from v2.0.6 remains **exactly the same**:

- âŒ **No changes** to `csrc/bindings.cpp`
- âŒ **No changes** to `csrc/core/device.cu`
- âŒ **No changes** to `csrc/core/tensor.cu`
- âŒ **No changes** to `csrc/ops/matmul.cu`
- âŒ **No changes** to `CMakeLists.txt`

### Existing Binaries Already Support v2.1.0 Features

The v2.0.6 binaries already have everything needed:

| Feature | v2.0.6 Binary Support | v2.1.0 API |
|---------|----------------------|------------|
| **FlashAttention** | âœ… Compiled in llama-server | Python wrapper |
| **CUDA Graphs** | âœ… PyTorch runtime feature | Python capture API |
| **Tensor Cores** | âœ… CUDA 12 runtime | Python config wrapper |
| **NF4 Quantization** | âœ… GGUF format support | Python implementation |
| **GGUF Conversion** | âœ… llama.cpp format | Python writer |
| **29 Quant Types** | âœ… All in llama.cpp | Python interface |

---

## Binary Package Details

### Using v2.0.6 Binaries

**Filename:** `llcuda-binaries-cuda12-t4-v2.0.6.tar.gz`
**Size:** 266 MB (278,892,158 bytes)
**SHA256:** `5a27d2e1a73ae3d2f1d2ba8cf557b76f54200208c8df269b1bd0d9ee176bb49d`

**Download URL:**
```
https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz
```

### Binary Contents

```
llcuda-binaries-cuda12-t4-v2.0.6.tar.gz
â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ llama-server          # 6.5 MB  - llama.cpp inference server
â”‚   â”œâ”€â”€ llama-cli             # 6.2 MB  - CLI tool
â”‚   â”œâ”€â”€ llama-quantize        # 1.8 MB  - Quantization tool
â”‚   â””â”€â”€ llama-embedding       # 6.0 MB  - Embedding tool
â””â”€â”€ lib/
    â”œâ”€â”€ libggml-cuda.so       # 219 MB  - GGML CUDA library
    â”œâ”€â”€ libllama.so           # 42 MB   - llama.cpp library
    â””â”€â”€ [other libraries]
```

### Features in v2.0.6 Binaries

**llama-server features:**
- âœ… FlashAttention support (`GGML_CUDA_FA=ON`)
- âœ… CUDA Graphs support (`GGML_CUDA_GRAPHS=ON`)
- âœ… All quantization types (Q4_K_M, Q5_K_M, Q8_0, F16, etc.)
- âœ… SM 7.5 code generation (Tesla T4 optimized)
- âœ… Tensor Core support
- âœ… GGUF v3 format support

**Build configuration:**
```cmake
CMAKE_CUDA_ARCHITECTURES=75        # Tesla T4 (SM 7.5)
GGML_CUDA_FA=ON                   # FlashAttention enabled
GGML_CUDA_GRAPHS=ON               # CUDA Graphs enabled
CMAKE_BUILD_TYPE=Release          # Optimized build
```

---

## How v2.1.0 Uses v2.0.6 Binaries

### 1. Quantization API

**v2.1.0 Implementation:**
- Pure Python: NF4 lookup tables, GGUF file format writer
- Uses PyTorch tensors for quantization operations
- No binary changes needed

**Binary Usage:**
- `llama-server` reads GGUF files created by Python API
- All quantization types already supported

### 2. Unsloth Integration API

**v2.1.0 Implementation:**
- Pure Python: Model loading, LoRA merging, GGUF export
- Uses existing PyTorch/Unsloth libraries
- No binary changes needed

**Binary Usage:**
- Exported GGUF files loaded by `llama-server`
- Standard GGUF format compatibility

### 3. CUDA Optimization API

**v2.1.0 Implementation:**
- CUDA Graphs: Python wrapper around `torch.cuda.CUDAGraph`
- Triton kernels: Pure Python JIT compilation
- Tensor Cores: Python config for `torch.backends.cuda`

**Binary Usage:**
- All features use PyTorch runtime
- No custom CUDA kernels in binaries

### 4. Advanced Inference API

**v2.1.0 Implementation:**
- FlashAttention: Python import wrapper for `flash_attn`
- KV-cache: Python management logic
- Batch inference: Python batching logic

**Binary Usage:**
- `llama-server` already has FlashAttention compiled in
- Python API just enables/configures it

---

## Bootstrap Configuration

The v2.1.0 bootstrap module is configured to use v2.0.6 binaries:

**File:** `llcuda/_internal/bootstrap.py`

```python
# Configuration for llcuda v2.1.0 (uses v2.0.6 binaries - 100% compatible)
GITHUB_RELEASE_URL = "https://github.com/waqasm86/llcuda/releases/download/v2.0.6"

# T4-only binary bundle (v2.0.6 binaries work with v2.1.0 - pure Python API layer)
T4_BINARY_BUNDLE = "llcuda-binaries-cuda12-t4-v2.0.6.tar.gz"  # 266 MB
```

### What Happens on First Import

1. **GPU Detection:** Verify Tesla T4 (SM 7.5+)
2. **Binary Download:** Download v2.0.6 binaries from GitHub (if not cached)
3. **Extraction:** Extract to `llcuda/binaries/cuda12/` and `llcuda/lib/`
4. **Configuration:** Set `LD_LIBRARY_PATH` and `LLAMA_SERVER_PATH`
5. **Ready:** All v2.1.0 APIs now available!

---

## Testing Compatibility

All v2.1.0 features have been tested with v2.0.6 binaries:

âœ… **18/18 tests passed** with v2.0.6 binaries

**Test results:**
```
Test Suite               Status    Details
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Quantization API         âœ… PASSED  NF4, GGUF, Dynamic
Unsloth Integration      âœ… PASSED  Load, Export, LoRA
CUDA Optimization        âœ… PASSED  Graphs, Triton, Tensor Cores
Advanced Inference       âœ… PASSED  FlashAttn, KV-cache, Batch
```

---

## Upgrade Process

### From v2.0.6 to v2.1.0

```bash
# Simply reinstall from GitHub
pip install --no-cache-dir --force-reinstall git+https://github.com/waqasm86/llcuda.git
```

**What happens:**
1. âœ… Python package updated to v2.1.0
2. âœ… New API modules imported
3. âœ… **Binaries remain unchanged** (already cached)
4. âœ… All v2.0.6 code continues to work

**No manual steps required!**

---

## Future Binary Updates

While v2.1.0 doesn't need new binaries, future versions may require updates if:

- New CUDA kernels are added
- llama.cpp is upgraded
- New CUDA features require compilation
- Performance optimizations at C++ level

For now, **v2.0.6 binaries work perfectly with v2.1.0!** âœ…

---

## Summary

| Aspect | Details |
|--------|---------|
| **Binary Version** | v2.0.6 |
| **llcuda Version** | v2.1.0 |
| **Compatibility** | âœ… 100% Compatible |
| **Rebuild Needed** | âŒ No |
| **Download URL** | v2.0.6 GitHub Release |
| **Size** | 266 MB |
| **Features** | FlashAttention, CUDA Graphs, Tensor Cores, All Quant Types |

**Bottom Line:** Install llcuda v2.1.0 and enjoy all new APIs with existing v2.0.6 binaries! ğŸš€
