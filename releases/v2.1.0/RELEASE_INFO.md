# llcuda v2.1.0 Release Information

**Release Date:** January 13, 2026
**Status:** Stable
**Target GPU:** Tesla T4 (SM 7.5)
**CUDA Version:** 12.x

---

## üöÄ Major Release: Complete Unsloth Integration with Advanced CUDA APIs

llcuda v2.1.0 introduces **four powerful API modules** that seamlessly integrate Unsloth fine-tuning with optimized CUDA inference for Tesla T4 GPUs.

### Key Features

#### 1. Quantization API (`llcuda.quantization`)
- **NF4 Quantization**: Block-wise 4-bit NormalFloat quantization with double quantization support
- **GGUF Conversion**: Complete GGUF v3 format support with 29 quantization types
- **Dynamic Quantization**: Intelligent VRAM-based quantization recommendations

#### 2. Unsloth Integration API (`llcuda.unsloth`)
- **Model Loading**: Direct loading of Unsloth fine-tuned models with 4-bit quantization
- **GGUF Export**: Export fine-tuned models to GGUF with automatic LoRA merging
- **LoRA Adapter Management**: Efficient adapter management and merging utilities

#### 3. CUDA Optimization API (`llcuda.cuda`)
- **CUDA Graphs**: 20-40% latency reduction for inference workloads
- **Triton Kernels**: Custom GPU operations with Triton integration
- **Tensor Core Utilities**: Leverage Tesla T4 Tensor Cores (SM 7.5)

#### 4. Advanced Inference API (`llcuda.inference`)
- **FlashAttention v2**: 2-3x speedup for long context inference
- **KV-Cache Optimization**: Efficient key-value cache management
- **Batch Inference**: Continuous batching and batch optimization

---

## üì¶ Binary Package Information

### v2.1.0 Binary Compatibility

**llcuda v2.1.0 uses the same v2.0.6 binaries** - they are 100% compatible because:

- All v2.1.0 features are **pure Python** layers on top of existing CUDA infrastructure
- No C++/CUDA code changes were made
- The binaries already support all features needed:
  - ‚úÖ FlashAttention (compiled in llama.cpp)
  - ‚úÖ CUDA Graphs (PyTorch runtime feature)
  - ‚úÖ Tensor Cores (CUDA 12 runtime feature)
  - ‚úÖ All quantization formats (GGUF/llama.cpp)

### Binary Package

**Filename:** `llcuda-binaries-cuda12-t4-v2.0.6.tar.gz`
**Size:** 266 MB (278,892,158 bytes)
**SHA256:** `5a27d2e1a73ae3d2f1d2ba8cf557b76f54200208c8df269b1bd0d9ee176bb49d`
**Download:** [GitHub Releases](https://github.com/waqasm86/llcuda/releases/download/v2.0.6/llcuda-binaries-cuda12-t4-v2.0.6.tar.gz)

### Binary Contents

1. **llama-server** (6.5 MB)
   - llama.cpp inference server
   - FlashAttention support
   - CUDA Graphs support
   - All quantization types

2. **libggml-cuda.so** (219 MB)
   - GGML CUDA library
   - Tesla T4 optimized (SM 7.5)
   - Tensor Core support

3. **Supporting Libraries**
   - Additional CUDA libraries
   - Dependencies for inference

---

## üéØ What Changed in v2.1.0

### New Python Modules (3,903 lines of code)

**Quantization API:**
- `llcuda/quantization/nf4.py` (300 lines)
- `llcuda/quantization/gguf.py` (462 lines)
- `llcuda/quantization/dynamic.py` (316 lines)

**Unsloth Integration:**
- `llcuda/unsloth/loader.py` (247 lines)
- `llcuda/unsloth/exporter.py` (287 lines)
- `llcuda/unsloth/adapter.py` (183 lines)

**CUDA Optimization:**
- `llcuda/cuda/graphs.py` (348 lines)
- `llcuda/cuda/triton_kernels.py` (487 lines)
- `llcuda/cuda/tensor_core.py` (385 lines)

**Advanced Inference:**
- `llcuda/inference/flash_attn.py` (283 lines)
- `llcuda/inference/kv_cache.py` (98 lines)
- `llcuda/inference/batch.py` (112 lines)

### Documentation (2,650+ lines)

- `API_REFERENCE.md` - Complete API documentation
- `QUICK_START.md` - 5-minute getting started guide
- `NEW_APIS_README.md` - v2.1+ feature overview
- `IMPLEMENTATION_SUMMARY.md` - Technical architecture
- `TEST_RESULTS.md` - Test results (18/18 passed)

### Examples & Tests

- `examples/complete_workflow_example.py` - Full workflow
- `examples/api_usage_examples.py` - API demonstrations
- `tests/test_new_apis.py` - 18 comprehensive tests

---

## ‚ö° Performance Improvements

- **CUDA Graphs**: 20-40% latency reduction
- **Tensor Cores**: 2-4x speedup for FP16/TF32 operations
- **FlashAttention**: 2-3x speedup for long context (8K+ tokens)
- **Dynamic Quantization**: Optimize VRAM while maintaining accuracy

---

## üìã Installation

### Method 1: From GitHub (Recommended)

```bash
pip install git+https://github.com/waqasm86/llcuda.git
```

### Method 2: Development Install

```bash
git clone https://github.com/waqasm86/llcuda.git
cd llcuda
pip install -e .
```

### First Run

On first import, llcuda will automatically:
1. Detect your Tesla T4 GPU
2. Download v2.0.6 binaries (266 MB, one-time download)
3. Configure paths and permissions

---

## üîÑ Upgrade from v2.0.6

v2.1.0 is **100% backward compatible** with v2.0.6:

```bash
# Simply reinstall from GitHub
pip install --no-cache-dir --force-reinstall git+https://github.com/waqasm86/llcuda.git
```

All existing v2.0.6 code will continue to work without modification.

---

## üìö Quick Start Example

```python
import llcuda
from llcuda.quantization import convert_to_gguf
from llcuda.unsloth import load_unsloth_model, export_to_llcuda

# Load Unsloth fine-tuned model
model, tokenizer = load_unsloth_model(
    "unsloth/Llama-3.2-1B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True
)

# Export to GGUF for llcuda inference
export_to_llcuda(
    model=model,
    tokenizer=tokenizer,
    output_path="finetuned_model.gguf",
    quant_type="Q4_K_M",
    merge_lora=True
)

# Run optimized inference
engine = llcuda.InferenceEngine()
engine.load_model("finetuned_model.gguf")
result = engine.infer("What is AI?", max_tokens=200)
print(result.text)
print(f"Speed: {result.tokens_per_sec:.1f} tokens/sec")
```

---

## üõ†Ô∏è System Requirements

### Minimum Requirements

- **GPU:** NVIDIA Tesla T4 (Compute Capability 7.5)
- **CUDA:** 12.x
- **Python:** 3.11+
- **RAM:** 8 GB system RAM
- **VRAM:** 12-16 GB recommended

### Tested Environments

- ‚úÖ Google Colab (Tesla T4 free tier)
- ‚úÖ Kaggle Notebooks (Tesla T4)
- ‚úÖ Local Tesla T4 workstations
- ‚úÖ Cloud instances with Tesla T4

---

## üìñ Documentation

- **GitHub Repository:** https://github.com/waqasm86/llcuda
- **Documentation Site:** https://llcuda.github.io/
- **Changelog:** [CHANGELOG.md](https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md)
- **Issues:** https://github.com/waqasm86/llcuda/issues

---

## üìÑ License

MIT License - see [LICENSE](https://github.com/waqasm86/llcuda/blob/main/LICENSE) file

---

## üìù Citation

If you use llcuda in your research, please cite:

```bibtex
@software{llcuda2026,
  author = {Waqas Muhammad},
  title = {llcuda: Fast LLM Inference on Tesla T4 GPUs},
  year = {2026},
  version = {2.1.0},
  url = {https://github.com/waqasm86/llcuda}
}
```

---

**Generated:** 2026-01-13
**Maintainer:** Waqas Muhammad (waqasm86@gmail.com)
