{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32eed8ef",
      "metadata": {},
      "source": [
        "## Step 1: Environment Verification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checks the Kaggle environment by querying available GPUs, their memory capacity, compute capability, and CUDA version to ensure readiness for llama-server configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf1e6ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd0d ENVIRONMENT CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# GPU check\n",
        "result = subprocess.run([\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \n",
        "                         \"--format=csv,noheader\"], capture_output=True, text=True)\n",
        "print(\"\\n\ud83d\udcca GPUs Available:\")\n",
        "for line in result.stdout.strip().split('\\n'):\n",
        "    print(f\"   {line}\")\n",
        "\n",
        "# CUDA version\n",
        "print(\"\\n\ud83d\udcca CUDA Version:\")\n",
        "!nvcc --version | grep release\n",
        "\n",
        "print(\"\\n\u2705 Environment ready for llama-server configuration\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b2c6025",
      "metadata": {},
      "source": [
        "## Step 2: Install llcuda and Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installs llcuda v2.2.0 and required dependencies (huggingface_hub, sseclient-py) from GitHub, then verifies the installation by displaying the version number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb85191",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n",
        "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
        "!pip install -q huggingface_hub sseclient-py\n",
        "\n",
        "import llcuda\n",
        "print(f\"\u2705 llcuda {llcuda.__version__} installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff35470",
      "metadata": {},
      "source": [
        "## Step 3: Understanding Server Configuration Options\n",
        "\n",
        "llama-server has many configuration options. Here's a comprehensive overview:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displays a comprehensive overview of llama-server configuration parameters organized by category (Model, GPU, Performance, Server), providing a reference guide for customizing server behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b81ee0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.server import ServerManager\n",
        "from llcuda.api.multigpu import MultiGPUConfig, SplitMode\n",
        "\n",
        "# Display all configuration options\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb LLAMA-SERVER CONFIGURATION OPTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "config_options = {\n",
        "    \"Model Settings\": {\n",
        "        \"--model, -m\": \"Path to GGUF model file\",\n",
        "        \"--alias, -a\": \"Model alias for API responses\",\n",
        "        \"--ctx-size, -c\": \"Context size (default: 4096)\",\n",
        "        \"--batch-size, -b\": \"Batch size for prompt processing\",\n",
        "        \"--ubatch-size\": \"Physical batch size (default: 512)\",\n",
        "    },\n",
        "    \"GPU Settings\": {\n",
        "        \"--n-gpu-layers, -ngl\": \"Layers to offload to GPU (99 = all)\",\n",
        "        \"--main-gpu, -mg\": \"Main GPU for computations\",\n",
        "        \"--tensor-split, -ts\": \"VRAM distribution across GPUs\",\n",
        "        \"--split-mode, -sm\": \"Split mode: layer, row, none\",\n",
        "    },\n",
        "    \"Performance\": {\n",
        "        \"--flash-attn, -fa\": \"Enable FlashAttention (faster)\",\n",
        "        \"--threads, -t\": \"CPU threads for generation\",\n",
        "        \"--threads-batch, -tb\": \"CPU threads for batch processing\",\n",
        "        \"--cont-batching\": \"Enable continuous batching\",\n",
        "        \"--parallel, -np\": \"Number of parallel sequences\",\n",
        "    },\n",
        "    \"Server Settings\": {\n",
        "        \"--host\": \"Host address (default: 127.0.0.1)\",\n",
        "        \"--port\": \"Port number (default: 8080)\",\n",
        "        \"--timeout\": \"Server timeout in seconds\",\n",
        "        \"--embeddings\": \"Enable embeddings endpoint\",\n",
        "    },\n",
        "}\n",
        "\n",
        "for category, options in config_options.items():\n",
        "    print(f\"\\n\ud83d\udccc {category}:\")\n",
        "    for flag, desc in options.items():\n",
        "        print(f\"   {flag:25} {desc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b95c505",
      "metadata": {},
      "source": [
        "## Step 4: Configuration Presets for Kaggle T4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Demonstrates llcuda's built-in configuration presets optimized for Kaggle T4 GPUs, including single GPU (15GB), dual GPU (30GB), and split-GPU setups for LLM+visualization workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12d6d414",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.multigpu import kaggle_t4_dual_config, colab_t4_single_config\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb KAGGLE T4 CONFIGURATION PRESETS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Single GPU configuration (use GPU 0 only)\n",
        "# Note: colab_t4_single_config works for Kaggle single T4 as well (same GPU)\n",
        "print(\"\\n\ud83d\udd39 Single T4 Configuration (15GB VRAM):\")\n",
        "single_config = colab_t4_single_config()\n",
        "print(f\"   GPU Layers: {single_config.n_gpu_layers}\")\n",
        "print(f\"   Context Size: {single_config.ctx_size}\")\n",
        "print(f\"   Batch Size: {single_config.batch_size}\")\n",
        "print(f\"   Flash Attention: {single_config.flash_attention}\")\n",
        "print(f\"   Best for: Models up to ~7B Q4_K_M\")\n",
        "\n",
        "# Dual GPU configuration (split across both)\n",
        "print(\"\\n\ud83d\udd39 Dual T4 Configuration (30GB VRAM total):\")\n",
        "dual_config = kaggle_t4_dual_config()\n",
        "print(f\"   GPU Layers: {dual_config.n_gpu_layers}\")\n",
        "print(f\"   Context Size: {dual_config.ctx_size}\")\n",
        "print(f\"   Tensor Split: {dual_config.tensor_split}\")\n",
        "print(f\"   Split Mode: {dual_config.split_mode}\")\n",
        "print(f\"   Flash Attention: {dual_config.flash_attention}\")\n",
        "print(f\"   Best for: Models up to ~13B Q4_K_M\")\n",
        "\n",
        "# Split GPU configuration (LLM on GPU 0, RAPIDS on GPU 1)\n",
        "print(\"\\n\ud83d\udd39 Split-GPU Configuration (Recommended):\")\n",
        "print(\"   GPU 0: llama-server (LLM inference)\")\n",
        "print(\"   GPU 1: RAPIDS/Graphistry (graph processing)\")\n",
        "print(\"   Best for: Combined LLM + visualization workflows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da13fa6e",
      "metadata": {},
      "source": [
        "## Step 5: Download Test Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloads a small Gemma 1B Q4_K_M GGUF model for testing various server configurations, reporting the download location and file size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a39360",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "# Download a small model for testing configurations\n",
        "MODEL_REPO = \"unsloth/gemma-3-1b-it-GGUF\"\n",
        "MODEL_FILE = \"gemma-3-1b-it-Q4_K_M.gguf\"\n",
        "\n",
        "print(f\"\ud83d\udce5 Downloading {MODEL_FILE} for configuration testing...\")\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=MODEL_REPO,\n",
        "    filename=MODEL_FILE,\n",
        "    local_dir=\"/kaggle/working/models\"\n",
        ")\n",
        "\n",
        "size_gb = os.path.getsize(model_path) / (1024**3)\n",
        "print(f\"\\n\u2705 Model downloaded: {model_path}\")\n",
        "print(f\"   Size: {size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c09c3a3",
      "metadata": {},
      "source": [
        "## Step 6: Basic Server Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starts llama-server with basic configuration settings including GPU layer offloading, context size, and batch parameters using ServerManager's API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d518b25b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.server import ServerManager\n",
        "\n",
        "# Create basic configuration settings (used as parameters to start_server)\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd27 BASIC SERVER CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Configuration parameters\n",
        "config = {\n",
        "    \"model_path\": model_path,\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": 8080,\n",
        "    \"gpu_layers\": 99,       # Offload all layers to GPU\n",
        "    \"ctx_size\": 4096,       # 4K context\n",
        "    \"batch_size\": 512,      # Batch size for prompt processing\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Configuration:\")\n",
        "print(f\"   Model: {config['model_path']}\")\n",
        "print(f\"   Host: {config['host']}:{config['port']}\")\n",
        "print(f\"   GPU Layers: {config['gpu_layers']}\")\n",
        "print(f\"   Context: {config['ctx_size']}\")\n",
        "\n",
        "# Start server using ServerManager.start_server() API\n",
        "server = ServerManager(server_url=f\"http://{config['host']}:{config['port']}\")\n",
        "print(\"\\n\ud83d\ude80 Starting server with basic configuration...\")\n",
        "\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=config['model_path'],\n",
        "        host=config['host'],\n",
        "        port=config['port'],\n",
        "        gpu_layers=config['gpu_layers'],\n",
        "        ctx_size=config['ctx_size'],\n",
        "        timeout=60,\n",
        "        verbose=True\n",
        "    )\n",
        "    print(\"\\n\u2705 Server started successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Server failed to start: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f127a0",
      "metadata": {},
      "source": [
        "## Step 7: Server Health Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monitors server health by querying the /health, /slots, and /props endpoints to check server status, available slots, and loaded model properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3037817f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfe5 SERVER HEALTH MONITORING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Health check\n",
        "try:\n",
        "    health = requests.get(\"http://127.0.0.1:8080/health\", timeout=5)\n",
        "    print(f\"\\n\ud83d\udcca Health Status: {health.json()}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Health check failed: {e}\")\n",
        "\n",
        "# Get server slots info\n",
        "try:\n",
        "    slots = requests.get(\"http://127.0.0.1:8080/slots\", timeout=5)\n",
        "    print(f\"\\n\ud83d\udcca Server Slots:\")\n",
        "    for slot in slots.json():\n",
        "        print(f\"   Slot {slot.get('id', 'N/A')}: {slot.get('state', 'unknown')}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Slots endpoint not available: {e}\")\n",
        "\n",
        "# Get model info\n",
        "try:\n",
        "    props = requests.get(\"http://127.0.0.1:8080/props\", timeout=5)\n",
        "    print(f\"\\n\ud83d\udcca Model Properties:\")\n",
        "    data = props.json()\n",
        "    print(f\"   Model: {data.get('default_generation_settings', {}).get('model', 'N/A')}\")\n",
        "    print(f\"   Context: {data.get('default_generation_settings', {}).get('n_ctx', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Props endpoint not available: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d0968c3",
      "metadata": {},
      "source": [
        "## Step 8: Stop Server and Test Advanced Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gracefully stops the running llama-server and waits 2 seconds for the port to be released before starting a new configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1a5cd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop current server\n",
        "print(\"\ud83d\uded1 Stopping current server...\")\n",
        "server.stop_server()\n",
        "\n",
        "import time\n",
        "time.sleep(2)  # Wait for port to be released\n",
        "\n",
        "print(\"\\n\u2705 Server stopped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54b06ed1",
      "metadata": {},
      "source": [
        "## Step 9: High-Performance Configuration\n",
        "\n",
        "Optimized for maximum throughput on Kaggle T4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configures llama-server for maximum throughput on Kaggle T4 with larger context (8K), batch sizes, and parallel request handling (4 slots) optimized for production workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b3ca62",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\u26a1 HIGH-PERFORMANCE CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# High-performance configuration parameters\n",
        "hp_config = {\n",
        "    \"model_path\": model_path,\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": 8080,\n",
        "    \n",
        "    # GPU settings - maximize GPU utilization\n",
        "    \"gpu_layers\": 99,\n",
        "    \n",
        "    # Context and batching\n",
        "    \"ctx_size\": 8192,      # Larger context\n",
        "    \"batch_size\": 1024,    # Larger batch for prompt processing\n",
        "    \"ubatch_size\": 512,    # Physical batch size\n",
        "    \n",
        "    # Parallelism\n",
        "    \"n_parallel\": 4,       # 4 parallel request slots\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb High-Performance Settings:\")\n",
        "print(f\"   Context Size: {hp_config['ctx_size']} tokens\")\n",
        "print(f\"   Batch Size: {hp_config['batch_size']}\")\n",
        "print(f\"   Parallel Slots: {hp_config['n_parallel']}\")\n",
        "\n",
        "# Create new server manager\n",
        "server = ServerManager(server_url=f\"http://{hp_config['host']}:{hp_config['port']}\")\n",
        "\n",
        "# Start with high-performance config\n",
        "print(\"\\n\ud83d\ude80 Starting server with high-performance configuration...\")\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=hp_config['model_path'],\n",
        "        host=hp_config['host'],\n",
        "        port=hp_config['port'],\n",
        "        gpu_layers=hp_config['gpu_layers'],\n",
        "        ctx_size=hp_config['ctx_size'],\n",
        "        batch_size=hp_config['batch_size'],\n",
        "        ubatch_size=hp_config['ubatch_size'],\n",
        "        n_parallel=hp_config['n_parallel'],\n",
        "        timeout=60,\n",
        "        verbose=True\n",
        "    )\n",
        "    print(\"\\n\u2705 High-performance server started!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Server failed to start: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12a2acb0",
      "metadata": {},
      "source": [
        "## Step 10: Benchmark Inference Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks inference performance by running multiple prompts and measuring tokens per second, total time, and average generation speed to evaluate configuration effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "645fd1ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from llcuda.api.client import LlamaCppClient\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca INFERENCE PERFORMANCE BENCHMARK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
        "\n",
        "# Benchmark parameters\n",
        "prompts = [\n",
        "    \"Explain quantum computing in simple terms.\",\n",
        "    \"Write a haiku about machine learning.\",\n",
        "    \"What are the benefits of GPU acceleration?\",\n",
        "]\n",
        "\n",
        "print(\"\\n\ud83c\udfc3 Running benchmark with 3 prompts...\\n\")\n",
        "\n",
        "total_tokens = 0\n",
        "total_time = 0\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    start = time.time()\n",
        "    \n",
        "    response = client.chat.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=100,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    tokens = response.usage.completion_tokens\n",
        "    \n",
        "    total_tokens += tokens\n",
        "    total_time += elapsed\n",
        "    \n",
        "    print(f\"   Prompt {i}: {tokens} tokens in {elapsed:.2f}s ({tokens/elapsed:.1f} tok/s)\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Benchmark Results:\")\n",
        "print(f\"   Total Tokens: {total_tokens}\")\n",
        "print(f\"   Total Time: {total_time:.2f}s\")\n",
        "print(f\"   Average Speed: {total_tokens/total_time:.1f} tokens/second\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6538c39c",
      "metadata": {},
      "source": [
        "## Step 11: GPU Memory Monitoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Monitors current GPU memory usage, available VRAM, and GPU utilization across all GPUs to track resource consumption during inference operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72be8f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca GPU MEMORY MONITORING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Current memory usage\n",
        "print(\"\\n\ud83d\udd39 Current GPU Memory Usage:\")\n",
        "!nvidia-smi --query-gpu=index,name,memory.used,memory.total,memory.free --format=csv\n",
        "\n",
        "# Memory usage over time (single snapshot)\n",
        "import subprocess\n",
        "result = subprocess.run(\n",
        "    [\"nvidia-smi\", \"--query-gpu=index,memory.used,utilization.gpu\", \"--format=csv,noheader\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "print(\"\\n\ud83d\udd39 GPU Utilization:\")\n",
        "for line in result.stdout.strip().split('\\n'):\n",
        "    parts = line.split(', ')\n",
        "    if len(parts) >= 3:\n",
        "        print(f\"   GPU {parts[0]}: {parts[1]} used, {parts[2]} utilization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54554a7",
      "metadata": {},
      "source": [
        "## Step 12: Command-Line Reference\n",
        "\n",
        "For running llama-server directly from command line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provides command-line reference examples for starting llama-server directly from terminal with various configurations (single GPU, dual GPU, high-performance, embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de3bfdb",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb COMMAND-LINE REFERENCE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cli_examples = f\"\"\"\n",
        "\ud83d\udd39 Basic Start:\n",
        "   llama-server -m {model_path} --host 0.0.0.0 --port 8080\n",
        "\n",
        "\ud83d\udd39 Single GPU (GPU 0, 15GB):\n",
        "   llama-server -m {model_path} \\\\\n",
        "       --host 0.0.0.0 --port 8080 \\\\\n",
        "       --n-gpu-layers 99 --main-gpu 0 \\\\\n",
        "       --ctx-size 4096 --flash-attn\n",
        "\n",
        "\ud83d\udd39 Dual GPU (30GB total):\n",
        "   llama-server -m {model_path} \\\\\n",
        "       --host 0.0.0.0 --port 8080 \\\\\n",
        "       --n-gpu-layers 99 \\\\\n",
        "       --tensor-split 0.5,0.5 \\\\\n",
        "       --split-mode layer \\\\\n",
        "       --ctx-size 8192 --flash-attn\n",
        "\n",
        "\ud83d\udd39 High-Performance:\n",
        "   llama-server -m {model_path} \\\\\n",
        "       --host 0.0.0.0 --port 8080 \\\\\n",
        "       --n-gpu-layers 99 --flash-attn \\\\\n",
        "       --ctx-size 8192 --batch-size 1024 \\\\\n",
        "       --parallel 4 --cont-batching \\\\\n",
        "       --threads 4 --threads-batch 4\n",
        "\n",
        "\ud83d\udd39 With Embeddings:\n",
        "   llama-server -m {model_path} \\\\\n",
        "       --host 0.0.0.0 --port 8080 \\\\\n",
        "       --n-gpu-layers 99 --flash-attn \\\\\n",
        "       --embeddings\n",
        "\"\"\"\n",
        "\n",
        "print(cli_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8bc155",
      "metadata": {},
      "source": [
        "## Step 13: Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performs final cleanup by stopping the server, freeing GPU resources, and displaying the final memory status to confirm all resources have been released."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58236462",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop server\n",
        "print(\"\ud83d\uded1 Stopping server...\")\n",
        "server.stop_server()\n",
        "\n",
        "print(\"\\n\u2705 Server stopped. Resources freed.\")\n",
        "\n",
        "# Final GPU status\n",
        "print(\"\\n\ud83d\udcca Final GPU Memory Status:\")\n",
        "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac5776e",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Summary\n",
        "\n",
        "You've learned:\n",
        "1. \u2705 Server configuration options\n",
        "2. \u2705 Kaggle T4 presets (single/dual GPU)\n",
        "3. \u2705 High-performance tuning\n",
        "4. \u2705 Health monitoring\n",
        "5. \u2705 Command-line reference\n",
        "\n",
        "## Configuration Tips for Kaggle T4\n",
        "\n",
        "| Model Size | Quantization | VRAM | Context | Config |\n",
        "|------------|--------------|------|---------|--------|\n",
        "| 1-3B | Q4_K_M | ~2GB | 8192 | Single T4 |\n",
        "| 4-7B | Q4_K_M | ~5GB | 4096 | Single T4 |\n",
        "| 8-13B | Q4_K_M | ~8GB | 4096 | Dual T4 |\n",
        "| 13-30B | IQ3_XS | ~12GB | 2048 | Dual T4 |\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** [03-multi-gpu-inference](03-multi-gpu-inference-llcuda-v2.2.0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}