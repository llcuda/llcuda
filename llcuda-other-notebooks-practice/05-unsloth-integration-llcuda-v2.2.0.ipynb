{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f0c4fcb",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifies GPU availability and CUDA version to ensure the environment is ready for Unsloth fine-tuning and subsequent llcuda deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31f0af2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd0d GPU ENVIRONMENT CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check GPUs\n",
        "!nvidia-smi --query-gpu=index,name,memory.total --format=csv\n",
        "\n",
        "# CUDA version\n",
        "print(\"\\n\ud83d\udcca CUDA Version:\")\n",
        "!nvcc --version | grep release\n",
        "\n",
        "print(\"\\n\u2705 Environment ready for Unsloth + llcuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8584dd45",
      "metadata": {},
      "source": [
        "## Step 2: Install Unsloth and llcuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installs Unsloth for efficient fine-tuning, llcuda v2.2.0 for inference, and additional dependencies (datasets, trl) required for the training workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2cd09a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udce6 Installing Unsloth and llcuda...\")\n",
        "\n",
        "# Install Unsloth (fast installation)\n",
        "!pip install -q unsloth\n",
        "\n",
        "# Install llcuda v2.2.0 (avoid --force-reinstall to prevent breaking RAPIDS/cupy)\n",
        "!pip install -q git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
        "\n",
        "# Additional dependencies\n",
        "!pip install -q datasets trl\n",
        "\n",
        "# Verify installations\n",
        "try:\n",
        "    import llcuda\n",
        "    print(f\"\\n\u2705 llcuda {llcuda.__version__} installed\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c llcuda import failed: {e}\")\n",
        "\n",
        "try:\n",
        "    from unsloth import FastLanguageModel\n",
        "    print(\"\u2705 Unsloth installed\")\n",
        "except ImportError as e:\n",
        "    print(f\"\u26a0\ufe0f Unsloth import issue: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ac151c",
      "metadata": {},
      "source": [
        "## Step 3: Load Base Model with Unsloth\n",
        "\n",
        "We'll use Gemma-3 1B as it's fast to fine-tune on T4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loads Gemma-3 1B base model using Unsloth's FastLanguageModel with 4-bit quantization for memory-efficient fine-tuning on T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b31df05",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udce5 LOADING BASE MODEL WITH UNSLOTH\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Model configuration\n",
        "model_name = \"unsloth/gemma-3-1b-it\"  # Small model for demo\n",
        "max_seq_length = 2048\n",
        "\n",
        "print(f\"\\n\ud83d\udce5 Loading {model_name}...\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=torch.float16,\n",
        "    load_in_4bit=True,  # Use 4-bit for training\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Model loaded!\")\n",
        "print(f\"   Model: {model_name}\")\n",
        "print(f\"   Max Sequence Length: {max_seq_length}\")\n",
        "print(f\"   Precision: 4-bit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257229ae",
      "metadata": {},
      "source": [
        "## Step 4: Add LoRA Adapters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adds LoRA (Low-Rank Adaptation) adapters to the model targeting attention and MLP layers, enabling parameter-efficient fine-tuning with only a small fraction of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ac530a",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd27 ADDING LORA ADAPTERS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,                # LoRA rank\n",
        "    lora_alpha=32,       # LoRA alpha\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Count trainable parameters\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "print(f\"\\n\u2705 LoRA adapters added!\")\n",
        "print(f\"   Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
        "print(f\"   Total params: {total:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8df930",
      "metadata": {},
      "source": [
        "## Step 5: Prepare Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loads Alpaca dataset and formats it with instruction/input/response structure suitable for supervised fine-tuning, preparing 500 examples for demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6636ad16",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca PREPARING TRAINING DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load a small dataset for demo (Alpaca format)\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:500]\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Dataset loaded: {len(dataset)} examples\")\n",
        "print(f\"\\n\ud83d\udccb Sample data:\")\n",
        "print(dataset[0])\n",
        "\n",
        "# Format for training (Alpaca prompt format)\n",
        "def format_alpaca(example):\n",
        "    instruction = example.get(\"instruction\", \"\")\n",
        "    input_text = example.get(\"input\", \"\")\n",
        "    output = example.get(\"output\", \"\")\n",
        "    \n",
        "    if input_text:\n",
        "        prompt = f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\n",
        "{output}\"\"\"\n",
        "    \n",
        "    return {\"text\": prompt}\n",
        "\n",
        "dataset = dataset.map(format_alpaca)\n",
        "print(f\"\\n\u2705 Dataset formatted for training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2480a22",
      "metadata": {},
      "source": [
        "## Step 6: Train with SFTTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trains the model using SFTTrainer with optimized settings (8-bit AdamW, gradient accumulation, FP16) for 30 steps as a quick demonstration of the fine-tuning workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4f2f9cd",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfcb\ufe0f TRAINING MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Training configuration (quick demo - increase for real training)\n",
        "training_args = SFTConfig(\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=5,\n",
        "    max_steps=30,  # Quick demo - use more for real training\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=5,\n",
        "    output_dir=\"./unsloth_output\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "print(\"\\n\ud83c\udfcb\ufe0f Starting training...\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Max steps: {training_args.max_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\u2705 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00e7361a",
      "metadata": {},
      "source": [
        "## Step 7: Export to GGUF Format\n",
        "\n",
        "This is the key step - converting from Unsloth to llama.cpp compatible format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exports the fine-tuned model to GGUF format with Q4_K_M quantization using Unsloth's built-in export functionality, making it compatible with llama.cpp and llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "966f4412",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udce6 EXPORTING TO GGUF FORMAT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Output path\n",
        "OUTPUT_DIR = \"/kaggle/working/gguf_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Unsloth's built-in GGUF export\n",
        "print(\"\\n\ud83d\udce6 Exporting to GGUF with Q4_K_M quantization...\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    OUTPUT_DIR,\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\",  # K-quant for best quality/size\n",
        ")\n",
        "\n",
        "# Find the exported file\n",
        "gguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n",
        "print(f\"\\n\u2705 GGUF export complete!\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Files: {gguf_files}\")\n",
        "\n",
        "if gguf_files:\n",
        "    gguf_path = os.path.join(OUTPUT_DIR, gguf_files[0])\n",
        "    size_mb = os.path.getsize(gguf_path) / (1024**2)\n",
        "    print(f\"   Size: {size_mb:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates a compressed tar.gz archive of the GGUF model for easy download and distribution, with alternative download methods for Kaggle environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfe910b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## Step 7.2: Create and Download GGUF Archive\n",
        "\n",
        "# %%\n",
        "import tarfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udce6 CREATING GGUF ARCHIVE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Paths\n",
        "GGUF_DIR = \"/kaggle/working/gguf_output\"\n",
        "ARCHIVE_PATH = \"/kaggle/working/fine-tuned-model.tar.gz\"\n",
        "\n",
        "# Check if GGUF directory exists and contains files\n",
        "if not os.path.exists(GGUF_DIR):\n",
        "    print(f\"\u274c GGUF directory not found: {GGUF_DIR}\")\n",
        "elif not os.listdir(GGUF_DIR):\n",
        "    print(f\"\u26a0\ufe0f GGUF directory is empty: {GGUF_DIR}\")\n",
        "else:\n",
        "    print(f\"\ud83d\udcc1 GGUF directory: {GGUF_DIR}\")\n",
        "    print(\"\ud83d\udccb Contents:\")\n",
        "    for item in os.listdir(GGUF_DIR):\n",
        "        size = os.path.getsize(os.path.join(GGUF_DIR, item)) / (1024**2)\n",
        "        print(f\"   \u2022 {item} ({size:.1f} MB)\")\n",
        "\n",
        "# Create tar.gz archive\n",
        "print(f\"\\n\ud83d\udce6 Creating archive: {ARCHIVE_PATH}\")\n",
        "try:\n",
        "    with tarfile.open(ARCHIVE_PATH, \"w:gz\") as tar:\n",
        "        tar.add(GGUF_DIR, arcname=os.path.basename(GGUF_DIR))\n",
        "    \n",
        "    archive_size = os.path.getsize(ARCHIVE_PATH) / (1024**2)\n",
        "    print(f\"\u2705 Archive created: {ARCHIVE_PATH}\")\n",
        "    print(f\"   Size: {archive_size:.1f} MB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\u274c Failed to create archive: {e}\")\n",
        "    raise\n",
        "\n",
        "# Download the archive\n",
        "\"\"\"\n",
        "print(\"\\n\u2b07\ufe0f  Downloading archive...\")\n",
        "try:\n",
        "    # For Kaggle/Colab\n",
        "    from google.colab import files\n",
        "    files.download(ARCHIVE_PATH)\n",
        "    print(\"\u2705 Download initiated!\")\n",
        "except ImportError:\n",
        "    print(\"\u26a0\ufe0f  Not in Colab environment. Archive saved locally:\")\n",
        "    print(f\"   {ARCHIVE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Could not initiate download: {e}\")\n",
        "    print(f\"\ud83d\udcc1 Archive available at: {ARCHIVE_PATH}\")\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Alternative: Provide download link in Kaggle\n",
        "print(\"\\n\ud83d\udd17 Alternative download methods:\")\n",
        "print(f\"1. Direct path: {ARCHIVE_PATH}\")\n",
        "print(\"2. In Kaggle Notebooks, use the 'Data' tab on the right\")\n",
        "print(\"3. Copy to Google Drive:\")\n",
        "print(f\"   !cp {ARCHIVE_PATH} /content/drive/MyDrive/\")\n",
        "print(\"4. Use kaggle API:\")\n",
        "print(f\"   !kaggle datasets create -p {os.path.dirname(ARCHIVE_PATH)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6705b5",
      "metadata": {},
      "source": [
        "## Step 8: Clear GPU Memory Before Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clears GPU memory by deleting training objects and emptying CUDA cache to free up VRAM for deploying the fine-tuned model with llcuda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e598c6d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "print(\"\ud83e\uddf9 Clearing GPU memory...\")\n",
        "\n",
        "# Delete training objects\n",
        "del model\n",
        "del trainer\n",
        "del tokenizer\n",
        "\n",
        "# Clear CUDA cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\ud83d\udcca GPU Memory After Cleanup:\")\n",
        "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv\n",
        "\n",
        "print(\"\\n\u2705 GPU memory cleared for inference\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dbfec0e",
      "metadata": {},
      "source": [
        "## Step 9: Deploy with llcuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Locates the exported GGUF file and deploys it using llcuda's ServerManager, starting llama-server with GPU acceleration for inference testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a45266",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# ## Step 9: Deploy with llcuda\n",
        "\n",
        "# %%\n",
        "# Find GGUF file\n",
        "import os\n",
        "gguf_path = None\n",
        "for root, dirs, files in os.walk(\"/kaggle/working\"):\n",
        "    for f in files:\n",
        "        if f.endswith(\".gguf\") and \"gemma\" in f.lower():\n",
        "            gguf_path = os.path.join(root, f)\n",
        "            break\n",
        "    if gguf_path:\n",
        "        break\n",
        "\n",
        "if gguf_path is None:\n",
        "    raise FileNotFoundError(\"\u274c Fine-tuned GGUF file not found!\")\n",
        "\n",
        "print(f\"\ud83d\udce5 Found fine-tuned model: {gguf_path}\")\n",
        "\n",
        "# %%\n",
        "from llcuda.server import ServerManager\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\ude80 DEPLOYING FINE-TUNED MODEL WITH LLCUDA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\ud83d\udce5 Loading: {gguf_path}\")\n",
        "\n",
        "server = ServerManager()\n",
        "print(\"\\n\ud83d\ude80 Starting llama-server...\")\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=gguf_path,\n",
        "        host=\"127.0.0.1\",\n",
        "        port=8090,\n",
        "        gpu_layers=32,      # GPU layers for acceleration\n",
        "        ctx_size=2048,\n",
        "        timeout=180,        # Increased timeout for startup\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Server failed to start: {e}\")\n",
        "    server.stop_server()\n",
        "    raise\n",
        "\n",
        "if server.check_server_health(timeout=180):\n",
        "    print(\"\\n\u2705 Fine-tuned model deployed with llcuda!\")\n",
        "    print(f\"   API endpoint: http://127.0.0.1:8090\")\n",
        "    print(f\"   Model: {os.path.basename(gguf_path)}\")\n",
        "    \n",
        "    # Test the server\n",
        "    print(\"\\n\ud83d\udd0d Testing server health...\")\n",
        "    if server.check_server_health():\n",
        "        print(\"   \u2705 Server is responding\")\n",
        "    else:\n",
        "        print(\"   \u274c Server not responding\")\n",
        "else:\n",
        "    print(\"\\n\u274c Server failed to start\")\n",
        "    server.stop_server()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6296af75",
      "metadata": {},
      "source": [
        "## Step 10: Test Your Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tests the fine-tuned model using both direct HTTP requests to llama-server and llcuda's client API, evaluating responses on Alpaca-formatted prompts and chat completions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3fbd86",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 10: Test Your Fine-Tuned Model\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83e\uddea TESTING FINE-TUNED MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test with direct HTTP request to llama-server (most reliable)\n",
        "import requests\n",
        "\n",
        "test_prompts = [\n",
        "    \"### Instruction:\\nExplain what machine learning is.\\n\\n### Response:\",\n",
        "    \"### Instruction:\\nWrite a short poem about coding.\\n\\n### Response:\",\n",
        "    \"### Instruction:\\nWhat are the benefits of GPU acceleration?\\n\\n### Response:\",\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    print(f\"\\n\ud83d\udccb Test {i}:\")\n",
        "    print(f\"   Prompt: {prompt[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"http://127.0.0.1:8090/completion\",\n",
        "            json={\n",
        "                \"prompt\": prompt,\n",
        "                \"max_tokens\": 100,\n",
        "                \"temperature\": 0.7,\n",
        "                \"stop\": [\"###\", \"\\n\\n\"]\n",
        "            },\n",
        "            timeout=30\n",
        "        )\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            answer = result.get(\"content\", \"\").strip()\n",
        "            print(f\"   \u2705 Response: {answer[:200]}\")\n",
        "        else:\n",
        "            print(f\"   \u274c Error: HTTP {response.status_code}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Request failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca Testing with llcuda Client API\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test with proper llcuda client API\n",
        "try:\n",
        "    from llcuda.api.client import LlamaCppClient\n",
        "    \n",
        "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
        "    \n",
        "    # Test 1: Using chat completions (correct method)\n",
        "    print(\"\\n\ud83d\udcac Chat Completion Test:\")\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    print(f\"   \u2705 Answer: {chat_response.choices[0].message.content[:200]}\")\n",
        "    \n",
        "    # Test 2: Using completion endpoint with training format\n",
        "    print(\"\\n\ud83d\udcdd Fine-tuned Format Test:\")\n",
        "    completion_response = requests.post(\n",
        "        \"http://127.0.0.1:8090/completion\",\n",
        "        json={\n",
        "            \"prompt\": \"What is machine learning?\",\n",
        "            \"max_tokens\": 50,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    if completion_response.status_code == 200:\n",
        "        result = completion_response.json()\n",
        "        print(f\"   \u2705 Direct response: {result.get('content', '')[:200]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   \u26a0\ufe0f  Error with client API: {e}\")\n",
        "\n",
        "print(\"\\n\ud83c\udfaf Fine-tuned Model Successfully Tested!\")\n",
        "print(\"   \u2705 Direct HTTP API: Working\")\n",
        "print(\"   \ud83d\udd17 Endpoint: http://127.0.0.1:8090/completion\")\n",
        "print(\"   \ud83d\udccb Format: Use Alpaca-style prompts for best results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30713f5a",
      "metadata": {},
      "source": [
        "## Step 11: Compare with Chat API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compares fine-tuned model performance between Alpaca instruction format and standard chat format, demonstrating compatibility with both /completion and /v1/chat/completions endpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa0ab695",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 11: Compare with Chat API\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcac CHAT COMPLETION COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# First, test with the original fine-tuned Alpaca format\n",
        "print(\"\\n\ud83e\uddea Test 1: Fine-tuned Alpaca Format\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "alpaca_prompt = \"### Instruction:\\nWhat did you learn during fine-tuning?\\n\\n### Response:\"\n",
        "\n",
        "alpaca_response = requests.post(\n",
        "    \"http://127.0.0.1:8090/completion\",\n",
        "    json={\n",
        "        \"prompt\": alpaca_prompt,\n",
        "        \"max_tokens\": 150,\n",
        "        \"temperature\": 0.7,\n",
        "        \"stop\": [\"###\", \"\\n\\n\"]\n",
        "    },\n",
        "    timeout=30\n",
        ")\n",
        "\n",
        "if alpaca_response.status_code == 200:\n",
        "    alpaca_result = alpaca_response.json()\n",
        "    alpaca_answer = alpaca_result.get(\"content\", \"\").strip()\n",
        "    print(f\"\ud83d\udcdd Prompt: {alpaca_prompt[:60]}...\")\n",
        "    print(f\"\u2705 Response: {alpaca_answer[:300]}\")\n",
        "    \n",
        "    if \"content\" in alpaca_result:\n",
        "        # Estimate tokens (rough approximation: 1 token \u2248 4 chars)\n",
        "        prompt_tokens_approx = len(alpaca_prompt) // 4\n",
        "        response_tokens_approx = len(alpaca_answer) // 4\n",
        "        print(f\"\ud83d\udcca Usage (approx):\")\n",
        "        print(f\"   Prompt tokens: ~{prompt_tokens_approx}\")\n",
        "        print(f\"   Completion tokens: ~{response_tokens_approx}\")\n",
        "else:\n",
        "    print(f\"\u274c Error: HTTP {alpaca_response.status_code}\")\n",
        "\n",
        "# Second, test with standard chat format\n",
        "print(\"\\n\\n\ud83e\udd16 Test 2: Standard Chat Format\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    from llcuda.api.client import LlamaCppClient\n",
        "    client = LlamaCppClient(base_url=\"http://127.0.0.1:8090\")\n",
        "    \n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=\"llama\",  # Use generic model name\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant fine-tuned on Alpaca dataset.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What did you learn during fine-tuning?\"}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    if hasattr(chat_response, 'choices') and len(chat_response.choices) > 0:\n",
        "        chat_answer = chat_response.choices[0].message.content\n",
        "        print(f\"\ud83d\udcdd Prompt: What did you learn during fine-tuning?\")\n",
        "        print(f\"\u2705 Response: {chat_answer[:300]}\")\n",
        "        \n",
        "        if hasattr(chat_response, 'usage'):\n",
        "            print(f\"\ud83d\udcca Usage (exact):\")\n",
        "            print(f\"   Prompt tokens: {chat_response.usage.prompt_tokens}\")\n",
        "            print(f\"   Completion tokens: {chat_response.usage.completion_tokens}\")\n",
        "        else:\n",
        "            print(f\"\ud83d\udcca Usage: Token information not available in response\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"\u26a0\ufe0f  Chat API test failed: {e}\")\n",
        "    \n",
        "    # Fallback to completion endpoint for chat\n",
        "    print(\"\\n\ud83d\udd04 Fallback: Using completion endpoint for chat-style\")\n",
        "    simple_prompt = \"You are a helpful assistant. What did you learn during fine-tuning?\"\n",
        "    \n",
        "    simple_response = requests.post(\n",
        "        \"http://127.0.0.1:8090/completion\",\n",
        "        json={\n",
        "            \"prompt\": simple_prompt,\n",
        "            \"max_tokens\": 150,\n",
        "            \"temperature\": 0.7\n",
        "        },\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if simple_response.status_code == 200:\n",
        "        simple_result = simple_response.json()\n",
        "        simple_answer = simple_result.get(\"content\", \"\").strip()\n",
        "        print(f\"\ud83d\udcdd Prompt: {simple_prompt[:60]}...\")\n",
        "        print(f\"\u2705 Response: {simple_answer[:300]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83d\udcca COMPARISON RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\u2705 Both Alpaca format and Chat format work with your fine-tuned model!\")\n",
        "print(\"\ud83d\udcdd Tip: Use Alpaca format for best results (matches training data)\")\n",
        "print(\"\ud83d\udd17 API Endpoints:\")\n",
        "print(\"   - /completion : For Alpaca-style prompts\")\n",
        "print(\"   - /v1/chat/completions : For OpenAI-compatible chat\")\n",
        "print(\"\\n\ud83c\udfaf Your llcuda server is fully functional!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "113ccfe2",
      "metadata": {},
      "source": [
        "## Step 12: Save Model for Later Use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creates a deployment-ready bundle by copying the GGUF model with descriptive naming, generating comprehensive metadata JSON, and packaging everything into a tar.gz archive with usage instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f714ec49",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Step 12: Save and Document Your Fine-Tuned Model\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcbe SAVING AND DOCUMENTING FINE-TUNED MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Find the GGUF file (created by Unsloth export)\n",
        "OUTPUT_DIR = \"/kaggle/working/gguf_output\"\n",
        "gguf_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.gguf')]\n",
        "\n",
        "if not gguf_files:\n",
        "    # Search in working directory as backup\n",
        "    for root, dirs, files in os.walk(\"/kaggle/working\"):\n",
        "        for f in files:\n",
        "            if f.endswith(\".gguf\") and \"gemma\" in f.lower():\n",
        "                gguf_files = [os.path.join(root, f)]\n",
        "                break\n",
        "\n",
        "if gguf_files:\n",
        "    if isinstance(gguf_files[0], str) and os.path.isabs(gguf_files[0]):\n",
        "        src = gguf_files[0]\n",
        "    else:\n",
        "        src = os.path.join(OUTPUT_DIR, gguf_files[0])\n",
        "    \n",
        "    # Create descriptive filename\n",
        "    model_name = \"my-finetuned-gemma-3-1b\"\n",
        "    dst = f\"/kaggle/working/{model_name}-Q4_K_M.gguf\"\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcc2 Found GGUF file: {os.path.basename(src)}\")\n",
        "    print(f\"   Source: {src}\")\n",
        "    print(f\"   Size: {os.path.getsize(src) / (1024**2):.1f} MB\")\n",
        "    \n",
        "    # Copy to working directory\n",
        "    shutil.copy(src, dst)\n",
        "    \n",
        "    print(f\"\\n\u2705 Model saved to: {dst}\")\n",
        "    print(f\"   Saved size: {os.path.getsize(dst) / (1024**2):.1f} MB\")\n",
        "    \n",
        "    # Create model metadata file\n",
        "    metadata = {\n",
        "        \"model_name\": model_name,\n",
        "        \"base_model\": \"unsloth/gemma-3-1b-it\",\n",
        "        \"quantization\": \"Q4_K_M\",\n",
        "        \"training_data\": \"Alpaca-cleaned (500 samples)\",\n",
        "        \"training_config\": {\n",
        "            \"lora_rank\": 16,\n",
        "            \"lora_alpha\": 32,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"batch_size\": 2,\n",
        "            \"learning_rate\": 2e-4,\n",
        "            \"trained_steps\": 30\n",
        "        },\n",
        "        \"export_format\": \"GGUF\",\n",
        "        \"export_tool\": \"Unsloth + llama.cpp\",\n",
        "        \"deployment_engine\": \"llcuda v2.2.0\",\n",
        "        \"api_endpoints\": {\n",
        "            \"completion\": \"http://127.0.0.1:8090/completion\",\n",
        "            \"chat\": \"http://127.0.0.1:8090/v1/chat/completions\"\n",
        "        },\n",
        "        \"optimal_prompt_format\": \"### Instruction:\\\\n{instruction}\\\\n\\\\n### Response:\",\n",
        "        \"file_size_mb\": round(os.path.getsize(dst) / (1024**2), 2),\n",
        "        \"created\": \"2026-01-19\",\n",
        "        \"environment\": \"Kaggle 2x Tesla T4\"\n",
        "    }\n",
        "    \n",
        "    metadata_path = f\"/kaggle/working/{model_name}-metadata.json\"\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcdd Metadata saved: {metadata_path}\")\n",
        "    \n",
        "    # Create archive for easy download\n",
        "    import tarfile\n",
        "    archive_path = f\"/kaggle/working/{model_name}-bundle.tar.gz\"\n",
        "    with tarfile.open(archive_path, \"w:gz\") as tar:\n",
        "        tar.add(dst, arcname=os.path.basename(dst))\n",
        "        tar.add(metadata_path, arcname=os.path.basename(metadata_path))\n",
        "    \n",
        "    print(f\"\\n\ud83d\udce6 Archive created: {archive_path}\")\n",
        "    print(f\"   Archive size: {os.path.getsize(archive_path) / (1024**2):.1f} MB\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\ud83d\udd27 USAGE INSTRUCTIONS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 Quick Start with llcuda:\")\n",
        "    print(\"```python\")\n",
        "    print(\"from llcuda.server import ServerManager\")\n",
        "    print(\"\")\n",
        "    print(\"# Start server\")\n",
        "    print(\"server = ServerManager()\")\n",
        "    print(f\"server.start_server(model_path='{dst}', port=8090)\")\n",
        "    print(\"\")\n",
        "    print(\"# Test with Alpaca format (recommended)\")\n",
        "    print('prompt = \"### Instruction:\\\\\\\\nYour question here\\\\\\\\n\\\\\\\\n### Response:\"')\n",
        "    print(\"import requests\")\n",
        "    print(\"response = requests.post('http://127.0.0.1:8090/completion',\")\n",
        "    print(\"  json={'prompt': prompt, 'max_tokens': 100, 'temperature': 0.7})\")\n",
        "    print(\"```\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udca1 Quick Start with llcuda Client API:\")\n",
        "    print(\"```python\")\n",
        "    print(\"from llcuda.api.client import LlamaCppClient\")\n",
        "    print(\"\")\n",
        "    print(\"client = LlamaCppClient(base_url='http://127.0.0.1:8090')\")\n",
        "    print(\"response = client.chat.completions.create(\")\n",
        "    print(\"  model='gemma-3-1b',\")\n",
        "    print(\"  messages=[{'role': 'user', 'content': 'Your question'}],\")\n",
        "    print(\"  max_tokens=100\")\n",
        "    print(\")\")\n",
        "    print(\"```\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Model Performance:\")\n",
        "    print(\"   \u2705 Supports both /completion and /v1/chat/completions endpoints\")\n",
        "    print(\"   \u2705 Works with Alpaca-style prompts\")\n",
        "    print(\"   \u2705 Token usage tracking available\")\n",
        "    print(\"   \u2705 GPU accelerated with llcuda\")\n",
        "    \n",
        "    print(\"\\n\ud83c\udfaf Next Steps:\")\n",
        "    print(\"   1. Download the archive from Kaggle Data tab\")\n",
        "    print(\"   2. Deploy on any system with llcuda installed\")\n",
        "    print(\"   3. Use with Ollama: ollama create my-model -f ./Modelfile\")\n",
        "    print(\"   4. Share via HuggingFace Hub\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n\u274c No GGUF file found. Please check the export in Step 7.\")\n",
        "    print(\"   Expected in: /kaggle/working/ or /kaggle/working/gguf_output/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7066e68",
      "metadata": {},
      "source": [
        "## Step 13: Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stops the llama-server and displays final GPU memory status to confirm all resources have been properly released after the fine-tuning and deployment workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a972de0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\uded1 Stopping server...\")\n",
        "server.stop_server()\n",
        "\n",
        "print(\"\\n\u2705 Server stopped\")\n",
        "print(\"\\n\ud83d\udcca Final GPU Status:\")\n",
        "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35c6449",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Summary\n",
        "\n",
        "### Complete Workflow:\n",
        "1. \u2705 Installed Unsloth + llcuda\n",
        "2. \u2705 Loaded base model with 4-bit quantization\n",
        "3. \u2705 Added LoRA adapters for efficient training\n",
        "4. \u2705 Fine-tuned on custom dataset\n",
        "5. \u2705 Exported to GGUF (Q4_K_M)\n",
        "6. \u2705 Deployed with llcuda llama-server\n",
        "7. \u2705 Ran inference on fine-tuned model\n",
        "\n",
        "### Key llcuda + Unsloth Integration:\n",
        "\n",
        "```python\n",
        "from llcuda.unsloth import export_to_llcuda\n",
        "\n",
        "# After Unsloth training\n",
        "export_to_llcuda(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    output_path=\"my-model.gguf\",\n",
        "    quant_type=\"Q4_K_M\"\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** [06-split-gpu-graphistry](06-split-gpu-graphistry-llcuda-v2.2.0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}