{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "81a43b95",
      "metadata": {},
      "source": [
        "## Step 1: Install llcuda and Check Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installs llcuda v2.2.0 with fresh binaries and verifies GPU availability by displaying GPU index, name, and total memory for GGUF quantization testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90338ebb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install llcuda v2.2.0 (force fresh install to ensure correct binaries)\n",
        "!pip install -q --no-cache-dir --force-reinstall git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
        "\n",
        "import llcuda\n",
        "print(f\"\u2705 llcuda {llcuda.__version__} installed\")\n",
        "\n",
        "# Check GPU\n",
        "print(\"\\n\ud83d\udcca GPU Info:\")\n",
        "!nvidia-smi --query-gpu=index,name,memory.total --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8008eb9",
      "metadata": {},
      "source": [
        "## Step 2: Understanding Quantization Types\n",
        "\n",
        "GGUF supports multiple quantization types, organized into families:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displays comprehensive GGUF quantization type reference organized by family (Legacy, K-Quants, I-Quants, Full Precision) with bits/weight, quality scores, and imatrix requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e7f9b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.gguf import QUANT_TYPE_INFO, QuantTypeInfo\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udccb GGUF QUANTIZATION TYPES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Group by family\n",
        "families = {\n",
        "    \"Legacy (Basic)\": [\"Q4_0\", \"Q4_1\", \"Q5_0\", \"Q5_1\", \"Q8_0\"],\n",
        "    \"K-Quants (Recommended)\": [\"Q2_K\", \"Q3_K_S\", \"Q3_K_M\", \"Q3_K_L\", \"Q4_K_S\", \"Q4_K_M\", \"Q5_K_S\", \"Q5_K_M\", \"Q6_K\"],\n",
        "    \"I-Quants (Ultra-Low)\": [\"IQ2_XXS\", \"IQ2_XS\", \"IQ2_S\", \"IQ3_XXS\", \"IQ3_XS\", \"IQ3_S\", \"IQ3_M\", \"IQ4_XS\", \"IQ4_NL\"],\n",
        "    \"Full Precision\": [\"F16\", \"F32\", \"BF16\"],\n",
        "}\n",
        "\n",
        "for family_name, types in families.items():\n",
        "    print(f\"\\n\ud83d\udd39 {family_name}:\")\n",
        "    print(f\"   {'Type':<12} {'Bits/Weight':<12} {'Quality':<10} {'Notes'}\")\n",
        "    print(f\"   {'-'*60}\")\n",
        "    \n",
        "    for qtype in types:\n",
        "        if qtype in QUANT_TYPE_INFO:\n",
        "            info = QUANT_TYPE_INFO[qtype]\n",
        "            quality_stars = \"\u2605\" * int(info.quality_score * 5)\n",
        "            notes = \"Needs imatrix\" if info.requires_imatrix else \"\"\n",
        "            print(f\"   {qtype:<12} {info.bits_per_weight:<12.2f} {quality_stars:<10} {notes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b9033a",
      "metadata": {},
      "source": [
        "## Step 3: Quantization Size Calculator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculates estimated GGUF file sizes for common models (1B to 70B parameters) across different quantization types to help predict VRAM requirements and storage needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4634fc0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.gguf import estimate_gguf_size\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcca GGUF SIZE CALCULATOR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Common model sizes\n",
        "model_sizes = {\n",
        "    \"Gemma-3 1B\": 1,\n",
        "    \"Gemma-3 4B\": 4,\n",
        "    \"Llama-3.2 3B\": 3,\n",
        "    \"Llama-3.1 8B\": 8,\n",
        "    \"Qwen2.5 14B\": 14,\n",
        "    \"Llama-3.1 70B\": 70,\n",
        "}\n",
        "\n",
        "# Quantization types to compare\n",
        "quant_types = [\"Q4_K_M\", \"Q5_K_M\", \"Q6_K\", \"Q8_0\", \"IQ3_XS\", \"F16\"]\n",
        "\n",
        "print(f\"\\n{'Model':<18} | \", end=\"\")\n",
        "for qt in quant_types:\n",
        "    print(f\"{qt:<10}\", end=\"\")\n",
        "print()\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for model_name, params_b in model_sizes.items():\n",
        "    print(f\"{model_name:<18} | \", end=\"\")\n",
        "    for qt in quant_types:\n",
        "        size_gb = estimate_gguf_size(params_b, qt)\n",
        "        print(f\"{size_gb:<10.1f}\", end=\"\")\n",
        "    print(\" GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607b25c2",
      "metadata": {},
      "source": [
        "## Step 4: Kaggle T4 Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provides quantization recommendations for various model sizes on single T4 (15GB) and dual T4 (30GB) configurations, indicating which models fit and optimal quantization types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d565caa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.gguf import recommend_quant_for_kaggle, estimate_gguf_size\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83c\udfaf KAGGLE T4 QUANTIZATION RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test various model sizes (in billions)\n",
        "test_models = [\n",
        "    (\"Gemma-3 1B\", 1_000_000_000),\n",
        "    (\"Llama-3.2 3B\", 3_000_000_000),\n",
        "    (\"Gemma-3 4B\", 4_000_000_000),\n",
        "    (\"Llama-3.1 8B\", 8_000_000_000),\n",
        "    (\"Qwen2.5 14B\", 14_000_000_000),\n",
        "    (\"Llama-3.1 70B\", 70_000_000_000),\n",
        "]\n",
        "\n",
        "print(\"\\n\ud83d\udd39 Single T4 (15GB VRAM):\")\n",
        "print(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Fits?'}\")\n",
        "print(f\"   {'-'*55}\")\n",
        "\n",
        "for model_name, params in test_models:\n",
        "    rec = recommend_quant_for_kaggle(params, dual_t4=False)\n",
        "    if rec['fits']:\n",
        "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     \u2705\")\n",
        "    else:\n",
        "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     \u274c Too large\")\n",
        "\n",
        "print(\"\\n\ud83d\udd39 Dual T4 (30GB VRAM):\")\n",
        "print(f\"   {'Model':<18} {'Recommended':<12} {'Est. Size':<10} {'Fits?'}\")\n",
        "print(f\"   {'-'*55}\")\n",
        "\n",
        "for model_name, params in test_models:\n",
        "    rec = recommend_quant_for_kaggle(params, dual_t4=True)\n",
        "    if rec['fits']:\n",
        "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     \u2705\")\n",
        "    else:\n",
        "        print(f\"   {model_name:<18} {rec['quant_type']:<12} {rec['estimated_size_gb']:.1f} GB     \u274c Too large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7001e562",
      "metadata": {},
      "source": [
        "## Step 5: K-Quants Deep Dive\n",
        "\n",
        "K-Quants are the recommended choice for most use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explains K-Quants mixed-precision approach with detailed guidance on Q4_K_M, Q5_K_M, Q6_K, and Q3_K_M variants including use cases, quality trade-offs, and size comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f961c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcd8 K-QUANTS DETAILED GUIDE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "k_quant_guide = \"\"\"\n",
        "K-Quants use a sophisticated mixed-precision approach:\n",
        "- Attention layers: Higher precision (more important for quality)\n",
        "- Feed-forward layers: Lower precision (less sensitive)\n",
        "\n",
        "\ud83d\udd39 Naming Convention:\n",
        "   Q{bits}_K_{size}\n",
        "   \u2514\u2500 bits: Base quantization (2,3,4,5,6)\n",
        "      \u2514\u2500 K: K-quant family marker\n",
        "         \u2514\u2500 size: S=Small, M=Medium, L=Large\n",
        "\n",
        "\ud83d\udd39 Recommended K-Quants:\n",
        "\n",
        "   Q4_K_M (4.85 bits/weight) \u2b50 RECOMMENDED\n",
        "   \u251c\u2500\u2500 Best balance of size and quality\n",
        "   \u251c\u2500\u2500 ~30% smaller than FP16\n",
        "   \u2514\u2500\u2500 Minimal quality loss\n",
        "\n",
        "   Q5_K_M (5.69 bits/weight) - HIGH QUALITY\n",
        "   \u251c\u2500\u2500 Better quality than Q4_K_M\n",
        "   \u251c\u2500\u2500 Good for creative writing\n",
        "   \u2514\u2500\u2500 ~20% larger than Q4_K_M\n",
        "\n",
        "   Q6_K (6.59 bits/weight) - NEAR LOSSLESS\n",
        "   \u251c\u2500\u2500 Almost FP16 quality\n",
        "   \u251c\u2500\u2500 Good for technical tasks\n",
        "   \u2514\u2500\u2500 ~35% larger than Q4_K_M\n",
        "\n",
        "   Q3_K_M (3.89 bits/weight) - MEMORY SAVER\n",
        "   \u251c\u2500\u2500 For larger models on limited VRAM\n",
        "   \u251c\u2500\u2500 Some quality degradation\n",
        "   \u2514\u2500\u2500 ~20% smaller than Q4_K_M\n",
        "\"\"\"\n",
        "print(k_quant_guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a9e958",
      "metadata": {},
      "source": [
        "## Step 6: I-Quants for Large Models\n",
        "\n",
        "I-Quants enable running 70B+ models on limited hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describes I-Quants (Importance-Matrix Quants) for extreme compression, focusing on IQ3_XS for running 70B models on dual T4 GPUs with importance matrix requirements and quality considerations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f2704f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcd8 I-QUANTS FOR LARGE MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "i_quant_guide = \"\"\"\n",
        "I-Quants (Importance-Matrix Quants) use importance matrices\n",
        "to determine which weights are most critical for quality.\n",
        "\n",
        "\ud83d\udd39 Key Requirements:\n",
        "   \u26a0\ufe0f  Require importance matrix (imatrix) for good quality\n",
        "   \u26a0\ufe0f  Without imatrix, quality suffers significantly\n",
        "   \u2705 Pre-made imatrix GGUFs are available on HuggingFace\n",
        "\n",
        "\ud83d\udd39 I-Quant Types:\n",
        "\n",
        "   IQ3_XS (3.30 bits/weight) \u2b50 BEST FOR 70B\n",
        "   \u251c\u2500\u2500 Fits 70B models in ~25GB VRAM\n",
        "   \u251c\u2500\u2500 Surprisingly good quality with imatrix\n",
        "   \u2514\u2500\u2500 Ideal for Kaggle dual T4 (30GB)\n",
        "\n",
        "   IQ4_XS (4.25 bits/weight) - HIGH QUALITY LOW SIZE\n",
        "   \u251c\u2500\u2500 Better than Q4_K_M in some benchmarks\n",
        "   \u251c\u2500\u2500 Slightly smaller file size\n",
        "   \u2514\u2500\u2500 Great for 13B-34B models\n",
        "\n",
        "   IQ2_XS (2.31 bits/weight) - EXTREME COMPRESSION\n",
        "   \u251c\u2500\u2500 For 70B+ when VRAM is very limited\n",
        "   \u251c\u2500\u2500 Noticeable quality degradation\n",
        "   \u2514\u2500\u2500 Use only when necessary\n",
        "\n",
        "\ud83d\udd39 70B Model on Kaggle Dual T4:\n",
        "   Model: Llama-3.1-70B-Instruct-GGUF\n",
        "   Quant: IQ3_XS (~25GB)\n",
        "   Config: tensor-split 0.5,0.5\n",
        "   Context: 2048 (limited by VRAM)\n",
        "\"\"\"\n",
        "print(i_quant_guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def3b025",
      "metadata": {},
      "source": [
        "## Step 7: Interactive Quant Selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prints llcuda's interactive quantization guide providing comprehensive reference for selecting optimal quantization types based on model size, VRAM constraints, and quality requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91968e13",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.gguf import print_quant_guide\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83c\udfaf INTERACTIVE QUANTIZATION GUIDE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Print the full guide\n",
        "print_quant_guide()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5936cbe2",
      "metadata": {},
      "source": [
        "## Step 8: Download and Test Different Quantizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloads multiple quantization variants (Q4_K_M, Q5_K_M, Q8_0) of Gemma-3 1B from HuggingFace for side-by-side comparison testing and benchmarking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d785df8e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udce5 DOWNLOAD GGUF MODELS FOR COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Available Gemma-3 1B quantizations from Unsloth\n",
        "models_to_test = {\n",
        "    \"Q4_K_M\": \"gemma-3-1b-it-Q4_K_M.gguf\",\n",
        "    \"Q5_K_M\": \"gemma-3-1b-it-Q5_K_M.gguf\",\n",
        "    \"Q8_0\": \"gemma-3-1b-it-Q8_0.gguf\",\n",
        "}\n",
        "\n",
        "REPO = \"unsloth/gemma-3-1b-it-GGUF\"\n",
        "MODEL_DIR = \"/kaggle/working/models\"\n",
        "\n",
        "print(f\"\\n\ud83d\udce5 Downloading from {REPO}...\\n\")\n",
        "\n",
        "downloaded = {}\n",
        "for quant, filename in models_to_test.items():\n",
        "    print(f\"   Downloading {quant}...\", end=\" \")\n",
        "    try:\n",
        "        path = hf_hub_download(\n",
        "            repo_id=REPO,\n",
        "            filename=filename,\n",
        "            local_dir=MODEL_DIR\n",
        "        )\n",
        "        size_mb = os.path.getsize(path) / (1024**2)\n",
        "        downloaded[quant] = path\n",
        "        print(f\"\u2705 {size_mb:.0f} MB\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c {e}\")\n",
        "\n",
        "print(f\"\\n\u2705 Downloaded {len(downloaded)} models for comparison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223a69b6",
      "metadata": {},
      "source": [
        "## Step 9: Benchmark Different Quantizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks inference performance across different quantization types by starting separate servers for each variant, measuring tokens/second, and comparing quality versus speed trade-offs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d856fda5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.server import ServerManager\n",
        "from llcuda.api.client import LlamaCppClient\n",
        "import time\n",
        "import socket\n",
        "\n",
        "def get_free_port():\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcca QUANTIZATION BENCHMARK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "test_prompt = \"Explain what CUDA is in exactly 3 sentences.\"\n",
        "results = {}\n",
        "\n",
        "for quant, model_path in downloaded.items():\n",
        "    print(f\"\\n\ud83d\udd39 Testing {quant}...\")\n",
        "    port = get_free_port()\n",
        "    server = ServerManager(server_url=f\"http://127.0.0.1:{port}\")\n",
        "    try:\n",
        "        server.start_server(\n",
        "            model_path=model_path,\n",
        "            host=\"127.0.0.1\",\n",
        "            port=port,\n",
        "            gpu_layers=32,\n",
        "            ctx_size=2048,\n",
        "            timeout=120,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Failed to start: {e}\")\n",
        "        if hasattr(server, 'server_process') and server.server_process:\n",
        "            try:\n",
        "                _, err = server.server_process.communicate(timeout=5)\n",
        "                if err:\n",
        "                    print('   [Server stderr]:', err.decode(errors='ignore'))\n",
        "            except Exception:\n",
        "                pass\n",
        "        continue\n",
        "    if not server.check_server_health(timeout=120):\n",
        "        print(f\"   \u274c Server not healthy\")\n",
        "        server.stop_server()\n",
        "        continue\n",
        "    client = LlamaCppClient(base_url=f\"http://127.0.0.1:{port}\")\n",
        "    try:\n",
        "        client.chat.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "            max_tokens=10\n",
        "        )\n",
        "        start = time.time()\n",
        "        response = client.chat.create(\n",
        "            messages=[{\"role\": \"user\", \"content\": test_prompt}],\n",
        "            max_tokens=100,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        elapsed = time.time() - start\n",
        "        tokens = response.usage.completion_tokens\n",
        "        tok_per_sec = tokens / elapsed\n",
        "        results[quant] = {\n",
        "            \"tokens\": tokens,\n",
        "            \"time\": elapsed,\n",
        "            \"tok_per_sec\": tok_per_sec,\n",
        "            \"response\": response.choices[0].message.content[:100]\n",
        "        }\n",
        "        print(f\"   \u2713 Tokens: {tokens}, Time: {elapsed:.2f}s, Speed: {tok_per_sec:.1f} tok/s\")\n",
        "    except Exception as e:\n",
        "        print(f\"   \u274c Inference failed: {e}\")\n",
        "    server.stop_server()\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83d\udcca BENCHMARK SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "if results:\n",
        "    for quant, data in results.items():\n",
        "        print(f\"   {quant:<12}: {data['tok_per_sec']:.1f} tok/s\")\n",
        "else:\n",
        "    print(\"   No successful benchmarks completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b2e6c06",
      "metadata": {},
      "source": [
        "## Step 10: Creating Custom Quantizations\n",
        "\n",
        "Use llama-quantize to create your own quantized models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provides reference guide for creating custom GGUF quantizations using llama-quantize, including importance matrix generation for I-quants and llcuda API usage examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "997e52aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udd27 CREATING CUSTOM QUANTIZATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "quantize_guide = \"\"\"\n",
        "To quantize a model yourself, use llama-quantize:\n",
        "\n",
        "\ud83d\udd39 Basic Usage:\n",
        "   llama-quantize input.gguf output.gguf Q4_K_M\n",
        "\n",
        "\ud83d\udd39 With Importance Matrix (for I-quants):\n",
        "   # First, generate importance matrix\n",
        "   llama-imatrix -m model.gguf -f calibration.txt -o imatrix.dat\n",
        "   \n",
        "   # Then quantize with imatrix\n",
        "   llama-quantize --imatrix imatrix.dat input.gguf output.gguf IQ3_XS\n",
        "\n",
        "\ud83d\udd39 Available in llcuda:\n",
        "   from llcuda.quantization import quantize_model\n",
        "   \n",
        "   quantize_model(\n",
        "       input_path=\"model-f16.gguf\",\n",
        "       output_path=\"model-q4km.gguf\",\n",
        "       quant_type=\"Q4_K_M\"\n",
        "   )\n",
        "\n",
        "\ud83d\udd39 From Unsloth/HuggingFace:\n",
        "   Most models on HuggingFace are already pre-quantized.\n",
        "   Look for repos with '-GGUF' suffix:\n",
        "   - unsloth/gemma-3-4b-it-GGUF\n",
        "   - unsloth/Llama-3.2-3B-Instruct-GGUF\n",
        "   - bartowski/Qwen2.5-14B-Instruct-GGUF\n",
        "\"\"\"\n",
        "print(quantize_guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a376df4",
      "metadata": {},
      "source": [
        "## Step 11: Quick Reference Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Displays quick reference table mapping model sizes and quantization types to Kaggle T4 feasibility (single/dual GPU), with size estimates and practical deployment notes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35089e61",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udccb QUICK REFERENCE: MODEL + QUANT \u2192 KAGGLE FEASIBILITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "reference = \"\"\"\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Model       \u2502 Quant     \u2502 Size (GB) \u2502 Kaggle T4  \u2502 Notes                  \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 1B params   \u2502 Q4_K_M    \u2502 0.6       \u2502 \u2705 Single  \u2502 Fast, high quality     \u2502\n",
        "\u2502 1B params   \u2502 Q8_0      \u2502 1.1       \u2502 \u2705 Single  \u2502 Best quality for 1B    \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 3B params   \u2502 Q4_K_M    \u2502 1.8       \u2502 \u2705 Single  \u2502 Recommended            \u2502\n",
        "\u2502 3B params   \u2502 Q5_K_M    \u2502 2.1       \u2502 \u2705 Single  \u2502 Higher quality         \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 4B params   \u2502 Q4_K_M    \u2502 2.4       \u2502 \u2705 Single  \u2502 \u2b50 Sweet spot          \u2502\n",
        "\u2502 4B params   \u2502 Q6_K      \u2502 3.3       \u2502 \u2705 Single  \u2502 Near lossless          \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 7-8B params \u2502 Q4_K_M    \u2502 4.5       \u2502 \u2705 Single  \u2502 Popular choice         \u2502\n",
        "\u2502 7-8B params \u2502 Q5_K_M    \u2502 5.3       \u2502 \u2705 Single  \u2502 Better for coding      \u2502\n",
        "\u2502 7-8B params \u2502 Q6_K      \u2502 6.0       \u2502 \u26a0\ufe0f Single  \u2502 Tight fit, 4K ctx      \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 13-14B      \u2502 Q4_K_M    \u2502 8.0       \u2502 \u26a0\ufe0f Single  \u2502 2K context max         \u2502\n",
        "\u2502 13-14B      \u2502 Q4_K_M    \u2502 8.0       \u2502 \u2705 Dual    \u2502 Split across GPUs      \u2502\n",
        "\u2502 13-14B      \u2502 IQ3_XS    \u2502 5.5       \u2502 \u2705 Single  \u2502 Quality trade-off      \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 30-34B      \u2502 Q4_K_M    \u2502 19        \u2502 \u2705 Dual    \u2502 tensor-split 0.5,0.5   \u2502\n",
        "\u2502 30-34B      \u2502 IQ3_XS    \u2502 12        \u2502 \u26a0\ufe0f Single  \u2502 Low context            \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 70B params  \u2502 IQ3_XS    \u2502 25        \u2502 \u2705 Dual    \u2502 \u2b50 70B on Kaggle!      \u2502\n",
        "\u2502 70B params  \u2502 IQ2_XS    \u2502 19        \u2502 \u2705 Dual    \u2502 Lower quality          \u2502\n",
        "\u2502 70B params  \u2502 Q4_K_M    \u2502 40        \u2502 \u274c         \u2502 Too large              \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "Legend:\n",
        "  \u2705 Works well    \u26a0\ufe0f Possible with limits    \u274c Won't fit\n",
        "\"\"\"\n",
        "print(reference)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "837c8bdb",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Summary\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Q4_K_M** is the recommended default - best balance of size and quality\n",
        "2. **Q5_K_M** for better quality when VRAM allows\n",
        "3. **IQ3_XS** for fitting large models (70B) on limited hardware\n",
        "4. Always check HuggingFace for pre-quantized models (faster than doing it yourself)\n",
        "\n",
        "### Kaggle T4 Guidelines:\n",
        "- Single T4 (15GB): Up to 8B Q4_K_M comfortably\n",
        "- Dual T4 (30GB): Up to 70B IQ3_XS or 34B Q4_K_M\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** [05-unsloth-integration](05-unsloth-integration-llcuda-v2.2.0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}