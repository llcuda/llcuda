{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a8cf5402",
      "metadata": {},
      "source": [
        "## Step 1: Verify Dual GPU Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifies the dual T4 GPU setup on Kaggle by enumerating available GPUs, computing total VRAM (30GB), and checking CUDA version for multi-GPU inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5253aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd0d DUAL GPU ENVIRONMENT CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get GPU info\n",
        "result = subprocess.run(\n",
        "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.total,compute_cap\", \"--format=csv,noheader\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "gpus = result.stdout.strip().split('\\n')\n",
        "print(f\"\\n\ud83d\udcca Detected {len(gpus)} GPU(s):\")\n",
        "for gpu in gpus:\n",
        "    print(f\"   {gpu}\")\n",
        "\n",
        "if len(gpus) >= 2:\n",
        "    print(\"\\n\u2705 Dual T4 environment confirmed!\")\n",
        "    print(\"   Total VRAM: 30GB (15GB \u00d7 2)\")\n",
        "else:\n",
        "    print(\"\\n\u26a0\ufe0f Only 1 GPU detected!\")\n",
        "    print(\"   Enable 'GPU T4 x2' in Kaggle settings.\")\n",
        "\n",
        "# CUDA version\n",
        "print(\"\\n\ud83d\udcca CUDA Version:\")\n",
        "!nvcc --version | grep release"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfa5ca7c",
      "metadata": {},
      "source": [
        "## Step 2: Install llcuda v2.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installs llcuda v2.2.0 without force-reinstall to preserve RAPIDS/cuDF compatibility by avoiding numpy reinstallation that breaks C extensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fb3eff5",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "# Install llcuda v2.2.0\n",
        "# NOTE: We avoid --force-reinstall here to preserve RAPIDS compatibility.\n",
        "# Using --force-reinstall reinstalls numpy which breaks cupy/cudf C extensions.\n",
        "!pip install -q --no-cache-dir git+https://github.com/llcuda/llcuda.git@v2.2.0\n",
        "!pip install -q huggingface_hub sseclient-py\n",
        "\n",
        "import llcuda\n",
        "print(f\"\u2705 llcuda {llcuda.__version__} installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a2febcb",
      "metadata": {},
      "source": [
        "## Step 3: Understanding Multi-GPU Options\n",
        "\n",
        "llama.cpp provides several flags for multi-GPU configuration:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explains llama.cpp multi-GPU configuration flags including tensor-split (VRAM distribution), split-mode (layer/row/none), main-gpu selection, and layer offloading parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c722131f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.multigpu import MultiGPUConfig, SplitMode, GPUInfo\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb MULTI-GPU CONFIGURATION OPTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "\ud83d\udd39 --tensor-split, -ts\n",
        "   Distributes VRAM usage across GPUs.\n",
        "   Example: --tensor-split 0.5,0.5 (50% each GPU)\n",
        "   Example: --tensor-split 0.7,0.3 (70% GPU0, 30% GPU1)\n",
        "\n",
        "\ud83d\udd39 --split-mode, -sm  \n",
        "   How to split the model across GPUs:\n",
        "   \u2022 'layer' - Split by transformer layers (default, recommended)\n",
        "   \u2022 'row'   - Split by matrix rows (can be slower)\n",
        "   \u2022 'none'  - Disable multi-GPU (single GPU only)\n",
        "\n",
        "\ud83d\udd39 --main-gpu, -mg\n",
        "   Primary GPU for small tensors and scratch buffers.\n",
        "   Default: 0 (first GPU)\n",
        "\n",
        "\ud83d\udd39 --n-gpu-layers, -ngl\n",
        "   Number of layers to offload to GPU(s).\n",
        "   Use 99 to offload all layers.\n",
        "\"\"\")\n",
        "\n",
        "# Show split mode enum\n",
        "print(\"\\n\ud83d\udccb Split Modes:\")\n",
        "for mode in SplitMode:\n",
        "    print(f\"   {mode.name}: {mode.value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdfed953",
      "metadata": {},
      "source": [
        "## Step 4: GPU Info Utility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uses llcuda's GPU detection API to query detailed information about each GPU including total/free VRAM, compute capability, and calculates total available memory across all GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058365ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.multigpu import detect_gpus, get_free_vram\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca GPU INFORMATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get detailed GPU info using detect_gpus (actual API function)\n",
        "gpus = detect_gpus()\n",
        "\n",
        "for gpu in gpus:\n",
        "    print(f\"\\n\ud83d\udd39 GPU {gpu.id}: {gpu.name}\")\n",
        "    print(f\"   Total VRAM: {gpu.memory_total_gb:.1f} GB\")\n",
        "    print(f\"   Free VRAM: {gpu.memory_free_gb:.1f} GB\")\n",
        "    if gpu.compute_capability:\n",
        "        print(f\"   Compute Capability: {gpu.compute_capability}\")\n",
        "\n",
        "# Calculate total available VRAM\n",
        "total_vram = sum(gpu.memory_free_gb for gpu in gpus)\n",
        "print(f\"\\n\ud83d\udcca Total Available VRAM: {total_vram:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9140cd97",
      "metadata": {},
      "source": [
        "## Step 5: Download a Larger Model for Multi-GPU Testing\n",
        "\n",
        "We'll use Gemma-3-4B which benefits from dual-GPU distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloads Gemma-3-4B Q4_K_M GGUF model (2.5GB) optimized for multi-GPU testing, which benefits from distribution across dual T4 GPUs for improved performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4b9bc3",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "# For multi-GPU testing, use a 4B model\n",
        "MODEL_REPO = \"unsloth/gemma-3-4b-it-GGUF\"\n",
        "MODEL_FILE = \"gemma-3-4b-it-Q4_K_M.gguf\"\n",
        "\n",
        "print(f\"\ud83d\udce5 Downloading {MODEL_FILE}...\")\n",
        "print(f\"   This ~2.5GB model will be split across both GPUs.\")\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=MODEL_REPO,\n",
        "    filename=MODEL_FILE,\n",
        "    local_dir=\"/kaggle/working/models\"\n",
        ")\n",
        "\n",
        "size_gb = os.path.getsize(model_path) / (1024**3)\n",
        "print(f\"\\n\u2705 Model downloaded: {model_path}\")\n",
        "print(f\"   Size: {size_gb:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d35c0e9",
      "metadata": {},
      "source": [
        "## Step 6: Tensor-Split Configurations\n",
        "\n",
        "Different ways to distribute the model across GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Demonstrates different tensor-split configurations (50/50 equal, 70/30 GPU0-heavy, 100/0 single-GPU) with use cases for balanced workloads, mixed tasks, and reserved GPU scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1a16bd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.api.multigpu import MultiGPUConfig, SplitMode\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udccb TENSOR-SPLIT CONFIGURATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "configs = [\n",
        "    {\n",
        "        \"name\": \"Equal Split (50/50)\",\n",
        "        \"tensor_split\": [0.5, 0.5],\n",
        "        \"description\": \"Equal distribution across both GPUs\",\n",
        "        \"use_case\": \"Default, balanced workload\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPU 0 Heavy (70/30)\",\n",
        "        \"tensor_split\": [0.7, 0.3],\n",
        "        \"description\": \"More VRAM on GPU 0\",\n",
        "        \"use_case\": \"When GPU 1 needed for other tasks\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GPU 0 Only (100/0)\",\n",
        "        \"tensor_split\": [1.0, 0.0],\n",
        "        \"description\": \"Single GPU mode\",\n",
        "        \"use_case\": \"When GPU 1 reserved for RAPIDS/Graphistry\"\n",
        "    },\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs, 1):\n",
        "    print(f\"\\n\ud83d\udd39 Config {i}: {config['name']}\")\n",
        "    print(f\"   Tensor Split: {config['tensor_split']}\")\n",
        "    print(f\"   Description: {config['description']}\")\n",
        "    print(f\"   Use Case: {config['use_case']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d58e3e2c",
      "metadata": {},
      "source": [
        "## Step 7: Start Server with Dual-GPU Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Starts llama-server with dual-GPU configuration using 50/50 tensor split, 8K context, and parallel request handling to distribute the model equally across both T4 GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433d461a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from llcuda.server import ServerManager\n",
        "from llcuda.api.multigpu import SplitMode\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\ude80 STARTING DUAL-GPU SERVER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Dual-GPU configuration parameters\n",
        "dual_config = {\n",
        "    \"model_path\": model_path,\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": 8080,\n",
        "    \n",
        "    # Multi-GPU settings\n",
        "    \"gpu_layers\": 99,              # Offload all layers\n",
        "    \"tensor_split\": \"0.5,0.5\",     # Equal split (as comma-separated string)\n",
        "    \n",
        "    # Performance\n",
        "    \"ctx_size\": 8192,\n",
        "    \"batch_size\": 1024,\n",
        "    \n",
        "    # Parallelism\n",
        "    \"n_parallel\": 4,\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Dual-GPU Configuration:\")\n",
        "print(f\"   Model: {model_path.split('/')[-1]}\")\n",
        "print(f\"   Tensor Split: GPU0=50%, GPU1=50%\")\n",
        "print(f\"   Context Size: {dual_config['ctx_size']}\")\n",
        "\n",
        "# Start server\n",
        "server = ServerManager(server_url=f\"http://{dual_config['host']}:{dual_config['port']}\")\n",
        "print(\"\\n\ud83d\ude80 Starting server...\")\n",
        "\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=dual_config['model_path'],\n",
        "        host=dual_config['host'],\n",
        "        port=dual_config['port'],\n",
        "        gpu_layers=dual_config['gpu_layers'],\n",
        "        ctx_size=dual_config['ctx_size'],\n",
        "        batch_size=dual_config['batch_size'],\n",
        "        n_parallel=dual_config['n_parallel'],\n",
        "        timeout=120,\n",
        "        verbose=True,\n",
        "        # Multi-GPU tensor split\n",
        "        tensor_split=dual_config['tensor_split']\n",
        "    )\n",
        "    print(\"\\n\u2705 Dual-GPU server started successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Server failed to start: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c50fb0",
      "metadata": {},
      "source": [
        "## Step 8: Verify Multi-GPU Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifies multi-GPU tensor distribution by checking VRAM usage on both GPUs after model load, confirming that both GPUs show memory consumption when tensor-split is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "299a7e2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca GPU MEMORY DISTRIBUTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check memory usage on both GPUs\n",
        "result = subprocess.run(\n",
        "    [\"nvidia-smi\", \"--query-gpu=index,name,memory.used,memory.total,utilization.gpu\", \n",
        "     \"--format=csv,noheader\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "print(\"\\n\ud83d\udcca GPU Memory After Model Load:\")\n",
        "for line in result.stdout.strip().split('\\n'):\n",
        "    parts = line.split(', ')\n",
        "    if len(parts) >= 4:\n",
        "        idx, name, used, total = parts[0], parts[1], parts[2], parts[3]\n",
        "        print(f\"   GPU {idx}: {used} / {total}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Both GPUs should show VRAM usage if tensor-split is working.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fceb983b",
      "metadata": {},
      "source": [
        "## Step 9: Benchmark Multi-GPU Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmarks multi-GPU inference performance using longer prompts across both GPUs, measuring tokens per second, total generation time, and average throughput to evaluate dual-GPU efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3690e5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from llcuda.api.client import LlamaCppClient\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca MULTI-GPU PERFORMANCE BENCHMARK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "client = LlamaCppClient(base_url=\"http://127.0.0.1:8080\")\n",
        "\n",
        "# Longer prompts to test multi-GPU throughput\n",
        "prompts = [\n",
        "    \"Write a detailed explanation of how GPU parallelism works in deep learning.\",\n",
        "    \"Explain the architecture of a transformer model step by step.\",\n",
        "    \"Describe the CUDA programming model and its key concepts.\",\n",
        "    \"What are the advantages of using multiple GPUs for inference?\",\n",
        "    \"Explain tensor parallelism vs pipeline parallelism.\",\n",
        "]\n",
        "\n",
        "print(f\"\\n\ud83c\udfc3 Running benchmark with {len(prompts)} prompts...\\n\")\n",
        "\n",
        "total_input_tokens = 0\n",
        "total_output_tokens = 0\n",
        "total_time = 0\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    start = time.time()\n",
        "    \n",
        "    response = client.chat.create(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    input_tokens = response.usage.prompt_tokens\n",
        "    output_tokens = response.usage.completion_tokens\n",
        "    \n",
        "    total_input_tokens += input_tokens\n",
        "    total_output_tokens += output_tokens\n",
        "    total_time += elapsed\n",
        "    \n",
        "    tok_per_sec = output_tokens / elapsed\n",
        "    print(f\"   Prompt {i}: {output_tokens} tokens in {elapsed:.2f}s ({tok_per_sec:.1f} tok/s)\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Benchmark Results:\")\n",
        "print(f\"   Total Input Tokens: {total_input_tokens}\")\n",
        "print(f\"   Total Output Tokens: {total_output_tokens}\")\n",
        "print(f\"   Total Time: {total_time:.2f}s\")\n",
        "print(f\"   Average Generation Speed: {total_output_tokens/total_time:.1f} tokens/second\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b558e2",
      "metadata": {},
      "source": [
        "## Step 10: Test Different Split Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stops the current llama-server instance and waits 2 seconds for port release before testing alternative tensor-split configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7db403",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop current server\n",
        "print(\"\ud83d\uded1 Stopping server for reconfiguration...\")\n",
        "server.stop_server()\n",
        "\n",
        "import time\n",
        "time.sleep(2)\n",
        "print(\"\u2705 Server stopped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tests an asymmetric 70/30 tensor split configuration where GPU 0 handles 70% of the model and GPU 1 handles 30%, useful when GPU 1 needs reserved memory for other tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8741132f",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udd27 TESTING 70/30 SPLIT CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 70/30 split - more on GPU 0\n",
        "config_70_30 = {\n",
        "    \"model_path\": model_path,\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": 8080,\n",
        "    \"gpu_layers\": 99,\n",
        "    \"tensor_split\": \"0.7,0.3\",  # 70% GPU0, 30% GPU1\n",
        "    \"ctx_size\": 8192,\n",
        "}\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Configuration:\")\n",
        "print(f\"   Tensor Split: GPU0=70%, GPU1=30%\")\n",
        "print(f\"   Use Case: When GPU1 needs memory for other tasks\")\n",
        "\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=config_70_30['model_path'],\n",
        "        host=config_70_30['host'],\n",
        "        port=config_70_30['port'],\n",
        "        gpu_layers=config_70_30['gpu_layers'],\n",
        "        ctx_size=config_70_30['ctx_size'],\n",
        "        timeout=60,\n",
        "        verbose=True,\n",
        "        tensor_split=config_70_30['tensor_split']\n",
        "    )\n",
        "    print(\"\\n\u2705 70/30 split server started!\")\n",
        "    \n",
        "    # Check memory distribution\n",
        "    print(\"\\n\ud83d\udcca Memory Distribution:\")\n",
        "    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Failed to start: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5da3269",
      "metadata": {},
      "source": [
        "## Step 11: Split-GPU Mode for LLM + RAPIDS\n",
        "\n",
        "Configure for running LLM on GPU 0 while reserving GPU 1 for RAPIDS/Graphistry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configures split-GPU mode with 100% of the LLM on GPU 0 (tensor-split 1.0,0.0), leaving GPU 1 completely free for RAPIDS, cuGraph, or Graphistry visualization workloads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff9bd02",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop current server\n",
        "server.stop_server()\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83c\udfaf SPLIT-GPU MODE: LLM + RAPIDS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "This configuration runs the LLM entirely on GPU 0,\n",
        "leaving GPU 1 free for RAPIDS/cuGraph/Graphistry.\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502       GPU 0 (15GB)      \u2502        GPU 1 (15GB)           \u2502\n",
        "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n",
        "\u2502  \u2502  llama-server   \u2502    \u2502   \u2502  RAPIDS / Graphistry    \u2502 \u2502\n",
        "\u2502  \u2502  (Full Model)   \u2502    \u2502   \u2502  (Graph Visualization)  \u2502 \u2502\n",
        "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\"\"\")\n",
        "\n",
        "# Single GPU configuration (GPU 0 only)\n",
        "split_gpu_config = {\n",
        "    \"model_path\": model_path,\n",
        "    \"host\": \"127.0.0.1\",\n",
        "    \"port\": 8080,\n",
        "    \"gpu_layers\": 99,\n",
        "    \"tensor_split\": \"1.0,0.0\",  # 100% on GPU 0\n",
        "    \"ctx_size\": 4096,  # Smaller context to fit in single GPU\n",
        "}\n",
        "\n",
        "try:\n",
        "    server.start_server(\n",
        "        model_path=split_gpu_config['model_path'],\n",
        "        host=split_gpu_config['host'],\n",
        "        port=split_gpu_config['port'],\n",
        "        gpu_layers=split_gpu_config['gpu_layers'],\n",
        "        ctx_size=split_gpu_config['ctx_size'],\n",
        "        timeout=60,\n",
        "        verbose=True,\n",
        "        tensor_split=split_gpu_config['tensor_split']\n",
        "    )\n",
        "    print(\"\\n\u2705 Split-GPU mode server started!\")\n",
        "    print(\"   GPU 0: llama-server\")\n",
        "    print(\"   GPU 1: Available for RAPIDS/Graphistry\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udcca Memory Distribution:\")\n",
        "    !nvidia-smi --query-gpu=index,memory.used,memory.total --format=csv\n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c Failed to start: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa4e1778",
      "metadata": {},
      "source": [
        "## Step 12: Verify GPU 1 is Free for RAPIDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checks GPU 1 availability by querying free VRAM to confirm it has >14GB available for RAPIDS workloads after the LLM is loaded exclusively on GPU 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe283bc0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"\ud83d\udcca GPU 1 AVAILABILITY CHECK\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\"nvidia-smi\", \"--query-gpu=index,memory.used,memory.free\", \"--format=csv,noheader\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "lines = result.stdout.strip().split('\\n')\n",
        "if len(lines) >= 2:\n",
        "    gpu1_info = lines[1].split(', ')\n",
        "    used = gpu1_info[1].strip()\n",
        "    free = gpu1_info[2].strip()\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca GPU 1 Status:\")\n",
        "    print(f\"   Memory Used: {used}\")\n",
        "    print(f\"   Memory Free: {free}\")\n",
        "    \n",
        "    # Parse free memory\n",
        "    free_mb = int(free.replace(' MiB', ''))\n",
        "    if free_mb > 14000:  # > 14GB free\n",
        "        print(f\"\\n\u2705 GPU 1 has {free_mb/1024:.1f} GB free - Ready for RAPIDS!\")\n",
        "    else:\n",
        "        print(f\"\\n\u26a0\ufe0f GPU 1 has limited free memory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c23da2",
      "metadata": {},
      "source": [
        "## Step 13: Cleanup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performs final cleanup by stopping llama-server and displaying GPU memory status to confirm all resources are released across both GPUs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2bdf5c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\uded1 Stopping server...\")\n",
        "server.stop_server()\n",
        "\n",
        "print(\"\\n\u2705 Server stopped\")\n",
        "print(\"\\n\ud83d\udcca Final GPU Status:\")\n",
        "!nvidia-smi --query-gpu=index,memory.used,memory.free --format=csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e9a1f2",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Summary\n",
        "\n",
        "You've learned:\n",
        "1. \u2705 Multi-GPU configuration options (tensor-split, split-mode)\n",
        "2. \u2705 Equal split (50/50) for maximum model size\n",
        "3. \u2705 Asymmetric split (70/30) for mixed workloads\n",
        "4. \u2705 Single-GPU mode (100/0) to reserve GPU 1 for other tasks\n",
        "5. \u2705 Performance benchmarking across GPUs\n",
        "\n",
        "## Configuration Quick Reference\n",
        "\n",
        "| Use Case | Tensor Split | GPU 0 | GPU 1 |\n",
        "|----------|--------------|-------|-------|\n",
        "| Max Model Size | 0.5, 0.5 | LLM (50%) | LLM (50%) |\n",
        "| LLM + Light Task | 0.7, 0.3 | LLM (70%) | LLM (30%) |\n",
        "| LLM Only on GPU 0 | 1.0, 0.0 | LLM (100%) | Free for other tasks |\n",
        "\n",
        "---\n",
        "\n",
        "**Next:** [04-gguf-quantization](04-gguf-quantization-llcuda-v2.2.0.ipynb)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}