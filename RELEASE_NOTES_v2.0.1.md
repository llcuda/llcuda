# llcuda v2.0.1 Release Notes

**Release Date**: January 7, 2025
**Type**: Maintenance/Cleanup Release
**Target**: Tesla T4 GPU (SM 7.5) - Google Colab

---

## ğŸ¯ Overview

llcuda v2.0.1 is a maintenance release focused on **project cleanup and PyPI package optimization**. This release ensures the PyPI package stays under 100 MB by excluding large binaries, which are now downloaded on first use.

---

## âœ¨ What's Changed

### Package Optimization
- **Excluded large binaries from PyPI wheel** (previously would have been 466+ MB)
- **PyPI package size reduced to ~70 KB** (from potential 500+ MB)
- Large binaries (llcuda_cpp.so, llcuda-binaries-cuda12-t4.tar.gz) are **downloaded on first import**
- Bootstrap mechanism handles binary downloads automatically

### Repository Cleanup (~265 MB saved)
- Removed duplicate backup files (`__init___backup.py`, `__init___pure.py`)
- Removed empty nested directory structure in `llcuda/` package
- Removed obsolete CMakeLists.txt and llcuda_py.cpp from package directory
- Removed 15+ obsolete documentation files from v1.x era
- Removed duplicate binary tarballs

### Configuration Improvements
- Updated .gitignore with comprehensive patterns to prevent large file uploads
- Added explicit exclusion patterns for *.so, *.gguf, *.tar.gz files
- Updated pyproject.toml with [tool.setuptools.exclude-package-data]
- Ensured no model files (.gguf) can be accidentally uploaded

---

## ğŸ“¦ Installation

```bash
pip install llcuda==2.0.1
```

**First import** will download T4-optimized binaries (264 MB, one-time):
```python
import llcuda  # Triggers automatic binary download
```

Subsequent imports use cached binaries (~instant).

---

## ğŸš€ Quick Start

### Tensor API
```python
from llcuda.core import Tensor, DType

A = Tensor.zeros([2048, 2048], dtype=DType.Float16, device=0)
B = Tensor.zeros([2048, 2048], dtype=DType.Float16, device=0)
C = A @ B  # cuBLAS with Tensor Cores
```

### HTTP Server API
```python
import llcuda

engine = llcuda.InferenceEngine()
engine.load_model("gemma-3-1b-Q4_K_M", silent=True)
result = engine.infer("Hello, world!", max_tokens=50)
print(result.text)
```

---

## ğŸ“Š Package Sizes

| Component | Size | Included in PyPI? |
|-----------|------|-------------------|
| PyPI wheel | 54 KB | âœ… Yes |
| Source tarball | 67 KB | âœ… Yes |
| llcuda_cpp.so | 466 MB | âŒ No - Downloaded on first use |
| T4 binaries | 264 MB | âŒ No - Downloaded on first use |

**Total PyPI download**: ~70 KB (vs 730 MB if binaries were included)

---

## ğŸ”§ What's NOT Changed

### Core Functionality (Unchanged)
- âœ… Native Tensor API works identically
- âœ… HTTP Server API works identically
- âœ… FlashAttention support unchanged
- âœ… CUDA Graphs optimization unchanged
- âœ… Tesla T4 targeting unchanged
- âœ… All performance benchmarks remain same

### Dependencies (Unchanged)
- Python 3.11+
- CUDA 12.x
- Tesla T4 GPU (SM 7.5)
- numpy>=1.24.0, requests>=2.31.0, huggingface_hub>=0.20.0, tqdm>=4.65.0

---

## ğŸ› Known Issues

None specific to v2.0.1.

---

## âš ï¸ Breaking Changes

**None** - This is a backward-compatible maintenance release.

---

## ğŸ“š Migration from v2.0.0

No code changes needed! Simply upgrade:

```bash
pip install --upgrade llcuda
```

On first import after upgrade, binaries will be re-downloaded if needed.

---

## ğŸ”— Links

- **PyPI**: https://pypi.org/project/llcuda/2.0.1/
- **GitHub**: https://github.com/waqasm86/llcuda
- **Changelog**: https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md
- **Issues**: https://github.com/waqasm86/llcuda/issues

---

## ğŸ™ Acknowledgments

- Built on [llama.cpp](https://github.com/ggerganov/llama.cpp)
- FlashAttention from [Dao et al.](https://github.com/Dao-AILab/flash-attention)
- Designed for [Unsloth](https://github.com/unslothai/unsloth) integration

---

**Version**: 2.0.1
**Release Type**: Maintenance
**Backward Compatible**: Yes
**License**: MIT
