{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# llcuda v1.1.6 Quick Start Guide\n\nThis notebook demonstrates the **clean, ultra-lightweight PyTorch-style API** in llcuda v1.1.6.\n\n**Requirements:**\n- Python 3.11+\n- NVIDIA GPU with CUDA support (Compute 5.0-8.9)\n- `pip install llcuda==1.1.6` (62 KB package - binaries auto-downloaded)\n\n**First-time Setup:**\nOn first import, llcuda will automatically download:\n- Optimized binaries (~700 MB) based on your GPU\n- Default model (~770 MB) from Hugging Face\nThis is a one-time download taking 3-5 minutes.\n\n**What's New in v1.1.6:**\n- Clean project structure (reduced from 14GB+ to <100MB)\n- Python 3.11+ focused optimization\n- Streamlined codebase for better development\n- Ultra-lightweight 62KB package\n- Updated documentation and examples\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Import and Auto-Configuration\n\nImporting llcuda automatically:\n- Downloads binaries and models on first run (one-time setup)\n- Detects your GPU and configures optimal settings\n- Configures all paths and libraries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llcuda\n",
    "import sys\n",
    "\n",
    "print(f\"llcuda version: {llcuda.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Print comprehensive system information\n",
    "llcuda.print_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Inference - Zero Configuration\n",
    "\n",
    "The simplest way to use llcuda. Just load a model and infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load model - auto-downloads from HuggingFace with user confirmation\n",
    "engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
    "\n",
    "print(\"\\nâœ“ Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Simple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "result = engine.infer(\n",
    "    prompt=\"What is artificial intelligence?\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Generated Text:\")\n",
    "print(\"=\"*60)\n",
    "print(result.text)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Tokens Generated: {result.tokens_generated}\")\n",
    "print(f\"  Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "print(f\"  Latency: {result.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. List Available Models\n\nllcuda v1.1.6 includes curated models in the registry and supports all CUDA compute capabilities (5.0-8.9) with clean, optimized codebase."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda.models import list_registry_models\n",
    "\n",
    "models = list_registry_models()\n",
    "\n",
    "print(f\"Available Models in Registry: {len(models)}\\n\")\n",
    "for i, (name, info) in enumerate(models.items(), 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   {info['description']}\")\n",
    "    print(f\"   Size: {info['size_mb']} MB\")\n",
    "    print(f\"   Recommended VRAM: {info['min_vram_gb']} GB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about CUDA programming.\",\n",
    "    \"What are the benefits of GPU acceleration?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = engine.infer(prompt, max_tokens=80, temperature=0.7)\n",
    "    print(result.text)\n",
    "    print(f\"\\nâš¡ {result.tokens_per_sec:.1f} tok/s | {result.latency_ms:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is natural language processing?\"\n",
    "]\n",
    "\n",
    "print(\"Running batch inference...\\n\")\n",
    "results = engine.batch_infer(batch_prompts, max_tokens=50)\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(batch_prompts, results), 1):\n",
    "    print(f\"{i}. {prompt}\")\n",
    "    print(f\"   â†’ {result.text[:100]}...\")\n",
    "    print(f\"   âš¡ {result.tokens_per_sec:.1f} tok/s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics\n",
    "\n",
    "Get detailed P50/P95/P99 latency statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLatency Statistics:\")\n",
    "latency = metrics['latency']\n",
    "print(f\"  Mean: {latency['mean_ms']:.2f} ms\")\n",
    "print(f\"  p50:  {latency['p50_ms']:.2f} ms\")\n",
    "print(f\"  p95:  {latency['p95_ms']:.2f} ms\")\n",
    "print(f\"  p99:  {latency['p99_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\nThroughput Statistics:\")\n",
    "throughput = metrics['throughput']\n",
    "print(f\"  Total Tokens: {throughput['total_tokens']}\")\n",
    "print(f\"  Total Requests: {throughput['total_requests']}\")\n",
    "print(f\"  Tokens/sec: {throughput['tokens_per_sec']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hardware Auto-Configuration\n",
    "\n",
    "llcuda automatically detects your GPU and configures optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and GPU info\n",
    "if llcuda.check_cuda_available():\n",
    "    print(\"âœ“ CUDA is available!\\n\")\n",
    "    \n",
    "    gpu_info = llcuda.get_cuda_device_info()\n",
    "    if gpu_info:\n",
    "        print(f\"CUDA Version: {gpu_info['cuda_version']}\")\n",
    "        print(f\"Number of GPUs: {len(gpu_info['gpus'])}\\n\")\n",
    "        \n",
    "        for i, gpu in enumerate(gpu_info['gpus']):\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Name: {gpu['name']}\")\n",
    "            print(f\"  Memory: {gpu['memory']}\")\n",
    "            print(f\"  Driver: {gpu['driver_version']}\")\n",
    "else:\n",
    "    print(\"âŒ CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Local GGUF Files\n",
    "\n",
    "You can also use local GGUF model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find local GGUF models\n",
    "models = llcuda.find_gguf_models()\n",
    "\n",
    "if models:\n",
    "    print(f\"Found {len(models)} local GGUF models:\\n\")\n",
    "    for i, model in enumerate(models[:5], 1):  # Show first 5\n",
    "        size_mb = model.stat().st_size / (1024 * 1024)\n",
    "        print(f\"{i}. {model.name}\")\n",
    "        print(f\"   Size: {size_mb:.1f} MB\\n\")\n",
    "else:\n",
    "    print(\"No local GGUF models found. Use registry models instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temperature Comparison\n",
    "\n",
    "Compare outputs with different temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a creative opening sentence for a science fiction story.\"\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "\n",
    "print(\"Comparing Different Temperatures\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = engine.infer(\n",
    "        prompt=prompt,\n",
    "        max_tokens=60,\n",
    "        temperature=temp\n",
    "    )\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Performance (Optional)\n",
    "\n",
    "Create a simple plot of latencies if matplotlib is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    latencies = engine._metrics['latencies']\n",
    "    \n",
    "    if latencies:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Latency over time\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(latencies, marker='o', linewidth=2)\n",
    "        plt.xlabel('Request Number')\n",
    "        plt.ylabel('Latency (ms)')\n",
    "        plt.title('Inference Latency Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(latencies, bins=20, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Latency (ms)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Latency Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No metrics available yet. Run some inferences first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib not installed. Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Context Manager Usage\n",
    "\n",
    "Use llcuda with Python context managers for automatic cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager automatically handles cleanup\n",
    "with llcuda.InferenceEngine() as engine:\n",
    "    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
    "    \n",
    "    result = engine.infer(\n",
    "        \"Explain the benefits of context managers in Python.\",\n",
    "        max_tokens=80\n",
    "    )\n",
    "    \n",
    "    print(result.text)\n",
    "    print(f\"\\nâš¡ {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "# Engine automatically cleaned up after context exit\n",
    "print(\"\\nâœ“ Resources automatically cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "When you're done, unload the model to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload model and stop server\n",
    "engine.unload_model()\n",
    "print(\"âœ“ Server stopped and resources cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Summary - llcuda v1.1.6 Features\n\nYou've learned how to:\n- âœ… **Clean ultra-lightweight architecture** - 62 KB PyPI package (vs 51KB v1.1.5)\n- âœ… **Universal GPU support** - Compute capabilities 5.0-8.9 (8 architectures)\n- âœ… **Cloud platform ready** - Works on Colab, Kaggle, local JupyterLab\n- âœ… **Zero-configuration setup** - Auto-downloads binaries and models\n- âœ… **Smart model loading** - Auto-download from Hugging Face\n- âœ… **Hardware auto-configuration** - Automatic GPU detection\n- âœ… **Single and batch inference** - Efficient processing\n- âœ… **Performance metrics** - P50/P95/P99 latency tracking\n- âœ… **Curated models** - Ready to use out of the box\n- âœ… **Context manager support** - Automatic cleanup\n- âœ… **Python 3.11+ focus** - Optimized for modern Python\n- âœ… **Clean project structure** - Streamlined for development\n\n### What's New in v1.1.6:\n- **Project Cleanup** - Reduced repository from 14GB+ to <100MB\n- **Python 3.11+ Optimization** - Explicit testing and support\n- **Ultra-Light Package** - 62KB wheel, 61KB source distribution\n- **Clean Codebase** - Streamlined for better maintainability\n- **Updated Documentation** - All docs refreshed for v1.1.6\n- **Better Git Performance** - Faster cloning and operations\n\n### Maintained Features from v1.1.5:\n- **Hybrid Bootstrap Architecture** - Auto-download binaries/models\n- **Multi-GPU Support** - SM 5.0, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6, 8.9\n- **Cloud Platforms** - Colab (T4, P100, V100, A100), Kaggle (T4)\n- **Professional Distribution** - Matches PyTorch/TensorFlow pattern\n\n### Supported Platforms:\n- âœ… **Google Colab** - T4 (~15 tok/s), P100 (~18 tok/s), V100, A100\n- âœ… **Kaggle** - T4 (~15 tok/s)\n- âœ… **Local JupyterLab** - All NVIDIA GPUs with Compute 5.0+\n- âœ… **GeForce/RTX Series** - 940M, GTX 10xx, RTX 20xx/30xx/40xx\n\n### Next Steps:\n1. Try different models from the registry\n2. Test on Google Colab or Kaggle\n3. Experiment with temperature and other parameters\n4. Build applications using llcuda\n5. Check out the documentation: [PyPI](https://pypi.org/project/llcuda/1.1.6/) | [GitHub](https://github.com/waqasm86/llcuda) | [Docs](https://waqasm86.github.io/)\n\n---\n\n**Happy Inferencing with llcuda v1.1.6! ðŸš€**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}