{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llcuda v1.0.0 Quick Start Guide\n",
    "\n",
    "This notebook demonstrates the **PyTorch-style zero-configuration API** in llcuda v1.0.0.\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3.11+\n",
    "- NVIDIA GPU with CUDA support (tested on GeForce 940M)\n",
    "- `pip install llcuda` (includes all CUDA binaries)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and Auto-Configuration\n",
    "\n",
    "Importing llcuda automatically configures all paths and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llcuda\n",
    "import sys\n",
    "\n",
    "print(f\"llcuda version: {llcuda.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Print comprehensive system information\n",
    "llcuda.print_system_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Inference - Zero Configuration\n",
    "\n",
    "The simplest way to use llcuda. Just load a model and infer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "engine = llcuda.InferenceEngine()\n",
    "\n",
    "# Load model - auto-downloads from HuggingFace with user confirmation\n",
    "engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
    "\n",
    "print(\"\\n‚úì Model loaded and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Simple Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "result = engine.infer(\n",
    "    prompt=\"What is artificial intelligence?\",\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Generated Text:\")\n",
    "print(\"=\"*60)\n",
    "print(result.text)\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Tokens Generated: {result.tokens_generated}\")\n",
    "print(f\"  Speed: {result.tokens_per_sec:.1f} tok/s\")\n",
    "print(f\"  Latency: {result.latency_ms:.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. List Available Models\n",
    "\n",
    "llcuda v1.0.0 includes 11 curated models in the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llcuda.models import list_registry_models\n",
    "\n",
    "models = list_registry_models()\n",
    "\n",
    "print(f\"Available Models in Registry: {len(models)}\\n\")\n",
    "for i, (name, info) in enumerate(models.items(), 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   {info['description']}\")\n",
    "    print(f\"   Size: {info['size_mb']} MB\")\n",
    "    print(f\"   Recommended VRAM: {info['min_vram_gb']} GB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Explain quantum computing in simple terms.\",\n",
    "    \"Write a haiku about CUDA programming.\",\n",
    "    \"What are the benefits of GPU acceleration?\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i}: {prompt}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = engine.infer(prompt, max_tokens=80, temperature=0.7)\n",
    "    print(result.text)\n",
    "    print(f\"\\n‚ö° {result.tokens_per_sec:.1f} tok/s | {result.latency_ms:.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Inference\n",
    "\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is deep learning?\",\n",
    "    \"What is natural language processing?\"\n",
    "]\n",
    "\n",
    "print(\"Running batch inference...\\n\")\n",
    "results = engine.batch_infer(batch_prompts, max_tokens=50)\n",
    "\n",
    "for i, (prompt, result) in enumerate(zip(batch_prompts, results), 1):\n",
    "    print(f\"{i}. {prompt}\")\n",
    "    print(f\"   ‚Üí {result.text[:100]}...\")\n",
    "    print(f\"   ‚ö° {result.tokens_per_sec:.1f} tok/s\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics\n",
    "\n",
    "Get detailed P50/P95/P99 latency statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = engine.get_metrics()\n",
    "\n",
    "print(\"Performance Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nLatency Statistics:\")\n",
    "latency = metrics['latency']\n",
    "print(f\"  Mean: {latency['mean_ms']:.2f} ms\")\n",
    "print(f\"  p50:  {latency['p50_ms']:.2f} ms\")\n",
    "print(f\"  p95:  {latency['p95_ms']:.2f} ms\")\n",
    "print(f\"  p99:  {latency['p99_ms']:.2f} ms\")\n",
    "\n",
    "print(\"\\nThroughput Statistics:\")\n",
    "throughput = metrics['throughput']\n",
    "print(f\"  Total Tokens: {throughput['total_tokens']}\")\n",
    "print(f\"  Total Requests: {throughput['total_requests']}\")\n",
    "print(f\"  Tokens/sec: {throughput['tokens_per_sec']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hardware Auto-Configuration\n",
    "\n",
    "llcuda automatically detects your GPU and configures optimal settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and GPU info\n",
    "if llcuda.check_cuda_available():\n",
    "    print(\"‚úì CUDA is available!\\n\")\n",
    "    \n",
    "    gpu_info = llcuda.get_cuda_device_info()\n",
    "    if gpu_info:\n",
    "        print(f\"CUDA Version: {gpu_info['cuda_version']}\")\n",
    "        print(f\"Number of GPUs: {len(gpu_info['gpus'])}\\n\")\n",
    "        \n",
    "        for i, gpu in enumerate(gpu_info['gpus']):\n",
    "            print(f\"GPU {i}:\")\n",
    "            print(f\"  Name: {gpu['name']}\")\n",
    "            print(f\"  Memory: {gpu['memory']}\")\n",
    "            print(f\"  Driver: {gpu['driver_version']}\")\n",
    "else:\n",
    "    print(\"‚ùå CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Local GGUF Files\n",
    "\n",
    "You can also use local GGUF model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find local GGUF models\n",
    "models = llcuda.find_gguf_models()\n",
    "\n",
    "if models:\n",
    "    print(f\"Found {len(models)} local GGUF models:\\n\")\n",
    "    for i, model in enumerate(models[:5], 1):  # Show first 5\n",
    "        size_mb = model.stat().st_size / (1024 * 1024)\n",
    "        print(f\"{i}. {model.name}\")\n",
    "        print(f\"   Size: {size_mb:.1f} MB\\n\")\n",
    "else:\n",
    "    print(\"No local GGUF models found. Use registry models instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Temperature Comparison\n",
    "\n",
    "Compare outputs with different temperature settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a creative opening sentence for a science fiction story.\"\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "\n",
    "print(\"Comparing Different Temperatures\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    result = engine.infer(\n",
    "        prompt=prompt,\n",
    "        max_tokens=60,\n",
    "        temperature=temp\n",
    "    )\n",
    "    print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Performance (Optional)\n",
    "\n",
    "Create a simple plot of latencies if matplotlib is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    latencies = engine._metrics['latencies']\n",
    "    \n",
    "    if latencies:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Latency over time\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(latencies, marker='o', linewidth=2)\n",
    "        plt.xlabel('Request Number')\n",
    "        plt.ylabel('Latency (ms)')\n",
    "        plt.title('Inference Latency Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency distribution\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(latencies, bins=20, edgecolor='black', alpha=0.7)\n",
    "        plt.xlabel('Latency (ms)')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Latency Distribution')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No metrics available yet. Run some inferences first.\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"matplotlib not installed. Install with: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Context Manager Usage\n",
    "\n",
    "Use llcuda with Python context managers for automatic cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager automatically handles cleanup\n",
    "with llcuda.InferenceEngine() as engine:\n",
    "    engine.load_model(\"gemma-3-1b-Q4_K_M\")\n",
    "    \n",
    "    result = engine.infer(\n",
    "        \"Explain the benefits of context managers in Python.\",\n",
    "        max_tokens=80\n",
    "    )\n",
    "    \n",
    "    print(result.text)\n",
    "    print(f\"\\n‚ö° {result.tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "# Engine automatically cleaned up after context exit\n",
    "print(\"\\n‚úì Resources automatically cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup\n",
    "\n",
    "When you're done, unload the model to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unload model and stop server\n",
    "engine.unload_model()\n",
    "print(\"‚úì Server stopped and resources cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary - llcuda v1.0.0 Features\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ **Zero-configuration setup** - just `pip install llcuda`\n",
    "- ‚úÖ **Smart model loading** - auto-download from HuggingFace registry\n",
    "- ‚úÖ **Hardware auto-configuration** - automatic VRAM detection\n",
    "- ‚úÖ **Single and batch inference** - efficient processing\n",
    "- ‚úÖ **Performance metrics** - P50/P95/P99 latency tracking\n",
    "- ‚úÖ **11 curated models** - ready to use out of the box\n",
    "- ‚úÖ **Context manager support** - automatic cleanup\n",
    "\n",
    "### What's New in v1.0.0:\n",
    "- **Bundled CUDA binaries** - No manual llama-server setup\n",
    "- **Auto-configuration on import** - No LLAMA_SERVER_PATH needed\n",
    "- **Model registry** - 11 pre-configured models\n",
    "- **PyTorch-style API** - Familiar interface for ML engineers\n",
    "\n",
    "### Next Steps:\n",
    "1. Try different models from the registry\n",
    "2. Experiment with temperature and other parameters\n",
    "3. Build applications using llcuda\n",
    "4. Check out the documentation: [PyPI](https://pypi.org/project/llcuda/) | [GitHub](https://github.com/waqasm86/llcuda)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Inferencing! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
