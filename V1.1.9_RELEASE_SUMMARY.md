# llcuda v1.1.9 - Release Summary

**Release Date**: 2025-01-03
**Status**: ‚úÖ Live on PyPI and GitHub

---

## üéØ What Was Fixed

### Critical Bug Fix: llama-server Detection

**Problem**: In v1.1.8, the bootstrap correctly downloaded binaries to the package directory (`~/.local/lib/python3.XX/site-packages/llcuda/binaries/cuda12/llama-server`), but the ServerManager wasn't looking there. Users got "llama-server not found" errors.

**Solution**: Updated `find_llama_server()` in [server.py](llcuda/server.py) to prioritize the package binaries directory.

**New Search Priority**:
1. `LLAMA_SERVER_PATH` environment variable
2. **Package binaries directory** ‚Üê NEW! (Priority #2)
3. `LLAMA_CPP_DIR` environment variable
4. Cache directories (with Colab/Kaggle paths)
5. Project paths
6. System paths

**Code Changes** ([server.py:298-321](llcuda/server.py#L298-L321)):
```python
# Priority 2: Package's installed binaries (from bootstrap extraction)
try:
    import llcuda
    package_dir = Path(llcuda.__file__).parent
    binaries_paths = [
        package_dir / "binaries" / "cuda12" / "llama-server",
        package_dir / "binaries" / "llama-server",
    ]
    for path in binaries_paths:
        if path.exists():
            # Setup library path for package binaries
            lib_dir = package_dir / "lib"
            if lib_dir.exists():
                lib_path_str = str(lib_dir.absolute())
                current_ld_path = os.environ.get("LD_LIBRARY_PATH", "")
                if lib_path_str not in current_ld_path:
                    os.environ["LD_LIBRARY_PATH"] = f"{lib_path_str}:{current_ld_path}" if current_ld_path else lib_path_str
            return path
except:
    pass
```

### New Feature: Silent Mode

**Problem**: llama-server prints warnings and status messages to stdout, cluttering Jupyter notebook output.

**Solution**: Added `silent=True` parameter to suppress all llama-server output.

**Code Changes**:
- [llcuda/__init__.py:166](llcuda/__init__.py#L166) - Added `silent` parameter to `load_model()`
- [server.py:417-430](llcuda/server.py#L417-L430) - Implemented silent mode in `start_server()`

**Usage**:
```python
engine.load_model("gemma-3-1b-Q4_K_M", silent=True)
```

**Implementation** ([server.py:580-590](llcuda/server.py#L580-L590)):
```python
if silent:
    # Suppress all output
    self.server_process = subprocess.Popen(
        cmd,
        stdout=subprocess.DEVNULL,
        stderr=subprocess.DEVNULL,
        start_new_session=True,
    )
else:
    # Capture output for error reporting
    self.server_process = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        start_new_session=True,
    )
```

---

## üì¶ Release Artifacts

### PyPI Package
- **URL**: https://pypi.org/project/llcuda/1.1.9/
- **Wheel**: llcuda-1.1.9-py3-none-any.whl (54 KB)
- **Source**: llcuda-1.1.9.tar.gz (55 KB)
- **Upload Time**: 2026-01-02 22:05:30 UTC

### GitHub Release
- **URL**: https://github.com/waqasm86/llcuda/releases/tag/v1.1.9
- **Tag**: v1.1.9
- **Assets**:
  - llcuda-1.1.9-py3-none-any.whl
  - llcuda-1.1.9.tar.gz
- **Binary Archive**: Use v1.1.7 binaries (llcuda-binaries-cuda12.tar.gz - 161 MB)

---

## üöÄ Installation & Usage

### Quick Start

```python
# Install
pip install llcuda==1.1.9

# Import (downloads binaries on first run, NO model download)
import llcuda

# Initialize engine
engine = llcuda.InferenceEngine()

# Load model with silent mode (downloads model on first run)
engine.load_model("gemma-3-1b-Q4_K_M", silent=True)

# Run inference
result = engine.infer("What is AI?", max_tokens=100)
print(result.text)
```

### Google Colab / Kaggle

Use the new test notebook:

1. **Open in Colab**: [colab_test_v1.1.9.ipynb](https://colab.research.google.com/github/waqasm86/llcuda/blob/main/examples/colab_test_v1.1.9.ipynb)
2. **Select Runtime**: Change runtime type ‚Üí T4 GPU
3. **Run All Cells**

Or use the Python script:
```bash
!pip install llcuda==1.1.9
!wget https://raw.githubusercontent.com/waqasm86/llcuda/main/examples/colab_test_v1.1.9.py
!python3 colab_test_v1.1.9.py
```

---

## üìù Files Modified

### Version Updates
- [pyproject.toml](pyproject.toml#L13) - Version 1.1.9
- [llcuda/__init__.py](llcuda/__init__.py#L9) - Version 1.1.9
- [README.md](README.md#L13-L46) - Version 1.1.9 section
- [CHANGELOG.md](CHANGELOG.md#L10-L42) - Version 1.1.9 entry

### Bug Fixes
- [llcuda/server.py](llcuda/server.py#L298-L321) - Added package binaries directory to search
- [llcuda/server.py](llcuda/server.py#L334-L338) - Added Colab/Kaggle cache paths
- [llcuda/server.py](llcuda/server.py#L580-L590) - Implemented silent mode

### New Features
- [llcuda/__init__.py](llcuda/__init__.py#L166) - Added `silent` parameter
- [examples/colab_test_v1.1.9.py](examples/colab_test_v1.1.9.py) - New test script
- [examples/colab_test_v1.1.9.ipynb](examples/colab_test_v1.1.9.ipynb) - New test notebook
- [examples/README.md](examples/README.md) - Examples documentation

---

## üîÑ Version History

### v1.1.9 (2025-01-03) - Current
- ‚úÖ Fixed llama-server detection
- ‚úÖ Added silent mode
- ‚úÖ Improved Colab/Kaggle compatibility

### v1.1.8 (2025-01-03)
- Fixed bootstrap URL (v1.1.6 ‚Üí v1.1.7)
- Removed automatic model download on import
- Improved binary extraction logic

### v1.1.7 (2025-01-03)
- CUDA 12.8 support
- Optimized binary distribution (551MB ‚Üí 161MB)
- Enhanced GPU compatibility

### v1.1.6 (2025-01-03)
- Project cleanup
- Repository size reduction (14GB ‚Üí <100MB)
- Python 3.11+ focus

---

## ‚úÖ Testing

### Automated Tests
Run the test script to verify all fixes:
```bash
python3 examples/colab_test_v1.1.9.py
```

### Manual Tests
1. **Import Test**: `import llcuda` should NOT download models
2. **Server Detection**: `ServerManager().find_llama_server()` should find package binaries
3. **Silent Mode**: `engine.load_model(..., silent=True)` should have no llama-server output
4. **Inference**: `engine.infer(...)` should work correctly

### Expected Output
```
‚úÖ llcuda imported successfully
‚úÖ llama-server found at: /home/user/.local/lib/python3.11/site-packages/llcuda/binaries/cuda12/llama-server
‚úÖ Model loaded successfully
‚úÖ Inference successful
```

---

## üêõ Known Issues

None reported for v1.1.9.

If you encounter issues:
1. Check GPU compatibility (NVIDIA with compute capability 5.0+)
2. Verify CUDA installation (11.0+ or 12.0+)
3. Try verbose mode: `engine.load_model(..., verbose=True)`
4. Report at: https://github.com/waqasm86/llcuda/issues

---

## üîó Links

- **PyPI**: https://pypi.org/project/llcuda/1.1.9/
- **GitHub**: https://github.com/waqasm86/llcuda
- **Releases**: https://github.com/waqasm86/llcuda/releases
- **Documentation**: https://waqasm86.github.io/
- **Issues**: https://github.com/waqasm86/llcuda/issues
- **Changelog**: https://github.com/waqasm86/llcuda/blob/main/CHANGELOG.md

---

## üë• Credits

**Developed by**: Waqas Muhammad (waqasm86@gmail.com)

**Generated with**: [Claude Code](https://claude.com/claude-code)
**Co-Authored-By**: Claude Sonnet 4.5 <noreply@anthropic.com>

**Core Dependencies**:
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inference engine
- [Hugging Face](https://huggingface.co) - Model hosting
- NVIDIA CUDA Toolkit 12.8

---

**License**: MIT
**Python Support**: 3.11+
**CUDA Support**: 11.0+ and 12.0+ (12.8 recommended)
**GPU Support**: NVIDIA with compute capability 5.0-8.9 (Maxwell to Ada Lovelace)
