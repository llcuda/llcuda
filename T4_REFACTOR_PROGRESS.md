# llcuda v2.0 T4-Only Refactoring Progress

**Date**: January 6, 2026
**Status**: Phase 2 Complete âœ…
**Next**: Create T4-specific build script and test

---

## Completed Tasks âœ…

### Phase 1: Cleanup & Archive (DONE)
1. âœ… Created `archive/v1.x/` directory structure
2. âœ… Moved 940M build scripts to archive
3. âœ… Deleted 940M binary package (freed 26 MB)
4. âœ… Archived v1.x release-info directory (19 files)
5. âœ… Archived outdated documentation (README variants, organization docs)
6. âœ… Archived 940M GPU specifications

**Result**: 19 files archived, 26 MB freed

### Phase 2: Build System Update (DONE)
1. âœ… Updated CMakeLists.txt to target SM 7.5 (Tesla T4) only
2. âœ… Added Tensor Core optimization flags
3. âœ… Added FlashAttention support indicators
4. âœ… Added explicit T4 code generation (`-gencode=arch=compute_75,code=sm_75`)
5. âœ… Added PTX verbosity for debugging (`--ptxas-options=-v`)
6. âœ… Added CPU native optimizations (`-march=native`)

### Phase 3: Bootstrap Refactoring (DONE)
1. âœ… Updated `llcuda/_internal/bootstrap.py` for T4-only
2. âœ… Added GPU compatibility verification (SM 7.5+ check)
3. âœ… Removed multi-GPU architecture selection
4. âœ… Added clear error messages for incompatible GPUs
5. âœ… Updated to download T4 binaries from v2.0.0 release
6. âœ… Enhanced user-facing messages

### Phase 4: GGUF Parser (DONE)
1. âœ… Created `llcuda/gguf_parser.py`
2. âœ… Implemented zero-copy memory-mapped tensor access
3. âœ… Added support for all GGUF v3 features
4. âœ… Implemented metadata parsing
5. âœ… Added tensor info extraction
6. âœ… Created comprehensive test suite (`tests/test_gguf_parser.py`)

### Phase 5: Documentation Update (DONE)
1. âœ… Updated `README.md` for T4-only focus
2. âœ… Removed all 940M references
3. âœ… Added v2.0 native Tensor API documentation
4. âœ… Added Google Colab quick start guide
5. âœ… Updated performance benchmarks for T4
6. âœ… Added GGUF parser usage examples

### Phase 6: Package Metadata Update (DONE)
1. âœ… Updated `pyproject.toml` to v2.0.0
2. âœ… Updated description for Unsloth integration focus
3. âœ… Added pybind11 to build requirements
4. âœ… Added package data for binaries and .so files
5. âœ… Updated keywords for T4, tensor-api, flashattention
6. âœ… Changed status to "Beta" (Development Status :: 4)

---

## Comprehensive Project Analysis ðŸ“Š

Based on thorough exploration:

### Current llcuda Structure
- **Total files**: ~195 files
- **V1.x Python package**: Functional HTTP wrapper (InferenceEngine)
- **V2.0 Tensor API**: Phase 1 complete (Tensor, Device, MatMul)
- **T4 Binaries**: 264 MB (libggml-cuda.so.0.9.5 = 219 MB)

### T4 Build Configuration (from Colab notebook)
```cmake
-DCMAKE_CUDA_ARCHITECTURES="75"      # T4 specific
-DGGML_CUDA_FA=ON                    # FlashAttention
-DGGML_CUDA_FA_ALL_QUANTS=ON        # FA for all quantization types
-DGGML_CUDA_GRAPHS=ON                # CUDA Graphs optimization
-DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 # Multi-GPU batching
-DBUILD_SHARED_LIBS=ON               # Shared libraries
```

### llama.cpp Server Insights
- **Continuous batching**: Multi-user request handling
- **Slot-based architecture**: Parallel decoding
- **KV cache checkpoints**: Default 8 per slot
- **Cache reuse**: Via KV shifting
- **Speculative decoding**: Draft model support

### GGUF Format Spec
- **Magic**: `0x47474655` ("GGUF")
- **Version**: 3 (big-endian support)
- **Alignment**: Default 32 bytes
- **Quantization types**: 40+ formats including Q4_K_M, NF4, MXFP4
- **Metadata**: Key-value pairs with type information

---

## Remaining Tasks (Prioritized)

### HIGH PRIORITY (This Week)

#### 1. Update bootstrap.py for T4-Only
**File**: `llcuda/_internal/bootstrap.py`

**Changes needed**:
```python
def verify_gpu_compatibility():
    """Verify GPU is SM 7.5+ (Turing or newer)"""
    # Check compute capability
    # Raise error if < 7.5
    # Warn if not exactly T4 but compatible

def download_t4_binaries():
    """Download T4 CUDA 12 binaries (264 MB)"""
    url = "https://github.com/waqasm86/llcuda/releases/download/v2.0.0/llcuda-binaries-cuda12-t4.tar.gz"
    # Download and extract to llcuda/binaries/cuda12/

# Remove 940M download logic
# Remove GPU detection logic for multiple architectures
```

#### 2. Create GGUF Parser
**New file**: `llcuda/gguf_parser.py`

**Features**:
- Parse GGUF header (magic, version, tensor count)
- Read metadata key-value pairs
- Extract tensor information (name, shape, type, offset)
- Support all quantization types
- Memory-mapped tensor access

**Usage**:
```python
from llcuda.gguf_parser import GGUFReader

with GGUFReader("model.gguf") as reader:
    print(f"Model: {reader.metadata['general.name']}")
    print(f"Tensors: {len(reader.tensors)}")

    for name, info in reader.tensors.items():
        print(f"{name}: {info['shape']} ({info['type']})")
```

#### 3. Update README.md
**File**: `README.md`

**New content**:
- Remove all 940M references
- Emphasize T4-only focus
- Highlight FlashAttention and Tensor Cores
- Add Google Colab quick start
- Update performance benchmarks (T4-specific)
- Add Unsloth integration examples

#### 4. Update pyproject.toml
**File**: `pyproject.toml`

**Changes**:
```toml
[project]
name = "llcuda"
version = "2.0.0"
description = "CUDA 12 inference backend for Unsloth - Tesla T4 optimized"
requires-python = ">=3.11"

[project.optional-dependencies]
dev = ["pytest", "black", "mypy"]

[tool.setuptools.package-data]
llcuda = ["*.so", "binaries/**/*"]
```

---

### MEDIUM PRIORITY (Next Week)

#### 5. Implement Continuous Batching
**New files**:
- `llcuda/server/batching.py` - Slot-based scheduler
- `llcuda/server/request.py` - Request types
- `llcuda/server/slot.py` - Slot management

**Based on llama.cpp patterns**:
- RequestSlot class (IDLE, PROCESSING_PROMPT, GENERATING states)
- ContinuousBatchScheduler (step-by-step inference)
- KV cache management
- Context shifting

#### 6. Add CUDA Graphs Support
**New file**: `llcuda/cuda_graphs.py`

**Features**:
- Capture CUDA graph for fixed input shapes
- Replay graph for inference
- Reduce kernel launch overhead
- Optimize for repeated inference

#### 7. Create Unsloth Export Method
**Target**: Contribute to Unsloth via PR

**Function**: `save_pretrained_llcuda()`
- Merge LoRA adapters
- Quantize to NF4 (optional)
- Save in safetensors format
- Add llcuda metadata

#### 8. Implement Unsloth Model Loader
**New file**: `llcuda/models/unsloth_loader.py`

**Features**:
- Load safetensors from Unsloth
- Detect model architecture (Llama, Gemma, Mistral)
- Apply quantization metadata
- Create llcuda.Module instance

---

### LOW PRIORITY (Month 2+)

#### 9. NF4 Quantization Kernels (Phase 2)
- Implement NF4 block structure
- Write dequantization CUDA kernel
- Implement quantized matmul
- Test against bitsandbytes

#### 10. Flash Attention 2 Implementation (Phase 3)
- Tiled attention algorithm
- Forward pass CUDA kernel
- Backward pass (for future training)
- Optimize for T4 architecture

#### 11. Model Architecture Support (Phase 4)
- Llama (1, 2, 3, 3.1, 3.2, 3.3)
- Gemma (2, 3)
- Mistral/Mixtral (MoE)

#### 12. Multi-GPU Support (Phase 6)
- Tensor parallelism
- Pipeline parallelism
- NCCL communication
- Hybrid parallelism

---

## Testing Plan

### Unit Tests
- âœ… `tests/test_tensor_api.py` - Tensor operations (done)
- ðŸ”² `tests/test_gguf_parser.py` - GGUF parsing
- ðŸ”² `tests/test_batching.py` - Continuous batching
- ðŸ”² `tests/test_cuda_graphs.py` - CUDA graphs

### Integration Tests
- ðŸ”² `tests/test_unsloth_integration.py` - End-to-end workflow
- ðŸ”² `tests/test_server.py` - Server API
- ðŸ”² `tests/test_model_loading.py` - Model loading

### Performance Tests
- ðŸ”² `benchmarks/bench_throughput.py` - Tokens/sec
- ðŸ”² `benchmarks/bench_latency.py` - TTFT, ITL
- ðŸ”² `benchmarks/bench_memory.py` - VRAM usage

---

## Documentation Updates Needed

### High Priority
1. âœ… `REFACTOR_T4_ONLY_PLAN.md` - Master plan (done)
2. âœ… `T4_REFACTOR_PROGRESS.md` - This file (done)
3. ðŸ”² `README.md` - Main README (T4-focused)
4. ðŸ”² `docs/BUILD_GUIDE.md` - T4 build instructions
5. ðŸ”² `docs/QUICK_START.md` - Google Colab quick start

### Medium Priority
6. ðŸ”² `docs/GGUF_FORMAT.md` - GGUF integration guide
7. ðŸ”² `docs/UNSLOTH_INTEGRATION.md` - Unsloth workflow
8. ðŸ”² `docs/API_REFERENCE.md` - Complete API docs
9. ðŸ”² `examples/colab_demo_v2.ipynb` - Colab demo

### Low Priority
10. ðŸ”² Migration guide (v1.x â†’ v2.0)
11. ðŸ”² Performance tuning guide
12. ðŸ”² Multi-GPU setup guide

---

## Key Decisions Made

### 1. T4-Only Focus âœ…
**Rationale**:
- Google Colab standard GPU
- Tensor Core support (SM 7.5+)
- FlashAttention compatibility
- Single target = better optimization

**Tradeoff**:
- Loses 940M support (Maxwell users)
- Smaller potential user base
- BUT: Better performance for target users

### 2. Keep V1.x Compatibility âœ…
**Rationale**:
- Existing users need migration path
- HTTP server mode still useful
- GGUF model support via llama.cpp

**Approach**:
- V1.x: `InferenceEngine()` for HTTP server
- V2.0: `from llcuda.core import Tensor` for native API
- Dual API in single package

### 3. Python-First Design âœ…
**Rationale**:
- Matches Unsloth's Python ecosystem
- PyTorch-style API familiar to users
- Easy integration with HuggingFace

**Implementation**:
- Core kernels in C++/CUDA
- Python wrapper with pybind11
- High-level API in pure Python

### 4. Static Linking âœ…
**Rationale**:
- Simpler deployment (no .so dependencies)
- Self-contained package
- Works in Colab without extra setup

**Tradeoff**:
- Larger binary size (466 MB)
- Longer compile time
- BUT: Better user experience

---

## Next Immediate Steps

**Today** (Priority 1):
1. Update `llcuda/_internal/bootstrap.py` for T4-only
2. Create `llcuda/gguf_parser.py` implementation
3. Update `README.md` for T4 focus
4. Update `pyproject.toml` to v2.0.0

**Tomorrow** (Priority 2):
5. Create T4-specific build script (`scripts/build_t4_native.sh`)
6. Test complete build on local system
7. Verify T4 binaries work correctly
8. Start continuous batching implementation

**This Week** (Priority 3):
9. Implement CUDA Graphs wrapper
10. Create Unsloth export method (draft PR)
11. Begin Unsloth model loader
12. Update all documentation

---

## Questions for User

1. **Binary Distribution**: Should we upload the 264 MB T4 binary package to GitHub releases or use a CDN?

2. **Version Scheme**: Use v2.0.0 immediately or start with v2.0.0-alpha for testing?

3. **PyPI Package**: Include compiled extension in wheel (466 MB) or download on first import?

4. **Backward Compatibility**: Keep v1.x API in v2.0 package or create separate llcuda-legacy package?

5. **Unsloth PR**: Should we contact Unsloth team now or wait until v2.0 is more mature?

---

## Success Metrics

### Phase 1 (Week 1) âœ… COMPLETE
- [x] Archive 940M files
- [x] Update CMakeLists.txt for T4

### Phase 2 (Week 1) âœ… COMPLETE
- [x] GGUF parser working
- [x] Bootstrap updated
- [x] README updated
- [x] pyproject.toml updated to v2.0.0

### Phase 2 (Next Week)
- [ ] Continuous batching implemented
- [ ] CUDA Graphs working
- [ ] Basic Unsloth integration
- [ ] Clean build on Colab

### Phase 3 (Month 1)
- [ ] Full Unsloth export/import
- [ ] Performance benchmarks
- [ ] Documentation complete
- [ ] PyPI package published

---

## Resources

### T4 Binaries
- **Location**: `/media/waqasm86/External1/Project-Nvidia/llcuda/release-packages/llcuda-binaries-cuda12-t4.tar.gz`
- **Size**: 264 MB
- **Contents**: llama-server + lib*.so files
- **Build from**: Colab notebook `p4_colab_llama_cpp.ipynb`

### Key References
- llama.cpp server: https://github.com/ggml-org/llama.cpp/tree/master/tools/server
- GGUF spec: https://github.com/ggml-org/ggml/blob/master/docs/gguf.md
- Unsloth: https://github.com/unslothai/unsloth
- Flash Attention: https://github.com/Dao-AILab/flash-attention

---

**Status**: Ready for Phase 2 (Bootstrap + GGUF)
**Blockers**: None
**ETA for v2.0.0-alpha**: 1 week
